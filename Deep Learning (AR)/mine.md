<img src="./media/image1.jpeg"
style="width:8.26791in;height:11.66667in" />­­­

<span dir="rtl">الحوسبة التكيفية والتعليم الآلي</span>

<span dir="rtl">فرانسيس باخ</span>

<span dir="rtl">قائمة كاملة بالكتب المنشورة في سلسلة الحوسبة التكيفية
والتعليم الآلي</span>

<span dir="rtl">في الجزء الخلفي من هذا الكتاب.</span>

<span dir="rtl">التعليم المعزز:</span>

<span dir="rtl">مقدمة</span>

<span dir="rtl">الطبعة الثانية</span>

<span dir="rtl">ريتشارد ساتون وأندرو ج. بارتو</span>

<span dir="rtl">مطبعة معهد ماساتشوستس للتكنولوجيا</span>

<span dir="rtl">كامبريدج، ماساتشوستس</span>

<span dir="rtl">لندن، إنجلترا</span>

<span dir="rtl">مقدمة الطبعة الثانية</span>:

<span dir="rtl">شهدت السنوات العشرون منذ نشر الطبعة الأولى من هذا الكتاب
تقدمًا هائلًا في **الذكاء الاصطناعي** </span>**(Artificial
Intelligence)**<span dir="rtl">، مدفوعًا بشكل كبير بالتطورات في **التعليم
الآلي**</span> **<span dir="rtl">(</span>Machine
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**<span dir="rtl">،
بما في ذلك **التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">على الرغم من أن القوة الحاسوبية الهائلة التي أصبحت متاحة
كانت مسؤولة عن بعض هذه التطورات، فإن الابتكارات في **النظريات**</span>
**(Theory)** <span dir="rtl">و**الخوارزميات**</span> **(Algorithms)**
<span dir="rtl">كانت أيضًا قوى دافعة. في مواجهة هذا التقدم، كان إصدار
طبعة ثانية من كتابنا لعام 1998 أمرًا مستحقًا منذ فترة طويلة، وقد بدأنا هذا
المشروع أخيرًا في عام 2012. كان هدفنا من الطبعة الثانية هو نفس هدفنا من
الطبعة الأولى: تقديم شرح واضح وبسيط للأفكار والخوارزميات الرئيسية في
**التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">بطريقة تكون مفهومة للقراء في جميع التخصصات ذات الصلة.
تظل هذه الطبعة بمثابة مقدمة، ونحتفظ بالتركيز على **الخوارزميات**</span>
**(Algorithms)** <span dir="rtl">الأساسية للتعليم عبر الإنترنت</span>
**(Online Learning)<span dir="rtl">.</span>** <span dir="rtl">تتضمن هذه
الطبعة بعض الموضوعات الجديدة التي أصبحت ذات أهمية خلال السنوات الفاصلة،
كما قمنا بتوسيع تغطية الموضوعات التي نفهمها الآن بشكل أفضل. لكننا لم
نحاول تقديم تغطية شاملة لهذا المجال، الذي انفجر في العديد من الاتجاهات
المختلفة. نعتذر عن الاضطرار إلى ترك معظم هذه المساهمات</span>.

<span dir="rtl">كما في الطبعة الأولى، اخترنا عدم إنتاج معالجة رسمية
صارمة لـ **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**<span dir="rtl">،
أو صياغته بأكثر المصطلحات عمومية. ومع ذلك، فإن فهمنا الأعمق لبعض
الموضوعات منذ الطبعة الأولى تطلب استخدام المزيد من **الرياضيات**</span>
**(Mathematics)** <span dir="rtl">لشرحها؛ لقد وضعنا الأجزاء الأكثر
رياضية في مربعات مظللة يمكن لأولئك الذين لا يميلون إلى **الرياضيات**
</span>**(Mathematics) <span dir="rtl"></span>**<span dir="rtl">تخطيها.
استخدمنا أيضًا ترميزًا مختلفًا قليلًا عن الترميز المستخدم في الطبعة الأولى.
في التدريس، وجدنا أن الترميز الجديد يساعد في معالجة بعض نقاط الارتباك
الشائعة. يبرز الترميز الجديد الفرق بين المتغيرات العشوائية، التي يتم
تمثيلها بحروف كبيرة، وتكراراتها، التي تمثل بحروف صغيرة. على سبيل المثال،
الحالة</span> **(State)**<span dir="rtl">، الإجراء</span>
**(Action)**<span dir="rtl">، والمكافأة</span> **(Reward)
<span dir="rtl"></span>**<span dir="rtl">في الخطوة الزمنية</span> $`t`$
<span dir="rtl">يتم تمثيلها بـ</span> $`S_{t}`$
<span dir="rtl">و</span>$`A_{t}`$ <span dir="rtl">و</span>$`R_{t}`$
<span dir="rtl"></span>​ <span dir="rtl">على التوالي، بينما القيم
المحتملة لها يمكن تمثيلها  
بـ</span> $`s`$ <span dir="rtl">و</span>$`a`$
<span dir="rtl">و</span>$`r`$<span dir="rtl">.</span>
<span dir="rtl">إلى جانب ذلك، من الطبيعي استخدام الأحرف الصغيرة لوظائف
القيمة  
**(**</span>**Value <span dir="rtl"></span>Functions<span dir="rtl">)
</span>**<span dir="rtl">مثل</span> $`v_{\pi}`$ <span dir="rtl">وتقييد
الأحرف الكبيرة على تقديراتها الجدولية **(**</span>**Tabular
<span dir="rtl"></span>Estimates<span dir="rtl">)
</span>**<span dir="rtl">مثل</span>
$`Q_{t}(s,a)`$<span dir="rtl">.</span> **<span dir="rtl">وظائف القيمة
التقريبية</span> (Approximate Value Functions)** <span dir="rtl">هي
وظائف حتمية للبارامترات العشوائية وبالتالي هي أيضًا في حروف صغيرة  
(مثل</span>
$`\left( e.g.,\widehat{v}\left( s,w_{t} \right) \approx v_{\pi}(s) \right)`$<span dir="rtl">).</span>
**<span dir="rtl">المتجهات</span> (Vectors)**<span dir="rtl">، مثل
**متجه الوزن**</span> **<span dir="rtl">(</span>Weight
<span dir="rtl"></span>Vector<span dir="rtl">)</span>**
$`\mathbf{w}_{\mathbf{t}}`$​ <span dir="rtl">(الذي كان يُشار إليه سابقًا
بـ</span> $`{\ \ \theta}_{t}`$​<span dir="rtl">) ومتجه **الميزة**</span>
**<span dir="rtl">(</span>Feature
<span dir="rtl"></span>Vector<span dir="rtl">)</span>**
$`X_{t}`$<span dir="rtl">(الذي كان يُشار إليه سابقًا بـ</span>
$`\phi_{t}`$​<span dir="rtl">)، تكون بالخط العريض وتُكتب بحروف صغيرة حتى
لو كانت متغيرات عشوائية. يتم حجز الأحرف الكبيرة بالخط العريض لـ
**المصفوفات** </span>**(Matrices)**<span dir="rtl">.</span>
<span dir="rtl">في الطبعة الأولى، استخدمنا ترميزًا خاصًا</span>​
$`\mathcal{P}_{\mathcal{s}\mathcal{s}'}^{\mathcal{a}}و\ \mathcal{R}_{\mathcal{s}\mathcal{s}'}^{\mathcal{a}}`$​
<span dir="rtl">لاحتمالات الانتقال والمكافآت المتوقعة. كانت إحدى نقاط
الضعف في ذلك الترميز أنه لم يصف بشكل كامل ديناميكيات المكافآت، حيث يعطي
فقط توقعاتها، وهو ما يكفي لـ **البرمجة الديناميكية**</span>
**<span dir="rtl">(</span>(Dynamic Programming
<span dir="rtl"></span>**<span dir="rtl">ولكنه</span>
<span dir="rtl">غير كافٍ لـ **التعليم المعزز (**</span>**Reinforcement
Learning<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">نقطة ضعف أخرى هي الإفراط في استخدام الرموز العلوية
والسفلية. في هذه الطبعة، نستخدم الترميز الصريح</span>
$`p\left( s',r \middle| s,a \right)`$ <span dir="rtl">للاحتمال المشترك
للحالة التالية والمكافأة المعطاة للحالة الحالية والإجراء. جميع التغييرات
في الترميز مُلخصة في جدول في الصفحة</span> xix.

<span dir="rtl">الطبعة الثانية توسعت بشكل كبير، وتم تغيير تنظيمها على
مستوى عالٍ. بعد الفصل التمهيدي الأول، تم تقسيم الطبعة الثانية إلى ثلاثة
أجزاء جديدة. الجزء الأول (الفصول 2-8) يعالج قدر الإمكان من **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">دون الخروج
عن الحالة الجدولية</span> **(Tabular Case)
<span dir="rtl"></span>**<span dir="rtl">التي يمكن إيجاد حلول دقيقة لها.
نغطي كل من طرق التعليم والتخطيط للحالة الجدولية، بالإضافة إلى توحيدها في
طرق</span> n-step <span dir="rtl">و</span>Dyna<span dir="rtl">.</span>
<span dir="rtl">العديد من الخوارزميات المعروضة في هذا الجزء جديدة في
الطبعة الثانية، بما في ذلك</span> **UCB**<span dir="rtl">،</span>
**Expected Sarsa**<span dir="rtl">،</span> **Double
Learning**<span dir="rtl">،  
</span>**Tree-backup**<span dir="rtl">،</span>
$`\mathbf{Q}\left( \mathbf{\sigma} \right)`$<span dir="rtl">،</span>
**RTDP**<span dir="rtl">، **و**</span>**MCTS<span dir="rtl">.</span>**
<span dir="rtl">معالجة الحالة الجدولية أولاً، وبشكل شامل، تمكن الأفكار
الأساسية من التطور في أبسط بيئة ممكنة. الجزء الثاني من الكتاب (الفصول
9-13) مخصص لتوسيع الأفكار إلى **تقريب الدوال** </span>**(Function
Approximation)**<span dir="rtl">.</span> <span dir="rtl">يتضمن فصولًا
جديدة حول **الشبكات العصبية الاصطناعية** </span>**(Artificial Neural
Networks)**<span dir="rtl">، **الأساس فورييه**</span>
**<span dir="rtl">(</span>Fourier
<span dir="rtl"></span>Basis<span dir="rtl">)</span>**<span dir="rtl">،</span>
**LSTD**<span dir="rtl">، **طرق القواعد الأساسية**
</span>**(Kernel-based Methods)**<span dir="rtl">،</span>
**Gradient-TD** **<span dir="rtl">و</span>Emphatic-TD**<span dir="rtl">،
**طرق متوسط المكافأة** </span>**(Average-Reward
Methods)**<span dir="rtl">،</span> **True Online
<span dir="rtl"></span>**$`\text{TD}\left( \mathbf{\lambda} \right)`$<span dir="rtl">،
وطرق **سياسة التدرج** </span>**(Policy-gradient
Methods)**<span dir="rtl">.</span> <span dir="rtl">الطبعة الثانية توسعت
بشكل كبير في معالجة **التعليم خارج السياسة** </span>**(Off-policy
Learning)**<span dir="rtl">، أولاً للحالة الجدولية في الفصول  
5-7، ثم مع **تقريب الدوال**</span> **(Function Approximation)**
<span dir="rtl">في الفصول 11 و12. تغيير آخر هو أن الطبعة الثانية تفصل
بين فكرة عرض المستقبل</span> **(Forward-View)
<span dir="rtl"></span>**<span dir="rtl">للتمهيد</span> n-step
<span dir="rtl">(المعالجة الآن بشكل كامل في الفصل 7) وفكرة عرض
الرجوع</span> **(Backward-View)
<span dir="rtl"></span>**<span dir="rtl">لآثار الأهلية
**(**</span>**Eligibility Traces<span dir="rtl">)</span>**
<span dir="rtl">(التي تُعالج الآن بشكل مستقل في الفصل 12).  
الجزء الثالث من الكتاب يحتوي على فصول جديدة كبيرة حول علاقات **التعليم
المعزز** </span>**(Reinforcement Learning)
<span dir="rtl"></span>**<span dir="rtl">بعلم النفس</span>
**(Psychology) <span dir="rtl"></span>**<span dir="rtl">(الفصل 14) وعلوم
الأعصاب</span> **(Neuroscience)
<span dir="rtl"></span>**<span dir="rtl">(الفصل 15)، وكذلك فصل محدث
لدراسات الحالة يتضمن لعب **ألعاب الأتاري** </span>**(Atari Game
Playing)**<span dir="rtl">، **استراتيجية المراهنة لواتسون**</span>
**<span dir="rtl">(</span>Watson’s Wagering
Strategy<span dir="rtl">)</span>**<span dir="rtl">، وبرامج لعب</span>
**Go <span dir="rtl"></span>**<span dir="rtl">مثل</span> **AlphaGo
<span dir="rtl"></span>**<span dir="rtl">و</span>**AlphaGo Zero
<span dir="rtl"></span>**<span dir="rtl">(الفصل 16). ومع ذلك، من الضروري
أننا قمنا بتضمين فقط مجموعة صغيرة من كل ما تم إنجازه في هذا المجال. تعكس
اختياراتنا اهتماماتنا الطويلة الأمد بالطرق الرخيصة الخالية من النماذج
التي يجب أن تتوسع بشكل جيد للتطبيقات الكبيرة. يتضمن الفصل الأخير الآن
مناقشة للتأثيرات المجتمعية المستقبلية لـ **التعليم المعزز**
</span>**(Reinforcement Learning)<span dir="rtl">.</span>**
<span dir="rtl">سواء للأفضل أو للأسوأ، فإن الطبعة الثانية أكبر بحوالي
الضعف من الطبعة الأولى</span>.

<span dir="rtl">هذا الكتاب مصمم ليُستخدم كنص أساسي لدورة دراسية واحدة أو
دورتين دراسيتين على مدار فصلين دراسيين في **التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">.</span>
<span dir="rtl">لدورة دراسية واحدة، يجب تغطية الفصول العشرة الأولى
بالترتيب وتشكل نواة جيدة، يمكن إضافة مواد من الفصول الأخرى، من كتب أخرى
مثل</span> Bertsekas and Tsitsiklis (1996)<span dir="rtl">،</span>
Wiering and van Otterlo (2012)<span dir="rtl">، و</span>Szepesvári
(2010)<span dir="rtl">، أو من الأدبيات، وفقًا للذوق. اعتمادًا على خلفية
الطلاب، قد تكون بعض المواد الإضافية حول **التعليم الخاضع
للإشراف**</span> **(Supervised Learning)** <span dir="rtl">عبر الإنترنت
مفيدة. تعتبر أفكار **الخيارات**</span> **(Options)**
<span dir="rtl">ونماذج **الخيارات**</span> **(Option Models)**
<span dir="rtl">إضافة طبيعية  
(</span>Sutton, Precup and Singh, 1999<span dir="rtl">).</span>
<span dir="rtl">يمكن أن تغطي دورة دراسية على مدار فصلين دراسيين جميع
الفصول بالإضافة إلى المواد التكميلية. يمكن أيضًا استخدام الكتاب كجزء من
دورات أوسع حول **التعليم الآلي** </span>**(Machine
Learning)**<span dir="rtl">، **الذكاء الاصطناعي** </span>**(Artificial
Intelligence)**<span dir="rtl">، أو **الشبكات العصبية** </span>**(Neural
Networks)**<span dir="rtl">.</span> <span dir="rtl">في هذه الحالة، قد
يكون من المرغوب فيه تغطية فقط جزء من المواد. نوصي بتغطية الفصل 1 للحصول
على لمحة موجزة، الفصل 2 حتى القسم 2.4، الفصل 3، ثم اختيار الأقسام من
الفصول المتبقية وفقًا للوقت والاهتمامات. يعتبر الفصل 6 الأكثر أهمية
للموضوع وللبقية الكتاب. يجب أن تغطي دورة تركز على **التعليم
الآلي**</span> **(Machine Learning)** <span dir="rtl">أو **الشبكات
العصبية**</span> **(Neural Networks)** <span dir="rtl">الفصول 9 و10،
ويجب أن تغطي دورة تركز على **الذكاء الاصطناعي**</span> **(Artificial
Intelligence)** <span dir="rtl">أو **التخطيط**</span> **(Planning)**
<span dir="rtl">الفصل 8. عبر الكتاب، تم تمييز الأقسام والفصول الأكثر
صعوبة وغير الضرورية لبقية الكتاب بعلامة</span>
**(\*)<span dir="rtl">.</span>** <span dir="rtl">يمكن تجاوزها في القراءة
الأولى دون خلق مشاكل لاحقًا. تم تمييز بعض التمارين أيضًا بعلامة</span>
**(\*) <span dir="rtl"></span>**<span dir="rtl">للإشارة إلى أنها أكثر
تقدمًا وغير ضرورية لفهم المادة الأساسية للفصل</span>.

**<span dir="rtl">معظم الفصول تنتهي بقسم بعنوان "ملاحظات بيبليوغرافية
وتاريخية</span>"** <span dir="rtl">حيث نمنح الفضل لمصادر الأفكار المقدمة
في ذلك الفصل، ونوفر إرشادات لمزيد من القراءة والبحث الجاري، ونصف الخلفية
التاريخية ذات الصلة. على الرغم من محاولاتنا لجعل هذه الأقسام موثوقة
وشاملة، فمن المؤكد أننا تركنا بعض الأعمال السابقة المهمة. لذلك نعتذر مرة
أخرى، ونرحب بتصحيحات وإضافات لإدراجها في النسخة الإلكترونية من
الكتاب</span>.

<span dir="rtl">كما هو الحال في الطبعة الأولى، فإن هذه الطبعة من الكتاب
مكرسة لذكرى **هاري كلوبف**</span> **<span dir="rtl">(</span>A. Harry
<span dir="rtl"></span>Klopf<span dir="rtl">).</span>**
<span dir="rtl">كان **هاري** هو من قدمنا لبعضنا البعض، وكانت أفكاره حول
الدماغ والذكاء الاصطناعي هي التي أطلقت رحلتنا الطويلة في **التعليم
المعزز**</span> **<span dir="rtl">(</span>Reinforcement
Learning<span dir="rtl">).</span>** <span dir="rtl">**هاري**، الذي كان
متدربًا في علم الأعصاب وكان مهتمًا بالذكاء الآلي، كان عالمًا كبيرًا مرتبطًا
بمديرية</span> **Avionics
<span dir="rtl"></span>**<span dir="rtl">التابعة لمكتب البحث العلمي
للقوات الجوية **(**</span>**Air Force Office of Scientific Research –
AFOSR<span dir="rtl">)</span>** <span dir="rtl">في قاعدة رايت باترسون
الجوية في أوهايو. كان غير راضٍ عن الأهمية الكبيرة التي تُعطى لعمليات البحث
عن التوازن، بما في ذلك **التوازن الداخلي**</span> **(Homeostasis)**
<span dir="rtl">وطرق تصنيف الأنماط القائمة على تصحيح الأخطاء، في تفسير
الذكاء الطبيعي وفي توفير أساس للذكاء الآلي. أشار إلى أن الأنظمة التي
تحاول تحقيق أقصى قدر من شيء ما (أياً كان ذلك) تختلف نوعياً عن الأنظمة التي
تسعى إلى التوازن، وجادل بأن الأنظمة التي تهدف إلى **التعظيم**</span>
**(Maximization)** <span dir="rtl">هي المفتاح لفهم الجوانب المهمة للذكاء
الطبيعي ولإنشاء **ذكاء اصطناعي** </span>**(Artificial
Intelligences)**<span dir="rtl">.</span> <span dir="rtl">كان **هاري** له
دور حاسم في الحصول على تمويل من</span> **AFOSR
<span dir="rtl"></span>**<span dir="rtl">لمشروع لتقييم الجدارة العلمية
لهذه الأفكار والأفكار ذات الصلة. تم تنفيذ هذا المشروع في أواخر
السبعينيات في جامعة ماساتشوستس في أمهرست</span> **(UMass
Amherst)**<span dir="rtl">، في البداية تحت إشراف **مايكل أربيب**
</span>**(Michael Arbib)**<span dir="rtl">، **ويليام كيلمر**
</span>**(William Kilmer)**<span dir="rtl">، و**نيكو سبينيلي**
</span>**(Nico Spinelli)**<span dir="rtl">، أساتذة في قسم علوم الحاسوب
والمعلومات في</span> **UMass Amherst**<span dir="rtl">، وأعضاء مؤسسين
لمركز **السيبرنتيكا** لعلوم الأعصاب في الجامعة، وهي مجموعة بعيدة النظر
تركز على تقاطع علوم الأعصاب والذكاء الاصطناعي.</span>
**<span dir="rtl">بارتو</span> (Barto)**<span dir="rtl">، وهو حاصل على
درجة الدكتوراه حديثاً من جامعة ميشيغان، تم تعيينه كباحث ما بعد الدكتوراه
في المشروع. في الوقت نفسه، كان **ساتون**
</span>**(Sutton)**<span dir="rtl">، طالبًا جامعيًا يدرس علوم الحاسوب وعلم
النفس في **ستانفورد** </span>**(Stanford)**<span dir="rtl">، قد بدأ
يتواصل مع **هاري** بشأن اهتمامهما المشترك بدور توقيت المحفز في التكييف
الكلاسيكي. اقترح **هاري** على مجموعة</span> **UMass
<span dir="rtl"></span>**<span dir="rtl">أن **ساتون** سيكون إضافة رائعة
للمشروع. وهكذا، أصبح **ساتون** طالب دراسات عليا في</span>
**UMass**<span dir="rtl">، وحصل على درجة الدكتوراه بإشراف **بارتو**،
الذي أصبح أستاذاً مشاركاً. الدراسة الخاصة بـ **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">كما هو مقدم في هذا الكتاب
هي بحق نتيجة لذلك المشروع الذي بدأه **هاري** وألهمه بأفكاره. علاوة على
ذلك، كان **هاري** مسؤولاً عن جمعنا، نحن المؤلفين، معًا فيما كان تفاعلاً
طويلًا وممتعًا. من خلال تكريس هذا الكتاب لـ **هاري**، نكرم مساهماته
الأساسية، ليس فقط في مجال **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">، ولكن أيضًا في تعاوننا. كما نشكر **أساتذة
أربيب**</span> **(Arbib)** <span dir="rtl">وكيلمر</span> **(Kilmer)**
**<span dir="rtl">وسبينيلي</span> (Spinelli)** <span dir="rtl">على
الفرصة التي قدموها لنا لبدء استكشاف هذه الأفكار. وأخيرًا، نشكر</span>
**AFOSR <span dir="rtl"></span>**<span dir="rtl">على دعمه السخي خلال
السنوات الأولى من أبحاثنا، و</span>**NSF
<span dir="rtl"></span>**<span dir="rtl">على دعمه السخي خلال العديد من
السنوات التالية</span>.

<span dir="rtl">لدينا الكثير من الأشخاص الذين نشكرهم على إلهامهم
ومساعدتهم في هذه الطبعة الثانية. كل من شكرناهم على إلهامهم ومساعدتهم في
الطبعة الأولى يستحقون امتناننا العميق لهذه الطبعة أيضًا، التي لم تكن
لتوجد لولا مساهماتهم في الطبعة الأولى. إلى تلك القائمة الطويلة يجب أن
نضيف الكثيرين الذين ساهموا بشكل خاص في الطبعة الثانية. طلابنا على مر
السنوات العديدة التي قمنا بتدريس هذه المادة ساهموا بطرق لا تعد ولا تحصى:
من كشف الأخطاء، إلى تقديم الحلول، وليس أقلها - شعورهم بالحيرة في الأماكن
التي كان بإمكاننا أن نشرحها بشكل أفضل. نشكر بشكل خاص **مارثا ستينستروب**
</span>**(Martha Steenstrup)
<span dir="rtl"></span>**<span dir="rtl">لقراءتها وتقديم تعليقات مفصلة
في جميع أنحاء الكتاب. الفصول حول **علم النفس**</span> **(Psychology)**
<span dir="rtl">و**علوم الأعصاب**</span> **(Neuroscience)**
<span dir="rtl">لم يكن من الممكن كتابتها بدون مساعدة العديد من الخبراء
في تلك المجالات. نشكر **جون مور**</span> **(John Moore)**
<span dir="rtl">على تدريسه لنا بصبر على مدى سنوات عديدة حول تجارب
التعليم الحيواني، النظرية، وعلوم **الأعصاب**
</span>**(Neuroscience)**<span dir="rtl">، وعلى قراءته الدقيقة للعديد من
المسودات للفصول 14 و15. نشكر أيضًا **مات بوتفينيك** </span>**(Matt
Botvinick)**<span dir="rtl">، **ناثانيال داو** </span>**(Nathaniel
Daw)**<span dir="rtl">، **بيتر ديان** </span>**(Peter
Dayan)**<span dir="rtl">، و**يائيل نيف**</span> **(Yael Niv)**
<span dir="rtl">على تعليقاتهم العميقة على مسودات هذه الفصول، وتوجيههم
الأساسي عبر الأدبيات الضخمة، واعتراضهم على العديد من أخطائنا في المسودات
المبكرة. بالطبع، الأخطاء المتبقية في هذه الفصول - ويجب أن يكون هناك
بعض - هي كلياً مسؤوليتنا. نشكر **فيل توماس**</span>
**<span dir="rtl">(</span>Phil
<span dir="rtl"></span>Thomas<span dir="rtl">)</span>**
<span dir="rtl">على مساعدته في جعل هذه الفصول مفهومة لغير علماء النفس
وغير علماء الأعصاب، ونشكر **بيتر ستيرلينغ**</span> **(Peter Sterling)**
<span dir="rtl">على مساعدته في تحسين الشرح. نحن ممتنون لـ **جيم هوك**
</span>**(Jim Houk) <span dir="rtl"></span>**<span dir="rtl">لتعريفنا
بموضوع معالجة المعلومات في **العقد القاعدية**</span> **(Basal Ganglia)**
<span dir="rtl">ولتنبيهنا إلى جوانب أخرى ذات صلة بـ **علوم
الأعصاب**</span> **(Neuroscience)**. **<span dir="rtl">خوسيه
مارتينيز</span> <span dir="rtl">(</span>José
<span dir="rtl"></span>Martínez<span dir="rtl">)</span>**<span dir="rtl">،
**تيري سجينوسكي** </span>**Terry
Sejnowski)<span dir="rtl">)</span>**<span dir="rtl">، **ديفيد سيلفر**
</span>**(David Silver)**<span dir="rtl">،  
**جيري تسيسورو** </span>**(Gerry Tesauro)**<span dir="rtl">، **جورجيوس
ثيوشاروس**</span> **<span dir="rtl">(</span>Georgios
Theocharous<span dir="rtl">)</span>**<span dir="rtl">، و**فيل
توماس**</span> **(Phil Thomas)** <span dir="rtl">ساعدونا بسخاء في فهم
تفاصيل تطبيقاتهم لـ **التعليم المعزز** </span>**(Reinforcement Learning)
<span dir="rtl"></span>**<span dir="rtl">لإدراجها في فصل دراسات الحالة،
وقدموا تعليقات مفيدة على مسودات هذه الأقسام. نحن مدينون بشكر خاص لـ
**ديفيد سيلفر**</span> **(David Silver)** <span dir="rtl">لمساعدتنا على
فهم أفضل لـ **مونت كارلو شجرة البحث**</span> **(Monte Carlo Tree
Search)** <span dir="rtl">وبرامج اللعب  
</span>**DeepMind Go<span dir="rtl">.</span>** <span dir="rtl">نشكر
**جورج كونيداريس**</span> **(George Konidaris)** <span dir="rtl">على
مساعدته في قسم الأساس فورييه</span> **(Fourier
Basis)**<span dir="rtl">.</span> **<span dir="rtl">إميليو كارتوني</span>
(Emilio Cartoni)**<span dir="rtl">، **توماس سيدربورغ** </span>**(Thomas
Cederborg)**<span dir="rtl">، **ستيفان ديرنباخ**</span>**Stefan
Dernbach) <span dir="rtl">)</span>**<span dir="rtl">، **كليمنس
روزينباوم** </span>**(Clemens Rosenbaum)**<span dir="rtl">، **باتريك
تايلور** </span>**(Patrick Taylor)**<span dir="rtl">، **توماس
كولين**</span> **<span dir="rtl">(</span>Thomas
<span dir="rtl"></span>Colin<span dir="rtl">)</span>**<span dir="rtl">،
**وبيير-لوك بايكون**</span> **(Pierre-Luc Bacon)**
<span dir="rtl">ساعدونا بعدة طرق مهمة نشكرهم عليها كثيرًا</span>.

<span dir="rtl">يريد **ساتون**</span> **(Sutton)** <span dir="rtl">أيضًا
أن يشكر أعضاء مختبر **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**
**<span dir="rtl">والذكاء الاصطناعي</span> (Artificial Intelligence)**
<span dir="rtl">في **جامعة ألبرتا**</span>
**<span dir="rtl">(</span>University of
<span dir="rtl"></span>Alberta<span dir="rtl">)</span>**
<span dir="rtl">على مساهماتهم في الطبعة الثانية. يدين بفضل خاص لـ
**روبام محمود**</span> **<span dir="rtl">(</span>Rupam
<span dir="rtl"></span>Mahmood<span dir="rtl">)</span>**
<span dir="rtl">على المساهمات الأساسية في معالجة **طرق مونت كارلو خارج
السياسة  
(**</span>**Off-policy Monte Carlo Methods<span dir="rtl">)</span>**
<span dir="rtl">في الفصل 5، و **حميد معي**</span> **(Hamid Maei)**
<span dir="rtl">على مساعدته في تطوير المنظور حول **التعليم خارج
السياسة**</span> **(Off-policy Learning)** <span dir="rtl">المقدم في
الفصل 11، و **إريك جريفز**</span> **(Eric Graves)** <span dir="rtl">على
إجراء التجارب في الفصل 13، و **شانغتونغ تشانغ** </span>**(Shangtong
Zhang)** <span dir="rtl">على تكرار النتائج التجريبية والتحقق منها
تقريبًا، و **كريس دي أسيس**</span> **<span dir="rtl">(</span>Kris De
Asis<span dir="rtl">)</span>** <span dir="rtl">على تحسين المحتوى الفني
الجديد للفصول 7 و12، و **هارم فان سيجن**</span>
**<span dir="rtl">(</span>Harm van Seijen<span dir="rtl">)</span>**
<span dir="rtl">على الرؤى التي أدت إلى فصل **طرق**</span> **n-step**
<span dir="rtl">عن **آثار الأهلية** </span>**(Eligibility Traces)**
<span dir="rtl">و(مع **هادو فان هاسلت**</span>**(Hado van
Hasselt)**<span dir="rtl">) على الأفكار المتعلقة بالتكافؤ الدقيق بين
العرض الأمامي والخلفي لآثار الأهلية المقدمة في الفصل 12. يقدر
**ساتون**</span> **(Sutton)** <span dir="rtl">أيضًا الدعم والحرية التي
منحها له **حكومة ألبرتا**</span> **(Government of Alberta)**
**<span dir="rtl">والمجلس الوطني للعلوم والهندسة في كندا
(</span>National Science and Engineering Research Council of
Canada<span dir="rtl">)</span>** <span dir="rtl">طوال الفترة التي تم
فيها تصور الطبعة الثانية وكتابتها. على وجه الخصوص، يرغب في شكر **راندي
جوبل**</span> **<span dir="rtl">(</span>Randy
<span dir="rtl"></span>Goebel<span dir="rtl">)
</span>**<span dir="rtl">على خلق بيئة داعمة وبعيدة النظر للبحث في
ألبرتا. يريد أيضًا أن يشكر</span> **DeepMind
<span dir="rtl"></span>**<span dir="rtl">على دعمهم في الأشهر الستة
الأخيرة من كتابة الكتاب. أخيرًا، نحن مدينون بالشكر للعديد من القراء
اليقظين لمسودات الطبعة الثانية التي نشرناها على الإنترنت. لقد وجدوا
العديد من الأخطاء التي فاتتنا وأبلغونا عن نقاط محتملة للحيرة</span>.

<span dir="rtl">مقدمة الطبعة الأولى</span>:

<span dir="rtl">في أواخر عام 1979، بدأنا نركز على ما يعرف الآن بـ
**التعليم المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**<span dir="rtl">.
كنا كلانا في **جامعة ماساتشوستس** </span>**(University of
Massachusetts)**<span dir="rtl">، نعمل على أحد أوائل المشاريع لإحياء
فكرة أن شبكات العناصر التكيفية المشابهة للخلايا العصبية قد تكون نهجًا
واعدًا لتحقيق الذكاء الاصطناعي التكيفي. استكشف المشروع **"نظرية
الهتروستاتية للأنظمة التكيفية"** التي طورها **هاري كلوبف** </span>**(A.
Harry Klopf)**<span dir="rtl">.</span> <span dir="rtl">كان عمل **هاري**
مصدرًا غنيًا بالأفكار، وسمح لنا باستكشافها نقديًا ومقارنتها مع التاريخ
الطويل للأعمال السابقة في الأنظمة التكيفية. أصبحت مهمتنا هي تفكيك هذه
الأفكار وفهم علاقاتها وأهميتها النسبية. لا يزال هذا العمل مستمرًا حتى
اليوم، ولكن في عام 1979 أدركنا أن أبسط هذه الأفكار، التي كانت منذ فترة
طويلة مفروغًا منها، قد تلقت اهتمامًا قليلًا بشكل مدهش من منظور حسابي. كانت
هذه الفكرة ببساطة هي فكرة نظام التعليم الذي يريد شيئًا ما، الذي يتكيف مع
سلوكه من أجل تعظيم إشارة خاصة من بيئته. كانت هذه فكرة نظام التعليم
**"المتعة** </span>**(Hedonistic)"<span dir="rtl">،</span>**
<span dir="rtl">أو كما نقول الآن، فكرة **التعليم المعزز**</span>
**(Reinforcement Learning)<span dir="rtl">.</span>**

<span dir="rtl">مثل الآخرين، كان لدينا شعور بأن **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">قد تم
استكشافه بشكل كامل في الأيام الأولى من **السايبرنتيكا**</span>
**(Cybernetics) <span dir="rtl"></span>**<span dir="rtl">والذكاء
**الاصطناعي**</span> **<span dir="rtl">(</span>Artificial
<span dir="rtl"></span>Intelligence<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">ولكن عند الفحص الدقيق، وجدنا أنه لم يتم استكشافه إلا
قليلاً. في حين أن **التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">قد حفز بوضوح بعض الدراسات الحاسوبية الأولى للتعليم، إلا
أن معظم هؤلاء الباحثين انتقلوا إلى أمور أخرى، مثل تصنيف الأنماط
**(**</span>**Pattern
Classification<span dir="rtl">)</span>**<span dir="rtl">، **التعليم
الخاضع للإشراف** </span>**(Supervised Learning)**<span dir="rtl">،
و**التحكم التكيفي** </span>**(Adaptive Control)**<span dir="rtl">، أو
أنهم قد تخلوا عن دراسة التعليم تمامًا. ونتيجة لذلك، تلقت القضايا الخاصة
بتعلم كيفية الحصول على شيء من البيئة اهتمامًا أقل. بالنظر إلى الوراء، كان
التركيز على هذه الفكرة هو الخطوة الحاسمة التي وضعت هذا الفرع من البحث في
الحركة. لم يكن بالإمكان تحقيق أي تقدم كبير في الدراسة الحاسوبية لـ
**التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">حتى تم التعرف على أن مثل هذه الفكرة الأساسية لم يتم
استكشافها بشكل كامل بعد</span>.

<span dir="rtl">لقد قطع المجال شوطًا طويلاً منذ ذلك الحين، حيث تطور ونضج
في عدة اتجاهات. أصبح **التعليم المعزز** </span>**(Reinforcement
Learning) <span dir="rtl"></span>**<span dir="rtl">تدريجيًا واحدًا من أكثر
مجالات البحث نشاطًا في **التعليم الآلي** </span>**(Machine
Learning)**<span dir="rtl">، **الذكاء الاصطناعي** </span>**(Artificial
Intelligence)**<span dir="rtl">، وبحوث **الشبكات العصبية**
</span>**(Neural Networks)<span dir="rtl">.</span>** <span dir="rtl">لقد
تطور المجال ليشمل أسسًا رياضية قوية وتطبيقات مثيرة للإعجاب. دراسة
**التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">الحاسوبية أصبحت الآن مجالًا كبيرًا، مع مئات الباحثين
النشطين حول العالم في تخصصات متنوعة مثل **علم النفس**
</span>**(Psychology)**<span dir="rtl">، **نظرية التحكم**
</span>**(Control Theory)**<span dir="rtl">، **الذكاء الاصطناعي**
</span>**(Artificial Intelligence)**<span dir="rtl">، و**علوم الأعصاب**
</span>**(Neuroscience)<span dir="rtl">.</span>** <span dir="rtl">كانت
المساهمات التي أرست وطورت العلاقات مع نظرية **التحكم الأمثل**
</span>**(Optimal Control Theory)
<span dir="rtl"></span>**<span dir="rtl">والبرمجة **الديناميكية**</span>
**(Dynamic Programming)** <span dir="rtl">مهمة بشكل خاص. لا يزال التحدي
العام للتعليم من التفاعل لتحقيق الأهداف بعيدًا عن الحل، لكن فهمنا له قد
تحسن بشكل كبير. يمكننا الآن وضع أفكار مكونة، مثل **التعليم بالتفاضل
الزمني**</span> **<span dir="rtl">(</span>Temporal-Difference
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**<span dir="rtl">،
**البرمجة الديناميكية** </span>**(Dynamic
Programming)**<span dir="rtl">، و**تقريب الدوال**</span>
**<span dir="rtl">(</span>Function
<span dir="rtl"></span>Approximation<span dir="rtl">)</span>**<span dir="rtl">،
في منظور متماسك بالنسبة للتحدي العام</span>.

<span dir="rtl">كان هدفنا من كتابة هذا الكتاب هو تقديم شرح واضح وبسيط
للأفكار والخوارزميات الرئيسية في **التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">.</span>
<span dir="rtl">أردنا أن تكون معالجتنا لهذه الموضوعات متاحة للقراء في
جميع التخصصات ذات الصلة، ولكن لم نتمكن من تغطية جميع هذه المنظورات
بالتفصيل. في الغالب، تناولنا الموضوع من وجهة نظر **الذكاء
الاصطناعي**</span> **(Artificial Intelligence)**
<span dir="rtl">والهندسة. تركنا تغطية الروابط مع المجالات الأخرى للآخرين
أو لوقت آخر. كما اخترنا عدم إنتاج معالجة رسمية صارمة لـ **التعليم
المعزز** </span>**(Reinforcement Learning)**<span dir="rtl">.</span>
<span dir="rtl">لم نسعَ إلى أعلى مستوى ممكن من التجريد الرياضي ولم نعتمد
على صيغة النظرية والدليل. حاولنا اختيار مستوى من التفاصيل الرياضية يشير
إلى الاتجاهات الصحيحة لمن هم مائلون للرياضيات دون أن يشتت من بساطة
وإمكانية تعميم الأفكار الأساسية</span>.

<span dir="rtl">بمعنى ما، كنا نعمل نحو هذا الكتاب لمدة ثلاثين عامًا،
ولدينا الكثير من الناس لنشكرهم. أولاً، نشكر أولئك الذين ساعدونا شخصيًا في
تطوير النظرة العامة المقدمة في هذا الكتاب:</span> **<span dir="rtl">هاري
كلوبف</span> <span dir="rtl">(</span>Harry
<span dir="rtl"></span>Klopf<span dir="rtl">)</span>**<span dir="rtl">،
لمساعدته لنا في إدراك أن **التعليم المعزز**</span> **(Reinforcement
Learning)** <span dir="rtl">يحتاج إلى إحياء؛ **كريس واتكينز**
</span>**(Chris Watkins)**<span dir="rtl">، **ديمتري بيرتسيكاس**
</span>**(Dimitri Bertsekas)**<span dir="rtl">، **جون تسيتيكليس**
</span>**(John Tsitsiklis)**<span dir="rtl">، و **بول ويربوس**
</span>**(Paul Werbos)**<span dir="rtl">، لمساعدتنا في رؤية قيمة
العلاقات مع **البرمجة الديناميكية** </span>**(Dynamic
Programming)**<span dir="rtl">؛ **جون مور**</span> **(John Moore)**
<span dir="rtl">و **جيم كيهوي** </span>**(Jim Kehoe)**<span dir="rtl">،
على الرؤى والإلهامات من نظرية التعليم الحيواني؛ **أوليفر سيلفريدج**
</span>**(Oliver Selfridge)**<span dir="rtl">، لتأكيده على اتساع وأهمية
التكيف؛ وبشكل عام، زملاؤنا وطلابنا الذين ساهموا بطرق لا حصر لها:</span>
**<span dir="rtl">رون ويليامز</span> (Ron Williams)**<span dir="rtl">،
**تشارلز أندرسون** </span>**(Charles Anderson)**<span dir="rtl">،
**ساتيندر سينغ** </span>**(Satinder Singh)**<span dir="rtl">،</span>

**<span dir="rtl">حر مهدييفن</span> (Sridhar
Mahadevan)**<span dir="rtl">، **ستيف برادتكي** </span>**(Steve
Bradtke)**<span dir="rtl">، **بوب كريتس** </span>**(Bob
Crites)**<span dir="rtl">، **بيتر ديان** </span>**(Peter
Dayan)**<span dir="rtl">، و**ليمن بيرد** </span>**(Leemon
Baird)**<span dir="rtl">.</span> <span dir="rtl">تم إثراء نظرتنا  
لـ **التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">بشكل كبير من خلال مناقشاتنا مع **بول كوهين**
</span>**(Paul Cohen)**<span dir="rtl">، **بول أوتكوف** </span>**(Paul
Utgoff)**<span dir="rtl">، **مارثا ستينستروب** </span>**(Martha
Steenstrup)**<span dir="rtl">، **جيري تسيسورو** </span>**(Gerry
Tesauro)**<span dir="rtl">، **مايك جوردان** </span>**(Mike
Jordan)**<span dir="rtl">، **ليزلي كيلبلينغ**</span>
**<span dir="rtl">(</span>Leslie
<span dir="rtl"></span>Kaelbling<span dir="rtl">)</span>**<span dir="rtl">،
**أندرو مور** </span>**(Andrew Moore)**<span dir="rtl">، **كريس أتكسون**
</span>**(Chris Atkeson)**<span dir="rtl">، **توم ميتشل** </span>**(Tom
Mitchell)**<span dir="rtl">، **نيلس نيلسون** </span>**(Nils
Nilsson)**<span dir="rtl">، **ستيوارت راسل** </span>**(Stuart
Russell)**<span dir="rtl">، **توم ديترتش** </span>**(Tom
Dietterich)**<span dir="rtl">، **توم دين** </span>**(Tom
Dean)**<span dir="rtl">، و**بوب نارندرا** </span>**(Bob
Narendra)<span dir="rtl">.</span>**

<span dir="rtl">نشكر **مايكل ليتمن** </span>**(Michael
Littman)**<span dir="rtl">، **جيري تسيسورو** </span>**(Gerry
Tesauro)**<span dir="rtl">، **بوب كريتس** </span>**(Bob
Crites)**<span dir="rtl">، **ساتيندر سينغ** </span>**(Satinder
Singh)**<span dir="rtl">، و**وي زانغ**</span> **(Wei Zhang)**
<span dir="rtl">على توفير التفاصيل الخاصة بالفقرات 4.7، 15.1، 15.4،
15.4، و15.6 على التوالي. نشكر **مكتب البحث العلمي للقوات الجوية**
</span>**(Air Force Office of Scientific Research)**<span dir="rtl">،
**المؤسسة الوطنية للعلوم** </span>**(National Science
Foundation)**<span dir="rtl">، و**مختبرات جي تي إي** </span>**(GTE
Laboratories)** <span dir="rtl">على دعمهم الطويل وبُعد نظرهم</span>.

<span dir="rtl">كما نود أن نشكر العديد من الأشخاص الذين قرأوا مسودات هذا
الكتاب وقدموا تعليقات قيمة، بما في ذلك **توم كالت** </span>**(Tom
Kalt)**<span dir="rtl">، **جون تسيتيكليس** </span>**(John
Tsitsiklis)**<span dir="rtl">، **باول سيخوش**</span>
**<span dir="rtl">(</span>Pawel
<span dir="rtl"></span>Cichosz<span dir="rtl">)</span>**<span dir="rtl">،
**أولي غالمو** </span>**(Olle Gällmo)**<span dir="rtl">، **تشاك
أندرسون** </span>**(Chuck Anderson)**<span dir="rtl">، **ستيوارت راسل**
</span>**(Stuart Russell)**<span dir="rtl">، **بن فان روي**
</span>**(Ben Van Roy)**<span dir="rtl">، **بول ستينستروب**</span>
**<span dir="rtl">(</span>Paul
<span dir="rtl"></span>Steenstrup<span dir="rtl">)</span>**<span dir="rtl">،
**بول كوهين** </span>**(Paul Cohen)**<span dir="rtl">، **سريدحر
مهدييفن** </span>**(Sridhar Mahadevan)**<span dir="rtl">، **جيتي
راندلوف** </span>**(Jette Randlov)**<span dir="rtl">، **برايان شيبرد**
</span>**(Brian Sheppard)**<span dir="rtl">، **توماس أوكونيل**
</span>**(Thomas O’Connell)**<span dir="rtl">، **ريتشارد كوجينز**
</span>**(Richard Coggins)**<span dir="rtl">، **كريستينا فيرسينو**
</span>**(Cristina Versino)**<span dir="rtl">، **جون هييت**
</span>**(John H. Hiett)**<span dir="rtl">، **أندرياس بادل**
</span>**(Andreas Badelt)**<span dir="rtl">، **جي بونتي** </span>**(Jay
Ponte)**<span dir="rtl">، **جو بيك** </span>**(Joe
Beck)**<span dir="rtl">، **جاستس بيتر** </span>**(Justus
Piater)**<span dir="rtl">، **مارثا ستينستروب** </span>**(Martha
Steenstrup)**<span dir="rtl">، **ساتيندر سينغ** </span>**(Satinder
Singh)**<span dir="rtl">، **تومي ياكولا** </span>**(Tommi
Jaakkola)**<span dir="rtl">، **ديميتري بيرتسيكاس** </span>**(Dimitri
Bertsekas)**<span dir="rtl">، **توربيورن إيكمان** </span>**(Torbjörn
Ekman)**<span dir="rtl">، **كريستينا بيوركمان** </span>**(Christina
Björkman)**<span dir="rtl">، **جاكوب كارلستروم** </span>**(Jakob
Carlström)**<span dir="rtl">، و **أولي بالمجرين** </span>**(Olle
Palmgren)**<span dir="rtl">.</span> <span dir="rtl">وأخيرًا، نشكر **جوين
ميتشل.**</span>

<span dir="rtl">ملخص الترميز (</span>Summary of
Notation<span dir="rtl">)</span>

<span dir="rtl">تُستخدم الأحرف الكبيرة للمتغيرات العشوائية، بينما تستخدم
الأحرف الصغيرة لقيم المتغيرات العشوائية</span>

<span dir="rtl">قيم المتغيرات العشوائية والدوال القياسية. الكميات
المطلوب أن تكون متجهات ذات قيم حقيقية تُكتب بخط عريض وبأحرف صغيرة (حتى لو
كانت متغيرات عشوائية).</span>

<span dir="rtl">أما المصفوفات فتُكتب بأحرف كبيرة.</span>

|  |  |
|----|----|
| <span dir="rtl">علاقة المساواة صحيحة بحكم التعريف</span> | 
``` math
\doteq
``` |
| <span dir="rtl">متساوية تقريبًا</span> | 
``` math
\approx
``` |
| <span dir="rtl">يتناسب مع</span> | 
``` math
\propto
``` |
| <span dir="rtl">احتمال أن يأخذ المتغير العشوائي</span> X <span dir="rtl">القيمة</span> x | 
``` math
PrX = x
``` |
| <span dir="rtl">المتغير العشوائي</span> X <span dir="rtl">المحدد من التوزيع</span> p(x)<span dir="rtl">=</span> Pr<span dir="rtl">{</span>X<span dir="rtl">=</span>x<span dir="rtl">}</span> | X ~ p |
| <span dir="rtl"></span>$`\mathbf{\ \ E}\left\lbrack \mathbf{X} \right\rbrack\mathbf{=}\sum_{\mathbf{x}}^{}{\mathbf{p}\left( \mathbf{x} \right)}\mathbf{x\ }`$<span dir="rtl">التوقع للقيمة العشوائية</span> X <span dir="rtl"></span> | 𝔼\[X\] |
| <span dir="rtl">قيمة</span> a <span dir="rtl">التي عندها تأخذ</span> f(a) <span dir="rtl">قيمتها القصوى</span> | argmax***a f**(a)* |
| <span dir="rtl">اللوغاريتم الطبيعي لـ</span> x | In $`x`$ |
| <span dir="rtl">أساس اللوغاريتم الطبيعي،</span> e <span dir="rtl"></span>$`\mathbf{\approx}`$ <span dir="rtl"></span>2.71828<span dir="rtl">، مرفوع القوة الى</span> $`\mathbf{x;\,}\mathbf{e}^{\mathbf{ln}\mathbf{x}}\mathbf{= x\ \ \ \ \ }`$ | 
``` math
\mathbb{e}^{x}
``` |
| <span dir="rtl">مجموعة الأعداد الحقيقية</span> | 
``` math
\mathbb{R}
``` |
| <span dir="rtl">الدالة</span> f <span dir="rtl">من عناصر المجموعة</span> X <span dir="rtl">إلى عناصر المجموعة</span> Y | 
``` math
f:x \rightarrow y
``` |
| <span dir="rtl">تعيين</span> | 
``` math
\leftarrow
``` |
| <span dir="rtl">الفاصل الزمني الحقيقي بين</span> a <span dir="rtl">و</span>b <span dir="rtl">بما في ذلك</span> b <span dir="rtl">ولكن لا يشمل</span> a | (a, b\] |
| <span dir="rtl">احتمالية اتخاذ إجراء عشوائي في سياسة "</span> $`\mathbf{\varepsilon}`$<span dir="rtl">-</span>greed<span dir="rtl">".</span> | 
``` math
\varepsilon
``` |
| <span dir="rtl">حجم خطوة</span> parameter | 
``` math
\alpha,\beta
``` |
| <span dir="rtl">معدل الخصم</span> parameter | 
``` math
\gamma
``` |
| parameter <span dir="rtl">معدل التلاشي لأثر الاهلية</span> | 
``` math
\leftthreetimes
``` |
| <span dir="rtl">دالة المؤشر (العبارة المنطقية = 1 إذا كانت العبارة المنطقية صحيحة، وإلا 0)</span> | 
``` math
\mathbb{1\ }predicate
``` |
| In a multi-arm bandit problem: |  |
| <span dir="rtl">عدد الإجراءات (الأذرع)</span> number of actions (arms) | 
``` math
\kappa
``` |
| <span dir="rtl">الخطوة الزمنية المنفصلة أو عدد الخطوات</span> | t |
| <span dir="rtl">القيمة الحقيقية (المكافأة المتوقعة) للإجراء</span> a | 
``` math
q*(a)
``` |
| <span dir="rtl">التقدير في الوقت</span> t <span dir="rtl">لـ</span> $`\mathbf{q*}\left( \mathbf{a} \right)`$ | 
``` math
Q_{t}(\alpha)
``` |
| <span dir="rtl">عدد المرات التي تم فيها اختيار الإجراء</span> a <span dir="rtl">قبل الوقت</span> t | 
``` math
N_{t}(a)
``` |
| <span dir="rtl">التفضيل المتعلم لاختيار الإجراء</span> a <span dir="rtl">في الوقت</span> t | 
``` math
H_{t}(a)
``` |
| <span dir="rtl">احتمالية اختيار الإجراء</span> a <span dir="rtl">في الوقت</span> t | 
``` math
\pi_{t}(a)
``` |
| <span dir="rtl">التقدير في الوقت</span> t <span dir="rtl">للمكافأة المتوقعة المعطاة</span> $`\mathbf{\pi}_{\mathbf{t}}`$ | 
``` math
{\overline{R}}_{t}
``` |
| In a Markov Decision Process: |  |
| <span dir="rtl">الحالات</span> | 
``` math
\mathcal{S,}\mathcal{S}'
``` |
| <span dir="rtl">إجراء</span> | 
``` math
\alpha
``` |
| <span dir="rtl">مكافأة</span> | 
``` math
r
``` |
| <span dir="rtl">مجموعة جميع الحالات غير النهائية</span> | 
``` math
\mathcal{S}
``` |
| <span dir="rtl">مجموعة جميع الحالات، بما في ذلك الحالة النهائية</span> | 
``` math
\mathcal{S +}
``` |
| <span dir="rtl">مجموعة جميع الإجراءات المتاحة في الحالة</span> s | 
``` math
A\left( \mathcal{S} \right)
``` |
|  |  |
| <span dir="rtl">مجموعة فرعية</span> | 
``` math
\subset
``` |
| <span dir="rtl">عدد العناصر في المجموعة</span> S | 
``` math
\left| \mathcal{S} \right|
``` |
| <span dir="rtl">الخطوة الزمنية المنفصلة</span> | t |
| <span dir="rtl">الخطوة الزمنية النهائية لحلقة، أو الحلقة التي تشمل الخطوة الزمنية</span> t | 
``` math
T,T(t)
``` |
| <span dir="rtl">الإجراء في الوقت</span> t | 
``` math
A_{t}
``` |
| <span dir="rtl">الحالة في الوقت</span> t<span dir="rtl">، والتي تعتمد عادةً، بطريقة احتمالية،</span> $`\mathbf{S}_{\mathbf{t - 1}}\text{ and  }\mathbf{A}_{\mathbf{t - 1}}`$ | 
``` math
S_{t}
``` |
| <span dir="rtl">المكافأة في الوقت</span> t<span dir="rtl">، والتي تعتمد عادةً، بطريقة احتمالية،</span> $`\mathbf{S}_{\mathbf{t - 1}}\text{ and  }\mathbf{A}_{\mathbf{t - 1}}`$ | 
``` math
R_{t}
``` |
| <span dir="rtl">السياسة (قاعدة اتخاذ القرار)</span> | 
``` math
\pi
``` |
| <span dir="rtl">الإجراء الذي يتم اتخاذه في الحالة</span> s <span dir="rtl">تحت السياسة الحتمية</span> $`\mathbf{\pi}`$ | 
``` math
\pi\left( \mathcal{S} \right)
``` |
| <span dir="rtl">احتمالية اتخاذ الإجراء</span> a <span dir="rtl">في الحالة</span> s <span dir="rtl">تحت السياسة العشوائية</span> $`\mathbf{\pi}`$ | 
``` math
\pi\left( \alpha \middle| \mathcal{S} \right)
``` |
|  |  |
| <span dir="rtl">العائد بعد الوقت</span> t | 
``` math
G_{t}
``` |
| <span dir="rtl">الأفق، الخطوة الزمنية التي يتم النظر إليها في الرؤية المستقبلية</span> | 
``` math
h
``` |
| <span dir="rtl">العائد ذو</span> n <span dir="rtl">خطوة من</span> t+1 <span dir="rtl">إلى</span> t+n <span dir="rtl">، أو إلى</span> h <span dir="rtl">(معدل و مخصوم )</span> | 
``` math
G_{t:t + n}\ ,G_{t:h}
``` |
| <span dir="rtl">العائد الثابت (غير مخصوم وغير معدل) من</span> t+1<span dir="rtl">إلى</span>h <span dir="rtl">(القسم 5.8)</span> | 
``` math
{\overline{G}}_{t:h}
``` |
| return (Section 12.1) | 
``` math
G_{t}^{\leftthreetimes}
``` |
| <span dir="rtl">العائد المصحح المقطوع</span> | 
``` math
G_{t:h}^{\leftthreetimes}
``` |
| <span dir="rtl">العائد المصحح بقيم الحالة أو الإجراء المقدرة</span> | 
``` math
G_{t}^{\leftthreetimes s}\ ,\ G_{t}^{\leftthreetimes a}
``` |
|  |  |
| <span dir="rtl">احتمالية الانتقال إلى الحالة</span> s′ <span dir="rtl">مع المكافأة</span> r<span dir="rtl">، من الحالة</span> s <span dir="rtl">عند اتخاذ الفعل</span> a | 
``` math
p\left( \mathcal{S}',\alpha \middle| \mathcal{S,}\alpha \right)
``` |
| <span dir="rtl">احتمالية الانتقال إلى الحالة</span> s′ <span dir="rtl">من الحالة</span> s <span dir="rtl">عند اتخاذ الفعل</span> a | 
``` math
p\left( \mathcal{S}' \middle| \mathcal{S,}\alpha \right)
``` |
| <span dir="rtl">المكافأة الفورية المتوقعة عند الانتقال من الحالة</span> s <span dir="rtl">بعد اتخاذ الفعل</span> a | 
``` math
p\left( \mathcal{S,}\alpha \right)
``` |
| <span dir="rtl">المكافأة الفورية المتوقعة عند الانتقال من الحالة</span> s <span dir="rtl">إلى الحالة</span> s′ <span dir="rtl">تحت الفعل</span> a | 
``` math
p\left( \mathcal{S,}\alpha,\mathcal{S}' \right)
``` |
|  |  |
| <span dir="rtl">قيمة الحالة</span> $`\mathbf{s}`$ <span dir="rtl"></span> <span dir="rtl">تحت السياسة</span> π <span dir="rtl">(العائد المتوقع)</span> | 
``` math
v_{\pi}(s)
``` |
| <span dir="rtl">قيمة الحالة</span> $`\mathbf{s}`$ <span dir="rtl"></span> <span dir="rtl">تحت السياسة المثلى</span> | 
``` math
v_{*}(s)
``` |
| <span dir="rtl">قيمة اتخاذ الفعل</span> a <span dir="rtl">في الحالة</span> s <span dir="rtl">تحت السياسة</span> π | 
``` math
q_{\pi}(s,a)
``` |
| <span dir="rtl">قيمة اتخاذ الفعل</span> a <span dir="rtl">في الحالة</span> $`\mathbf{s}`$ <span dir="rtl"></span> <span dir="rtl">تحت السياسة المثلى</span> | 
``` math
q_{*}(s,a)
``` |
| <span dir="rtl">تقديرات مصفوفة لدالة قيمة الحالة</span> $`\mathbf{v}_{\mathbf{\pi}}`$ <span dir="rtl"></span> <span dir="rtl">أو</span> v∗ | 
``` math
V{,V}_{t}
``` |
| <span dir="rtl">تقديرات مصفوفة لدالة قيمة الإجراء</span> $`\mathbf{q}_{\mathbf{\pi}}`$ <span dir="rtl"></span> <span dir="rtl">أو</span> q∗ | 
``` math
Q,Q_{t}
``` |
| <span dir="rtl">قيمة الإجراء التقريبية المتوقعة</span> | 
``` math
{\overline{V}}_{t}(s)
``` |
| <span dir="rtl">الهدف للتقدير عند الزمن</span> t | 
``` math
U_{t}
``` |
| <span dir="rtl">الخطأ الزمني للفروقات</span> (TD) <span dir="rtl">عند الزمن</span> t <span dir="rtl">(متغير عشوائي)</span> | 
``` math
\delta_{t}
``` |
| <span dir="rtl">أشكال الخطأ الزمني للفروقات</span> (TD) <span dir="rtl">الخاصة بالحالة والإجراء</span> | 
``` math
\delta_{t}^{S},\delta_{t}^{a}
``` |
| <span dir="rtl">في طرق</span> n<span dir="rtl">\_الخطوات،</span> n <span dir="rtl">هو عدد خطوات الإقحام</span> | 
``` math
n
``` |
| <span dir="rtl">الأبعاد—عدد المكونات في</span> w | 
``` math
d
``` |
| <span dir="rtl">الأبعاد البديلة—عدد المكونات في</span> θ | 
``` math
d'
``` |
| <span dir="rtl">متجه بحجم</span> d <span dir="rtl">للأوزان التي تقوم على دالة القيمة التقريبية</span> | 
``` math
\mathbf{w},\mathbf{w}_{t}
``` |
| <span dir="rtl">العنصر</span> i <span dir="rtl">من متجه الأوزان القابل للتعليم</span> | 
``` math
w_{i},w_{t,i}
``` |
| <span dir="rtl">التقدير التقريبي لقيمة الحالة</span> s <span dir="rtl">بالنظر إلى متجه الأوزان</span> w | 
``` math
\widehat{v}(s,w)
``` |
| <span dir="rtl">ترميز بديل لـ</span> v^(s,w) | 
``` math
v_{w}(s)
``` |
| <span dir="rtl">التقدير التقريبي لقيمة زوج الحالة – الفعل</span> s,a <span dir="rtl">بالنسبة لمتجه الأوزان</span> w | 
``` math
\widehat{q}(s,a,w)
``` |
| <span dir="rtl">متجه عمودي للمشتقات الجزئية لـ</span> v^(s,w) <span dir="rtl">بالنسبة لـ</span> w | 
``` math
\nabla\widehat{v}(s,w)
``` |
| <span dir="rtl">متجه عمودي للمشتقات الجزئية لـ</span> q^​(s,a,w) <span dir="rtl">بالنسبة لـ</span> w | 
``` math
\nabla\widehat{q}(s,a,w)
``` |
|  |  |
| <span dir="rtl">متجه الخصائص المرئية عند التواجد في الحالة</span> s | 
``` math
x(s)
``` |
| <span dir="rtl">متجه الخصائص المرئية عند التواجد في الحالة</span> s <span dir="rtl">واتخاذ الفعل</span> a | 
``` math
x(s,a)
``` |
| <span dir="rtl">العنصر</span> i <span dir="rtl">من المتجه</span> x(s) <span dir="rtl">أو</span> x(s,a) | 
``` math
x_{i}(s),\chi_{i}(s,a)
``` |
| <span dir="rtl">اختصار لـ</span> x(St​) <span dir="rtl">أو</span> x($`\mathbf{S}_{\mathbf{t}}`$​,$`\mathbf{A}_{\mathbf{t}}`$​) | 
``` math
x_{t}
``` |
| <span dir="rtl">الضرب الداخلي للمتجهات</span> | 
``` math
w^{\top}x
``` |
| <span dir="rtl">متجه ثانوي بحجم</span> d <span dir="rtl">للأوزان، يُستخدم لتعلم</span> w | 
``` math
v,v_{t}
``` |
| <span dir="rtl">متجه بحجم</span> d <span dir="rtl">لآثار الأهلية عند الزمن</span> t | 
``` math
z_{t}
``` |
|  |  |
| <span dir="rtl">متجه المعاملات للسياسة المستهدفة</span> | 
``` math
\theta,\theta_{t}
``` |
| <span dir="rtl">احتمالية اتخاذ الفعل</span> a <span dir="rtl">في الحالة</span> s <span dir="rtl">بالنظر إلى متجه المعاملات</span> θ | 
``` math
\pi\left( \left. \ a \right|s,\theta \right)
``` |
| <span dir="rtl">السياسة المطابقة للمعامل</span> θ | 
``` math
\pi_{\theta}
``` |
| <span dir="rtl">متجه عمودي للمشتقات الجزئية لـ</span> π(a∣s,θ) <span dir="rtl">بالنسبة لـ</span> θ | 
``` math
\nabla\pi\left( \left. \ a \right|s,\theta \right)
``` |
| <span dir="rtl">مقياس الأداء للسياسة</span> $`\mathbf{\pi}_{\mathbf{\theta}}`$ | 
``` math
J(\theta)
``` |
| <span dir="rtl">متجه عمودي للمشتقات الجزئية لـ</span> J(θ) <span dir="rtl">بالنسبة لـ</span> θ | 
``` math
\nabla J(\theta)
``` |
| <span dir="rtl">تفضيل اختيار الفعل</span> a <span dir="rtl">في الحالة</span> s <span dir="rtl">بناءً على</span> θ | 
``` math
h(s,a,\theta)
``` |
| <span dir="rtl">سياسة السلوك المستخدمة لاختيار الأفعال أثناء التعليم عن سياسة الهدف</span> π | 
``` math
b\left( \left. \ a \right|s \right)
``` |
| <span dir="rtl">دالة الأساس</span> b:S→ℝ <span dir="rtl">لأساليب تدرج السياسة</span> | 
``` math
b(s)
``` |
| <span dir="rtl">عامل التفرع لمدير القرارات</span> Markovian (MDP) <span dir="rtl">أو شجرة البحث</span> | 
``` math
b
``` |
| <span dir="rtl">نسبة العينة الهامة من الزمن</span> t <span dir="rtl">حتى الزمن</span> h | 
``` math
{\rho_{t}}_{:h}
``` |
| <span dir="rtl">نسبة العينة الهامة عند الزمن</span> t <span dir="rtl">فقط</span> ρt​≐ρt​:t​​ | 
``` math
\rho_{t}
``` |
| <span dir="rtl">المكافأة المتوسطة (معدل المكافأة) للسياسة</span> π | 
``` math
r(\pi)
``` |
| <span dir="rtl">تقدير لـ</span> r(π) <span dir="rtl">عند الزمن</span> t | 
``` math
{\overline{R}}_{t}
``` |
| <span dir="rtl">توزيع على السياسة فوق الحالات</span> | 
``` math
\mu(s)
``` |
| <span dir="rtl">متجه بحجم</span> ∣S∣ <span dir="rtl">يحتوي على</span> μ(s) <span dir="rtl">لجميع</span> s <span dir="rtl"></span>$`\mathbf{\ni}`$ <span dir="rtl"></span>S | 
``` math
\mu
``` |
| <span dir="rtl">القاعدة المربعة لدالة القيمة الموزونة بـ</span> μ | 
``` math
\left\| v \right\|^{2}
``` |
| <span dir="rtl">العدد المتوقع للزيارات إلى الحالة</span> s <span dir="rtl">في كل حلقة</span> | 
``` math
\eta(s)^{\mu}
``` |
| <span dir="rtl">مشغل الإسقاط لدوال القيمة</span> | 
``` math
\Pi
``` |
| <span dir="rtl">مشغل بيلمان لدوال القيمة</span> | 
``` math
B_{\pi}
``` |
| <span dir="rtl">مصفوفة بحجم</span> d×d | 
``` math
A
``` |
| <span dir="rtl">متجه ذو بعد</span> d | 
``` math
b
``` |
| <span dir="rtl">نقطة ثابتة في التعليم من الفرق الزمني</span> (TD) | 
``` math
w_{TD}
``` |
| <span dir="rtl">مصفوفة الوحدة</span> | 
``` math
I
``` |
| <span dir="rtl">مصفوفة بحجم</span> ∣S∣×∣S∣ <span dir="rtl">للاحتمالات الانتقالية بين الحالات تحت</span> π | 
``` math
P
``` |
| <span dir="rtl">مصفوفة قطرية بحجم</span> ∣S∣×∣S∣ <span dir="rtl">تحتوي على</span> μ <span dir="rtl">على القطر</span> | 
``` math
D
``` |
| <span dir="rtl">مصفوفة بحجم</span> $`\left| \mathbf{S} \right|\mathbf{\times d}`$ <span dir="rtl">حيث تكون</span> x(s) <span dir="rtl">كصفوف لها</span> | 
``` math
X
``` |
| <span dir="rtl">خطأ بيلمان (خطأ</span> TD <span dir="rtl">المتوقع)</span> <span dir="rtl">لـ</span> $`\mathbf{v}_{\mathbf{w}}`$​ <span dir="rtl">عند الحالة</span> s | 
``` math
\overline{\delta}w(s)
``` |
| <span dir="rtl">متجه خطأ بيلمان، مع المكونات</span> | 
``` math
{\overline{\delta}}_{w},BE
``` |
| <span dir="rtl">خطأ القيمة المربع المتوسط</span> | 
``` math
\overline{VE}(w)
``` |
| <span dir="rtl">خطأ بيلمان المربع المتوسط</span> | 
``` math
\overline{BE}(w)
``` |
| <span dir="rtl">خطأ بيلمان المربع المتوسط المُتوقع</span> | 
``` math
\overline{PBE}(w)
``` |
| <span dir="rtl">خطأ الفرق الزمني المربع المتوسط</span> | 
``` math
\overline{TDE}(w)
``` |
| <span dir="rtl">خطأ العائد المربع المتوسط</span> | 
``` math
\overline{RE}(w)
``` |

<span dir="rtl">الفصل الأول</span>  
<span dir="rtl">مقدمة</span>

<span dir="rtl">إن الفكرة التي تقول إننا نتعلم من خلال التفاعل مع بيئتنا
هي على الأرجح أول ما يتبادر إلى ذهننا عندما نفكر في طبيعة التعليم. عندما
يلعب الرضيع أو يلوّح بذراعيه أو ينظر حوله، فإنه لا يكون لديه معلم واضح،
ولكنه يمتلك اتصالاً مباشرًا بين حواسه وحركاته مع بيئته. ممارسة هذا الاتصال
تُنتج ثروة من المعلومات حول السبب والنتيجة، وحول عواقب الأفعال، وحول ما
يجب فعله لتحقيق الأهداف. طوال حياتنا، مثل هذه التفاعلات هي بلا شك مصدر
رئيسي للمعرفة حول بيئتنا وعن أنفسنا. سواء كنا نتعلم قيادة السيارة أو
إجراء محادثة، فإننا ندرك بشكل حاد كيفية استجابة بيئتنا لما نقوم به،
ونسعى للتأثير على ما يحدث من خلال سلوكنا. التعليم من التفاعل هو فكرة
أساسية تقوم عليها تقريبًا جميع نظريات التعليم والذكاء</span>.

<span dir="rtl">في هذا الكتاب نستكشف نهجًا حسابيًا للتعليم من التفاعل.
بدلاً من التنظير بشكل مباشر حول كيفية تعلم الناس أو الحيوانات، نركز بشكل
أساسي على استكشاف حالات تعلم مثالية وتقييم فعالية طرق التعليم المختلفة.
بمعنى آخر، نتبنى منظور باحث أو مهندس في مجال الذكاء الاصطناعي. نستكشف
تصاميم للآلات تكون فعالة في حل مشاكل التعليم ذات الاهتمام العلمي أو
الاقتصادي، مع تقييم التصاميم من خلال التحليل الرياضي أو التجارب
الحسابية. النهج الذي نستكشفه، والذي يُعرف بالتعليم المعزز، يركز بشكل أكبر
على التعليم الموجه نحو الأهداف من خلال التفاعل مقارنة بالنهج الأخرى في
تعلم الآلة</span>.

### 1.1 <span dir="rtl">التعليم المعزز</span>

<span dir="rtl">التعليم المعزز هو عملية تعلم ماذا يجب أن نفعل، أي كيفية
ربط المواقف بالإجراءات، وذلك بهدف تعظيم إشارة المكافأة العددية. المتعلم
لا يُخبره أحد بالإجراءات التي يجب اتخاذها، بل يجب عليه اكتشاف أي
الإجراءات تؤدي إلى تحقيق أكبر قدر من المكافآت من خلال تجربتها. في
الحالات الأكثر إثارة وتحديًا، قد تؤثر الإجراءات ليس فقط على المكافأة
الفورية ولكن أيضًا على الوضع التالي، ومن خلال ذلك، على جميع المكافآت
اللاحقة. هاتان السمتان - البحث من خلال التجربة والخطأ والمكافأة
المؤجلة - هما أهم ميزتين تميزان التعليم المعزز</span>.

<span dir="rtl">التعليم المعزز، مثل العديد من المواضيع التي تنتهي
أسماؤها بـ</span> "ing"<span dir="rtl">، مثل تعلم الآلة وتسلق الجبال،
يمثل في الوقت ذاته مشكلة، وفئة من طرق الحل التي تعمل بشكل جيد على هذه
المشكلة، والمجال الذي يدرس هذه المشكلة وطرق حلها. من الملائم استخدام اسم
واحد للإشارة إلى هذه الأمور الثلاثة، ولكن من الضروري في الوقت نفسه إبقاء
هذه الأمور منفصلة بشكل مفاهيمي. على وجه الخصوص، فإن التمييز بين المشاكل
وطرق الحل مهم جدًا في التعليم المعزز؛ إذ أن الفشل في إجراء هذا التمييز هو
مصدر للعديد من الالتباسات</span>.

<span dir="rtl">نقوم بتشكيل مشكلة التعليم المعزز باستخدام أفكار من نظرية
الأنظمة الديناميكية، وتحديدًا باعتبارها عملية التحكم الأمثل في العمليات
العشوائية لماركوف ذات المعرفة الجزئية. يجب أن ننتظر حتى الفصل الثالث
للحصول على تفاصيل هذا التشكيل، لكن الفكرة الأساسية هي ببساطة التقاط
الجوانب الأكثر أهمية للمشكلة الحقيقية التي يواجهها وكيل التعليم أثناء
تفاعله مع بيئته لتحقيق هدف ما. يجب أن يكون وكيل التعليم قادرًا على إدراك
حالة بيئته إلى حد ما، ويجب أن يكون قادرًا على اتخاذ إجراءات تؤثر على تلك
الحالة. يجب أيضًا أن يكون لدى الوكيل هدف أو أهداف تتعلق بحالة البيئة.
تهدف عمليات اتخاذ القرار لماركوف إلى تضمين هذه الجوانب الثلاثة -
الإدراك، والعمل، والهدف - في أبسط أشكالها الممكنة دون تبسيط أي منها إلى
حد التفاهة. أي طريقة تكون مناسبة لحل مثل هذه المشاكل نعتبرها طريقة
للتعليم المعزز</span>.

<span dir="rtl">التعليم المعزز يختلف عن التعليم الخاضع للإشراف، وهو
النوع من التعليم الذي يتم دراسته في معظم الأبحاث الحالية في مجال تعلم
الآلة. التعليم الخاضع للإشراف هو التعليم من مجموعة تدريبية من الأمثلة
البارامترية التي يقدمها مشرف خارجي ذو معرفة. كل مثال هو وصف لحالة معينة
مع تحديد - يُسمى التسمية - للإجراء الصحيح الذي يجب على النظام اتخاذه في
تلك الحالة، والذي يكون غالبًا تحديد الفئة التي تنتمي إليها الحالة. الهدف
من هذا النوع من التعليم هو أن يتمكن النظام من التعميم أو الاستقراء في
استجاباته بحيث يتصرف بشكل صحيح في مواقف لم تكن موجودة في مجموعة التدريب.
هذا نوع مهم من التعليم، لكنه وحده لا يكفي للتعليم من التفاعل. في المشاكل
التفاعلية، يكون من غير العملي غالبًا الحصول على أمثلة لسلوك مرغوب فيه
تكون صحيحة وتمثل جميع الحالات التي يجب أن يتصرف فيها الوكيل. في الأراضي
غير المستكشفة - حيث يتوقع أن يكون التعليم الأكثر فائدة - يجب أن يكون
الوكيل قادرًا على التعليم من تجربته الخاصة</span>.

<span dir="rtl">التعليم المعزز يختلف أيضًا عما يسميه باحثو تعلم الآلة
"**التعليم** **غير** **الخاضع** **للإشراف**"، الذي يتعلق عادةً بالعثور
على بنية مخفية في مجموعات من البيانات **غير** **البارامترية**. قد يبدو
أن مصطلحي التعليم الخاضع للإشراف والتعليم غير الخاضع للإشراف يصنفان جميع
نماذج تعلم الآلة بشكل شامل، لكنهما لا يفعلان ذلك. على الرغم من أن المرء
قد يشعر بالميل إلى اعتبار التعليم المعزز نوعًا من التعليم غير الخاضع
للإشراف لأنه لا يعتمد على أمثلة للسلوك الصحيح، إلا أن التعليم المعزز
يهدف إلى تعظيم إشارة المكافأة بدلاً من محاولة العثور على بنية مخفية. يمكن
أن يكون اكتشاف البنية في تجربة الوكيل مفيدًا بالتأكيد في التعليم المعزز،
لكن هذا وحده لا يعالج مشكلة التعليم المعزز في تعظيم إشارة المكافأة.
لذلك، نعتبر التعليم المعزز نمطًا ثالثًا من أنماط تعلم الآلة، إلى جانب
التعليم الخاضع للإشراف والتعليم غير الخاضع للإشراف وربما أنماط
أخرى</span>

<span dir="rtl">أحد التحديات التي تنشأ في التعليم المعزز، والتي لا تظهر
في أنواع التعليم الأخرى، هو التوازن بين الاستكشاف</span>
(**exploration**) <span dir="rtl">والاستغلال</span>
(**exploitation**)<span dir="rtl">.</span> <span dir="rtl">للحصول على
الكثير من المكافآت، يجب على وكيل التعليم المعزز أن يفضل الإجراءات التي
جربها في الماضي ووجد أنها فعالة في تحقيق المكافآت. ولكن لاكتشاف مثل هذه
الإجراءات، يجب عليه تجربة إجراءات لم يختبرها من قبل. يجب على الوكيل أن
يستغل</span> (**exploit**) <span dir="rtl">ما قد جربه بالفعل للحصول على
المكافأة، ولكنه يحتاج أيضًا إلى الاستكشاف</span> (**explore**)
<span dir="rtl">لتحسين اختياراته المستقبلية. التحدي هو أنه لا يمكن
متابعة الاستكشاف</span> (**exploration**) <span dir="rtl">أو
الاستغلال</span> (**exploitation**) <span dir="rtl">بشكل حصري دون الفشل
في المهمة. يجب على الوكيل تجربة مجموعة متنوعة من الإجراءات وتفضيل تلك
التي تبدو الأفضل بشكل تدريجي. في مهمة عشوائية، يجب تجربة كل إجراء عدة
مرات للحصول على تقدير موثوق للمكافأة المتوقعة. لقد تم دراسة معضلة
الاستكشاف</span> (**exploration**) <span dir="rtl">والاستغلال</span>
(**exploitation**) <span dir="rtl">بشكل مكثف من قبل علماء الرياضيات لعدة
عقود، لكنها لا تزال غير محلولة. في الوقت الحالي، نلاحظ ببساطة أن مسألة
التوازن بين الاستكشاف</span> (**exploration**)
<span dir="rtl">والاستغلال</span> (**exploitation**) <span dir="rtl">لا
تنشأ حتى في التعليم الخاضع للإشراف والتعليم غير الخاضع للإشراف، على
الأقل في أشكالهما النقية.</span>

<span dir="rtl">ميزة رئيسية أخرى في التعليم المعزز هي أنه يأخذ بعين
الاعتبار بشكل صريح مشكلة الوكيل الموجه نحو الأهداف الذي يتفاعل مع بيئة
غير مؤكدة. وهذا يتناقض مع العديد من الأساليب التي تعتبر مشكلات فرعية دون
معالجة كيفية تناسبها في الصورة الأكبر. على سبيل المثال، لقد ذكرنا أن
العديد من أبحاث تعلم الآلة تركز على التعليم الخاضع للإشراف دون تحديد
كيفية استخدام هذه القدرة في النهاية. وقد طور باحثون آخرون نظريات للتخطيط
مع أهداف عامة، ولكن دون النظر إلى دور التخطيط في اتخاذ القرارات في الوقت
الحقيقي، أو مسألة مصدر النماذج التنبؤية الضرورية للتخطيط. على الرغم من
أن هذه الأساليب قد حققت العديد من النتائج المفيدة، إلا أن تركيزها على
المشكلات الفرعية المعزولة يعتبر قيدًا كبيرًا</span>.

<span dir="rtl">التعليم المعزز يتبع نهجًا معكوسًا، بدءًا من وكيل تفاعلي
وكامل يسعى لتحقيق الأهداف. جميع وكلاء التعليم المعزز لديهم أهداف صريحة،
ويمكنهم استشعار جوانب من بيئاتهم، ويمكنهم اختيار إجراءات للتأثير على
بيئاتهم. علاوة على ذلك، يُفترض عادةً من البداية أن الوكيل يجب أن يعمل رغم
وجود عدم يقين كبير حول البيئة التي يواجهها. عندما يتضمن التعليم المعزز
التخطيط، يجب أن يتناول التفاعل بين التخطيط واختيار الإجراءات في الوقت
الحقيقي، بالإضافة إلى مسألة كيفية الحصول على نماذج البيئة وتحسينها.
عندما يتضمن التعليم المعزز التعليم الخاضع للإشراف، فإنه يفعل ذلك لأسباب
محددة تحدد أي القدرات حاسمة وأيها ليست كذلك. لكي يتقدم البحث في التعليم،
يجب عزل المشكلات الفرعية المهمة ودراستها، ولكن يجب أن تكون مشكلات فرعية
تلعب أدوارًا واضحة في الوكلاء التفاعليين والكاملين الذين يسعون لتحقيق
الأهداف، حتى لو لم يتمكن بعد من ملء جميع تفاصيل الوكيل الكامل</span>.

<span dir="rtl">عندما نتحدث عن وكيل تفاعلي وكامل يسعى لتحقيق الأهداف،
فإننا لا نعني دائمًا كائنًا كاملاً أو روبوتًا كاملًا. هذه أمثلة واضحة، لكن
الوكيل التفاعلي والكامل الذي يسعى لتحقيق الأهداف يمكن أن يكون أيضًا مكونًا
من نظام أكبر سلوكياً. في هذه الحالة، يتفاعل الوكيل مباشرة مع باقي النظام
الأكبر ويتفاعل بشكل غير مباشر مع بيئة النظام الأكبر. مثال بسيط على ذلك
هو وكيل يراقب مستوى شحن بطارية الروبوت ويرسل أوامر إلى بنية التحكم في
الروبوت. بيئة هذا الوكيل هي باقي الروبوت إلى جانب بيئة الروبوت. يجب
النظر إلى ما هو أبعد من الأمثلة الأكثر وضوحًا للوكلاء وبيئاتهم لتقدير
عمومية إطار التعليم المعزز</span>.

<span dir="rtl">أحد أكثر الجوانب إثارة في "التعليم المعزز</span>
(reinforcement learning) <span dir="rtl">الحديث هو تفاعله الجوهري
والمثمر مع تخصصات الهندسة والعلوم الأخرى. يعتبر التعليم المعزز</span>
(reinforcement learning) <span dir="rtl">جزءًا من اتجاه طويل الأمد في
الذكاء الاصطناعي وتعلم الآلة نحو التكامل الأكبر مع "الإحصاء</span>
(**statistics**)<span dir="rtl">، والتحسين</span>
(**optimization**)<span dir="rtl">، وغيرها من المواضيع الرياضية. على
سبيل المثال، قدرة بعض أساليب "التعليم المعزز</span> (**reinforcement**
**learning**)" <span dir="rtl">على التعليم باستخدام "تقريبات ذات
معاملات</span> (**parameterized** **approximators**)"
<span dir="rtl">تتعامل مع "لعنة الأبعاد</span>
<span dir="rtl">(</span>**curse** **of**
<span dir="rtl"></span>**dimensionality**<span dir="rtl">)</span>"
<span dir="rtl">الكلاسيكية في أبحاث العمليات ونظرية التحكم. بشكل أكثر
تميزًا، تفاعل التعليم المعزز</span> (reinforcement learning)
<span dir="rtl">أيضًا بشكل قوي مع علم النفس</span> (**psychology**)
<span dir="rtl">وعلم الأعصاب</span> (**neuroscience**)"<span dir="rtl">،
مع تحقيق فوائد كبيرة للطرفين. من بين جميع أشكال "تعلّم الآلة</span>
(**machine** **learning**)<span dir="rtl">، يعتبر "التعليم المعزز</span>
(reinforcement learning) <span dir="rtl">الأقرب إلى نوع التعليم الذي
يقوم به البشر والحيوانات الأخرى، وقد استلهم العديد من خوارزميات التعليم
المعزز</span> (reinforcement learning) <span dir="rtl">الأساسية من أنظمة
التعليم البيولوجية. كما أن التعليم المعزز</span> (reinforcement
learning) <span dir="rtl">قدم مساهمات أيضًا من خلال نموذج نفسي</span>
<span dir="rtl">(</span>**psychological**
<span dir="rtl"></span>**model**<span dir="rtl">)</span>
<span dir="rtl">للتعليم الحيواني الذي يتطابق بشكل أفضل مع بعض البيانات
التجريبية، ومن خلال نموذج مؤثر لأجزاء من "نظام المكافآت</span>
(**reward** **system**) <span dir="rtl">في الدماغ. يتناول هذا الكتاب
أفكار "التعليم المعزز</span> (reinforcement learning)
<span dir="rtl">التي تتعلق بـ الهندسة</span> (**engineering**)
<span dir="rtl">والذكاء الاصطناعي</span> (**artificial**
**intelligence**)<span dir="rtl">، مع تلخيص الروابط بـ علم النفس</span>
(**psychology**) <span dir="rtl">وعلم الأعصاب</span> (**neuroscience**)
<span dir="rtl">في الفصول 14 و15</span>.

<span dir="rtl">أخيرًا، يعتبر **التعليم المعزز**</span> **(reinforcement
learning)** <span dir="rtl">أيضًا جزءًا من اتجاه أكبر في الذكاء الاصطناعي
نحو مبادئ عامة بسيطة. منذ أواخر الستينيات، افترض العديد من الباحثين في
الذكاء الاصطناعي أنه لا توجد مبادئ عامة يمكن اكتشافها، وأن الذكاء ناتج
بدلاً من ذلك عن امتلاك عدد ضخم من الحيل والإجراءات والخوارزميات الخاصة.
كان يُقال أحيانًا إنه إذا تمكنا من إدخال عدد كافٍ من الحقائق ذات الصلة إلى
آلة، على سبيل المثال مليونًا أو مليارًا، فإنها ستصبح ذكية. كانت الطرق
المعتمدة على المبادئ العامة، مثل البحث أو التعليم، تُصنف على أنها **طرق**
**ضعيفة**</span> <span dir="rtl">(</span>**weak**
<span dir="rtl"></span>**methods**<span dir="rtl">)، في حين كانت الطرق
المعتمدة على المعرفة المحددة تُسمى **طرق قوية**</span>
<span dir="rtl">(</span>**strong**
<span dir="rtl"></span>**methods**<span dir="rtl">)</span>
<span dir="rtl">هذه النظرة لا تزال شائعة اليوم، لكنها ليست سائدة. من
وجهة نظرنا، كانت ببساطة مبكرة جدًا: لم يُبذل جهد كافٍ في البحث عن المبادئ
العامة لنستنتج أنه لا يوجد منها. يشمل الذكاء الاصطناعي الحديث الآن
الكثير من البحث الذي يبحث عن المبادئ العامة للتعليم، والبحث، واتخاذ
القرارات. ليس واضحًا مدى عودة المؤشر إلى الوراء، لكن بحث **التعليم
المعزز**</span> <span dir="rtl">(</span>**reinforcement**
<span dir="rtl"></span>**learning**<span dir="rtl">) هو بالتأكيد جزء من
العودة نحو مبادئ أبسط وأقل في **الذكاء** **الاصطناعي**</span>.

<u>1.2 <span dir="rtl">أمثلة:</span></u> <span dir="rtl">طريقة جيدة لفهم
**التعليم المعزز**</span> **(reinforcement learning)**
<span dir="rtl">هي النظر في بعض الأمثلة والتطبيقات الممكنة التي وجهت
تطوره</span>.

<span dir="rtl">لاعب شطرنج ماهر يقوم بحركة. يتم اتخاذ القرار استنادًا إلى
كل من التخطيط—توقع الردود والردود المضادة المحتملة—والأحكام الفورية
والحدسية لجاذبية المواقع والحركات المحددة</span>.

<span dir="rtl">يقوم جهاز تحكم تكييفي بتعديل معايير تشغيل مصفاة النفط في
الوقت الفعلي. يعمل الجهاز على تحسين توازن العائد/التكلفة/الجودة استنادًا
إلى التكاليف الهامشية المحددة دون الالتزام الصارم بالنقاط المحددة أصلاً
من قبل المهندسين</span>. <span dir="rtl"></span>

<span dir="rtl">يصارع عجل الغزال ليقف على قدميه بعد دقائق من ولادته. بعد
نصف ساعة، يكون قد بدأ بالجري بسرعة 20 ميلًا في الساعة</span>.

<span dir="rtl">يقرر روبوت متنقل ما إذا كان يجب عليه دخول غرفة جديدة
بحثًا عن مزيد من النفايات لجمعها أو بدء البحث عن طريقه للعودة إلى محطة
شحن البطارية. يتخذ قراره بناءً على مستوى شحن البطارية الحالي ومدى سرعة
وسهولة العثور على شاحن البطارية في الماضي</span>.

<span dir="rtl">فيليب يعد فطوره. عند فحصه عن كثب، تكشف هذه النشاطات
البسيطة ظاهريًا عن شبكة معقدة من السلوكيات المشروطة والعلاقات بين الأهداف
والأهداف الفرعية المتشابكة: المشي إلى الخزانة، فتحها، اختيار علبة حبوب،
ثم الوصول إلى العلبة، والإمساك بها، واسترجاعها. تتطلب الحصول على وعاء،
ملعقة، وعلبة حليب تسلسلات سلوكية معقدة ومتناسقة وتفاعلية. يتضمن كل خطوة
سلسلة من حركات العين للحصول على المعلومات وتوجيه الوصول والحركة. يتم
اتخاذ قرارات سريعة بشكل مستمر حول كيفية حمل الأشياء أو ما إذا كان من
الأفضل نقل بعضها إلى طاولة الطعام قبل الحصول على أشياء أخرى. كل خطوة
توجهها الأهداف، مثل الإمساك بملعقة أو الوصول إلى الثلاجة، وتخدم أهدافًا
أخرى، مثل الحصول على الملعقة لتناول الطعام بمجرد إعداد الحبوب، وأخيرًا
الحصول على التغذية. سواء كان يدرك ذلك أم لا، فإن فيليب يصل إلى معلومات
حول حالة جسده التي تحدد احتياجاته الغذائية، مستوى جوعه، وتفضيلاته
الغذائية</span>.

<span dir="rtl">تشترك هذه الأمثلة في ميزات أساسية للغاية بحيث يسهل
تجاهلها. جميعها تشمل التفاعل بين وكيل نشط يتخذ قرارات وبيئته، حيث يسعى
الوكيل لتحقيق هدفه على الرغم من عدم اليقين بشأن بيئته. يُسمح لإجراءات
الوكيل بتأثير الحالة المستقبلية للبيئة (مثل: الوضع التالي في الشطرنج،
مستوى الخزانات في المصفاة، موقع الروبوت التالي ومستوى الشحن المستقبلي
لبطاريته)، وبالتالي تؤثر على الأفعال والفرص المتاحة للوكيل في أوقات
لاحقة. يتطلب الاختيار الصحيح أخذ العواقب غير المباشرة. والمتأخرة
للإجراءات في الاعتبار، وبالتالي قد يتطلب النظر إلى المستقبل أو
التخطيط</span>

<span dir="rtl">في الوقت نفسه، في جميع هذه الأمثلة، لا يمكن التنبؤ
الكامل بتأثيرات الإجراءات؛ لذا يجب على الوكيل مراقبة بيئته بشكل متكرر
والتفاعل بشكل مناسب. على سبيل المثال، يجب على فيليب أن يراقب الحليب الذي
يصبه في وعاء الحبوب الخاص به ليمنع فائضه. جميع هذه الأمثلة تتضمن أهدافًا
تكون صريحة بمعنى أن الوكيل يمكنه تقييم تقدمه نحو هدفه بناءً على ما يمكنه
استشعاره مباشرة. لاعب الشطرنج.</span>

<span dir="rtl">يعرف ما إذا كان قد فاز أم لا، ومراقب المصفاة يعرف مقدار
النفط الذي يتم إنتاجه، والعجل الغزال يعرف متى يسقط، والروبوت المتنقل
يعرف متى تنفد بطارياته، وفيليب يعرف ما إذا كان يستمتع بفطوره أم
لا</span>. <span dir="rtl"></span>

<span dir="rtl">في جميع هذه الأمثلة، يمكن للوكيل استخدام تجربته لتحسين
أدائه بمرور الوقت. لاعب الشطرنج ينمي حدسه الذي يستخدمه لتقييم المواقع،
مما يحسن لعبه؛ العجل الغزال يحسن كفاءته في الجري؛ فيليب يتعلم تبسيط
عملية إعداد فطوره. المعرفة التي يجلبها الوكيل إلى المهمة في البداية—سواء
من تجربته السابقة مع المهام ذات الصلة أو التي تم تضمينها في تصميمه أو
تطوره—تؤثر على ما هو مفيد أو سهل التعليم، ولكن التفاعل مع البيئة أمر
أساسي لتعديل السلوك لاستغلال ميزات محددة من المهمة.</span>

**<u>1.3 <span dir="rtl">عناصر التعليم المعزز</span></u>**

<span dir="rtl">بجانب الوكيل والبيئة، يمكن تحديد أربعة عناصر رئيسية في
نظام التعليم المعزز: سياسة</span> (Policy)<span dir="rtl">، إشارة
مكافأة</span> (Reward Signal)<span dir="rtl">، دالة قيمة</span> (Value
Function)<span dir="rtl">، ونموذج للبيئة</span>
<span dir="rtl">(</span>Model of the Environment<span dir="rtl">)</span>
<span dir="rtl">اختياريًا</span>.

<span dir="rtl">تحدد السياسة</span> (Policy) <span dir="rtl">طريقة تصرف
الوكيل المتعلم في وقت معين. بشكل عام، السياسة هي تحويل من الحالات
المدركة للبيئة إلى الإجراءات التي يجب اتخاذها عندما يكون الوكيل في تلك
الحالات. تتوافق مع ما يُسمى في علم النفس مجموعة من قواعد
التحفيز–الاستجابة أو الروابط. في بعض الحالات، قد تكون السياسة دالة بسيطة
أو جدول بحث، بينما في حالات أخرى قد تشمل حسابات واسعة مثل عملية بحث.
السياسة هي جوهر وكيل التعليم المعزز بمعنى أنها وحدها كافية لتحديد
السلوك. بشكل عام، قد تكون السياسات عشوائية</span>
(Stochastic)<span dir="rtl">، حيث تحدد احتمالات لكل إجراء</span>.

<span dir="rtl">إشارة المكافأة</span> (**Reward** **Signal**)
<span dir="rtl">تحدد هدف مشكلة التعليم المعزز. في كل خطوة زمنية، ترسل
البيئة إلى الوكيل المتعلم عددًا واحدًا يُسمى المكافأة. الهدف الوحيد للوكيل
هو تعظيم المكافأة الإجمالية التي يتلقاها على المدى الطويل. لذلك، تحدد
إشارة المكافأة ما هي الأحداث الجيدة والسيئة بالنسبة للوكيل. في نظام
بيولوجي، قد نفكر في المكافآت على أنها نظير لتجارب المتعة أو الألم. فهي
الخصائص الفورية والمحددة للمشكلة التي يواجهها الوكيل. إشارة المكافأة هي
الأساس الرئيسي لتعديل السياسة؛ إذا كانت إحدى الإجراءات التي يختارها
السياسة تتبعها مكافأة منخفضة، فقد يتم تغيير السياسة لاختيار إجراء آخر في
تلك الحالة في المستقبل. بشكل عام، قد تكون إشارات المكافأة دوال عشوائية
لحالة البيئة والإجراءات المتخذة</span>.

<span dir="rtl">بينما تشير إشارة المكافأة إلى ما هو جيد من الناحية
الفورية، تحدد دالة القيمة</span> (**Value** **Function**)
<span dir="rtl">ما هو جيد على المدى الطويل. بشكل عام، قيمة الحالة هي
مقدار المكافأة التي يمكن للوكيل توقع جمعها في المستقبل، بدءًا من تلك
الحالة. بينما تحدد المكافآت الجاذبية الفورية والداخلية للحالات البيئية،
تشير القيم إلى الجاذبية الطويلة الأمد للحالات بعد أخذ الحالات التي من
المحتمل أن تليها والمكافآت المتاحة في تلك الحالات بعين الاعتبار. على
سبيل المثال، قد تعطي حالة ما دائمًا مكافأة فورية منخفضة لكنها لا تزال ذات
قيمة عالية لأنها تُتبع بانتظام بحالات أخرى تعطي مكافآت عالية. أو قد يكون
العكس صحيحًا. لتوضيح ذلك بالتحليل البشري، المكافآت تشبه إلى حد ما المتعة
(إذا كانت عالية) والألم (إذا كانت منخفضة)، بينما القيم تتوافق مع حكم
أكثر تطورًا وبُعد نظر حول مدى رضا أو عدم رضا الشخص عن كون بيئته في حالة
معينة</span>.

<span dir="rtl">المكافآت تعتبر أساسية إلى حد ما، بينما القيم، باعتبارها
توقعات للمكافآت، تعتبر ثانوية. بدون المكافآت، لن تكون هناك قيم، والهدف
الوحيد من تقدير القيم هو تحقيق المزيد من المكافآت. ومع ذلك، فإن القيم هي
ما نهتم به أكثر عند اتخاذ القرارات وتقييمها. يتم اتخاذ اختيارات
الإجراءات بناءً على أحكام القيم. نحن نبحث عن الإجراءات التي تؤدي إلى
حالات ذات أعلى قيمة، وليس أعلى مكافأة، لأن هذه الإجراءات تحقق أكبر قدر
من المكافآت على المدى الطويل. للأسف، من الصعب جدًا تحديد القيم مقارنة
بتحديد المكافآت. المكافآت تُعطى أساسًا مباشرة من قبل البيئة، لكن القيم يجب
تقديرها وإعادة تقديرها من تسلسلات الملاحظات التي يجريها الوكيل طوال
عمره. في الواقع، فإن العنصر الأكثر أهمية في معظم خوارزميات التعليم
المعزز التي ندرسها هو طريقة لتقدير القيم بكفاءة. الدور المركزي لتقدير
القيم هو على الأرجح أهم شيء تم تعلمه عن التعليم المعزز خلال الستة عقود
الماضية</span>.

<span dir="rtl">العنصر الرابع والأخير في بعض أنظمة التعليم المعزز هو
نموذج للبيئة</span> (**environment**)<span dir="rtl">.</span>
<span dir="rtl">هذا النموذج يقلد سلوك البيئة، أو بشكل أكثر عمومية، يسمح
بإجراء استنتاجات حول كيفية تصرف البيئة. على سبيل المثال، بالنظر إلى
حالة</span> (**state**) <span dir="rtl">وإجراء</span>
(**action**)<span dir="rtl">، قد يتنبأ النموذج بالحالة التالية والمكافأة
التالية. تُستخدم النماذج في التخطيط</span>
<span dir="rtl">(</span>**planning**<span dir="rtl">)، والذي نعني به أي
طريقة لتحديد مسار العمل من خلال النظر في الحالات المستقبلية الممكنة قبل
أن يتم تجربتها فعلياً. تُسمى الأساليب التي تستخدم النماذج والتخطيط لحل
مشكلات التعليم المعزز بأساليب قائمة على النموذج
(</span>**model**-**based**
<span dir="rtl"></span>**methods**<span dir="rtl">)، على عكس الأساليب
الأبسط الخالية من النماذج</span> (**model**-**free** **methods**)
<span dir="rtl">التي تعتمد بشكل صريح على التعليم من التجربة والخطأ،
والتي تُعتبر تقريباً عكس التخطيط. في الفصل 8، نستعرض أنظمة التعليم المعزز
التي تتعلم في نفس الوقت من التجربة والخطأ، وتتعلم نموذجًا للبيئة</span>
(**environment**)<span dir="rtl">، وتستخدم النموذج في التخطيط. يمتد
التعليم المعزز الحديث من التعليم منخفض المستوى، الذي يعتمد على التجربة
والخطأ، إلى التخطيط المتعمد على مستوى عالٍ</span>.

**<u>1.4 <span dir="rtl">القيود والنطاق</span>
<span dir="rtl">(</span>Limitations and
Scope<span dir="rtl">)</span></u>**

<span dir="rtl">التعليم المعزز</span> (Reinforcement Learning)
<span dir="rtl">يعتمد بشكل كبير على مفهوم **الحالة**</span>
(**state**)—<span dir="rtl">كمدخل **للسياسة** </span>**(policy**)
**<span dir="rtl">ودالة القيمة</span> <span dir="rtl">(</span>value
function**<span dir="rtl">)، وكمدخل ومخرج من النموذج</span>
(**model**)<span dir="rtl">.</span> <span dir="rtl">بشكل غير رسمي،
يمكننا التفكير في الحالة على أنها إشارة تنقل إلى الوكيل فكرة عن "كيف
تبدو البيئة</span>" (how the environment is) <span dir="rtl">في وقت
معين. التعريف الرسمي للحالة كما نستخدمه هنا يتم تقديمه من خلال إطار
**عمليات** **اتخاذ** **القرارات** **ماركوف**</span> (**Markov**
**Decision** **Processes**) <span dir="rtl">كما هو موضح في الفصل 3. بشكل
أكثر عمومية، نشجع القارئ على اتباع المعنى غير الرسمي والتفكير في الحالة
على أنها أي معلومات متاحة للوكيل حول بيئته. بفعالية، نفترض أن إشارة
الحالة يتم إنتاجها بواسطة نظام معالجة مسبقة يعتبر جزءًا من بيئة الوكيل.
نحن لا نتناول مسائل بناء الحالة أو تغييرها أو تعلمها في هذا الكتاب
(باستثناء بشكل موجز في القسم 17.3). نتبع هذا النهج ليس لأننا نعتبر تمثيل
الحالة غير مهم، ولكن من أجل التركيز الكامل على قضايا اتخاذ القرار. بمعنى
آخر، اهتمامنا في هذا الكتاب ليس بتصميم إشارة الحالة، بل باتخاذ القرار
بشأن الإجراء الذي يجب اتخاذه بناءً على أي إشارة حالة متاحة</span>.

<span dir="rtl">معظم طرق **التعليم** **المعزز**</span>
(**Reinforcement** **Learning**) <span dir="rtl">التي نعتبرها في هذا
الكتاب مبنية على تقدير **دوال** **القيمة**</span> (**value**
**functions**)<span dir="rtl">، ولكن ليس من الضروري القيام بذلك لحل
مشكلات **التعليم** **المعزز**. على سبيل المثال، طرق الحل مثل
**الخوارزميات** **الجينية**</span> (**genetic**
**algorithms**)<span dir="rtl">، **البرمجة** **الجينية**</span>
(**genetic** **programming**)<span dir="rtl">، **التلدين**
**المحاكي**</span> (**simulated** **annealing**)<span dir="rtl">، وطرق
التحسين الأخرى لا تقدر دوال القيمة أبداً. هذه الطرق تطبق سياسات ثابتة
متعددة تتفاعل على مدى فترة زمنية ممتدة مع مثال منفصل للبيئة. السياسات
التي تحقق أكبر قدر من المكافآت، ونسخ عشوائية منها، تُنقل إلى الجيل التالي
من **السياسات**، وتتكرر العملية. نطلق على هذه الطرق اسم الطرق
التطورية</span> (**evolutionary** **methods**) <span dir="rtl">لأن عملها
مشابه لكيفية إنتاج التطور البيولوجي لكائنات ذات سلوك ماهر حتى وإن لم
تتعلم خلال حياتها الفردية. إذا كان مجال السياسات صغيراً بما فيه الكفاية،
أو يمكن هيكلته بحيث تكون السياسات الجيدة شائعة أو سهلة الاكتشاف—أو إذا
كان هناك الكثير من الوقت المتاح للبحث—فيمكن أن تكون الطرق التطورية
فعالة. بالإضافة إلى ذلك، تتمتع الطرق التطورية بميزات في المشكلات التي لا
يستطيع فيها الوكيل التعليمي استشعار الحالة الكاملة لبيئته</span>.

<span dir="rtl">نركز على طرق **التعليم** **المعزز**</span>
(**Reinforcement** **Learning**) <span dir="rtl">التي تتعلم أثناء
التفاعل مع البيئة، وهي طرق لا تقوم بها **الطرق** **التطورية**</span>
(**evolutionary** **methods**)<span dir="rtl">.</span>
<span dir="rtl">يمكن أن تكون الطرق القادرة على الاستفادة من تفاصيل
التفاعلات السلوكية الفردية أكثر كفاءة بكثير من الطرق التطورية في العديد
من الحالات. تتجاهل الطرق التطورية جزءاً كبيراً من هيكل مشكلة التعليم
المعزز المفيد: فهي لا تستخدم حقيقة أن السياسة التي تبحث عنها هي دالة من
الحالات إلى الإجراءات؛ ولا تلاحظ الحالات التي يمر بها الفرد خلال حياته،
أو الإجراءات التي يختارها. في بعض الحالات، يمكن أن تكون هذه المعلومات
مضللة (على سبيل المثال، عندما تُفهم الحالات بشكل خاطئ)، ولكن في الغالب
يجب أن تمكن من البحث بشكل أكثر كفاءة. على الرغم من أن التطور والتعليم
يشتركان في العديد من الميزات ويعملان معاً بشكل طبيعي، إلا أننا لا نعتبر
الطرق التطورية بحد ذاتها ملائمة بشكل خاص لمشكلات التعليم المعزز،
وبالتالي، نحن لا نتناولها في هذا الكتاب</span>.

**1.5 <span dir="rtl">مثال موسع:</span> (Tic-Tac-Toe)**

<span dir="rtl">لتوضيح الفكرة العامة للتعليم المعزز ومقارنتها بالطرق
الأخرى، نعتبر المثال التالي بتفصيل أكبر</span>.

**<span dir="rtl">"</span>X<span dir="rtl">","</span>O<span dir="rtl">"</span>
(Tic-Tac-Toe):**

<span dir="rtl">فكر في لعبة إكس أو المعروفة للأطفال. يلعب لاعبان
بالتناوب على لوحة بحجم ثلاث في ثلاث. أحد اللاعبين يلعب بعلامة</span> "X"
<span dir="rtl">والآخر بعلامة</span> "O" <span dir="rtl">حتى يفوز أحد
اللاعبين بوضع ثلاث علامات على التوالي، أفقيًا أو عموديًا أو قطريًا، كما فعل
لاعب</span> "X" <span dir="rtl">في اللعبة المعروضة على اليمين. إذا
امتلأت اللوحة دون أن يحقق أي لاعب ثلاث علامات على التوالي، فإن اللعبة
تكون تعادلًا</span>.

<span dir="rtl">نظرًا لأن اللاعب الماهر يمكنه اللعب بطريقة تجعله لا يخسر
أبدًا، دعنا نفترض أننا نلعب ضد لاعب غير مثالي، أي لاعب قد تكون حركاته
أحيانًا غير صحيحة وتسمح لنا بالفوز. في الوقت الحالي، دعنا نعتبر التعادل
والخسارة سيئين لنا على حد سواء. كيف يمكننا بناء لاعب يكتشف العيوب في
طريقة لعب خصمه ويتعلم لزيادة فرصه في الفوز؟</span>

<span dir="rtl">على الرغم من أن هذه مسألة بسيطة، إلا أنه لا يمكن حلها
بطريقة مرضية من خلال التقنيات التقليدية. على سبيل المثال، الحل
التقليدي</span> "**Minimax**" <span dir="rtl">من نظرية الألعاب ليس صحيحًا
هنا لأنه يفترض طريقة معينة في اللعب من قبل الخصم. على سبيل المثال،
اللاعب الذي يستخدم تقنية</span> "**Minimax**" <span dir="rtl">لن يصل
أبدًا إلى حالة لعبة يمكن أن يخسر منها، حتى لو كان في الواقع يفوز دائمًا من
تلك الحالة بسبب طريقة لعب غير صحيحة من قبل الخصم</span>.
<span dir="rtl">يمكن لطرق التحسين التقليدية لمشكلات اتخاذ القرار
المتسلسل، مثل البرمجة الديناميكية، حساب حل مثالي لأي خصم، ولكنها تتطلب
كمدخلات تحديدًا كاملاً لذلك الخصم، بما في ذلك الاحتمالات التي يختار بها
الخصم كل حركة في كل حالة على اللوحة. لنفترض أن هذه المعلومات غير متاحة
مسبقًا لهذه المشكلة، كما هو الحال بالنسبة للغالبية العظمى من المشكلات
العملية. من ناحية أخرى، يمكن تقدير هذه المعلومات من خلال التجربة، في هذه
الحالة عن طريق لعب العديد من الألعاب ضد الخصم</span>.
<span dir="rtl">أفضل ما يمكن القيام به في هذه المشكلة هو أولاً تعلم نموذج
لسلوك الخصم، حتى مستوى معين من الثقة، ثم تطبيق البرمجة الديناميكية لحساب
حل مثالي بناءً على نموذج الخصم التقريبي. في النهاية، هذا ليس مختلفًا كثيرًا
عن بعض طرق التعليم المعزز التي سنبحثها لاحقًا في هذا الكتاب</span>.

<span dir="rtl">طريقة تطورية تُطبق على هذه المشكلة ستبحث مباشرةً في مساحة
السياسات</span> (**policy**) <span dir="rtl">الممكنة للعثور على واحدة
ذات احتمال عالٍ للفوز ضد الخصم. هنا، السياسة</span> (**policy**)
<span dir="rtl">هي قاعدة تخبر اللاعب بأي حركة يجب اتخاذها في كل حالة من
حالات اللعبة—كل تكوين ممكن لـ</span> **X** <span dir="rtl">و</span>**O**
<span dir="rtl">على اللوحة المكونة من ثلاث مربعات بثلاثة مربعات. بالنسبة
لكل سياسة</span> (**policy**) <span dir="rtl">يتم النظر فيها، سيتم
الحصول على تقدير لاحتمالية فوزها من خلال لعب عدد معين من الألعاب ضد
الخصم. ستوجه هذه التقييمات السياسات</span> (**policy**)
<span dir="rtl">التي ستُفحص بعد ذلك</span>.

<span dir="rtl">طريقة تطورية نموذجية قد تستخدم تسلق التل</span>
(**hill**-**climbing**) <span dir="rtl">في مساحة السياسات</span>
(**policy**)<span dir="rtl">، حيث يتم توليد وتقييم السياسات</span>
(**policy**) <span dir="rtl">بشكل متعاقب في محاولة للحصول على تحسينات
تدريجية. أو، ربما يمكن استخدام خوارزمية على غرار الأساليب الجينية</span>
(**genetic**-**style** **algorithm**) <span dir="rtl">التي تحافظ على
مجموعة من السياسات</span> (**policy**) <span dir="rtl">وتقيّمها. حرفياً،
يمكن تطبيق مئات من طرق التحسين المختلفة</span>.

<span dir="rtl">إليك كيف سيتم التعامل مع مشكلة "تيك-تاك-تو" باستخدام
طريقة تعتمد على **دالة القيمة** (</span>value
<span dir="rtl"></span>function<span dir="rtl">).</span>
<span dir="rtl">أولاً، نقوم بإعداد جدول من الأرقام، واحد لكل حالة ممكنة
من حالات اللعبة. كل رقم سيكون أحدث تقدير لاحتمالية فوزنا من تلك الحالة.
نحن نعامل هذا التقدير كقيمة للحالة، والجدول الكامل هو **دالة القيمة**
المكتسبة. الحالة</span> $`A`$ <span dir="rtl">لها قيمة أعلى من
الحالة</span> $`B`$<span dir="rtl">، أو تعتبر "أفضل" من الحالة</span>
$`B`$<span dir="rtl">، إذا كان التقدير الحالي لاحتمالية فوزنا من</span>
$`A`$ <span dir="rtl">أعلى من ذلك من</span>
$`B`$<span dir="rtl">.</span>

<span dir="rtl">افتراضياً، إذا كنا دائماً نلعب بـ</span>
X<span dir="rtl">، فإن لجميع الحالات التي تحتوي على ثلاثة</span> X
<span dir="rtl">في صف واحد، احتمالية الفوز هي 1، لأننا قد فزنا بالفعل.
بالمثل، لجميع الحالات التي تحتوي على ثلاثة</span> O <span dir="rtl">في
صف واحد، أو التي تكون ممتلئة، الاحتمالية الصحيحة هي 0، حيث لا يمكننا
الفوز منها. نحن نحدد القيم الأولية لجميع الحالات الأخرى بـ 0.5، والتي
تمثل تقديراً لدينا فرصة بنسبة 50% للفوز</span>.

<span dir="rtl">نلعب بعد ذلك العديد من الألعاب ضد الخصم. لاختيار
حركاتنا، نقوم بفحص الحالات التي قد تترتب على كل حركة من حركاتنا الممكنة
(واحدة لكل مساحة فارغة على اللوحة) ونتحقق من قيمتها الحالية في الجدول.
في معظم الأوقات، نتحرك بشكل جشع، نختار الحركة التي تؤدي إلى الحالة ذات
أكبر قيمة، أي التي تحتوي على أعلى احتمالية مقدرة للفوز. ومع ذلك، نختار
أحيانًا عشوائيًا من بين الحركات الأخرى. تُسمى هذه الحركات بـ</span>
"**exploratory** **moves**" <span dir="rtl">(حركات استكشافية) لأنها
تجعلنا نختبر حالات قد لا نراها خلاف ذلك. يمكن رسم تسلسل الحركات التي تم
إجراؤها واعتبارها خلال اللعبة كما **في**</span> **<span dir="rtl">الشكل
1.1</span>**

<span dir="rtl">بينما نلعب، نقوم بتغيير قيم الحالات التي نجد أنفسنا فيها
خلال اللعبة. نحاول جعلها تقديرات أكثر دقة لاحتمالات الفوز. للقيام بذلك،
نُعيد **تحديث**</span> (**back** **up**) <span dir="rtl">قيمة الحالة بعد
كل **حركة** **جشعة**</span> (**greedy**) <span dir="rtl">إلى الحالة التي
كانت قبل الحركة، كما هو مقترح في الأسهم في (الشكل 1.1). بشكل أكثر دقة،
يتم تحديث القيمة الحالية للحالة السابقة لتكون أقرب إلى قيمة الحالة
اللاحقة. يمكن القيام بذلك عن طريق تحريك قيمة الحالة السابقة جزءًا من
الطريق نحو قيمة الحالة اللاحقة. إذا دعونا نستخدم</span> $`S_{t}`$
<span dir="rtl">لتمثيل الحالة قبل **الحركة** **الجشعة**
</span>**(greedy**)<span dir="rtl">،
و</span>$`\mathbf{\ s}_{\mathbf{t + 1}}`$<span dir="rtl">لحالة بعد
الحركة، فإن التحديث لتقدير قيمة</span> $`S_{t\ }`$<span dir="rtl">،
والذي يُرمز له بـ</span>
$`\mathbf{V}\left( \mathbf{S}_{\mathbf{t}} \right)`$<span dir="rtl">،
يمكن كتابته كالتالي</span>:

``` math
V\left( S_{t} \right) \leftarrow V\left( S_{t} \right) + \alpha\left\lbrack V\left( S_{t} + 1 \right) - V\left( S_{t} \right) \right\rbrack
```

<img src="./media/image2.png"
style="width:6.49861in;height:3.93958in" />

<span dir="rtl">**الشكل** **1.1**: تسلسل لحركات لعبة "**تيك-تاك-تو**
</span>**(tic-tac-toe)**<span dir="rtl">.</span> <span dir="rtl">تمثل
الخطوط السوداء الصلبة الحركات التي تم اتخاذها خلال اللعبة؛ بينما تمثل
الخطوط المتقطعة الحركات التي قمنا (لاعب التعليم المعزز لدينا) بالنظر
فيها ولكننا لم نقم بها. كانت حركتنا الثانية **حركة**
**استكشافية**</span> (**exploratory**)<span dir="rtl">، مما يعني أنها
اتخذت على الرغم من أن حركة شقيقة أخرى، التي تقود إلى</span>
$`e*`$<span dir="rtl">، كانت مصنفة أعلى. الحركات الاستكشافية لا تؤدي إلى
أي تعلم، ولكن كل واحدة من حركاتنا الأخرى تؤدي إلى ذلك، مما يتسبب في
تحديثات كما هو موضح بالأسهم الحمراء حيث يتم رفع القيم المقدرة في الشجرة
من العقد اللاحقة إلى العقد السابقة كما هو مفصل في النص</span>.

<span dir="rtl">حيث</span> α <span dir="rtl">هو كسر إيجابي صغير يُسمى
"معامل حجم الخطوة</span> (**step**-**size**
**parameter**)<span dir="rtl">، والذي يؤثر على معدل التعليم. هذه القاعدة
للتحديث هي مثال على طريقة تعلم "الفرق الزمني  
</span> <span dir="rtl">(</span>temporal-difference
learning<span dir="rtl">)، سُميت بذلك لأن التغييرات فيها تعتمد على
الفرق،</span> $`V(S_{t} + 1) - V(S_{t})`$ <span dir="rtl">بين التقديرات
في زمنين متتاليين</span>.

<span dir="rtl">الطريقة الموضحة أعلاه تؤدي أداءً جيدًا جدًا في هذه المهمة.
على سبيل المثال، إذا تم تقليل معامل حجم الخطوة</span> (**step**-**size**
**parameter**) <span dir="rtl">بشكل صحيح على مر الزمن، فإن هذه الطريقة
تتقارب، لأي خصم ثابت، إلى الاحتمالات الحقيقية للفوز من كل حالة عند اللعب
الأمثل من جانب لاعبنا. علاوة على ذلك، فإن الحركات التي تُتخذ</span>
<span dir="rtl">(باستثناء الحركات الاستكشافية</span> (**exploratory**
**moves**)<span dir="rtl">)</span> <span dir="rtl">هي في الواقع الحركات
المثلى ضد هذا الخصم (غير المثالي). بعبارة أخرى، الطريقة تتقارب إلى سياسة
مثلى للعب اللعبة ضد هذا الخصم. إذا لم يُخفض معامل حجم الخطوة إلى الصفر
تمامًا على مر الزمن، فإن هذا اللاعب أيضًا يلعب بشكل جيد ضد الخصوم الذين
يغيرون تدريجياً طريقة لعبهم</span>

<span dir="rtl">هذا المثال يوضح الفروقات بين **الطرق التطورية**</span>
(**evolutionary** **methods**) <span dir="rtl">وطرق **تعلم دوال
القيمة**</span> (**value** **function**
**methods**)<span dir="rtl">.</span> <span dir="rtl">لتقييم سياسة معينة،
تقوم الطريقة التطورية بتثبيت السياسة ولعب العديد من الألعاب ضد الخصم، أو
بمحاكاة العديد من الألعاب باستخدام نموذج للخصم. تعطي تكرارية الانتصارات
تقديرًا غير متحيز لاحتمالية الفوز بتلك السياسة، ويمكن استخدامها لتوجيه
اختيار السياسة التالية. ولكن، يتم تعديل كل سياسة فقط بعد العديد من
الألعاب، وتستخدم فقط النتيجة النهائية لكل لعبة: ما يحدث أثناء الألعاب
يُهمل. على سبيل المثال، إذا فاز اللاعب، فإن جميع تصرفاته في اللعبة تُمنح
الائتمان، بغض النظر عن كيفية كون التحركات المحددة قد تكون حاسمة للفوز.
حتى التحركات التي لم تحدث تُمنح الائتمان! بالمقابل، تتيح طرق **دوال
القيمة** (</span>**value** **function**
<span dir="rtl"></span>**methods**<span dir="rtl">)</span>
<span dir="rtl">تقييم الحالات الفردية. في النهاية، تبحث كل من الطرق
التطورية وطرق دوال القيمة في مساحة السياسات، ولكن تعلم **دالة
القيمة**</span> (**value** **function**) <span dir="rtl">يستفيد من
المعلومات المتاحة أثناء سير اللعب</span>.

<span dir="rtl">يوضح هذا المثال البسيط بعض الميزات الأساسية لطرق
**التعليم المعزز** (</span>**reinforcement**
<span dir="rtl"></span>**learning**<span dir="rtl">).</span>
<span dir="rtl">أولاً، هناك التركيز على التعليم أثناء التفاعل مع البيئة،
في هذه الحالة مع لاعب خصم. ثانيًا، هناك هدف واضح، ويتطلب السلوك الصحيح
التخطيط أو التنبؤ الذي يأخذ في الاعتبار تأثيرات تأخير اختيارات الفرد.
على سبيل المثال، سيتعلم اللاعب البسيط للتعليم المعزز وضع فخاخ متعددة
الحركات للخصم قصير النظر. من السمات اللافتة في حل التعليم المعزز أنه
يمكنه تحقيق تأثيرات التخطيط والنظر إلى الأمام دون استخدام نموذج للخصم
وبدون إجراء بحث صريح على التسلسلات الممكنة من الحالات والإجراءات
المستقبلية</span>.

<span dir="rtl">بينما يوضح هذا المثال بعض الميزات الأساسية للتعليم
المعزز، فهو بسيط لدرجة قد توحي بأن التعليم المعزز أكثر محدودية مما هو
عليه في الواقع. على الرغم من أن لعبة إكس-أو هي لعبة ثنائية الأشخاص، فإن
التعليم المعزز ينطبق أيضًا في حالة عدم وجود خصم خارجي، أي في حالة "اللعب
ضد الطبيعة". كما أن التعليم المعزز ليس مقتصرًا على المشكلات التي تنكسر
فيها السلوكيات إلى حلقات منفصلة، مثل الألعاب المنفصلة في إكس-أو، حيث يتم
الحصول على المكافأة فقط في نهاية كل حلقة. فهو ينطبق أيضًا عندما يستمر
السلوك إلى أجل غير مسمى وعندما يمكن تلقي مكافآت بمقادير مختلفة في أي
وقت. كما ينطبق التعليم المعزز على المشكلات التي لا تنكسر حتى إلى خطوات
زمنية منفصلة مثل الحركات في إكس-أو. المبادئ العامة تنطبق أيضًا على
المشكلات ذات الزمن المستمر، على الرغم من أن النظرية تصبح أكثر تعقيدًا
ونقوم بتجاوزها في هذا العرض التقديمي التمهيدي</span>.

<span dir="rtl">لعبة</span> **Tic-tac-toe** <span dir="rtl">لها مجموعة
حالات صغيرة ومحدودة نسبيًا، في حين أن **التعليم** **المعزز** يمكن
استخدامه عندما تكون مجموعة الحالات كبيرة جدًا، أو حتى لانهائية. على سبيل
المثال، جمع جيري تسوراو (1992، 1995) بين الخوارزمية الموضحة أعلاه وشبكة
عصبية صناعية لتعلم لعب لعبة</span> **backgammon**<span dir="rtl">، التي
تحتوي على حوالي</span> $`10^{20}`$ <span dir="rtl">حالة. مع هذا العدد
الكبير من الحالات، من المستحيل تجربة أكثر من جزء صغير منها. تعلم برنامج
**تسوراو** أن يلعب بشكل أفضل بكثير من أي برنامج سابق وأصبح في النهاية
أفضل من أفضل اللاعبين البشريين في العالم (انظر القسم 16.1). توفر الشبكة
العصبية الصناعية للبرنامج القدرة على التعميم من تجربته، بحيث يختار في
الحالات الجديدة حركات بناءً على المعلومات المحفوظة من حالات مماثلة واجهها
في الماضي، كما تحددها شبكته. مدى فعالية نظام التعليم المعزز في المشكلات
ذات مجموعات الحالات الكبيرة يرتبط ارتباطًا وثيقًا بكيفية تعميمه بشكل مناسب
من التجارب السابقة. وفي هذا الدور لدينا أكبر حاجة لطرق التعليم الخاضع
للإشراف مع **التعليم** **المعزز**. الشبكات العصبية الصناعية والتعليم
العميق (انظر القسم 9.6) ليست الطريقة الوحيدة، أو بالضرورة الأفضل، للقيام
بذلك</span>.

<span dir="rtl">في هذا المثال للعبة</span>
**Tic-tac-toe**<span dir="rtl">، بدأ التعليم بدون معرفة سابقة بخلاف
قواعد اللعبة، لكن التعليم المعزز لا يتطلب بأي حال من الأحوال رؤية "لوح
فارغ" للتعليم والذكاء. على العكس من ذلك، يمكن دمج المعلومات السابقة في
التعليم المعزز بطرق متنوعة قد تكون حاسمة للتعليم الفعّال (انظر، على سبيل
المثال، الأقسام 9.5، 17.4، و13.1). لدينا أيضًا الوصول إلى الحالة الحقيقية
في مثال  
</span>tac-toe" <span dir="rtl"></span>Tic-<span dir="rtl">"، بينما يمكن
تطبيق التعليم المعزز أيضًا عندما يكون جزء من الحالة مخفيًا، أو عندما تبدو
حالات مختلفة للمتعلمين متشابهة</span>.

<span dir="rtl">أخيرًا، كان بإمكان لاعب</span>
**Tic-tac-toe**<span dir="rtl">النظر إلى الأمام ومعرفة الحالات التي
ستنتج عن كل حركة من حركاته المحتملة. لتحقيق ذلك، كان يتعين عليه أن يمتلك
نموذجًا للعبة يتيح له التنبؤ بكيفية تغيير بيئته استجابةً للحركات التي قد
لا يقوم بها أبدًا. العديد من المشكلات تشبه هذا، لكن في حالات أخرى قد
يفتقر حتى النموذج قصير الأجل لتأثيرات الأفعال. يمكن تطبيق التعليم المعزز
في كلا الحالتين. النموذج ليس مطلوبًا، ولكن يمكن استخدام النماذج بسهولة
إذا كانت متاحة أو يمكن تعلمها (**الفصل** 8)</span>

<span dir="rtl">من ناحية أخرى، هناك طرق تعلم معزز لا تحتاج إلى أي نوع من
نماذج البيئة على الإطلاق. الأنظمة غير المعتمدة على النماذج لا يمكنها حتى
التفكير في كيفية تغيير بيئاتها استجابةً لإجراء واحد فقط. لاعب</span>
**Tic-tac-toe** <span dir="rtl">غير معتمد على النموذج بهذا المعنى فيما
يتعلق بخصمه: ليس لديه أي نموذج لخصمه. نظرًا لأن النماذج يجب أن تكون دقيقة
بما يكفي لتكون مفيدة، فإن الطرق غير المعتمدة على النماذج يمكن أن تكون
لها مزايا على الطرق الأكثر تعقيدًا عندما يكون العائق الحقيقي في حل
المشكلة هو صعوبة بناء نموذج بيئة دقيق بما فيه الكفاية. كما أن الطرق غير
المعتمدة على النماذج تعتبر عناصر أساسية للطرق المعتمدة على النماذج. في
هذا الكتاب، نخصص عدة فصول للطرق غير المعتمدة على النماذج قبل أن نناقش
كيف يمكن استخدامها كعناصر في طرق معتمدة على النماذج الأكثر
تعقيدًا</span>.

<span dir="rtl">يمكن استخدام التعليم المعزز على مستويات عالية ومنخفضة في
النظام. على الرغم من أن لاعب</span> <span dir="rtl">  
</span>**Tic-tac-toe** <span dir="rtl">تعلم فقط حول الحركات الأساسية
للعبة، إلا أنه لا شيء يمنع التعليم المعزز من العمل على مستويات أعلى حيث
قد يكون كل من "الإجراءات" نفسها تطبيقًا لطريقة حل مشكلة قد تكون معقدة. في
نظم التعليم الهرمية، يمكن أن يعمل التعليم المعزز في الوقت نفسه على عدة
مستويات</span>.

**Exercise 1.1: Self-Play**

<span dir="rtl">افترض أنه بدلاً من اللعب ضد خصم عشوائي، كان خوارزمية
التعليم المعزز الموضحة أعلاه تلعب ضد نفسها، مع تعلم كلا الجانبين. ماذا
تعتقد أنه سيحدث في هذه الحالة؟ هل ستتعلم سياسة مختلفة لاختيار
الحركات؟</span>

**Exercise 1.2: Symmetries**

<span dir="rtl">العديد من أوضاع</span> "Tic-tac-toe"
<span dir="rtl">تبدو مختلفة ولكنها في الحقيقة متشابهة بسبب التناظرات.
كيف يمكننا تعديل عملية التعليم الموصوفة أعلاه للاستفادة من ذلك؟ ما الطرق
التي يمكن أن يحسن بها ذلك عملية التعليم؟ الآن فكر مرة أخرى. افترض أن
الخصم لم يستفد من التناظرات. في هذه الحالة، هل ينبغي علينا الاستفادة
منها؟ هل صحيح إذًا أن الأوضاع المتناظرة يجب أن تكون لها نفس القيمة
بالضرورة؟</span>

**Exercise 1.3: Greedy Play
<span dir="rtl"></span>**<span dir="rtl">افترض أن لاعب التعليم المعزز
كان جشعاً، أي أنه دائماً ما لعب الحركة التي تجعله يصل إلى الوضع الذي يصنّفه
على أنه الأفضل. هل يمكن أن يتعلم أن يلعب بشكل أفضل، أم أسوأ، من لاعب غير
جشع؟ ما المشاكل التي قد تحدث؟</span>

**Exercise 1.4: Learning from Exploration**

<span dir="rtl">افترض أن التحديثات التعليمية حدثت بعد جميع الحركات، بما
في ذلك الحركات الاستكشافية. إذا تم تقليل معامل حجم الخطوة بشكل مناسب
بمرور الوقت (ولكن دون تغيير ميل الاستكشاف)، فإن قيم الحالات ستتقارب إلى
مجموعة مختلفة من الاحتمالات. ما هي (مفهومياً) مجموعتا الاحتمالات اللتان
يتم حسابهما عندما نتعلم، وعندما لا نتعلم من الحركات الاستكشافية؟ على
افتراض أننا نواصل إجراء الحركات الاستكشافية، أي مجموعة من الاحتمالات قد
تكون أفضل للتعليم؟ أي مجموعة ستؤدي إلى المزيد من الانتصارات؟</span>

**Exercise 1.5: Other Improvements**

<span dir="rtl">هل يمكنك التفكير في طرق أخرى لتحسين لاعب التعليم المعزز؟
هل يمكنك التفكير في أي طريقة أفضل لحل مشكلة</span> **Tic-tac-toe
<span dir="rtl"></span>**<span dir="rtl">كما تم طرحها؟</span>

**<u>1.6 Summary</u>**

<span dir="rtl">التعليم المعزز هو نهج حسابي لفهم وأتمتة التعليم الموجه
نحو الأهداف واتخاذ القرارات. يتميز عن الأساليب الحسابية الأخرى بتركيزه
على تعلم الوكيل من خلال التفاعل المباشر مع بيئته، دون الحاجة إلى إشراف
مثالي أو نماذج كاملة للبيئة. في رأينا، يعتبر التعليم المعزز المجال الأول
الذي يتناول بجدية القضايا الحسابية التي تنشأ عند التعليم من التفاعل مع
بيئة لتحقيق الأهداف طويلة الأجل</span>.

<span dir="rtl">يستخدم التعليم المعزز الإطار الرسمي لعمليات اتخاذ القرار
ماركوف</span> (MDPs) <span dir="rtl">لتعريف التفاعل بين الوكيل المتعلم
وبيئته من حيث الحالات، والإجراءات، والمكافآت. يهدف هذا الإطار إلى أن
يكون وسيلة بسيطة لتمثيل الخصائص الأساسية لمشكلة الذكاء الاصطناعي. تشمل
هذه الخصائص إحساسًا بالسببية والنتائج، وإحساسًا بعدم اليقين وعدم الحتمية،
ووجود أهداف صريحة</span>.

<span dir="rtl">مفاهيم القيمة ووظيفة القيمة هي مفتاح لمعظم طرق التعليم
المعزز التي نناقشها في هذا الكتاب. نحن نرى أن وظائف القيمة مهمة للبحث
الفعّال في مساحة السياسات. استخدام وظائف القيمة يميز طرق التعليم المعزز
عن الطرق التطورية التي تبحث مباشرة في مساحة السياسات مسترشدًا بتقييمات
السياسات الكاملة</span>.

**<u>1.7 <span dir="rtl">تاريخ مبكر لتعلم التعزيز</span></u>**

<span dir="rtl">التاريخ المبكر لتعلم التعزيز يحتوي على خيطين رئيسيين،
كلاهما طويل وغني، تم متابعتهما بشكل مستقل قبل أن يتشابكا في تعلم التعزيز
الحديث. يتعلق أحد هذين الخيطين بالتعليم من خلال التجربة والخطأ، ويعود
أصله إلى علم نفس تعلم الحيوانات. يمتد هذا الخيط عبر بعض من أوائل أعمال
الذكاء الاصطناعي وقاد إلى إحياء تعلم التعزيز في أوائل الثمانينيات. الخيط
الثاني يتعلق بمشكلة التحكم الأمثل وحلها باستخدام دوال القيمة والبرمجة
الديناميكية. في الغالب، لم يشمل هذا الخيط التعليم. كان الخيطان في معظم
الأحيان مستقلين، لكنهما تداخلا إلى حد ما حول خيط ثالث أقل وضوحًا يتعلق
بأساليب الفرق الزمني مثل تلك المستخدمة في مثال</span> "Tic-tac-toe"
<span dir="rtl">في هذا الفصل. اجتمعت الخيوط الثلاثة في أواخر الثمانينيات
لتكوين مجال تعلم التعزيز الحديث كما نقدمه في هذا الكتاب</span>.

<span dir="rtl">الخيط الذي يركز على التعليم من خلال التجربة والخطأ هو
الخيط الذي نحن الأكثر دراية به والذي لدينا أكثر ما نقوله عنه في هذه
التاريخية المختصرة. قبل أن نفعل ذلك، نناقش بإيجاز خيط التحكم
الأمثل</span>.

<span dir="rtl">مصطلح "التحكم الأمثل</span> ("**optimal** **control**")
<span dir="rtl">بدأ استخدامه في أواخر الخمسينيات من القرن العشرين لوصف
مشكلة تصميم وحدة تحكم تهدف إلى تقليل أو زيادة مقياس سلوك نظام ديناميكي
على مدى الزمن. تم تطوير إحدى الطرق لمعالجة هذه المشكلة في منتصف
الخمسينيات من قبل ريتشارد بيلمان وآخرين، وذلك من خلال توسيع نظرية تعود
إلى القرن التاسع عشر، والتي اقترحها هاملتون وجاكوبي. تعتمد هذه الطريقة
على مفهومي حالة النظام الديناميكي ودالة القيمة، أو "دالة العائد الأمثل
(</span>**optimal** <span dir="rtl"></span>**return**
**function**"<span dir="rtl">)، لتعريف معادلة وظيفية تُعرف اليوم بمعادلة
بيلمان</span> (**Bellman** **equation**)<span dir="rtl">. أصبحت مجموعة
الأساليب المستخدمة لحل مشاكل التحكم الأمثل عن طريق حل هذه المعادلة تعرف
بالبرمجة الديناميكية</span> (**dynamic** **programming**)
<span dir="rtl">(بيلمان، 1957</span>a<span dir="rtl">).</span>
<span dir="rtl">كما قدم بيلمان</span> (1957b) <span dir="rtl">النسخة
العشوائية المنفصلة من مشكلة التحكم الأمثل، المعروفة بعمليات اتخاذ القرار
ماركوف</span> ("**Markov** **decision** **processes**"
**MDPs**)<span dir="rtl">. قام رونالد هوارد (1960) بتطوير طريقة تكرار
السياسات لـ</span> **MDPs**<span dir="rtl">.</span> <span dir="rtl">جميع
هذه العناصر تُعتبر أساسيات في النظرية والخوارزميات الخاصة بالتعليم المعزز
الحديث</span>.

<span dir="rtl">البرمجة الديناميكية تُعتبر على نطاق واسع الطريقة الوحيدة
القابلة للتطبيق لحل مشكلات التحكم الأمثل العشوائية العامة. إنها تعاني
مما أطلق عليه بيلمان "لعنة الأبعاد"، مما يعني أن متطلباتها الحاسوبية
تنمو بشكل أسي مع عدد متغيرات الحالة، لكنها ما زالت أكثر كفاءة وأوسع
تطبيقًا من أي طريقة عامة أخرى. تم تطوير البرمجة الديناميكية بشكل مكثف منذ
أواخر الخمسينيات، بما في ذلك التوسعات إلى</span> **MDPs**
<span dir="rtl">القابلة للملاحظة جزئيًا</span>
<span dir="rtl">(استعرضها</span> Lovejoy<span dir="rtl">، 1991)، والعديد
من التطبيقات</span> <span dir="rtl">(استعرضها</span>
White<span dir="rtl">، 1985، 1988، 1993)، وطرق التقريب</span>
<span dir="rtl">(استعرضها</span> Rust<span dir="rtl">، 1996)، والطرق غير
المتزامنة</span> <span dir="rtl">(</span>Bertsekas<span dir="rtl">،
1982، 1983).</span> <span dir="rtl">تتوفر العديد من المعالجات الحديثة
الممتازة للبرمجة الديناميكية</span> <span dir="rtl">(مثل</span>
Bertsekas<span dir="rtl">، 2005، 2012؛</span> Puterman<span dir="rtl">،
1994؛</span> Ross<span dir="rtl">، 1983؛
و</span>Whittle<span dir="rtl">، 1982، 1983)</span>.
<span dir="rtl"></span>Bryson <span dir="rtl"></span>(1996)
<span dir="rtl">يقدم تاريخًا موثوقًا للتحكم الأمثل</span>.

<span dir="rtl">الاتصالات بين التحكم الأمثل والبرمجة الديناميكية، من
جهة، والتعليم، من جهة أخرى، كانت بطيئة في التعرف عليها. لا يمكننا أن
نكون متأكدين مما أدى إلى هذا الفصل، لكن السبب الرئيسي كان من المحتمل أن
يكون الفصل بين التخصصات المعنية وأهدافها المختلفة. قد يكون ساهم أيضًا في
ذلك الرأي السائد عن البرمجة الديناميكية كحساب غير متزامن يعتمد أساسًا على
نماذج النظام الدقيقة والحلول التحليلية لمعادلة بيلمان. علاوة على ذلك،
فإن أبسط شكل من البرمجة الديناميكية هو حساب يتقدم عكسيًا في الزمن، مما
يجعل من الصعب رؤية كيفية مشاركته في عملية تعلم يجب أن تتم في اتجاه
الأمام. بعض من أوائل الأعمال في البرمجة الديناميكية، مثل تلك التي قام
بها بيلمان ودريفوس (1959)، قد تُصنف الآن على أنها تتبع نهجًا تعلميًا. عمل
ويتن (1977) (المناقش أدناه) يتأهل بالتأكيد كدمج بين أفكار التعليم
والبرمجة الديناميكية. ووربوس (1987) جادل صراحةً بضرورة تعزيز الترابط بين
البرمجة الديناميكية وطرق التعليم وبأهمية البرمجة الديناميكية لفهم
الآليات العصبية والمعرفية. بالنسبة لنا، لم يحدث التكامل الكامل بين طرق
البرمجة الديناميكية والتعليم عبر الإنترنت حتى عمل كريس واتكنز في عام
1989، الذي كان تعامله مع التعليم المعزز باستخدام صيغة</span> MDP
<span dir="rtl">قد تم اعتماده على نطاق واسع. منذ ذلك الحين، تم تطوير هذه
العلاقات بشكل مكثف من قبل العديد من الباحثين، وعلى وجه الخصوص من قبل
ديميتري بيرتسيكاس وجون تسيتسكلس (1996)، الذين صكوا مصطلح "البرمجة
العصبية الديناميكية" للإشارة إلى دمج البرمجة الديناميكية والشبكات
العصبية الاصطناعية. مصطلح آخر حاليًا هو "البرمجة الديناميكية التقريبية".
تركز هذه الأساليب المختلفة على جوانب مختلفة من الموضوع، لكنها جميعًا
تشترك مع التعليم المعزز في الاهتمام بتجاوز نقاط ضعف البرمجة الديناميكية
الكلاسيكية</span>.

<span dir="rtl">نعتبر جميع الأعمال المتعلقة بالتحكم الأمثل أيضًا، من
ناحية ما، أعمالًا في مجال التعليم المعزز. نحن نعرّف طريقة التعليم المعزز
على أنها أي طريقة فعّالة لحل مشاكل التعليم المعزز، ومن الواضح الآن أن هذه
المشاكل ترتبط ارتباطًا وثيقًا بمشاكل التحكم الأمثل، خاصة مشاكل التحكم
الأمثل العشوائي مثل تلك التي تُصاغ على شكل عمليات اتخاذ القرارات
ماركوفية</span> (**MDPs**)<span dir="rtl">.</span> <span dir="rtl">وبناءً
على ذلك، يجب أن نعتبر طرق الحل في التحكم الأمثل، مثل البرمجة
الديناميكية، أيضًا كطرق للتعليم المعزز. نظرًا لأن معظم الأساليب التقليدية
تتطلب معرفة كاملة بالنظام الذي يتم التحكم فيه، فإن من غير المألوف قليلًا
القول بأنها جزء من التعليم المعزز. من ناحية أخرى، فإن العديد من
خوارزميات البرمجة الديناميكية تكون تزايدية وتكرارية. مثل طرق التعليم،
تصل تدريجيًا إلى الإجابة الصحيحة من خلال تقريبيات متعاقبة. كما نعرض في
بقية هذا الكتاب، فإن هذه التشابهات هي أكثر من مجرد سطحية. النظريات وطرق
الحل للحالات التي تتطلب معرفة كاملة وأخرى غير كاملة مترابطة لدرجة تجعلنا
نعتقد أنه يجب اعتبارها معًا كجزء من نفس الموضوع</span>.

<span dir="rtl">لنعد الآن إلى الخيط الرئيسي الآخر الذي أدى إلى المجال
الحديث للتعليم المعزز، وهو الخيط الذي يركز على فكرة التعليم من خلال
التجربة والخطأ. نعرض هنا النقاط الرئيسية للتواصل بين الأفكار، مع التطرق
إلى هذا الموضوع بمزيد من التفصيل في القسم 14.3. وفقًا لعلم النفس
الأمريكي</span> R. S. Woodworth (1938)<span dir="rtl">، فإن فكرة التعليم
من خلال التجربة والخطأ تعود إلى خمسينيات القرن التاسع عشر، حيث ناقش
ألكسندر بين التعليم من خلال "التحسس والتجربة"، واستخدم عالم الأحياء
السلوكي البريطاني وعالم النفس كونواي لويد مورغان في عام 1894 المصطلح
بشكل أكثر تحديدًا لوصف ملاحظاته لسلوك الحيوانات. ربما كان أول من عبر بشكل
مختصر عن جوهر التعليم من خلال التجربة والخطأ كمبدأ للتعليم هو إدوارد
ثورندايك</span>:

<span dir="rtl">من بين الاستجابات العديدة التي تتم في نفس الحالة، فإن
الاستجابات التي ترافقها أو تتبعها مباشرةً إحساس بالرضا لدى الحيوان،
ستكون، إذا كانت الأمور الأخرى متساوية، مرتبطة بشكل أقوى بتلك الحالة،
بحيث عندما تتكرر، سيكون من المرجح أن تتكرر هي أيضًا. أما الاستجابات التي
ترافقها أو تتبعها مباشرةً إحساس بعدم الراحة لدى الحيوان، فستضعف ارتباطها
بتلك الحالة، بحيث عندما تتكرر، سيكون من غير المرجح أن تحدث. كلما زادت
درجة الرضا أو عدم الراحة، زادت قوة أو ضعف الرابط. (ثورندايك، 1911، ص.
244)  
سمى ثورنديك هذا بـ 'قانون الأثر</span>' (Law of Effect)
<span dir="rtl">لأنه يصف تأثير الأحداث التعزيزية على ميل اختيار الأفعال.
قام ثورنديك لاحقًا بتعديل القانون ليتناسب بشكل أفضل مع البيانات اللاحقة
حول تعلم الحيوانات (مثل الفروق بين تأثيرات المكافأة والعقاب)، وقد أثار
القانون بصوره المختلفة الكثير من الجدل بين علماء التعليم (على سبيل
المثال، انظر غاليستل، 2005؛ هيرنشتاين، 1970؛ كيمبل، 1961، 1967؛ مازور،
1994). على الرغم من ذلك، يُعتبر قانون الأثر - بشكل أو بآخر - مبدأ
أساسيًا</span> underpinning <span dir="rtl">للكثير من السلوك **(على سبيل
المثال، هيلغارد وباور، 1975؛ دينيت، 1978؛ كامبل، 1960؛ كزيكو، 1995).**
وهو أساس النظريات التعليمية المؤثرة لكلارك هول</span> **(Clark Hull)**
<span dir="rtl">(</span>1943<span dir="rtl">، 1952)</span>
<span dir="rtl">وطرق التجريب المؤثرة لب. ف. سكينر</span> (B. F. Skinner)
(1938)<span dir="rtl">.</span>

<span dir="rtl">ظهر مصطلح 'التعزيز</span>' (reinforcement)
<span dir="rtl">في سياق تعلم الحيوانات بعد فترة طويلة من التعبير عن
قانون الأثر</span> (Law of Effect) <span dir="rtl">بواسطة ثورنديك، وظهر
لأول مرة في هذا السياق (على حد علمنا) في الترجمة الإنجليزية لعام 1927
لرسالة بافلوف عن الانعكاسات الشرطية. وصف بافلوف التعزيز بأنه تقوية نمط
من السلوك بسبب تلقي الحيوان لمثير - وهو معزز</span> (reinforcer) -
<span dir="rtl">في علاقة زمنية مناسبة مع مثير آخر أو مع استجابة. قام بعض
علماء النفس بتوسيع فكرة التعزيز لتشمل التضعيف بالإضافة إلى التقوية
للسلوك، ووسّعوا فكرة المعزز لتشمل ربما إغفال أو إنهاء المثير. ليُعتبر شيئًا
معززًا، يجب أن تستمر التقوية أو التضعيف بعد سحب المعزز؛ فالمثير الذي يجذب
انتباه الحيوان فقط أو الذي ينشط سلوكه دون أن يحدث تغييرات دائمة لن يُعتبر
معززًا</span>.

<span dir="rtl">ظهرت فكرة تنفيذ تعلم التجربة والخطأ على الكمبيوتر بين
الأفكار الأولى حول إمكانية الذكاء الاصطناعي. في تقرير عام 1948، وصف آلان
تورينج تصميمًا لـ 'نظام المتعة والألم</span>'
<span dir="rtl">(</span>pleasure-pain system<span dir="rtl">)</span>
<span dir="rtl">الذي يعمل وفقًا لمبادئ قانون الأثر</span> (Law of
Effect)<span dir="rtl">:</span>

<span dir="rtl">عندما يتم الوصول إلى تكوين يكون فيه الإجراء غير محدد،
يتم اختيار عشوائي للبيانات المفقودة ويتم إدخالها بشكل مؤقت في الوصف،
ويتم تطبيقها. عندما يحدث تحفيز للألم</span> (pain
stimulus)<span dir="rtl">، يتم إلغاء جميع الإدخالات المؤقتة، وعندما يحدث
تحفيز للمتعة</span> (pleasure stimulus)<span dir="rtl">، يتم تثبيتها
جميعًا</span>.

<span dir="rtl">تم بناء العديد من الآلات الكهربائية الميكانيكية الذكية
التي أظهرت التعليم من خلال التجربة والخطأ. قد تكون أقدمها آلة بناها
توماس روس (1933) التي كانت قادرة على العثور على طريقها عبر متاهة بسيطة
وتذكر المسار من خلال إعدادات المفاتيح. في عام 1951، بنى و. غراي والتر
نسخة من 'السلحفاة الميكانيكية</span>'
<span dir="rtl">(</span>Walter<span dir="rtl">، 1950)</span>
<span dir="rtl">القادرة على شكل بسيط من التعليم. في عام 1952، عرض كلود
شانون فأرًا يجري في متاهة يُدعى ثيسيوس</span> (Theseus)
<span dir="rtl">الذي استخدم التجربة والخطأ للعثور على طريقه عبر المتاهة،
مع المتاهة نفسها التي تتذكر الاتجاهات الناجحة عبر المغناطيسات والمفاتيح
تحت أرضيتها</span> <span dir="rtl">(انظر أيضًا</span>
Shannon<span dir="rtl">، 1951).</span> <span dir="rtl">وصف ج. أ. دويتش
(1954) آلة لحل المتاهة بناءً على نظريته السلوكية</span>
(Deutsch<span dir="rtl">، 1953</span>) <span dir="rtl">التي تحتوي على
بعض الخصائص المشتركة مع التعليم المعزز القائم على النماذج (الفصل 8). في
أطروحته لدرجة الدكتوراه، ناقش مارفن مينسكي (1954) نماذج حسابية للتعليم
المعزز وشرح بناءه لآلة تناظرية تتكون من مكونات أطلق عليها اسم</span>
SNARCs <span dir="rtl">(حسابات التعزيز العصبية العشوائية) تهدف إلى تشابه
الاتصالات المشبكية القابلة للتعديل في الدماغ (الفصل 15). يحتوي موقع
الويب</span> cyberneticzoo.com <span dir="rtl">على الكثير من المعلومات
حول هذه الآلات التعليمية الكهربائية الميكانيكية والعديد غيرها</span>.

<span dir="rtl">أدى بناء الآلات التعليمية الكهربائية الميكانيكية إلى
برمجة الحواسيب الرقمية لأداء أنواع مختلفة من التعليم، بعض منها طبق
التعليم من خلال التجربة والخطأ. وصف فارلي وكلارك (1954) محاكاة رقمية
لجهاز تعلم عبر الشبكات العصبية الذي تعلم من خلال التجربة والخطأ. ولكن
سرعان ما تحولت اهتماماتهم من التعليم عبر التجربة والخطأ إلى التعميم
والتعرف على الأنماط، أي من التعليم المعزز إلى التعليم الخاضع للإشراف
(كلارك وفارلي، 1955). بدأ هذا نمطًا من الالتباس حول العلاقة بين هذين
النوعين من التعليم. بدا أن العديد من الباحثين كانوا يعتقدون أنهم يدرسون
التعليم المعزز بينما كانوا يدرسون في الواقع التعليم الخاضع للإشراف. على
سبيل المثال، كان رواد الشبكات العصبية الاصطناعية مثل روزنبلات (1962)
وودرو وهوف (1960) مدفوعين بوضوح بالتعليم المعزز—استخدموا لغة المكافآت
والعقوبات—لكن الأنظمة التي درسها كانت أنظمة تعلم خاضعة للإشراف مناسبة
للتعرف على الأنماط والتعليم الإدراكي. حتى اليوم، يقلل بعض الباحثين
والكتب الدراسية من أهمية أو يخلطون بين هذين النوعين من التعليم. على سبيل
المثال، استخدمت بعض كتب الشبكات العصبية الاصطناعية مصطلح 'التجربة
والخطأ' لوصف الشبكات التي تتعلم من أمثلة التدريب. وهذا لبس مفهوم لأنه
بالرغم من أن هذه الشبكات تستخدم معلومات الخطأ لتحديث أوزان الاتصال، إلا
أن ذلك يتجاهل الطبيعة الأساسية للتعليم عبر التجربة والخطأ كاختيار
الإجراءات بناءً على تقييمات التغذية الراجعة التي لا تعتمد على معرفة ما
يجب أن تكون عليه الإجراءات الصحيحة</span>.

<span dir="rtl">نتيجة جزئية لهذه الالتباسات، أصبح البحث في التعليم
الحقيقي من خلال التجربة والخطأ نادرًا في الستينيات والسبعينيات، على الرغم
من وجود استثناءات ملحوظة. في الستينيات، استخدمت مصطلحات 'التعزيز'
و'التعليم المعزز' لأول مرة في الأدبيات الهندسية لوصف الاستخدامات
الهندسية للتعليم عبر التجربة والخطأ (على سبيل المثال، والتز وفو، 1965؛
مندل، 1966؛ فو، 1970؛ مندل وماكلارين، 1970). وكان من المؤثرين بشكل خاص
هو ورقة م</span>insky <span dir="rtl">خطوات نحو الذكاء الاصطناعي</span>
(Minsky, 1961)<span dir="rtl">، التي ناقشت عدة مسائل ذات صلة بالتعليم
عبر التجربة والخطأ، بما في ذلك التنبؤ، والتوقع، وما سماه مشكلة توزيع
الائتمان الأساسية للأنظمة المعززة المعقدة: كيف توزع الائتمان للنجاح بين
العديد من القرارات التي قد تكون قد ساهمت في تحقيقه؟ جميع الأساليب التي
نناقشها في هذا الكتاب موجهة، من ناحية ما، نحو حل هذه المشكلة. ورقة
مينيكي تستحق القراءة اليوم</span>

<span dir="rtl">في الفقرات التالية، نناقش بعض الاستثناءات الأخرى
والجزئية للإهمال النسبي للدراسة الحاسوبية والنظرية للتعليم الحقيقي من
خلال التجربة والخطأ في الستينيات والسبعينيات</span>.

<span dir="rtl">أحد الاستثناءات كان عمل الباحث النيوزيلندي جون أندريا،
الذي طور نظامًا يُسمى</span> STeLLA <span dir="rtl">تعلم من خلال التجربة
والخطأ في تفاعله مع بيئته. تضمن هذا النظام نموذجًا داخليًا للعالم، ولاحقًا،
"مونولوج داخلي" للتعامل مع مشاكل الحالة المخفية (أندريا، 1963، 1969أ،
ب). عمل أندريا اللاحق (1977) وضع المزيد من التركيز على التعليم من معلم،
ولكنه لا يزال شمل التعليم من خلال التجربة والخطأ، مع توليد أحداث جديدة
كأحد أهداف النظام. من ميزات هذا العمل كانت عملية "التسرب"، التي تم
تفصيلها بشكل أكبر في أندريا (1998)، والتي نفذت آلية توزيع الائتمان
مشابهة لعمليات تحديث التراجع التي نصفها. للأسف، لم يكن بحثه الرائد
معروفًا بشكل جيد ولم يكن له تأثير كبير على أبحاث التعليم المعزز اللاحقة.
ملخصات حديثة متاحة (أندريا، 2017أ، ب)</span>.

<span dir="rtl">كان عمل دونالد ميشي أكثر تأثيرًا. في عامي 1961 و1963 وصف
نظامًا بسيطًا للتعليم من خلال التجربة والخطأ لتعلم كيفية لعب إكس أو (أو
نوتس وكروسس) يُسمى</span> MENACE <span dir="rtl">(محرك نوتس وكروسس القابل
للتعليم في علبة الكبريت). يتكون من علبة كبريت لكل وضعية لعبة ممكنة،
تحتوي كل علبة على عدد من الخرز الملون، كل لون يمثل حركة محتملة من تلك
الوضعية. من خلال سحب خرز عشوائي من علبة الكبريت التي تتوافق مع الوضعية
الحالية في اللعبة، يمكن تحديد حركة</span> MENACE. <span dir="rtl">عند
انتهاء اللعبة، يتم إضافة أو إزالة الخرز من العلب المستخدمة خلال اللعب
لتقديم مكافآت أو عقوبات لقرارات</span> MENACE. <span dir="rtl">ميشي
وشامبرز (1968) وصفوا متعلمًا آخر لإكس أو يُسمى</span> GLEE
<span dir="rtl">(محرك تعلم الألعاب بتوقعات) ونظام تحكم للتعليم المعزز
يُسمى</span> BOXES<span dir="rtl">.</span> <span dir="rtl">طبقوا</span>
BOXES <span dir="rtl">على مهمة تعلم موازنة عمود مثبت على عربة متحركة
بناءً على إشارة فشل تحدث فقط عندما يسقط العمود أو تصل العربة إلى نهاية
المسار. تم تعديل هذه المهمة من العمل السابق لودرو وسميث (1964)، الذين
استخدموا طرق التعليم تحت الإشراف، بافتراض التعليم من معلم قادر بالفعل
على موازنة العمود. نسخة ميشي وشامبرز من موازنة العمود هي واحدة من أفضل
الأمثلة المبكرة لمهام التعليم المعزز في ظروف المعرفة غير الكاملة. أثرت
على الكثير من الأبحاث اللاحقة في التعليم المعزز، بدءًا من بعض دراساتنا
الخاصة (بارتو، ساتون، وأندرسون، 1983؛ ساتون، 1984). كان ميشي يبرز دائمًا
دور التجربة والخطأ والتعليم كأبعاد أساسية للذكاء الاصطناعي (ميشي،
1974)</span>.<span dir="rtl">  
قام ويدرو، غوبتا، ومايترا (1973) بتعديل خوارزمية أقل متوسط مربع</span>
(LMS) <span dir="rtl">لويدرو وهوف (1960) لإنتاج قاعدة تعلم تعزيزية
يمكنها التعليم من إشارات النجاح والفشل بدلاً من أمثلة التدريب. أطلقوا على
هذا الشكل من التعليم اسم "التكيف الانتقائي بالتمهيد" ووصفوه بأنه
"التعليم مع ناقد" بدلاً من "التعليم مع معلم". قاموا بتحليل هذه القاعدة
وأظهروا كيف يمكنها تعلم لعب البلاك جاك. كانت هذه محاولة معزولة في
التعليم المعزز من ويدرو، الذي كانت إسهاماته في التعليم تحت الإشراف أكثر
تأثيرًا. استخدامنا لمصطلح "الناقد" مستمد من ورقة ويدرو، غوبتا، ومايترا.
استخدم بوكانان، ميتشل، سميث، وجونسون (1978) بشكل مستقل مصطلح الناقد في
سياق تعلم الآلة (انظر أيضًا ديترتش وبوكانان، 1984)، ولكن بالنسبة لهم،
الناقد هو نظام خبير قادر على القيام بأكثر من تقييم
الأداء</span>.<span dir="rtl">  
</span>

<span dir="rtl">كان للبحث في آلات التعليم تأثير مباشر على الخيط المتعلق
بالتعليم عن طريق التجربة والخطأ الذي أدى إلى أبحاث التعليم المعزز
الحديثة. هذه هي الأساليب المستخدمة لحل مشكلة تعلم غير ترابطي، تُعرف
بمشكلة "الماكينة ذات الذراعين</span>" (k-armed bandit)
<span dir="rtl">بالتمثيل إلى ماكينة القمار، أو "الماكينة ذات الذراع
الواحد"، ولكن مع</span> k <span dir="rtl">ذراعًا (انظر الفصل 2). آلات
التعليم هي آلات بسيطة وقليلة الذاكرة لتحسين احتمالية المكافأة في هذه
المشاكل. بدأت آلات التعليم من العمل الذي قام به الرياضي والفيزيائي
الروسي م. ل. تسيتلين وزملاؤه في الستينيات (نُشر بعد وفاته في تسيتلين،
1973) وتم تطويرها بشكل مكثف منذ ذلك الحين ضمن مجال الهندسة (انظر
نارياندرا وثاتاتشار، 1974، 1989). شملت هذه التطورات دراسة آلات التعليم
العشوائية، وهي طرق لتحديث احتمالات الإجراءات بناءً على إشارات المكافأة.
على الرغم من أنها لم تُطور ضمن تقليد آلات التعليم العشوائية، فإن خوارزمية
ألوبيكس</span> (Alopex) <span dir="rtl">لهارث وتزاناكوس (1974) (التي
تعني خوارزمية استخراج الأنماط) هي طريقة عشوائية لاكتشاف العلاقات بين
الإجراءات والتعزيز التي أثرت على بعض أبحاثنا المبكرة (بارتو، ساتون،
وبروير، 1981). تم التنبؤ بآلات التعليم العشوائية من خلال أعمال سابقة في
علم النفس، بدءًا من جهود ويليام إستيس (1950) نحو نظرية إحصائية للتعليم
وتطويرها بواسطة آخرين (مثل بوش وموسيلر، 1955؛ ستيرنبرغ، 1963)</span>

<span dir="rtl">تُبنت نظريات التعليم الإحصائي التي طُورت في علم النفس من
قبل الباحثين في الاقتصاد، مما أدى إلى خيط من البحث في هذا المجال مكرس
للتعليم المعزز. بدأت هذه الأعمال في عام 1973 مع تطبيق نظرية التعليم لبوش
وموسيلر على مجموعة من النماذج الاقتصادية التقليدية (كروس، 1973). كان هدف
واحد من هذا البحث هو دراسة الوكلاء الاصطناعيين الذين يتصرفون بشكل يشبه
الناس الحقيقيين أكثر من الوكلاء الاقتصاديين المثاليين التقليديين (آرثر،
1991). توسعت هذه المقاربة إلى دراسة التعليم المعزز في سياق نظرية
الألعاب. تطور التعليم المعزز في الاقتصاد بشكل مستقل إلى حد كبير عن
الأعمال المبكرة في التعليم المعزز في الذكاء الاصطناعي، على الرغم من أن
نظرية الألعاب لا تزال موضوع اهتمام في كلا المجالين (خارج نطاق هذا
الكتاب). يناقش كاميرر (2011) تقليد التعليم المعزز في الاقتصاد، ويقدم
نوي، فرانك، ودي هووير (2012) نظرة عامة على الموضوع من وجهة نظر التمديدات
متعددة الوكلاء للمقاربة التي نقدمها في هذا الكتاب. التعليم المعزز في
سياق نظرية الألعاب هو موضوع مختلف تمامًا عن التعليم المعزز المستخدم في
البرامج للعب إكس-أو، الداما، وألعاب ترفيهية أخرى. انظر، على سبيل المثال،
سزيتا (2012) للحصول على نظرة عامة على هذا الجانب من التعليم المعزز
والألعاب</span>.

<span dir="rtl">حدد جون هولاند (1975) نظرية عامة للأنظمة التكيفية تعتمد
على المبادئ الانتقائية. كانت أعماله المبكرة تتعلق بالتجربة والخطأ بشكل
رئيسي في شكلها غير الترابطي، كما في الطرق التطورية ومشكلة الكازينوهات
ذات الذراع المتعدد. في عام 1976 وبشكل أكثر تفصيلاً في عام 1986، قدم أنظمة
التصنيف، وهي أنظمة تعلم معزز حقيقية تشمل الوظائف الترابطية والقيمية. كان
أحد المكونات الرئيسية لأنظمة التصنيف التي قدمها هولاند هو 'خوارزمية سحب
الدلو' لتخصيص الائتمان، والتي ترتبط ارتباطًا وثيقًا بخوارزمية الفروق
الزمنية المستخدمة في مثال إكس-أو الخاص بنا والمناقشة في الفصل 6. كان
مكونًا رئيسيًا آخر هو الخوارزمية الجينية، وهي طريقة تطورية كان دورها هو
تطوير تمثيلات مفيدة. لقد تم تطوير أنظمة التصنيف بشكل مكثف من قبل العديد
من الباحثين لتشكل فرعًا رئيسيًا من بحث التعليم المعزز</span>
<span dir="rtl">(راجع</span> Urbanowicz
<span dir="rtl">و</span>Moore<span dir="rtl">، 2009)، لكن الخوارزميات
الجينية—التي لا نعتبرها أنظمة تعلم معزز بحد ذاتها—تلقت اهتمامًا أكبر، كما
هو الحال مع أساليب الحوسبة التطورية الأخرى</span>
<span dir="rtl">(مثل</span> Fogel <span dir="rtl">و</span>Owens
<span dir="rtl">و</span>Walsh<span dir="rtl">، 1966،
و</span>Koza<span dir="rtl">، 1992).</span>

<span dir="rtl">الشخص الأكثر مسؤولية عن إحياء خيط التجربة والخطأ في
التعليم المعزز ضمن الذكاء الاصطناعي كان هاري كلوف (1972، 1975، 1982).
أدرك كلوف أن الجوانب الأساسية للسلوك التكيفي كانت تُفقد بينما بدأ
الباحثون في التعليم يركزون تقريبًا حصريًا على التعليم الخاضع للإشراف. ما
كان مفقودًا، وفقًا لكلوف، هو الجوانب الهيدونية للسلوك، الدافع لتحقيق بعض
النتائج من البيئة، للتحكم في البيئة نحو الأهداف المرغوبة والابتعاد عن
الأهداف غير المرغوبة (انظر القسم 15.9). هذه هي الفكرة الأساسية لتعلم
التجربة والخطأ. كانت أفكار كلوف مؤثرة بشكل خاص على المؤلفين لأن تقييمنا
لها</span> <span dir="rtl">(</span>Barto
<span dir="rtl">و</span>Sutton<span dir="rtl">،
1981</span>a<span dir="rtl">)</span> <span dir="rtl">قادنا إلى تقدير
الفارق بين التعليم الخاضع للإشراف والتعليم المعزز، وإلى تركيزنا في
النهاية على التعليم المعزز. كان الكثير من العمل المبكر الذي أنجزناه نحن
والزملاء موجهًا نحو إظهار أن التعليم المعزز والتعليم الخاضع للإشراف كانا
بالفعل مختلفين</span> <span dir="rtl">(</span>Barto
<span dir="rtl">و</span>Sutton<span dir="rtl">، 1981؛</span> Barto
<span dir="rtl">و</span>Sutton<span dir="rtl">،
1981</span>b<span dir="rtl">؛</span> Barto
<span dir="rtl">و</span>Anandan<span dir="rtl">، 1985).</span>
<span dir="rtl">أظهرت دراسات أخرى كيف يمكن للتعليم المعزز أن يعالج
مشكلات مهمة في تعلم الشبكات العصبية الاصطناعية، وبشكل خاص، كيف يمكن أن
ينتج خوارزميات تعلم للشبكات متعددة الطبقات</span> (Barto
<span dir="rtl">و</span>Anderson
<span dir="rtl">و</span>Sutton<span dir="rtl">، 1982؛</span> Barto
<span dir="rtl">و</span>Anderson<span dir="rtl">، 1985؛</span>
Barto<span dir="rtl">، 1985، 1986؛</span> Barto
<span dir="rtl">و</span>Jordan<span dir="rtl">، 1987؛ انظر القسم
15.10</span>.

<span dir="rtl">ننتقل الآن إلى الخيط الثالث في تاريخ التعليم المعزز، وهو
المتعلق بتعلم الفرق الزمني</span>
<span dir="rtl">(</span>temporal-difference
learning<span dir="rtl">).</span> <span dir="rtl">تتميز طرق تعلم الفرق
الزمني بأنها مدفوعة بالفرق بين التقديرات المتعاقبة زمنيًا لنفس الكمية—على
سبيل المثال، احتمال الفوز في مثال لعبة الإكس-أو. هذا الخيط أصغر وأقل
تميزًا من الخيطين الآخرين، لكنه لعب دورًا مهمًا بشكل خاص في المجال، جزئيًا
لأن طرق الفرق الزمني تبدو جديدة وفريدة من نوعها في التعليم
المعزز</span>.

<span dir="rtl">تعود أصول تعلم الفرق الزمني جزئيًا إلى علم نفس تعلم
الحيوانات، وبشكل خاص إلى مفهوم المعززات الثانوية</span> (secondary
reinforcers)<span dir="rtl">.</span> <span dir="rtl">المعزز الثانوي هو
منبه تم اقترانه مع معزز أولي مثل الطعام أو الألم، ونتيجة لذلك، اكتسب
خصائص تعزيزية مشابهة. قد يكون مينسكي (1954) هو أول من أدرك أن هذه
القاعدة النفسية قد تكون مهمة لأنظمة التعليم الاصطناعية. كان آرثر صامويل
(1959) هو الأول الذي اقترح وطبق طريقة تعلم شملت أفكار الفرق الزمني، كجزء
من برنامجه المشهور للعب الداما (انظر القسم 16.2)</span>.

<span dir="rtl">لم يشير صامويل إلى عمل مينسكي أو إلى الروابط المحتملة
بتعلم الحيوانات. يبدو أن إلهامه جاء من اقتراح كلود شانون (1950) بأن يمكن
برمجة الحاسوب لاستخدام دالة تقييم للعب الشطرنج، وأنه قد يكون قادرًا على
تحسين أدائه من خلال تعديل هذه الدالة عبر الإنترنت. (من الممكن أن تكون
هذه الأفكار من شانون قد أثرت أيضًا على بيلمان، ولكن لا نعرف أي دليل على
ذلك). ناقش مينسكي (1961) عمل صامويل بشكل مكثف في ورقته 'خطوات'، مقترحًا
الصلة بنظريات التعزيز الثانوي، سواء الطبيعية أو الاصطناعية</span>.

<span dir="rtl">كما ناقشنا، في العقد الذي تلى عمل مينسكي وصامويل، لم يتم
إجراء أي عمل حسابي كبير على التعليم عبر التجربة والخطأ، ويبدو أنه لم يُجرَ
أي عمل حسابي على الإطلاق على التعليم بالتفاضل الزمني. في عام 1972، جمع
كلوب بين التعليم عبر التجربة والخطأ وأحد المكونات المهمة للتعليم
بالتفاضل الزمني. كان كلوب مهتمًا بالمبادئ التي يمكن أن تتوسع لتشمل
التعليم في الأنظمة الكبيرة، وكان لذلك مفتونًا بمفاهيم التعزيز المحلي، حيث
يمكن أن تعزز الأجزاء الفرعية لنظام التعليم الشامل بعضها البعض. طور فكرة
'التعزيز العام'، حيث يرى كل مكون (اسميًا، كل خلية عصبية) جميع مدخلاته من
حيث التعزيز: المدخلات المثيرة كمكافآت والمدخلات المثبطة كعقوبات. هذه
الفكرة ليست نفسها ما نعرفه الآن على أنه التعليم بالتفاضل الزمني،
وفي</span> retrospect<span dir="rtl">، هي أبعد منها من عمل صامويل. من
ناحية أخرى، ربط كلوب الفكرة بالتعليم عبر التجربة والخطأ وربطها بقاعدة
البيانات التجريبية الضخمة في علم نفس التعليم الحيواني</span>.

<span dir="rtl">طور سوتون</span>
<span dir="rtl">(</span>1978a<span dir="rtl">،
1978</span>b<span dir="rtl">، 1978</span>c<span dir="rtl">)</span>
<span dir="rtl">أفكار كلوب بشكل أكبر، وخاصة الروابط مع نظريات التعليم
الحيواني، موضحًا قواعد التعليم التي يقودها التغيرات في التنبؤات المتعاقبة
زمنيًا. قام هو وبارتو بصقل هذه الأفكار وتطوير نموذج نفسي للتكييف
الكلاسيكي يعتمد على التعليم بالتفاضل الزمني (سوتون وبارتو،
1981</span>a<span dir="rtl">؛ بارتو وسوتون، 1982).</span>
<span dir="rtl">تلت ذلك عدة نماذج نفسية مؤثرة أخرى للتكييف الكلاسيكي
تعتمد على التعليم بالتفاضل الزمني (على سبيل المثال، كلوب، 1988؛ مور
وآخرون، 1986؛ سوتون وبارتو، 1987، 1990). بعض نماذج الأعصاب التي تم
تطويرها في هذا الوقت تفسر بشكل جيد من حيث التعليم بالتفاضل الزمني
(هوكينز وكاندل، 1984؛ بيرن وجينغريش وباكستر، 1990؛ جيلبرين وهوبفيلد
وتانك، 1985؛ تيساورو، 1986؛ فريستون وآخرون، 1994)، على الرغم من أنه في
معظم الحالات لم يكن هناك اتصال تاريخي</span>.

<span dir="rtl">كان عملنا المبكر على التعليم بالتفاضل الزمني متأثرًا بشدة
بنظريات التعليم الحيواني وأعمال كلوب. وتم التعرف على الروابط مع ورقة
مينسكي 'الخطوات' ولاعبي الداما لسامويل فقط في وقت لاحق. ومع ذلك، بحلول
عام 1981، كنا على دراية كاملة بكل الأعمال السابقة المذكورة أعلاه كجزء من
خيوط التعليم بالتفاضل الزمني وتجربة الأخطاء. في هذا الوقت، طورنا طريقة
لاستخدام التعليم بالتفاضل الزمني مع التعليم بالتجربة والخطأ، والمعروفة
بهندسة الممثل-الناقد، وطبقنا هذه الطريقة على مشكلة توازن القضيب الخاصة
بمشي وس</span> chambers <span dir="rtl">(بارتو وسوتون وأندرسون، 1983).
تم دراسة هذه الطريقة بشكل مكثف في أطروحة سوتون (1984) وامتدت لاستخدام
الشبكات العصبية ذات التغذية العكسية في أطروحة أندرسون (1986). في نفس
الوقت تقريبًا، أدخل هولاند (1986) أفكار التفاضل الزمني بشكل صريح في
أنظمته المصنفة على شكل خوارزمية الدلو. تم اتخاذ خطوة رئيسية بواسطة سوتون
(1988) بفصل التعليم بالتفاضل الزمني عن التحكم، معالجًا إياه كطريقة تنبؤ
عامة. قدمت تلك الورقة أيضًا خوارزمية</span> TD(") <span dir="rtl">وأثبتت
بعض خصائص تقاربها</span>.

<span dir="rtl">بينما كنا في مراحل إنهاء عملنا على بنية الممثل-الناقد في
عام 1981، اكتشفنا ورقة بحثية كتبها إيان ويتين</span>
<span dir="rtl">(</span>1977<span dir="rtl">،
1976</span>a<span dir="rtl">) والتي يبدو أنها أول منشور يتناول قاعدة
تعلم الفرق الزمني. اقترح ويتين الطريقة التي نطلق عليها الآن</span> TD(0)
<span dir="rtl">الجدولية لاستخدامها كجزء من وحدة تحكم تكيفية لحل مسائل
البرمجة الديناميكية</span> (MDPs). <span dir="rtl">تم تقديم هذا العمل
لأول مرة للنشر في مجلة علمية في عام 1974 وظهر أيضًا في رسالة الدكتوراه
الخاصة بويتين عام 1976. كان عمل ويتين من نسل تجارب أندريا المبكرة
مع</span> STeLLA <span dir="rtl">وأنظمة التعليم بالتجربة والخطأ الأخرى.
لذلك، فقد غطت ورقة ويتين لعام 1977 كلا من الخيطين الرئيسيين في أبحاث
التعليم التعزيزي—التعليم بالتجربة والخطأ والتحكم الأمثل—مع تقديم مساهمة
مبكرة مميزة في تعلم الفرق الزمني</span>.

<span dir="rtl">تم جمع خيوط تعلم الفرق الزمني والتحكم الأمثل بشكل كامل
في عام 1989 مع تطوير كريس واتكنز لـ</span>
Q-learning<span dir="rtl">.</span> <span dir="rtl">وسعت هذه الأعمال
ودمجت الأعمال السابقة في جميع الخيوط الثلاثة لأبحاث التعليم التعزيزي.
ساهم بول ويربوس (1987) في هذا الدمج من خلال التأكيد على تلاقي التعليم
بالتجربة والخطأ والبرمجة الديناميكية منذ عام 1977. بحلول وقت عمل واتكنز،
كان هناك نمو هائل في أبحاث التعليم التعزيزي، لا سيما في مجال التعليم
الآلي في الذكاء الاصطناعي، ولكن أيضًا في الشبكات العصبية الاصطناعية
والذكاء الاصطناعي بشكل عام. في عام 1992، جلب النجاح الرائع لبرنامج غاري
تسوراو للعب الطاولة،</span> TD-Gammon<span dir="rtl">، مزيدًا من الانتباه
إلى هذا المجال</span>.

<span dir="rtl">منذ نشر الطبعة الأولى من هذا الكتاب، تطور مجال فرعي
مزدهر من علم الأعصاب يركز على العلاقة بين خوارزميات التعليم التعزيزي
والتعليم التعزيزي في الجهاز العصبي. المسؤولون عن ذلك بشكل كبير هو
التشابه الغريب بين سلوك خوارزميات الفرق الزمني ونشاط خلايا الدوبامين
المنتجة في الدماغ، كما أشار عدد من الباحثين</span>
<span dir="rtl">(</span>Friston et al., 1994; Barto, 1995a; Houk, Adams,
and Barto, <span dir="rtl"></span>1995; Montague, Dayan, and Sejnowski,
1996; and Schultz, Dayan, and Montague, 1997<span dir="rtl">).</span>
<span dir="rtl">الفصل 15 يقدم مقدمة لهذا الجانب المثير من التعليم
التعزيزي. هناك مساهمات هامة أخرى تم تحقيقها في تاريخ التعليم التعزيزي
الحديث وهي كثيرة جدًا لذكرها في هذا الحساب المختصر؛ نشير إلى العديد من
هذه المساهمات في نهاية الفصول الفردية التي تظهر فيها</span>.

**<span dir="rtl">ملاحظات ببليوغرافية</span>**

<span dir="rtl">للحصول على تغطية عامة إضافية لتعلم التعزيز، نوصي القارئ
بالكتب التالية</span>: <span dir="rtl"></span>Szepesvári
(2010)<span dir="rtl">، و</span>Bertsekas
<span dir="rtl">و</span>Tsitsiklis (1996)<span dir="rtl">،
و</span>Kaelbling (1993a)<span dir="rtl">، و</span>Sugiyama
<span dir="rtl">و</span>Hachiya <span dir="rtl">و</span>Morimura
(2013)<span dir="rtl">.</span> <span dir="rtl">تشمل الكتب التي تتناول
منظور التحكم أو أبحاث العمليات تلك التي كتبها</span> Si
<span dir="rtl">و</span>Barto <span dir="rtl">و</span>Powell
<span dir="rtl">و</span>Wunsch
<span dir="rtl"></span>(2004)<span dir="rtl">، و</span>Powell
<span dir="rtl"></span>(2011)<span dir="rtl">، و</span>Lewis
<span dir="rtl">و</span>Liu
<span dir="rtl"></span>(2012)<span dir="rtl">، و</span>Bertsekas
<span dir="rtl"></span>(2012)<span dir="rtl">.</span>
<span dir="rtl">يضع استعراض</span> Cao (2009) <span dir="rtl">تعلم
التعزيز في سياق الأساليب الأخرى للتعليم وتحسين الأنظمة الديناميكية
العشوائية. تركز ثلاثة أعداد خاصة من مجلة</span> Machine Learning
<span dir="rtl">على تعلم التعزيز:</span> Sutton
<span dir="rtl"></span>(1992a)<span dir="rtl">، و</span>Kaelbling
<span dir="rtl"></span>(1996)<span dir="rtl">، و</span>Singh
<span dir="rtl"></span>(2002)<span dir="rtl">.</span>
<span dir="rtl">تقدم المراجعات المفيدة</span> Barto
<span dir="rtl"></span>(1995b)<span dir="rtl">؛ و</span>Kaelbling
<span dir="rtl">و</span>Littman <span dir="rtl">و</span>Moore
<span dir="rtl"></span>(1996)<span dir="rtl">؛ و</span>Keerthi
<span dir="rtl">و</span>Ravindran
<span dir="rtl"></span>(1997)<span dir="rtl">.</span>
<span dir="rtl">يوفر المجلد الذي حرره</span> Weiring
<span dir="rtl">و</span>van Otterlo <span dir="rtl"></span>(2012)
<span dir="rtl">نظرة عامة ممتازة على التطورات الحديثة</span>.

1.2 <span dir="rtl">كان المثال عن إفطار فيل في هذا الفصل مستوحى
من</span> Agre (1988)<span dir="rtl">.</span>

1.5 <span dir="rtl">الطريقة المستخدمة في مثال</span> tic-tac-toe
<span dir="rtl">تعتمد على التعليم بالتفاضل الزمني وتطويرها في الفصل
6</span>.

<span dir="rtl">الجزء الأول: طرق الحل الجدولية</span>

<span dir="rtl">في هذا الجزء من الكتاب، نعرض تقريبًا جميع الأفكار
الأساسية لخوارزميات التعليم المعزز بأبسط أشكالها: تلك التي تكون فيها
**فضاءات** **الحالة** والإجراء صغيرة بما يكفي لتُمثل **دوال** **القيمة**
**التقريبية** على شكل جداول أو **مصفوفات**</span>
<span dir="rtl">(</span>**Tables**<span dir="rtl">).</span>
<span dir="rtl">في هذه الحالة، **يمكن** **للأساليب** غالبًا أن تجد حلولًا
دقيقة، أي أنها تستطيع تحديد **دالة** **القيمة** **المثلى** **والسياسة**
**المثلى** بدقة</span> <span dir="rtl">(</span>**Exact**
**Solutions**<span dir="rtl">).</span> <span dir="rtl">وهذا يختلف عن
**الأساليب** **التقريبية** التي سيتم تناولها في **الجزء** **التالي**
**من** **الكتاب**، والتي توفر حلولًا تقريبية، لكنها يمكن أن تُطبق بفعالية
على مشاكل أكبر بكثير</span> (Approximate
Methods)<span dir="rtl">.</span>

<span dir="rtl">الفصل الأول من هذا الجزء يصف **طرق الحل للحالة الخاصة**
من مشكلة التعليم المعزز التي تحتوي على حالة واحدة فقط، والمعروفة
**بمشاكل** **البنديت**</span> (**Bandit**
**Problems**)<span dir="rtl">.</span> <span dir="rtl">الفصل الثاني يصف
**الصياغة** **العامة** **للمشكلة** التي نعالجها **طوال** **بقية**
**الكتاب**—وهي **عمليات اتخاذ القرار ماركوف المحدودة**</span>
<span dir="rtl">(</span>**Finite** <span dir="rtl"></span>**Markov**
**Decision** **Processes**<span dir="rtl">)</span>
—<span dir="rtl">وأفكارها الرئيسية، بما في ذلك **معادلات**
**بيلمان**</span> <span dir="rtl">(</span>**Bellman**
<span dir="rtl"></span>**Equations**<span dir="rtl">)</span>
<span dir="rtl">**ودوال** **القيمة**</span>
<span dir="rtl">(</span>**Value** **Functions**<span dir="rtl">).</span>

<span dir="rtl">الفصول الثلاثة التالية تعرض ثلاث فئات أساسية من الأساليب
لحل **مشاكل اتخاذ القرار ماركوف المحدودة**: **البرمجة
الديناميكية**</span> (Dynamic Programming)<span dir="rtl">، **وطرق مونت
كارلو**</span> (Monte Carlo Methods)<span dir="rtl">، **وتعلم الفرق
الزمني**</span> (Temporal-Difference Learning)<span dir="rtl">.</span>
<span dir="rtl">كل فئة من الأساليب لها ميزاتها وعيوبها. **الأساليب
البرمجية الديناميكية** متطورة رياضيًا، ولكنها **تتطلب نموذجًا كاملاً ودقيقًا
للبيئة**</span> <span dir="rtl">(</span>**Complete**
<span dir="rtl"></span>**and Accurate Model<span dir="rtl">).</span>
<span dir="rtl">طرق مونت كارلو</span>** <span dir="rtl">لا تحتاج إلى
نموذج وهي بسيطة من الناحية المفاهيمية، لكنها **ليست ملائمة للحسابات
التزايدية خطوة بخطوة**</span> <span dir="rtl">(</span>**Incremental
Computation**<span dir="rtl">).</span> <span dir="rtl">أخيرًا، **أساليب**
**الفرق الزمني** لا تحتاج إلى نموذج وتكون تزايدية بالكامل، **لكنها أكثر
تعقيدًا من ناحية التحليل**</span> **(Complexity in
Analysis)**<span dir="rtl">.</span> <span dir="rtl">كما تختلف الأساليب
أيضًا بطرق متعددة من حيث كفاءتها وسرعة تقاربها</span> (**Efficiency and
Speed of Convergence**)<span dir="rtl">.</span>

<span dir="rtl">الفصلين المتبقيان يصفان كيفية **دمج هذه الفئات الثلاث**
من الأساليب للحصول على أفضل مزايا كل منها. في أحد الفصول، نوضح كيفية دمج
**نقاط القوة في طرق مونت كارلو** مع **نقاط القوة في طرق الفرق**
**الزمني** عبر استخدام **التمهيد متعدد الخطوات**</span> (Multi-Step
Bootstrapping Methods)<span dir="rtl">.</span> <span dir="rtl">في الفصل
النهائي من هذا الجزء، نوضح كيف يمكن دمج طرق **تعلم الفرق الزمني** مع
**طرق تعلم النماذج** **والتخطيط** (**مثل البرمجة الديناميكية**) للحصول
على حل شامل وموحد لمشكلة التعليم المعزز الجدولية (</span>Unified
Solution to the Tabular Reinforcement Learning
Problem<span dir="rtl">)</span>.

<span dir="rtl">الفصل الثاني:</span>

Multi-armed Bandits

**<span dir="rtl">الماكينات ذات الأذرع المتعددة</span>**

<span dir="rtl">أكثر **ميزات** **التعليم المعزز** تميزًا عن أنواع التعليم
الأخرى هي أنه **يستخدم معلومات التدريب** التي **تقيم** **الأفعال
المتخذة** بدلاً من **توجيهها** عبر **تقديم الأفعال الصحيحة**. هذا ما يخلق
الحاجة للاستكشاف النشط، أي **البحث الصريح** عن **سلوك جيد**. تشير
**التغذية الراجعة التقييمية** </span>(**Evaluative** **Feedback**)
<span dir="rtl">البحتة إلى مدى جودة الفعل المتخذ، ولكنها **لا تحدد** ما
إذا كان **الفعل** هو **الأفضل أو الأسوأ** الممكن. من ناحية أخرى، **تشير
التغذية الراجعة** **التوجيهية**</span>
<span dir="rtl">(</span>**Instructive** **Feedback**<span dir="rtl">)
إلى الفعل الصحيح **الذي يجب اتخاذه**، بغض النظر عن الفعل المتخذ بالفعل.
هذا النوع من التغذية الراجعة هو **أساس التعليم الخاضع للإشراف**، الذي
يشمل أجزاء كبيرة من تصنيف الأنماط، والشبكات العصبية الاصطناعية، وتحديد
الأنظمة. في أشكالها النقية، تكون هاتان النوعيتان من التغذية الراجعة
مميزتين تمامًا: **التغذية الراجعة التقييمية**</span> (**Evaluative**
**Feedback**) <span dir="rtl">تعتمد بالكامل على الفعل المتخذ، بينما
**التغذية الراجعة التوجيهية**</span> (**Instructive** **Feedback**)
<span dir="rtl">مستقلة عن الفعل المتخذ</span>.

<span dir="rtl">في هذا الفصل، ندرس الجانب التقييمي للتعليم المعزز في
إطار مبسط، لا يتضمن تعلم التصرف في أكثر من حالة واحدة. هذا الإطار غير
الترابطي هو الإطار الذي تم فيه إنجاز معظم الأعمال السابقة التي تتضمن
**التغذية** **الراجعة** **التقييمية**</span>
<span dir="rtl">(</span>**Evaluative** **Feedback**<span dir="rtl">)،
وهو يتجنب الكثير من تعقيد **مشكلة** **التعليم المعزز الكامل**. دراسة هذه
الحالة تمكننا من رؤية الفرق بين **التغذية الراجعة التقييمية**
(</span>**Evaluative** **Feedback**<span dir="rtl">) **والتغذية الراجعة
التوجيهية** </span>**(Instructive Feedback)** <span dir="rtl">بوضوح،
وكيف يمكن دمجهما</span>.

<span dir="rtl">المشكلة غير الترابطية، التقييمية التي نستكشفها هي نسخة
بسيطة من مشكلة الماكينة ذات الأذرع المتعددة. نستخدم هذه المشكلة لتقديم
عدد من الأساليب الأساسية للتعليم التي نوسعها في الفصول اللاحقة لتطبيقها
على مشكلة التعليم المعزز الكاملة. في نهاية هذا الفصل، نقترب خطوة من
مشكلة التعليم المعزز الكامل من خلال مناقشة ما يحدث عندما تصبح مشكلة
الماكينة ذات الأذرع المتعددة ترابطية، أي عندما تُتخذ الأفعال في أكثر من
حالة واحدة</span>.

**<u>2.1 <span dir="rtl">مشكلة الماكينة ذات الأذرع المتعددة</span>
(k-armed Bandit Problem)</u>**

<span dir="rtl">فكر بعناية في مشكلة التعليم التالية. تواجه باستمرار
خيارًا بين عدة خيارات مختلفة</span> ($`\mathbf{k}`$) <span dir="rtl">أو
إجراءات مختلفة. بعد كل اختيار تتلقى مكافأة رقمية يتم اختيارها من توزيع
احتمالي ثابت يعتمد على الإجراء الذي قمت باختياره</span>.
<span dir="rtl">هدفك هو تعظيم إجمالي المكافأة المتوقعة على مدار فترة
زمنية معينة، على سبيل المثال، على مدار 1000 اختيار للإجراءات أو خطوات
زمنية</span>.

<span dir="rtl">هذه هي الصيغة الأصلية لمشكلة الماكينات ذات الأذرع
المتعددة، والتي سُميت بهذا الاسم بالتشابه مع آلة القمار ذات الذراع
الواحد، باستثناء أنها تحتوي على</span> $`\mathbf{k}`$
<span dir="rtl">أذرع بدلاً من واحدة. كل اختيار للعمل يشبه سحب ذراع من
أذرع آلة القمار، والعوائد هي الأرباح التي يتم الحصول عليها عند ضرب
الجائزة الكبرى. من خلال تكرار اختيار الأذرع، هدفك هو زيادة مكاسبك عن
طريق التركيز على الأذرع الأفضل. تشبيه آخر هو أن الطبيب يختار بين علاجات
تجريبية لمجموعة من المرضى بشكل خطير. كل إجراء هو اختيار علاج، وكل عائد
هو بقاء أو صحة المريض. اليوم، يُستخدم مصطلح "مشكلة الماكينات ذات الأذرع
المتعددة" أحياناً للإشارة إلى تعميم للمشكلة الموصوفة أعلاه، ولكن في هذا
الكتاب نستخدمه للإشارة فقط إلى هذه الحالة البسيطة</span>.

<span dir="rtl">في مشكلة الماكينات ذات الأذرع المتعددة، كل واحد من
الأذرع</span> $`\mathbf{k}`$ <span dir="rtl">له عائد متوقع أو متوسط يُعطى
عند اختيار هذا الذراع؛ دعنا نطلق على هذا العائد قيمة هذا الذراع. نُسمي
الذراع الذي يُختار في الزمن</span> **t
<span dir="rtl"></span>**<span dir="rtl">  
</span> <span dir="rtl">بـ</span> $`\mathbf{A}_{\mathbf{t}}`$
<span dir="rtl"></span>​<span dir="rtl">، والعائد المقابل له بـ</span>
$`\mathbf{R}_{\mathbf{t}}`$
<span dir="rtl"></span>​<span dir="rtl">.</span> <span dir="rtl">إذاً،
قيمة أي ذراع عشوائي</span> $`\mathbf{a}`$<span dir="rtl">، التي نُشير
إليها بـ</span>
$`\mathbf{q*}\left( \mathbf{a} \right)`$<span dir="rtl">، هي العائد
المتوقع عند اختيار</span> $`\mathbf{a}`$<span dir="rtl">.</span>

``` math
q*(a) = \text{E}\left\lbrack \left. \ R_{t} \right|A_{t} = a \right\rbrack
```

<span dir="rtl">إذا كنت تعرف قيمة كل إجراء، فستكون مشكلة الماكينات ذات
الأذرع المتعددة سهلة الحل: ستقوم دائماً باختيار الإجراء الذي له أعلى
قيمة. نحن نفترض أنك لا تعرف قيم الأفعال بيقين، رغم أنك قد تكون لديك
تقديرات. نُسمي تقدير قيمة الإجراء</span> $`\mathbf{a}`$
<span dir="rtl">في الزمن</span> t <span dir="rtl">بـ</span>
$`\mathbf{Q}_{\mathbf{t}}\left( \mathbf{a} \right)`$
<span dir="rtl">نرغب في أن يكون</span>
$`\mathbf{Q}_{\mathbf{t}}\left( \mathbf{a} \right)`$
<span dir="rtl">قريباً من  
</span>$`\mathbf{q*}\left( \mathbf{a} \right)`$**<span dir="rtl">.</span>**

<span dir="rtl">إذا كنت تحتفظ بتقديرات لقيم الأفعال، فإن في أي وقت سيكون
هناك على الأقل إجراء واحد له أعلى قيمة تقديرية. نُطلق على هذه الأفعال اسم
**الأفعال** **الجشعة**</span> <span dir="rtl">(</span>**greedy**
**actions**<span dir="rtl">).</span> <span dir="rtl">عندما تختار واحداً
من هذه الأفعال، نقول إنك **تستغل**</span> (**exploiting**)
<span dir="rtl">معرفتك الحالية بقيم الأفعال. إذا كنت تختار بدلاً من ذلك
واحداً من الأفعال **غير** **الجشعة**</span>
<span dir="rtl">(</span>**nongreedy** **actions**<span dir="rtl">)، فنحن
نقول إنك **تستكشف**</span>
<span dir="rtl">(</span>**exploring**<span dir="rtl">)، لأن هذا يتيح لك
تحسين تقديرك لقيمة الإجراء غير الجشع. **الاستغلال**</span>
(**exploitation**) <span dir="rtl">هو الشيء الصحيح لزيادة العائد المتوقع
في الخطوة الواحدة، ولكن **الاستكشاف**</span> (**exploration**)
<span dir="rtl">قد يؤدي إلى تحقيق عائد إجمالي أكبر على المدى
الطويل</span>.

<span dir="rtl">على سبيل المثال، افترض أن قيمة الإجراء الجشع معروفة
بيقين، بينما يُقدَّر أن عدة أفعال أخرى قريبة من حيث القيمة ولكن مع **عدم
يقين كبير**. **عدم اليقين** هو أن أحد هذه الأفعال الأخرى ربما يكون في
الواقع **أفضل من الإجراء الجشع**، لكنك لا تعرف أي منها. إذا كان لديك
العديد من الخطوات الزمنية القادمة لاختيار الأفعال، فقد يكون من الأفضل
استكشاف الأفعال غير الجشعة واكتشاف أي منها أفضل من الإجراء الجشع.
**العائد أقل** على المدى القصير خلال **الاستكشاف**</span>
(**exploration**)<span dir="rtl">، ولكنه **أعلى** **على المدى الطويل**
لأنك بعد اكتشاف الأفعال الأفضل يمكنك استغلالها عدة مرات. بما أنه ليس من
الممكن **استكشاف**</span> (**exploring**)
**<span dir="rtl">واستغلال</span>** (**exploiting**)
<span dir="rtl">الإجراء في نفس الوقت، فإننا غالباً ما نستخدم مصطلح
**الصراع**</span> (**conflict**) <span dir="rtl">بين
**الاستكشاف**</span> (**exploration**)
<span dir="rtl">**والاستغلال**</span>
(**exploitation**)<span dir="rtl">.</span>

<span dir="rtl">في أي حالة معينة، سواء كان من الأفضل
**الاستكشاف**</span> (**explore**) <span dir="rtl">أو
**الاستغلال**</span> (**exploit**) <span dir="rtl">يعتمد بشكل معقد على
القيم الدقيقة للتقديرات، **وعدم اليقين**، **وعدد الخطوات المتبقية**.
هناك العديد من الأساليب المتطورة لتحقيق توازن بين **الاستكشاف**</span>
(**exploration**) **<span dir="rtl">والاستغلال</span>**
(**exploitation**) <span dir="rtl">في صياغات رياضية محددة لمشكلة
الماكينات ذات الأذرع المتعددة والمشاكل ذات الصلة. ومع ذلك، فإن **معظم
هذه** **الأساليب تقوم على افتراضات قوية** بشأن **الثبات والمعرفة
السابقة** التي إما **تُخالف** أو يكون من **المستحيل** **التحقق منها** في
التطبيقات وفي **مشكلة التعليم المعزز الكامل** التي سنناقشها في الفصول
التالية. ضمانات الأمثلية أو الخسارة المحدودة لهذه الأساليب لا تقدم راحة
كبيرة عندما لا تنطبق افتراضات نظريتها</span>.

<span dir="rtl">في هذا الكتاب، لا نركز على تحقيق توازن بين
**الاستكشاف**</span> (**exploration**)
<span dir="rtl">**والاستغلال**</span> (**exploitation**)
<span dir="rtl">بطريقة متطورة؛ نحن نهتم فقط بتحقيق التوازن بينهما بشكل
عام. في هذا الفصل، نقدم **عدة طرق بسيطة** لتحقيق التوازن لمشكلة
الماكينات ذات الأذرع المتعددة ونوضح أنها تعمل بشكل أفضل بكثير من
الأساليب التي تعتمد دائماً على **الاستغلال**</span>
<span dir="rtl">(</span>**exploitation**<span dir="rtl">).</span>
<span dir="rtl">الحاجة إلى تحقيق توازن بين **الاستكشاف**</span>
<span dir="rtl">(</span>**exploration**<span dir="rtl">)
**والاستغلال**</span> (**exploitation**) <span dir="rtl">هي تحدٍ مميز
يظهر في التعليم المعزز؛ وبساطة نسخة المشكلة لدينا من الماكينات ذات
الأذرع المتعددة تمكننا من عرض هذا التحدي بشكل واضح جداً</span>.

<u>**2.2 <span dir="rtl">طرق تقييم الأفعال</span>**
<span dir="rtl"></span>(**Action-value Methods**)</u>

<span dir="rtl">نبدأ بالنظر عن كثب إلى الطرق المستخدمة لتقدير قيم
الأفعال واستخدام التقديرات لاتخاذ قرارات اختيار الأفعال، والتي نطلق
عليها مجتمعةً اسم **طرق تقييم الأفعال**</span> **(action-value
methods)**<span dir="rtl">.</span> <span dir="rtl">تذكر أن القيمة
الحقيقية للإجراء هي العائد المتوسط عند اختيار ذلك الإجراء. إحدى الطرق
الطبيعية لتقدير ذلك هي عن **طريق حساب متوسط العوائد** التي تم الحصول
عليها بالفعل</span>:

``` math
Q_{t}(\alpha) = \frac{Sum\ of\ Rewars\ When\ a\ taken\ prior\ of\ t}{Numbers\ of\ time\ a\ taken\ prior\ of\ t} = \frac{\Sigma_{\mathbb{i} = 1}^{t - 1}R_{i} \cdot \mathbb{1}A_{i = a}}{\sum_{i = 1}^{t - 1}\mathbb{1}A_{\dot{i} = a}}\ \ \ \ \ (2.1)
```

<span dir="rtl">حيث يشير</span> **predicate** <span dir="rtl">إلى
المتغير العشوائي الذي يكون 1 إذا كان الشرط صحيحًا و0 إذا لم يكن كذلك. إذا
كان المقام صفرًا، فإننا نعرّف</span> $`Q_{t}(\alpha)`$
<span dir="rtl">بدلاً من ذلك على أنه قيمة افتراضية، مثل 0. عندما يذهب
المقام نحو اللانهاية، وفقاً لقانون الأعداد الكبيرة، يتقارب</span>
$`Q_{t}(\alpha)`$ <span dir="rtl">إلى</span> $`q*(a)`$
<span dir="rtl">نُطلق على هذه الطريقة اسم طريقة المتوسط التجريبي</span>
(sample-average method) <span dir="rtl">لتقدير قيم الأفعال لأن كل تقدير
هو متوسط العوائد ذات الصلة. بالطبع، هذه مجرد طريقة واحدة لتقدير قيم
الأفعال، وليست بالضرورة الأفضل. ومع ذلك، دعنا نلتزم الآن بهذه الطريقة
البسيطة للتقدير وننتقل إلى مسألة كيفية استخدام التقديرات لاختيار
الأفعال</span>.

<span dir="rtl">أبسط قاعدة لاختيار الأفعال هي اختيار واحد من الأفعال
التي لها أعلى قيمة تقديرية، أي واحد من الأفعال الجشعة كما هو محدد في
القسم السابق. إذا كان هناك أكثر من إجراء جشع واحد، يتم اختيار واحد منها
بطريقة عشوائية أو بأي طريقة أخرى. نكتب هذه الطريقة لاختيار الأفعال
الجشعة كما يلي</span>:

``` math
{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ A}_{t} = \arg{\max_{a}Qt(a)}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2.2)
```

<span dir="rtl">حيث يشير</span> **argmax a** <span dir="rtl">إلى
الإجراء</span> a <span dir="rtl">الذي يتم تعظيم التعبير الذي يليه (مرة
أخرى، مع كسر الروابط بطريقة عشوائية). اختيار الأفعال الجشعة</span>
(greedy action selection) <span dir="rtl">يستغل دائمًا المعرفة الحالية
لتعظيم العائد الفوري؛ ولا يقضي أي وقت في تجربة الأفعال التي تبدو أقل
فعالية لمعرفة ما إذا كانت قد تكون أفضل فعلاً. بديل بسيط هو التصرف بطريقة
جشعة معظم الوقت، ولكن بين الحين والآخر، مثلاً باحتمالية صغيرة</span>
$`\varepsilon`$<span dir="rtl">، اختيار عشوائي من بين جميع الأفعال مع
احتمال متساوٍ، بشكل مستقل عن تقديرات قيم الأفعال. نُطلق على الطرق التي
تستخدم قاعدة اختيار الأفعال القريبة من الجشع هذه اسم
طرق</span>-<span dir="rtl">الجشع</span> <span dir="rtl">(</span>ε-greedy
methods<span dir="rtl">).</span> <span dir="rtl">ميزة هذه الطرق هي أنه،
في الحد الذي تزداد فيه عدد الخطوات، سيتم تجربة كل إجراء عددًا لانهائيًا من
المرات، مما يضمن أن جميع</span> $`Q_{t}(a)`$ <span dir="rtl">تتقارب
إلى</span>. $`q*(a)`$ <span dir="rtl">وهذا بالطبع يعني أن احتمالية
اختيار الإجراء الأمثل تتقارب إلى أكثر من</span> $`\varepsilon`$
<span dir="rtl"></span>−<span dir="rtl">1، أي تقريباً إلى يقين كبير. ومع
ذلك، فإن هذه مجرد ضمانات أفقية، ولا تعطي الكثير عن فعالية الطرق في
الممارسة العملية</span>.

**<span dir="rtl">التمرين 2.1:</span>** <span dir="rtl">في اختيار
الأفعال القريبة من الجشع</span> ($`\varepsilon`$-greedy action
selection)<span dir="rtl">، في حالة وجود إجراءين و</span> =0.5
<span dir="rtl"></span>$`\varepsilon`$ <span dir="rtl"></span>
<span dir="rtl">، ما هي احتمالية اختيار الإجراء الجشع؟</span>

<u>**2.3** **<span dir="rtl">اختبار الماكينات ذات الأذرع العشر</span>
(The 10-armed Testbed)**</u>

<span dir="rtl">لتقييم فعالية طرق تقييم الأفعال</span> greedy
<span dir="rtl">و</span>ϵ-greedy <span dir="rtl">بشكل تقريبي، قمنا
بمقارنتهما رقميًا على مجموعة من مشاكل الاختبار. كانت هذه مجموعة من 2000
مشكلة ماكينات ذات أذرع متعددة تم توليدها عشوائيًا، حيث</span> $`k =`$
10<span dir="rtl">.</span> <span dir="rtl">لكل مشكلة ماكينات ذات أذرع
متعددة، مثل تلك المعروضة في الشكل **2.1**، كانت قيم الأفعال،</span>

q\*(a), a=1…,10<span dir="rtl">،</span>

<img src="./media/image3.png" style="width:6.5in;height:4.02778in" />

<span dir="rtl">الشكل 2.1: مثال على مشكلة الماكينات ذات الأذرع المتعددة
من مجموعة الاختبار ذات **العشر أذرع**. تم اختيار القيمة الحقيقية</span>
$`\mathbf{q*}\left( \mathbf{a} \right)`$ <span dir="rtl">لكل من الأفعال
العشرة وفقًا لتوزيع طبيعي بمتوسط صفر وتباين وحدة، ثم تم اختيار المكافآت
الفعلية وفقًا لتوزيع طبيعي بمتوسط</span>
$`\mathbf{q*}\left( \mathbf{a} \right)`$ <span dir="rtl">وتباين وحدة،
كما تقترح هذه التوزيعات الرمادية</span>.

<span dir="rtl">تم اختيار القيم وفقًا لتوزيع طبيعي (غوسي) بمتوسط 0
وتباين 1. ثم، عندما طبق طريقة تعلم على هذه المشكلة واختارت
الإجراء</span> $`\mathbf{A}_{\mathbf{t}}`$ <span dir="rtl"></span>​
<span dir="rtl">في الخطوة الزمنية</span> t<span dir="rtl">، تم اختيار
المكافأة الفعلية</span> $`\mathbf{R}_{\mathbf{t}}`$
<span dir="rtl"></span> <span dir="rtl">من توزيع طبيعي بمتوسط</span> q∗
($`\mathbf{A}_{\mathbf{t}}`$​) <span dir="rtl">وتباين 1. تظهر هذه
التوزيعات باللون الرمادي في الشكل **2.1**. نسمي مجموعة مهام الاختبار هذه
مجموعة الاختبار ذات العشر أذرع. بالنسبة لأي طريقة تعلم، يمكننا قياس
أدائها وسلوكها مع تحسنها مع الخبرة على مدى **1000** خطوة زمنية عند
تطبيقها على إحدى مشاكل الماكينات ذات الأذرع المتعددة. هذه تشكل تجربة
واحدة. بتكرار ذلك على **2000** تجربة مستقلة، كل منها مع مشكلة ماكينات
ذات أذرع متعددة مختلفة، حصلنا على مقاييس للسلوك المتوسط لخوارزمية
التعليم</span>.

<span dir="rtl">**الشكل 2.2** يقارن بين طريقة</span> **greedy**
<span dir="rtl">وطريقتين</span> **greedy**<span dir="rtl">-</span>
$`\mathbf{\varepsilon}`$ <span dir="rtl"></span>
<span dir="rtl">(</span>$`\varepsilon`$ <span dir="rtl">=0.01 و</span>
$`\varepsilon`$ <span dir="rtl">=0.1)، كما هو موضح أعلاه، على مجموعة
الاختبار ذات العشر أذرع. جميع الطرق شكلت تقديراتها لقيم الأفعال باستخدام
تقنية المتوسط الحسابي للعينات. الرسم البياني العلوي يظهر الزيادة في
المكافأة المتوقعة مع الخبرة. تحسنت طريقة</span> **greedy**
<span dir="rtl">بشكل طفيف أسرع من الطرق الأخرى في البداية، لكنها استقرت
عند مستوى أدنى. حققت مكافأة لكل خطوة تبلغ حوالي 1 فقط، مقارنةً بأفضل
مكافأة ممكنة والتي تبلغ حوالي **1.55** في هذه المجموعة. كانت
طريقة</span> **greedy** <span dir="rtl">أسوأ بشكل ملحوظ على المدى
الطويل</span>

<img src="./media/image4.png" style="width:6.5in;height:5.41181in" />

<span dir="rtl">**الشكل 2.2:** الأداء المتوسط لطرق تقدير قيم
الأفعال</span> -**greedy**
<span dir="rtl"></span>$`\mathbf{\varepsilon}`$ <span dir="rtl"></span>
<span dir="rtl">على مجموعة الاختبار **ذات العشر** **أذرع**. هذه البيانات
هي متوسطات على مدى **2000** تجربة مع مشاكل ماكينات ذات أذرع متعددة
مختلفة. جميع الطرق استخدمت المتوسطات الحسابية للعينات كتقديرات لقيم
الأفعال</span>.

<span dir="rtl">غالبًا ما كانت طريقة</span> **greedy**
<span dir="rtl">تعلق في أداء أفعال دون المستوى الأمثل. يظهر الرسم
البياني السفلي أن طريقة</span> **greedy** <span dir="rtl">وجدت الإجراء
الأمثل في حوالي ثلث المهام فقط. في الثلثين الآخرين، كانت عيناتها الأولية
من الإجراء الأمثل مخيبة للآمال، ولم تعُد إليه أبدًا. كانت طرق</span>
**greedy** <span dir="rtl"></span>$`\mathbf{\varepsilon} -`$
<span dir="rtl"></span> <span dir="rtl">تؤدي في النهاية بشكل أفضل لأنها
استمرت في الاستكشاف وتحسين فرصها في التعرف على الإجراء الأمثل. كانت
طريقة</span> $`\mathbf{\varepsilon}`$ <span dir="rtl">**=0.1** تستكشف
أكثر، وعادة ما تجد الإجراء الأمثل في وقت أبكر، لكنها لم تختار ذلك
الإجراء أكثر من 91% من الوقت. تحسنت طريقة</span>
$`\mathbf{\varepsilon}`$ <span dir="rtl">=**0.01** بشكل أبطأ، لكنها في
النهاية كانت تؤدي بشكل أفضل من طريقة</span> $`\mathbf{\varepsilon}`$
<span dir="rtl">**=0.1** على كلا مقياسي الأداء المعروضين في الشكل. من
الممكن أيضًا تقليل مع مرور الوقت لمحاولة الحصول على أفضل ما في القيم
العالية والمنخفضة</span>.

<span dir="rtl">ميزة طرق</span> $`\mathbf{\varepsilon}`$ -**greedy**
<span dir="rtl">على طرق</span> **greedy** <span dir="rtl">تعتمد على
المهمة. على سبيل المثال، لنفترض أن تباين المكافآت كان أكبر، مثلاً 10 بدلاً
من 1. مع المكافآت الأكثر ضوضاء، يتطلب الأمر المزيد من الاستكشاف للعثور
على الإجراء الأمثل، ويجب أن تكون طرق</span>**greedy**
<span dir="rtl">-</span> $`\mathbf{\varepsilon}`$ <span dir="rtl">أفضل
نسبيًا مقارنةً بطريقة</span> **greedy**<span dir="rtl">.</span>
<span dir="rtl">من ناحية أخرى، إذا كانت تباينات المكافآت صفرية، فإن
طريقة</span> **greedy** <span dir="rtl">ستكون قادرة على معرفة القيمة
الحقيقية لكل إجراء بعد تجربته مرة واحدة. في هذه الحالة، قد تكون
طريقة</span> **greedy** <span dir="rtl">هي الأفضل لأنها ستجد الإجراء
الأمثل بسرعة ثم لا تستكشف أبدًا. ولكن حتى في الحالة الحتمية، هناك ميزة
كبيرة للاستكشاف إذا قمنا بتخفيف بعض الافتراضات الأخرى. على سبيل المثال،
لنفترض أن مهمة الماكينات ذات الأذرع المتعددة كانت غير ثابتة، أي أن القيم
الحقيقية للأفعال تتغير مع مرور الوقت. في هذه الحالة، يكون الاستكشاف
ضروريًا حتى في الحالة الحتمية للتأكد من أن أحد الأفعال غير الجشعة لم
يتغير ليصبح أفضل من الإجراء الجشع. كما سنرى في الفصول التالية، تعتبر عدم
الثبات هي الحالة الأكثر شيوعًا التي يتم مواجهتها في التعليم المعزز. حتى
إذا كانت المهمة الأساسية ثابتة وحتمية، يواجه المتعلم مجموعة من مهام
اتخاذ القرار الشبيهة بالماكينات ذات الأذرع المتعددة، والتي تتغير كل منها
مع مرور الوقت مع تقدم التعليم وتغير سياسة اتخاذ القرار لدى الوكيل. يتطلب
التعليم المعزز توازنًا بين الاستكشاف والاستغلال</span>.

**<span dir="rtl"><u>التمرين 2.2:</u> مثال على الماكينات ذات الأذرع
المتعددة</span>**

<span dir="rtl">اعتبر مشكلة ماكينات ذات أذرع متعددة</span> k
<span dir="rtl">حيث</span> k=4 <span dir="rtl">أفعال، والتي يُرمز لها بـ
1، 2، 3، و4. افترض تطبيق خوارزمية ماكينات ذات أذرع متعددة باستخدام
اختيار الأفعال بطريقة</span> " <span dir="rtl"></span>-greedy
<span dir="rtl"></span>$`\varepsilon`$ <span dir="rtl">، وتقديرات قيمة
الأفعال باستخدام المتوسط الحسابي للعينات، وتقديرات أولية لـ</span>
Q1(a)=0 <span dir="rtl">لجميع</span> a<span dir="rtl">.</span>
<span dir="rtl">لنفترض أن تسلسل الأفعال والمكافآت الأولي هو</span>:

- A1= 1, R1= -1

- A2=2, R2= 1

- A3= 2, R3= -2

- A4= 2, R4= 2

- A5= 3, R5= 0

<span dir="rtl">في بعض هذه الخطوات الزمنية، قد يكون قد حدثت حالة "، مما
يتسبب في اختيار إجراء عشوائي. في أي من هذه الخطوات الزمنية حدث ذلك
بالتأكيد؟ وفي أي من هذه الخطوات الزمنية قد يكون قد حدث ذلك؟</span>

<span dir="rtl">**<u>تمرين 2.3:</u>** في المقارنة الموضحة في الشكل 2.2،
أي طريقة ستؤدي بشكل أفضل على المدى الطويل من حيث المكافأة التراكمية
واحتمالية اختيار الإجراء الأفضل؟ كم ستكون أفضل؟ عبر عن إجابتك بشكل
كمي</span>.

**<u>2.4 <span dir="rtl">التنفيذ التزايدي</span> (Incremental
Implementation)</u>**

<span dir="rtl">لقد ناقشنا حتى الآن طرق تقدير قيم الإجراءات على أنها
متوسطات تجريبية للمكافآت التي لوحظت. ننتقل الآن إلى مسألة كيفية حساب هذه
المتوسطات بطريقة فعالة من حيث الحساب، وبشكل خاص، باستخدام ذاكرة ثابتة
وحساب ثابت لكل خطوة زمنية</span>.

<span dir="rtl">لتبسيط الكتابة، نركز على إجراء واحد فقط. لندع</span>
$`R_{i}`$ <span dir="rtl"></span> ​ <span dir="rtl">تشير الآن إلى
المكافأة المستلمة بعد الاختيار</span>
$`\ \mathbb{i}\ `$<span dir="rtl">لهذا الإجراء، ولندع</span> $`Q_{n}`$
<span dir="rtl"></span>​ <span dir="rtl">تمثل تقدير قيمة الإجراء بعد أن
تم اختياره</span> $`n - 1`$ <span dir="rtl">مرة، والتي يمكننا الآن
كتابتها ببساطة كـ</span>

``` math
Q_{n} = \frac{R1 + R2 + \ldots + R_{n} - 1}{n - 1}
```

<span dir="rtl">التنفيذ الواضح سيكون بالحفاظ على سجل لجميع المكافآت ثم
إجراء هذا الحساب كلما لزم الأمر الحصول على القيمة المقدرة. ومع ذلك، إذا
تم القيام بذلك، فإن متطلبات الذاكرة والحساب ستنمو مع مرور الوقت مع رؤية
المزيد من المكافآت. ستتطلب كل مكافأة إضافية ذاكرة إضافية لتخزينها وحساباً
إضافياً لحساب المجموع في البسط</span>.

<span dir="rtl">كما قد تتوقع، هذا ليس ضروريًا حقًا. من السهل ابتكار صيغ
تزايدية لتحديث المتوسطات مع متطلبات حسابية صغيرة وثابتة لمعالجة كل
مكافأة جديدة. بالنظر إلى</span> $`Q_{n}`$​
<span dir="rtl">والمكافأة</span> n-th<span dir="rtl">،</span>
$`R_{n}`$​<span dir="rtl">، يمكن حساب المتوسط الجديد لجميع
المكافآت</span> $`n`$ <span dir="rtl">باستخدام</span>

> 
> ``` math
> Q_{n} + 1 = \frac{1}{n}\sum_{i = 1}^{n}R_{i}
> ```
>
> ``` math
> = \frac{1}{n}\left( R_{n} + \sum_{i = 1}^{n - 1}R_{i} \right)
> ```

``` math
= \frac{1}{n}\begin{pmatrix}
n - 1 \\
R_{n} + (n - 1)\frac{1}{n - 1}\sum_{i = 1}^{}R_{i}
\end{pmatrix}
```

> 
> ``` math
> = \frac{1}{n}\left( R_{n} + nQ_{n} - Q_{n} \right)
> ```
>
> ``` math
> {= Q}_{n} + \frac{1}{n}\left\lbrack R_{n} - Q_{n} \right\rbrack
> ```

(2.3)

<span dir="rtl">التي تصح حتى عندما تكون</span> 1= $`n`$
<span dir="rtl">، مما يعطي</span> $`Q_{2} = R_{1}`$ <span dir="rtl">لأي
قيمة لـ</span> $`Q_{1}`$ <span dir="rtl"></span>​ <span dir="rtl">بشكل
عشوائي. هذا التنفيذ يتطلب ذاكرة فقط لـ</span> $`Q_{n}`$
<span dir="rtl">و</span> $`n`$<span dir="rtl">، وأيضًا الحساب البسيط
(2.3) لكل مكافأة جديدة</span>.

<span dir="rtl">تحديث القاعدة (2.3) هو من الشكل الذي يظهر بشكل متكرر
طوال هذا الكتاب. الشكل العام هو</span>

``` math
\text{New Estimate} \leftarrow \text{Old Estimate} + \text{Step Size}\left\lbrack \text{Target} - \text{Old Estimate} \right\rbrack
```

<span dir="rtl">التعبير</span> Target−Old Estimate <span dir="rtl">هو
خطأ في التقدير</span> (Error in the Estimate)<span dir="rtl">.</span>
<span dir="rtl">يتم تقليل هذا الخطأ بأخذ خطوة نحو الهدف</span>
(Target)<span dir="rtl">.</span> <span dir="rtl">يُفترض أن يشير الهدف إلى
اتجاه مرغوب للتحرك نحوه، رغم أنه قد يكون مضطربًا. على سبيل المثال، في
الحالة المذكورة أعلاه، يكون الهدف هو المكافأة</span>
$`nth`$<span dir="rtl">.</span>

<span dir="rtl">لاحظ أن **بارامتر** حجم الخطوة</span> (StepSize)
<span dir="rtl">المستخدم في الطريقة التزايدية</span>
<span dir="rtl">(</span>Incremental
<span dir="rtl"></span>Method<span dir="rtl">)</span>
<span dir="rtl">(المعادلة 2.3) يتغير من خطوة زمنية إلى أخرى. عند معالجة
المكافأة</span> $`nth`$ <span dir="rtl">للإجراء</span> (Action)
<span dir="rtl"></span>α<span dir="rtl">، تستخدم الطريقة **بارامتر** حجم
الخطوة</span> $`\frac{1}{n}`$​. <span dir="rtl">في هذا الكتاب، نشير إلى
**بارامتر** حجم الخطوة بواسطة</span> α <span dir="rtl">أو، بشكل أكثر
عمومية، بواسطة</span> $`a_{t}(a)`$<span dir="rtl">.</span>

<span dir="rtl">الكود الزائف</span> (**Pseudocode**)
<span dir="rtl">لخوارزمية كاملة لمشكلة الماكينات ذات الأذرع
المتعددة</span> <span dir="rtl">(</span>**Bandit**
<span dir="rtl"></span>**Problem**<span dir="rtl">)</span>
<span dir="rtl">باستخدام المتوسطات المحسوبة بشكل تزايدي واختيار الأفعال
بطريقة الإجراء الجشع العشوائي</span>
($`\mathbf{\varepsilon}`$-**greedy**) <span dir="rtl">موضح في المربع
أدناه. يُفترض أن تقوم الدالة</span>
$`\text{bandit}\left( \mathbf{a} \right)`$ <span dir="rtl">بتنفيذ
إجراء</span> (**Action**) <span dir="rtl">وتعيد المكافأة</span>
(**Reward**) <span dir="rtl">المقابلة</span>.

<span dir="rtl">خوارزمية بسيطة لماكينة الأذرع</span> (**A** **simple**
**bandit** **algorithm**)

<img src="./media/image5.png"
style="width:6.26806in;height:2.39306in" />

**<u>2.5 <span dir="rtl">تتبّع مشكلة غير ثابتة</span> (Tracking a
Nonstationary Problem)</u>**

<span dir="rtl">**طرق** **المتوسطات**</span> **(Averaging Methods)**
<span dir="rtl">التي نوقشت حتى الآن مناسبة لمشاكل **ماكينات الأذرع
الثابتة** </span>**(Stationary Bandit Problems)**<span dir="rtl">، أي
لمشاكل **ماكينات الأذرع** </span>**(Bandit Problems)**
<span dir="rtl">التي لا تتغير فيها احتمالات **المكافآت**</span>
**(Rewards)** <span dir="rtl">مع مرور الوقت. كما أُشير سابقًا، غالبًا ما
نواجه مشاكل في **التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">تكون **غير** **ثابتة**</span>
<span dir="rtl">(</span>**Nonstationary**<span dir="rtl">). في مثل هذه
الحالات، يكون من المنطقي إعطاء وزن أكبر للمكافآت الأخيرة مقارنة
بالمكافآت القديمة. واحدة من أكثر الطرق شيوعًا للقيام بذلك هي استخدام
**بارامتر حجم الخطوة الثابت**</span> **<span dir="rtl">(</span>Constant
<span dir="rtl"></span>Step-size
Parameter<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">على سبيل المثال، يتم تعديل قاعدة التحديث التزايدي (2.3)
لتحديث متوسط</span> $`\mathbf{Q}_{\mathbf{n}}`$ <span dir="rtl">للمكافآت
السابقة</span> $`\ \mathbf{n} - \mathbf{1}`$<span dir="rtl">لتكون</span>

``` math
\mathbf{Q}_{\mathbf{n + 1}}\mathbf{=}\mathbf{Q}_{\mathbf{n}}\mathbf{+}\mathbf{\alpha}\left\lbrack \mathbf{R}_{\mathbf{n}}\mathbf{-}\mathbf{Q}_{\mathbf{n}} \right\rbrack
```

**(2.5)**

<span dir="rtl">حيث أن **بارامتر حجم الخطوة**</span> **(Step-size
Parameter) α∈ (0,1\]** <span dir="rtl">ثابت. هذا يؤدي إلى أن يكون</span>
$`\mathbf{Q}_{\mathbf{n + 1}}`$ <span dir="rtl">متوسطًا مرجحًا للمكافآت
السابقة والتقدير الأولي</span> $`\mathbf{Q}_{\mathbf{1}}`$**:**

``` math
\mathbf{\ \ \ \ \ \ \ \ \ \ \ \ \ Q}_{\mathbf{n + 1}}\mathbf{=}\mathbf{Q}_{\mathbf{n}}\mathbf{+}\mathbf{\alpha}\left\lbrack \mathbf{R}_{\mathbf{n}}\mathbf{-}\mathbf{Q}_{\mathbf{n}} \right\rbrack\mathbf{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  =}\mathbf{\alpha}\mathbf{R}_{\mathbf{n}}\mathbf{+}\left( \mathbf{1 -}\mathbf{\alpha} \right)\mathbf{Q}_{\mathbf{n}}\mathbf{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  =}\mathbf{\alpha}\mathbf{R}_{\mathbf{n}}\mathbf{+}\left( \mathbf{1 -}\mathbf{\alpha} \right)\left\lbrack \mathbf{\alpha}\mathbf{R}_{\mathbf{n - 1}}\mathbf{+}\left( \mathbf{1 -}\mathbf{\alpha} \right)\mathbf{Q}_{\mathbf{n - 1}} \right\rbrack\mathbf{=}\mathbf{\alpha}\mathbf{R}_{\mathbf{n}}\mathbf{+}\left( \mathbf{1 -}\mathbf{\alpha} \right)\mathbf{\alpha}\mathbf{R}_{\mathbf{n - 1}}\mathbf{+}\left( \mathbf{1 -}\mathbf{\alpha} \right)^{\mathbf{2}}\mathbf{Q}_{\mathbf{n - 1}}\mathbf{=}\mathbf{\alpha}\mathbf{R}_{\mathbf{n}}\mathbf{+}\left( \mathbf{1 -}\mathbf{\alpha} \right)\mathbf{\alpha}\mathbf{R}_{\mathbf{n - 1}}\mathbf{+}\left( \mathbf{1 -}\mathbf{\alpha} \right)^{\mathbf{2}}\mathbf{\alpha}\mathbf{R}_{\mathbf{n - 2}}\mathbf{+}\mathbf{\cdots}\mathbf{+}\left( \mathbf{1 -}\mathbf{\alpha} \right)^{\mathbf{n - 1}}\mathbf{\alpha}\mathbf{R}_{\mathbf{1}}\mathbf{+}\left( \mathbf{1 -}\mathbf{\alpha} \right)^{\mathbf{n}}\mathbf{Q}_{\mathbf{1}}\mathbf{=}\left( \mathbf{1 -}\mathbf{\alpha} \right)^{\mathbf{n}}\mathbf{Q}_{\mathbf{1}}\mathbf{+}\sum_{\mathbf{i = 1}}^{\mathbf{n}}{\mathbf{\alpha}\left( \mathbf{1 -}\mathbf{\alpha} \right)^{\mathbf{n - i}}\mathbf{R}_{\mathbf{i}}}\mathbf{.}
```

**(2.6)**

<span dir="rtl">نطلق على هذا اسم **المتوسط الموزون** لأن مجموع الأوزان
هو</span>

<span dir="rtl"></span>$`(1 - a)^{n} + \sum_{i = 1}^{n}{a(1 - n)^{n - i}} = 1`$
<span dir="rtl">كما يمكنك التحقق بنفسك. لاحظ أن الوزن</span>
$`a(1 - n)^{n - i}`$ <span dir="rtl">المعطى للمكافأة</span> $`R_{i}`$
<span dir="rtl"></span>​ <span dir="rtl">يعتمد على عدد المكافآت الماضية
التي تم ملاحظتها، وهو  
</span>$`n - \mathbb{i}`$. <span dir="rtl">الكمية</span> $`1 - a`$
<span dir="rtl"></span> <span dir="rtl">أقل من 1، وبالتالي فإن الوزن
الممنوح لـ</span> $`R_{i}`$ <span dir="rtl"></span>​
<span dir="rtl">يتناقص كلما زاد عدد المكافآت المتداخلة. في الواقع،
يتناقص الوزن أسيًا وفقًا للأس</span> exponent <span dir="rtl">على</span>
$`1 - a`$ <span dir="rtl">(إذا كان</span> $`1 - a\  = \ 0`$
<span dir="rtl">، فإن كل الوزن يُعطى للمكافأة الأخيرة فقط،</span>
$`R_{n}`$​<span dir="rtl">، بسبب القاعدة التي تقول إن</span>
$`0^{0} = 1`$ <span dir="rtl">) وبناءً عليه، يُطلق على هذا أحيانًا اسم
**المتوسط الموزون بالتقادم الأسي (**</span>**(recency-weighted average
<span dir="rtl"></span>**<span dir="rtl">أحيانًا يكون من الملائم تغيير
معامل حجم الخطوة من خطوة إلى أخرى. دع</span> $`a_{n}(a)`$
<span dir="rtl"></span> <span dir="rtl">تشير إلى معامل حجم الخطوة
المستخدم لمعالجة المكافأة التي تم تلقيها بعد الاختيار</span> n
<span dir="rtl">للإجراء</span> a<span dir="rtl">.</span>
<span dir="rtl">كما لاحظنا، فإن اختيار</span> $`a_{n}(a) = \frac{1}{n}`$
<span dir="rtl"></span>​ <span dir="rtl">يؤدي إلى استخدام طريقة المتوسطات
التجريبية، والتي تضمن التقارب إلى القيم الحقيقية للإجراءات بموجب قانون
الأعداد الكبيرة. ولكن بالطبع، لا يُضمن التقارب لجميع اختيارات
تسلسل</span> $`\left\{ a_{n}(a) \right\}`$ <span dir="rtl">نظرية التقدير
العشوائي المعروفة تعطي لنا الشروط المطلوبة لضمان التقارب باحتمالية
1</span>:

$`\sum_{n = 1}^{\infty}{a_{n}(a)} = \infty\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{\ \ }\mathbf{(2.7)}`$
<span dir="rtl">و</span> $`\sum_{n = 1}^{}{a_{n}^{2}(a)} < \infty`$
<span dir="rtl"></span>

<span dir="rtl">الشرط الأول ضروري لضمان أن تكون الخطوات كبيرة بما يكفي
لتجاوز أي حالات أولية أو تقلبات عشوائية في النهاية. الشرط الثاني يضمن أن
تصبح الخطوات صغيرة بما يكفي لضمان التقارب في النهاية</span>.

<span dir="rtl">لاحظ أن كلا الشرطين للتقارب يتم تلبيتهما في حالة
المتوسطات التجريبية،</span>
$`\mathbf{a}_{\mathbf{n}}\left( \mathbf{a} \right)\mathbf{=}\frac{\mathbf{1}}{\mathbf{n}}`$<span dir="rtl">،
ولكن لا يتم تلبيتهما في حالة معامل حجم الخطوة الثابت،</span>
$`\mathbf{a}_{\mathbf{n}}\left( \mathbf{a} \right)`$. <span dir="rtl">في
الحالة الأخيرة، لا يتم تلقي الشرط الثاني، مما يشير إلى أن التقديرات لا
تتقارب تمامًا ولكن تواصل التغيير استجابة للمكافآت التي يتم تلقيها مؤخرًا.
كما ذكرنا أعلاه، هذا يكون في الواقع مرغوبًا في بيئة غير ثابتة، والمشاكل
التي تكون غير ثابتة بشكل فعال هي الأكثر شيوعًا في التعليم المعزز.
بالإضافة إلى ذلك، تسلسلات معاملات حجم الخطوة التي تلبي الشروط **(2.7)**
غالبًا ما تتقارب ببطء شديد أو تحتاج إلى ضبط كبير للحصول على معدل تقارب
مرضٍ. على الرغم من أن تسلسلات معاملات حجم الخطوة التي تلبي هذه الشروط
للتقارب تُستخدم في الأعمال النظرية، إلا أنها نادرًا ما تُستخدم في التطبيقات
والبحث التجريبي</span>.

**<span dir="rtl"><u>التمرين 2.4</u></span>**

<span dir="rtl">إذا كانت معاملات حجم الخطوة</span>
$`\mathbf{a}_{\mathbf{n}}`$​ <span dir="rtl">غير ثابتة، فإن
التقدير</span>$`\mathbf{Q}_{\mathbf{n}}`$ <span dir="rtl">يكون متوسطًا
موزونًا للمكافآت التي تم تلقيها مسبقًا، مع وزن مختلف عن ذلك المعطى في
المعادلة **(2.6)**. ما هو الوزن على كل مكافأة سابقة في الحالة العامة،
بطريقة مشابهة للمعادلة **(2.6)**، من حيث تسلسل معاملات حجم
الخطوة؟</span>

**<span dir="rtl"><u>التمرين 2.5 (برمجة)</u></span>**

<span dir="rtl">صمم ونفذ تجربة لتوضيح الصعوبات التي تواجهها طرق
المتوسطات التجريبية في المشاكل غير الثابتة. استخدم نسخة معدلة من مجموعة
الاختبار ذات العشر أذرع حيث تبدأ جميع القيم</span>
$`\mathbf{q*}\left( \mathbf{a} \right)`$ <span dir="rtl">متساوية ثم تتبع
مسيرات عشوائية مستقلة (على سبيل المثال، بإضافة زيادة موزعة بشكل طبيعي
بمتوسط صفر وانحراف معياري 0.01 إلى جميع القيم</span>
$`\mathbf{q*}\left( \mathbf{a} \right)`$ <span dir="rtl">في كل خطوة). قم
بإعداد رسومات بيانية مشابهة للشكل 2.2 لطريقة قيمة الإجراء باستخدام
المتوسطات التجريبية، التي يتم حسابها بشكل تدريجي، وطريقة قيمة الإجراء
الأخرى باستخدام معامل حجم خطوة ثابت،</span>
α=0.1<span dir="rtl">.</span> <span dir="rtl">استخدم</span> ϵ=0.1
<span dir="rtl">وجلسات أطول، مثل 10,000 خطوة</span>.

**<u>2.6 <span dir="rtl">القيم الأولية المتفائلة</span> (Optimistic
Initial Values) <span dir="rtl"></span></u>**

<span dir="rtl">جميع الأساليب التي ناقشناها حتى الآن تعتمد إلى حد ما على
التقديرات الأولية لقيمة الإجراءات</span>
$`\mathbf{Q}_{\mathbf{1}}\left( \mathbf{a} \right)`$<span dir="rtl">.</span>
<span dir="rtl">بلغة الإحصاء، هذه الأساليب تكون متحيزة بسبب تقديراتها
الأولية. بالنسبة لأساليب المتوسطات التجريبية، يختفي هذا التحيز بمجرد أن
يتم اختيار جميع الإجراءات على الأقل مرة واحدة. ولكن في الأساليب التي
تستخدم معامل حجم خطوة ثابت</span> **α**<span dir="rtl">، يكون التحيز
دائمًا، رغم أنه يتناقص بمرور الوقت كما هو موضح في المعادلة **(2.6)**. في
الممارسة العملية، هذا النوع من التحيز عادةً لا يمثل مشكلة، وقد يكون
أحيانًا مفيدًا جدًا. الجانب السلبي هو أن التقديرات الأولية تصبح، بشكل فعلي،
مجموعة من المعلمات التي يجب على المستخدم اختيارها، حتى لو كان الهدف هو
تعيينها جميعًا إلى الصفر. أما الجانب الإيجابي فهو أنها توفر طريقة سهلة
لتزويد النظام ببعض المعرفة السابقة حول مستوى المكافآت التي يمكن
توقعها</span>. <span dir="rtl"></span>

<span dir="rtl">يمكن أيضًا استخدام القيم الأولية للإجراءات كطريقة بسيطة
لتشجيع الاستكشاف</span> (**exploration**)<span dir="rtl">. افترض أنه
بدلًا من تعيين القيم الأولية للإجراءات إلى الصفر، كما فعلنا في مجموعة
الاختبار **ذات العشر** أذرع، قمنا بتعيينها جميعًا إلى **5**+. تذكر أن
القيم الحقيقية</span>
$`\mathbf{q*}\left( \mathbf{a} \right)\mathbf{\ }`$ <span dir="rtl">في
هذه المشكلة يتم اختيارها من توزيع طبيعي بمتوسط 0 وتباين 1. وبالتالي، فإن
التقدير الأولي بقيمة **5**+ متفائل للغاية</span>.

<span dir="rtl">لكن هذا التفاؤل يشجع أساليب قيمة الإجراء على الاستكشاف.
أيًا كانت الإجراءات التي يتم اختيارها في البداية، فإن المكافأة تكون أقل
من التقديرات الأولية؛ مما يدفع المتعلم إلى التبديل إلى إجراءات أخرى، إذ
يشعر "بخيبة أمل" من المكافآت التي يتلقاها. والنتيجة هي أن جميع الإجراءات
يتم تجربتها عدة مرات قبل أن تتقارب تقديرات القيم. يقوم النظام بقدر لا
بأس به من الاستكشاف حتى لو تم اختيار الإجراءات</span> "**greedy**”
<span dir="rtl">طوال الوقت</span>.

<span dir="rtl">توضح **الشكل 2.3** أداء طريقة</span> **greedy**
<span dir="rtl">باستخدام</span>
$`\mathbf{Q}_{\mathbf{1}}\left( \mathbf{a} \right)\mathbf{= + 5}`$
<span dir="rtl">لجميع الإجراءات</span> $`\mathbf{a}`$
<span dir="rtl">على مجموعة اختبار الماكينات ذات الأذرع العشرة</span>
(10-armed bandit testbed)<span dir="rtl">.</span>
<span dir="rtl">للمقارنة، يظهر أيضًا أداء طريقة</span>
"$`\mathbf{\varepsilon}`$**-greedy**” <span dir="rtl">مع</span>
$`\mathbf{Q}_{\mathbf{1}}\left( \mathbf{a} \right)\mathbf{= 0}`$<span dir="rtl">.</span>
<span dir="rtl">في البداية، تؤدي الطريقة المتفائلة أداءً أسوأ لأنها
تستكشف أكثر، ولكن في النهاية تؤدي بشكل أفضل لأن استكشافها يتناقص مع مرور
الوقت. نسمي هذه التقنية لتشجيع الاستكشاف **القيم الأولية المتفائلة**
</span>**(optimistic initial values)**<span dir="rtl">.</span>
<span dir="rtl">نعتبرها حيلة بسيطة يمكن أن تكون فعالة للغاية في
**المشاكل الثابتة**</span> (**stationary problems**)<span dir="rtl">،
لكنها بعيدة عن أن تكون نهجًا عامًا وفعالًا لتشجيع الاستكشاف. على سبيل
المثال، ليست هذه الطريقة مناسبة جيدًا للمشاكل غير الثابتة لأن دافعها
للاستكشاف يعتمد بشكل جوهري على القيم الأولية التي تصبح أقل أهمية مع مرور
الوقت</span>.

<img src="./media/image6.png" style="width:6.5in;height:2.47986in" />

**<span dir="rtl">الشكل 2.3: تأثير التقديرات الأولية المتفائلة لقيم
الإجراءات على مجموعة الاختبار ذات العشر أذرع</span>.**

<span dir="rtl">كلا الطريقتين استخدمتا معامل حجم خطوة ثابت</span>
α=0.1<span dir="rtl">.</span>

<span dir="rtl">هذه الطريقة مؤقتة بطبيعتها. إذا تغيرت المهمة وظهرت حاجة
جديدة للاستكشاف، فإن هذه الطريقة لن تكون مفيدة. في الواقع، أي طريقة تركز
على الظروف الأولية بشكل خاص من غير المحتمل أن تكون فعالة في الحالات
العامة غير الثابتة. بداية الوقت تحدث مرة واحدة فقط، ولذلك يجب ألا نركز
عليها كثيرًا. هذه الانتقادات تنطبق أيضًا على أساليب المتوسطات التجريبية،
التي تعامل أيضًا بداية الوقت كحدث خاص، حيث تدمج جميع المكافآت اللاحقة
بأوزان متساوية. ومع ذلك، فإن جميع هذه الأساليب بسيطة جدًا، وغالبًا ما يكون
أحدها – أو بعض التركيبات البسيطة منها – كافيًا في الممارسة العملية. في
بقية هذا الكتاب، سنستخدم بشكل متكرر العديد من هذه التقنيات البسيطة
للاستكشاف</span>.

**<span dir="rtl">التمرين 2.6: الارتفاعات الغامضة</span> (Mysterious
Spikes)**<span dir="rtl">  
النتائج المعروضة في **الشكل 2.3** يجب أن تكون موثوقة للغاية لأنها
متوسطات لأداء **2000** مهمة من مهام الماكينات ذات الأذرع المتعددة</span>
(10-armed bandit) <span dir="rtl">تم اختيارها عشوائيًا. إذًا، لماذا هناك
تذبذبات وارتفاعات مفاجئة في الجزء الأول من منحنى الطريقة المتفائلة؟
بعبارة أخرى، ما الذي قد يجعل هذه الطريقة تؤدي أداءً أفضل أو أسوأ بشكل خاص
في بعض الخطوات المبكرة؟</span>

**<u><span dir="rtl">التمرين 2.7: حيلة حجم الخطوة الثابتة غير المتحيزة
(</span>Unbiased Constant-Step-Size
<span dir="rtl"></span>Trick<span dir="rtl">)</span></u>**

<span dir="rtl">في معظم هذا الفصل، استخدمنا المتوسطات التجريبية لتقدير
قيم الإجراءات لأن المتوسطات التجريبية لا تنتج التحيز الأولي الذي تنتجه
أحجام الخطوات الثابتة (راجع التحليل الذي أدى إلى المعادلة</span>
(2.6)<span dir="rtl">). ومع ذلك، فإن المتوسطات التجريبية ليست حلاً مرضيًا
تمامًا لأنها قد تؤدي بشكل سيء في المشاكل غير الثابتة</span>.

<span dir="rtl">هل من الممكن تجنب التحيز الذي تحدثه أحجام الخطوات
الثابتة مع الاحتفاظ بمزاياها في المشاكل غير الثابتة؟ إحدى الطرق الممكنة
لتحقيق ذلك هي استخدام حجم خطوة يكون</span> ...

$`\frac{\beta_{n} = a}{{\overline{O}}_{n}}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2.8)\ `$
<span dir="rtl"></span>

<span dir="rtl">لاستيعاب المكافأة</span> $`\ n`$<span dir="rtl">للإجراء
معين، حيث</span> $`a < 0`$ <span dir="rtl">هو حجم خطوة ثابت تقليدي،
و</span> $`{\overline{o}}_{n}`$ <span dir="rtl"></span>​
<span dir="rtl">هو تتبع يبدأ من 0</span>:

$`\ \ \ \ \ \ \ \ \ (2.9)`$
<span dir="rtl"></span>$`{\overline{O}}_{n} = {\overline{o}}_{n - 1} + a\left( 1 - {\overline{O}}_{n} - 1 \right)`$
<span dir="rtl">الى</span> $`n \geq 0`$ <span dir="rtl">مع</span>
$`{\overline{O}}_{0} = 0`$

<span dir="rtl">قم بإجراء تحليل مشابه للتحليل الوارد في المعادلة (2.6)
لتظهر أن</span> $`Q_{n}`$ <span dir="rtl">هو متوسط مرجح للتجديد الأسي
بدون تحيز أولي</span>.

**<u>2.7 <span dir="rtl">اختيار الإجراء بناءً على حدود الثقة
العليا(</span>Upper-Confidence-Bound Action Selection<span dir="rtl">)  
</span></u>**

**<span dir="rtl">الاستكشاف</span> (Exploration)** <span dir="rtl">ضروري
لأنه دائمًا ما تكون هناك حالة من عدم اليقين بشأن دقة تقديرات قيمة
الإجراء</span> (**action**-**value**
**estimates**)<span dir="rtl">.</span> <span dir="rtl">الإجراءات
الجشعة</span> (**greedy** **actions**) <span dir="rtl">هي تلك التي تبدو
الأفضل في الوقت الحالي، لكن بعض الإجراءات الأخرى قد تكون في الواقع أفضل.
اختيار الإجراءات باستخدام سياسة</span> **-greedy
<span dir="rtl"></span>**$`\mathbf{\varepsilon}`$
<span dir="rtl"></span> <span dir="rtl">يجبر على تجربة الإجراءات غير
الجشعة، ولكن بشكل عشوائي، دون تفضيل لتلك التي تكون قريبة من الجشع أو
التي تتسم بعدم اليقين بشكل خاص. سيكون من الأفضل اختيار من بين الإجراءات
غير الجشعة بناءً على إمكاناتها في أن تكون مثالية فعلاً، مع الأخذ في
الاعتبار مدى قرب تقديراتها من أن تكون قصوى والشكوك في تلك التقديرات.
واحدة من الطرق الفعالة للقيام بذلك هي اختيار الإجراءات وفقًا لـ</span>

``` math
{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ A}_{t} = \arg{\max\left\lbrack Q_{t}(a) + c\sqrt{\frac{\ln t}{N_{t}(a)}\ } \right\rbrack}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2.10)
```

<span dir="rtl">حيث</span> $`\mathbf{\ln t}`$<span dir="rtl">تشير إلى
اللوغاريتم الطبيعي لـ</span> $`\mathbf{t}`$ <span dir="rtl">(العدد الذي
يجب رفع</span> $`\mathbb{e}`$ <span dir="rtl"></span>$`\approx`$
<span dir="rtl">2.71828 إليه لكي يساوي</span>
$`\mathbf{t}`$<span dir="rtl">**)**، و</span>**Nt(a)
<span dir="rtl"></span>**<span dir="rtl">تشير إلى عدد المرات التي تم
فيها اختيار الإجراء</span> $`\mathbf{a}`$ <span dir="rtl">قبل
الزمن</span> $`\mathbf{t}`$ <span dir="rtl">(المقام في المعادلة</span>
(2.1)<span dir="rtl">)، و</span>**c \> 0
<span dir="rtl"></span>**<span dir="rtl">تتحكم في درجة الاستكشاف</span>
(**exploration**)<span dir="rtl">.</span> <span dir="rtl">إذا
كانت</span>
$`\mathbf{N}_{\mathbf{t}}(\mathbf{a})\  = \ \mathbf{0}`$<span dir="rtl">،
فإن</span> $`\mathbf{a}`$ <span dir="rtl">يُعتبر إجراءً مُعظِمًا</span>
(**maximizing action**)<span dir="rtl">.</span>

<span dir="rtl">فكرة اختيار الإجراءات باستخدام **الحد الأعلى لثقة
التقدير**</span> **(UCB)** <span dir="rtl">هي أن الحد الجذري التربيعي هو
مقياس لعدم اليقين أو التباين في تقدير قيمة الإجراء</span>
$`\mathbf{a}`$**<span dir="rtl">.</span>** <span dir="rtl">الكمية التي
يتم تعظيمها هي بالتالي نوع من الحدود العليا للقيمة الحقيقية المحتملة
للإجراء</span> $`\mathbf{a}`$<span dir="rtl">، مع تحديد</span> **c
<span dir="rtl"></span>**<span dir="rtl">لمستوى الثقة. في كل مرة يتم
فيها اختيار</span> $`\mathbf{a}`$<span dir="rtl">، يُفترض أن عدم اليقين
ينخفض</span>:$`\mathbf{N}_{\mathbf{t}}(\mathbf{a})`$
<span dir="rtl">يزداد، وكما يظهر في المقام، ينخفض مصطلح عدم اليقين. من
ناحية أخرى، في كل مرة يتم فيها اختيار إجراء آخر غير</span>
$`\mathbf{a}`$<span dir="rtl">، يزداد</span> $`\mathbf{t}`$
<span dir="rtl">لكن</span> $`\mathbf{N}_{\mathbf{t}}(\mathbf{a})`$
<span dir="rtl">لا يتغير؛ لأن</span> $`\mathbf{t}`$ <span dir="rtl">يظهر
في البسط، فإن تقدير عدم اليقين يزداد. استخدام اللوغاريتم الطبيعي يعني أن
الزيادات تصبح أصغر بمرور الوقت، لكنها غير محدودة؛ جميع الإجراءات سيتم
اختيارها في النهاية، ولكن الإجراءات ذات تقديرات القيمة الأقل، أو التي تم
اختيارها بشكل متكرر بالفعل، ستختار بتردد متناقص بمرور الوقت</span>.

<span dir="rtl">تُظهِر النتائج التي تم الحصول عليها باستخدام</span> **UCB
<span dir="rtl"></span>**<span dir="rtl">في بيئة الاختبار ذات العشر أذرع
في **الشكل 2.4**. غالبًا ما يُؤدي</span> **UCB
<span dir="rtl"></span>**<span dir="rtl">أداءً جيدًا، كما هو موضح هنا،
لكنه أصعب من سياسة</span> **-greedy
<span dir="rtl"></span>**$`\mathbf{\varepsilon}`$
<span dir="rtl"></span> <span dir="rtl">في التمدد إلى أبعد من الماكينات
ذات الأذرع المتعددة إلى إعدادات التعليم المعزز الأكثر عمومية التي يتم
النظر فيها في بقية هذا الكتاب. واحدة من الصعوبات هي التعامل مع المشكلات
غير الثابتة؛ حيث ستكون هناك حاجة إلى طرق أكثر تعقيدًا من تلك التي تم
تقديمها في **القسم** **2.5**. صعوبة أخرى هي التعامل مع المساحات الكبيرة
للحالات، لا سيما عند استخدام تقريبات الدوال كما هو موضح في الجزء الثاني
من هذا الكتاب. في هذه الإعدادات المتقدمة، عادةً ما تكون فكرة اختيار
الإجراءات باستخدام</span> **UCB
<span dir="rtl"></span>**<span dir="rtl">غير عملية</span>.

<img src="./media/image7.png" style="width:6.5in;height:2.71875in" />

**<span dir="rtl">الشكل 2.4</span>:** <span dir="rtl">الأداء المتوسط
لاختيار الإجراءات باستخدام</span> **UCB
<span dir="rtl"></span>**<span dir="rtl">في بيئة الاختبار ذات العشر
أذرع. كما هو موضح، بشكل عام يؤدي</span> **UCB
<span dir="rtl"></span>**<span dir="rtl">أداءً أفضل من اختيار الإجراءات
باستخدام</span> **-greedy
<span dir="rtl"></span>**$`\mathbf{\varepsilon}`$ <span dir="rtl">،
باستثناء الخطوات الأولى</span> $`\mathbf{k}`$<span dir="rtl">، عندما
يختار بشكل عشوائي بين الإجراءات التي لم يتم تجربتها بعد</span>.

**<span dir="rtl">تمرين 2.8: ذروات</span> UCB**

<span dir="rtl">في الشكل 2.4، يظهر خوارزمية</span> **UCB
<span dir="rtl"></span>**<span dir="rtl">ذروة مميزة في الأداء في الخطوة
الحادية عشرة. لماذا يحدث ذلك؟ لاحظ أن إجابتك يجب أن تشرح بشكل كامل لماذا
تزداد المكافأة في الخطوة الحادية عشرة ولماذا تنخفض في الخطوات التالية.  
تلميح: إذا كان</span> **c = 1**<span dir="rtl">، فإن الذروة تكون أقل
بروزًا</span>.

**<u>2.8 <span dir="rtl">خوارزميات</span> Bandit
<span dir="rtl">التدرجية</span> (Gradient Bandit Algorithms)</u>**

<span dir="rtl">حتى الآن في هذا الفصل، تناولنا الأساليب التي تقدر قيم
الأفعال وتستخدم تلك التقديرات لاختيار الأفعال. هذه طريقة جيدة غالبًا،
لكنها ليست الطريقة الوحيدة الممكنة. في هذا القسم، نعتبر تعلم تفضيل رقمي
لكل إجراء</span> $`\mathbf{a}`$<span dir="rtl">، والذي نرمز له بـ</span>
$`\mathbf{H}_{\mathbf{t}}\left( \mathbf{a} \right)`$**<span dir="rtl">.</span>**
<span dir="rtl">كلما زادت درجة التفضيل، زادت احتمالية اتخاذ ذلك الإجراء،
ولكن التفضيل ليس له تفسير من حيث المكافأة. فقط التفضيل النسبي لإجراء
واحد على آخر هو المهم؛ إذا أضفنا 1000 إلى جميع تفضيلات الأفعال، فلن يكون
هناك تأثير على احتمالات الأفعال، التي تحدد وفقًا لتوزيع</span> **soft-max
<span dir="rtl"></span>**<span dir="rtl">(أي توزيع</span> **Gibbs
<span dir="rtl"></span>**<span dir="rtl">أو</span>
**Boltzmann<span dir="rtl">)</span>** <span dir="rtl">كما
يلي</span>:<span dir="rtl">  
</span>

``` math
\mathbf{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ P}_{\mathbf{r}}\left\{ \mathbf{A}_{\mathbf{t}}\mathbf{= a} \right\}\mathbf{=}\frac{\mathbf{\mathbb{e}}^{\mathbf{Ht}\left( \mathbf{a} \right)}}{\sum_{\mathbf{b = 1}}^{\mathbf{k}}\mathbf{\mathbb{e}}^{\mathbf{Ht}\left( \mathbf{b} \right)}}\mathbf{=}\mathbf{\pi}_{\mathbf{t}}\left( \mathbf{a} \right)\mathbf{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2.11)}
```

<span dir="rtl">حيث هنا قدمنا أيضًا تدوينًا جديدًا مفيدًا،</span>
$`\mathbf{\pi}_{\mathbf{t}}\left( \mathbf{a} \right)`$<span dir="rtl">،
لاحتمالية اتخاذ الإجراء</span> $`\mathbf{a}`$ <span dir="rtl">في
الزمن</span> $`\mathbf{t}`$**<span dir="rtl">.</span>**
<span dir="rtl">في البداية، تكون جميع تفضيلات الأفعال متساوية (على سبيل
المثال،</span> $`\mathbf{H}_{\mathbf{1}}(\mathbf{a})\  = \ \mathbf{0}`$
<span dir="rtl">لجميع الأفعال)، بحيث تكون لجميع الأفعال احتمالية متساوية
في الاختيار</span>.

<span dir="rtl">**<u>تمرين 2.9:</u>** إظهار أن توزيع</span> **soft-max**
<span dir="rtl">في حالة وجود إجراءين هو نفسه الذي يُعطى بواسطة دالة
اللوجستيك (أو الدالة السينية) التي تُستخدم كثيرًا في الإحصاء والشبكات
العصبية الاصطناعية</span>.

<span dir="rtl">هناك خوارزمية تعلم طبيعية لهذا الإعداد تعتمد على فكرة
الصعود التدرجي العشوائي. في كل خطوة، بعد اختيار الإجراء</span>
$`\mathbf{A}_{\mathbf{t}}`$ <span dir="rtl">وتلقي المكافأة</span>
$`\mathbf{R}_{\mathbf{t}}`$<span dir="rtl">، يتم تحديث تفضيلات الأفعال
كما يلي</span>:

``` math
H_{t} + 1\left( A_{t} \right) \doteq H_{t}\left( A_{t} \right) + a\left( R_{t} - {\overline{R}}_{t} \right)\left( 1 - \pi_{t}\left( A_{t} \right) \right),
```

``` math
H_{t} + 1(a) \doteq H_{t}(a) - \ a\left( R_{t} - {\overline{R}}_{t} \right)\pi_{t}(a),
```

``` math
{and\ 
}{for\ all\ a \neq A_{t},\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2.12)}
```

<span dir="rtl">حيث</span> **α \> 0
<span dir="rtl"></span>**<span dir="rtl">هو بارامتر حجم الخطوة، و</span>
$`{\overline{R}}_{t}\mathbb{\in R}`$<span dir="rtl">هو متوسط جميع
المكافآت حتى الزمن</span> **t**<span dir="rtl">، والذي يمكن حسابه بشكل
تدريجي كما هو موضح في **القسم 2.4** (أو **القسم 2.5** إذا كانت المشكلة
غير ثابتة). يعمل مصطلح</span> $`{\overline{R}}_{t}`$
<span dir="rtl">كقاعدة مقارنة مع المكافأة. إذا كانت المكافأة أعلى من
القاعدة، يتم زيادة احتمالية اتخاذ</span> $`\mathbf{A}_{\mathbf{t}}`$
<span dir="rtl">في المستقبل، وإذا كانت المكافأة أقل من القاعدة، يتم
تقليل الاحتمالية. تتحرك الإجراءات غير المختارة في الاتجاه
المعاكس</span>.

**<span dir="rtl">الشكل 2.5</span>:** <span dir="rtl">يُظهر نتائج
**خوارزمية** </span>**bandit** <span dir="rtl">التدرجية على نسخة معدلة
من بيئة الاختبار ذات العشر أذرع، حيث كانت المكافآت المتوقعة الحقيقية
تُختار وفقًا لتوزيع طبيعي بمتوسط 4+ بدلاً من الصفر (وبتباين وحدة كما كان من
قبل). إن رفع جميع المكافآت لا يؤثر على **خوارزمية** </span>**bandit**
<span dir="rtl">التدرجية بسبب مصطلح القاعدة للمكافأة، الذي يتكيف فورًا مع
المستوى الجديد. لكن إذا تم حذف القاعدة (أي إذا كانت</span>
$`\ {\overline{R}}_{t}`$ <span dir="rtl">تُعتبر صفرًا ثابتًا في
المعادلة</span> (2.12)<span dir="rtl">)، فإن الأداء سينخفض بشكل ملحوظ،
كما هو موضح في الشكل</span>.

<img src="./media/image8.png" style="width:6.5in;height:3.24167in" />

**<span dir="rtl">الشكل 2.5:</span>** <span dir="rtl">الأداء المتوسط
لخوارزمية</span> **bandit
<span dir="rtl"></span>**<span dir="rtl">التدرجية مع وبدون قاعدة مكافأة
في بيئة الاختبار ذات العشر أذرع عندما يتم اختيار</span>
$`\mathbf{\ \ q*}\left( \mathbf{a} \right)`$<span dir="rtl">لتكون قريبة
من 4+ بدلاً من قرب الصفر</span>.

**<span dir="rtl">خوارزمية</span> Bandit <span dir="rtl">التدرجية كصعود
تدرجي عشوائي</span> (Stochastic Gradient Ascent)**

<span dir="rtl">يمكن الحصول على رؤية أعمق لخوارزمية</span> **bandit
<span dir="rtl"></span>**<span dir="rtl">التدرجية من خلال فهمها كتقريب
عشوائي للصعود التدرجي. في الصعود التدرجي الدقيق، يتم زيادة تفضيل
الإجراء</span> $`\mathbf{H}_{\mathbf{t}}\left( \mathbf{a} \right)`$
<span dir="rtl">بشكل يتناسب مع تأثير الزيادة على الأداء</span>.

``` math
\mathbf{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ H}_{\mathbf{t + 1}}\left( \mathbf{a} \right)\mathbf{=}\mathbf{H}_{\mathbf{t}}\left( \mathbf{a} \right)\mathbf{+}\mathbf{\alpha}\frac{\mathbf{\partial}\mathbf{E}\left\lbrack \mathbf{R}_{\mathbf{t}} \right\rbrack}{\mathbf{\partial}\mathbf{H}_{\mathbf{t}}\left( \mathbf{a} \right)}\mathbf{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2.13)}
```

<span dir="rtl">حيث **مقياس** **الأداء** هنا هو **المكافأة**
**المتوقعة**</span>:

``` math
\mathbf{E}\left\lbrack \mathbf{R}_{\mathbf{t}} \right\rbrack\mathbf{=}\sum_{\mathbf{x}}^{}{\mathbf{\pi}_{\mathbf{t}}\left( \mathbf{x} \right)\mathbf{q}^{\mathbf{*}}\left( \mathbf{x} \right)}
```

<span dir="rtl">ومقياس تأثير الزيادة هو المشتق الجزئي لهذا المقياس
الأداء بالنسبة لتفضيل الإجراء. بالطبع، لا يمكن تنفيذ الصعود التدرجي بدقة
في حالتنا لأننا نفترض أننا لا نعرف</span>
$`\mathbf{\ q*}\left( \mathbf{x} \right)`$<span dir="rtl">، لكن في
الواقع، التحديثات في خوارزميتنا **(2.12)** تكون متساوية في القيمة
المتوقعة مع **(2.13)**، مما يجعل الخوارزمية مثالاً على الصعود التدرجي
العشوائي. الحسابات التي تظهر ذلك تتطلب فقط حساب التفاضل الأساسي، ولكنها
تتطلب عدة خطوات. أولاً، نلقي نظرة أقرب على التدرج الدقيق للأداء</span>:

``` math
\frac{\partial E\left\lbrack R_{t} \right\rbrack}{\partial H_{t}(a)} = \frac{\partial}{\partial H_{t}(a)}\left\lbrack \sum_{x}^{}{\pi_{t}(x)q^{*}(x)} \right\rbrack
```

``` math
= \sum_{x}^{}{q^{*}(x)\frac{\partial\pi_{t}(x)}{\partial H_{t}(a)}}
```

``` math
\ \ \ \ \ \ \ \ \ \ \ \  = \sum_{x}^{}{\left( q^{*}(x) - B_{t} \right)\frac{\partial\pi_{t}(x)}{\partial H_{t}(a)},}
```

<span dir="rtl">حيث</span> $`\mathbf{B}_{\mathbf{t}}`$<span dir="rtl">،
الذي يُسمى القاعدة، يمكن أن يكون أي عدد ثابت لا يعتمد على</span>
$`x`$<span dir="rtl">.</span> <span dir="rtl">يمكننا إدراج قاعدة هنا دون
تغيير المساواة لأن التدرج يُجمع إلى الصفر على جميع
الأفعال</span>$`\sum_{x}^{}\frac{\partial\pi_{t}(x)}{\partial H_{t}(a)} = 0 - as{\ \ H}_{t}(a)`$
<span dir="rtl">عندما يتغير</span>
$`\mathbf{H}_{\mathbf{t}}\left( \mathbf{a} \right)`$<span dir="rtl">،
تزيد احتمالية بعض الأفعال وتنخفض احتمالية البعض الآخر، لكن مجموع
التغييرات يجب أن يكون صفرًا لأن مجموع الاحتمالات هو دائمًا واحد</span>.
<span dir="rtl">التالي، نضرب كل مصطلح في المجموع بـ</span>
$`\pi_{t}(x)\text{/}\pi_{t}(x)`$

``` math
\frac{\partial E\left\lbrack R_{t} \right\rbrack}{\partial H_{t}(a)} = \sum_{x}^{}{\pi_{t}(x)\left( q^{*}(x) - B_{t} \right)\frac{\partial\pi_{t}(x)}{\partial H_{t}(a)}}\text{/}\pi_{t}(x)
```

<span dir="rtl">المعادلة الآن في شكل توقع، حيث نقوم بجمع جميع القيم
الممكنة</span> $`\mathbf{x}`$ <span dir="rtl">للمتغير العشوائي</span>
$`\mathbf{A}_{\mathbf{t}}`$<span dir="rtl">، ثم نضرب في احتمالية اتخاذ
تلك القيم. وبالتالي</span>:

``` math
= E\left\lbrack \left( q^{*}\left( A_{t} \right) - B_{t} \right)\frac{\partial\pi_{t}\left( A_{t} \right)}{\partial H_{t}(a)}\text{/}\pi_{t}\left( A_{t} \right) \right\rbrack
```

``` math
= E\left\lbrack \left( R_{t} - {\overline{R}}_{t} \right)\frac{\partial\pi_{t}\left( A_{t} \right)}{\partial H_{t}(a)}\text{/}\pi_{t}\left( A_{t} \right) \right\rbrack,
```

<span dir="rtl">حيث اخترنا هنا الخط الأساسي</span>
$`B_{t} = \overline{R_{t}}`$ <span dir="rtl">واستبدلنا</span> $`R_{t}`$
<span dir="rtl">بـ</span> $`q*\left( A_{t} \right)`$<span dir="rtl">  
وهذا مسموح لأن</span>
$`\ \ E\left\lbrack R_{t}\mid A_{t} \right\rbrack = q^{*}\left( A_{t} \right)`$<span dir="rtl">قريبًا
سنثبت أن</span>...<span dir="rtl">  
</span>$`\frac{\partial\pi_{t}(x)}{\partial H_{t}(a)} = \pi_{t}(x)\left( \mathbb{1}_{a = x} - \pi_{t}(a) \right)`$
<span dir="rtl">وحيث يُعرَّف</span> $`\mathbb{1}a = x`$
<span dir="rtl"></span> <span dir="rtl">ليكون 1 إذا كان</span>
a=x<span dir="rtl">، وإلا 0.  
بافتراض ذلك الآن، لدينا  
</span>=$`\mathbb{E}\left\lbrack \left( R_{t} - \overline{R_{t}} \right)*\left( \mathbb{1}_{a = At} - \pi_{t}(a) \right) \right\rbrack`$

<span dir="rtl">تذكر أن خطتنا كانت كتابة تدرج الأداء كتوقع لشيء يمكننا
أخذ عينات منه في كل خطوة، كما فعلنا للتو، ثم تحديثه في كل خطوة بشكل
متناسب مع العينة. باستبدال عينة من التوقع أعلاه بتدرج الأداء في المعادلة
(2.13)، نحصل على</span>:

``` math
H_{t + 1}(a) = H_{t}(a) + \alpha\left\lbrack \left( R_{t} - \overline{R_{t}} \right)\left( \mathbb{1}_{a = A_{t}} - \pi_{t}(a) \right) \right\rbrack
```

<span dir="rtl">التي قد تتعرف عليها باعتبارها مكافئة لخوارزميتنا
الأصلية</span> (2.12)<span dir="rtl">. بالتالي، يتبقى فقط أن نثبت
أن</span> <span dir="rtl">كما افترضنا.</span>

``` math
\frac{\partial\pi_{t}(x)}{\partial H_{t}(a)} = \pi_{t}(x)\left( \mathbb{1}_{a = x} - \pi_{t}(a) \right)
```

<span dir="rtl">تذكر قاعدة النسبية القياسية للمشتقات</span>:

``` math
\frac{\partial}{\partial x}\left\lbrack \frac{f(x)}{g(x)} \right\rbrack = \frac{g(x)\frac{\partial f(x)}{\partial x} - f(x)\frac{\partial g(x)}{\partial x}}{g(x)^{2}}
```

<span dir="rtl">باستخدام هذه القاعدة، يمكننا كتابة</span>

``` math
\frac{\partial\pi_{t}(x)}{\partial H_{t}(a)} = \frac{\partial}{\partial H_{t}(a)}\pi_{t}(x)
```

``` math
= \frac{\partial}{\partial H_{t}(a)}\left\lbrack \frac{e^{H_{t}(x)}}{\sum_{y = 1}^{k}e^{H_{t}(y)}} \right\rbrack
```

``` math
= \frac{\partial e^{H_{t}(x)}}{\partial H_{t}(a)} \cdot \frac{\sum_{y = 1}^{k}e^{H_{t}(y)} - e^{H_{t}(x)}\frac{\partial\sum_{y = 1}^{k}e^{H_{t}(y)}}{\partial H_{t}(a)}}{\left( \sum_{y = 1}^{k}e^{H_{t}(y)} \right)^{2}}
```

``` math
= \frac{\mathbb{1}_{a = x}e^{H_{t}(x)}\sum_{y = 1}^{k}e^{H_{t}(y)} - e^{H_{t}(x)}e^{H_{t}(a)}}{\left( \sum_{y = 1}^{k}e^{H_{t}(y)} \right)^{2}}
```

``` math
= \frac{\delta_{a = x}e^{H_{t}(x)}}{\sum_{y = 1}^{k}e^{H_{t}(y)}} - \frac{e^{H_{t}(x)}e^{H_{t}(a)}}{\left( \sum_{y = 1}^{k}e^{H_{t}(y)} \right)^{2}}
```

``` math
= \mathbb{1}_{a = x}\pi_{t}(x) - \pi_{t}(x)\pi_{t}(a)
```

``` math
= \pi_{t}(x)\left( \delta_{a = x} - \pi_{t}(a) \right)
```

<span dir="rtl">لقد أظهرنا للتو أن التحديث المتوقع في خوارزمية
"الروبوتات</span>" (gradient bandit algorithm) <span dir="rtl">يعادل
التدرج في المكافأة المتوقعة، وبالتالي فإن الخوارزمية هي مثال على
الارتفاع العشوائي للتدرج</span> (stochastic gradient
ascent)<span dir="rtl">.</span> <span dir="rtl">وهذا يضمن لنا أن
الخوارزمية تتمتع بخصائص تقارب قوية</span>.<span dir="rtl">  
لاحظ أننا لم نطلب أي خصائص لقاعدة المكافأة</span> (reward baseline)
<span dir="rtl">بخلاف أنها لا تعتمد على الإجراء المُختار. على سبيل
المثال، كان بإمكاننا تعيينها إلى صفر أو إلى 1000، وكانت الخوارزمية ستظل
مثالاً على الارتفاع العشوائي للتدرج</span> (stochastic gradient
ascent)<span dir="rtl">.</span> <span dir="rtl">اختيار قاعدة المكافأة لا
يؤثر على التحديث المتوقع للخوارزمية، لكنه يؤثر على تباين التحديث
وبالتالي على معدل التقارب (كما هو موضح، على سبيل المثال، في الشكل 2.5).
قد لا يكون اختيارها كمتوسط المكافآت هو الأفضل، ولكنه بسيط ويعمل بشكل جيد
في الممارسة العملية</span>.

**<u><span dir="rtl">2.9 البحث التوافقي (الروبوتات السياقية)</span>
(Contextual Bandits) Associative Search</u>**

<span dir="rtl">حتى الآن في هذا الفصل، تناولنا فقط المهام غير التوافقية،
أي المهام التي لا حاجة فيها لربط إجراءات مختلفة بمواقف مختلفة. في هذه
المهام، إما أن يحاول المتعلم العثور على أفضل إجراء واحد عندما تكون
المهمة ثابتة، أو يحاول تتبع أفضل إجراء مع تغييره بمرور الوقت عندما تكون
المهمة غير ثابتة. ومع ذلك، في مهمة التعليم التعزيزي العامة، هناك أكثر من
موقف واحد، والهدف هو تعلم سياسة: وهي رسم بياني من المواقف إلى الإجراءات
التي تكون الأفضل في تلك المواقف. لإعداد المسرح للمشكلة الكاملة، نناقش
بإيجاز أبسط طريقة يمكن من خلالها توسيع المهام غير التوافقية إلى الإعداد
التوافقي</span>.

<span dir="rtl">كمثال، لنفترض أن هناك عدة مهام مختلفة من نوع الماكينات
ذات الأذرع المتعددة، وفي كل خطوة تواجه واحدة من هذه المهام يتم اختيارها
عشوائيًا. وبالتالي، فإن مهمة الماكينات ذات الأذرع المتعددة تتغير عشوائيًا
من خطوة إلى خطوة. قد يظهر لك ذلك كمهمة واحدة من نوع الماكينات ذات الأذرع
المتعددة غير الثابتة، حيث تتغير القيم الحقيقية للإجراءات عشوائيًا من خطوة
إلى خطوة. يمكنك محاولة استخدام إحدى الطرق الموضحة في هذا الفصل التي يمكن
أن تتعامل مع عدم الثبات، ولكن ما لم تتغير القيم الحقيقية للإجراءات ببطء،
فإن هذه الطرق لن تعمل بشكل جيد</span>.

<span dir="rtl">الآن، لنفترض أنه عندما يتم اختيار مهمة الماكينات ذات
الأذرع المتعددة لك، يتم إعطاؤك بعض القرائن المميزة حول هويتها (لكن ليس
قيمها الإجرائية). ربما تواجه آلة حظ فعلية تغير لون شاشتها عندما تغير قيم
إجراءاتها. الآن يمكنك تعلم سياسة تربط كل مهمة، بالإشارة إلى اللون الذي
تراه، بأفضل إجراء يجب اتخاذه عند مواجهة تلك المهمة—على سبيل المثال، إذا
كان اللون أحمر، اختر الذراع 1؛ إذا كان أخضر، اختر الذراع 2. مع السياسة
الصحيحة، يمكنك عادةً القيام بعمل أفضل بكثير مما يمكنك في غياب أي معلومات
تميز بين مهمة الماكينات ذات الأذرع المتعددة وأخرى</span>.

<span dir="rtl">هذه هي مثال على مهمة البحث التوصيفي، والتي تُسمى بذلك
لأنها تتضمن كل من التعليم بالتجربة والخطأ للبحث عن أفضل الإجراءات،
وتوصيف هذه الإجراءات بال</span> **Situations** <span dir="rtl">التي تكون
الأفضل فيها. غالبًا ما تُسمى مهام البحث التوصيفي الآن بـ "الماكينات ذات
الأذرع المتعددة السياقية" في الأدبيات. تعتبر مهام البحث التوصيفي وسيطة
بين مشكلة الماكينات ذات الأذرع المتعددة والمشكلة الكاملة للتعليم المعزز.
فهي تشبه المشكلة الكاملة للتعليم المعزز من حيث أنها تتضمن تعلم سياسة،
ولكنها تشبه نسختنا من مشكلة الماكينات ذات الأذرع المتعددة من حيث أن كل
إجراء يؤثر فقط على المكافأة الفورية. إذا سمحنا للإجراءات بتأثير الوضع
التالي بالإضافة إلى المكافأة، فإننا نكون قد انتقلنا إلى المشكلة الكاملة
للتعليم المعزز. سنعرض هذه المشكلة في الفصل التالي ونناقش تداعياتها خلال
بقية الكتاب</span>.

### **<span dir="rtl"><u>التمرين 2.10</u></span>**

<span dir="rtl">افترض أنك تواجه مهمة ذات ذراعين</span> (2-armed bandit)
<span dir="rtl">حيث تتغير القيم الحقيقية للأفعال بشكل عشوائي من خطوة
زمنية إلى أخرى. على وجه التحديد، افترض أن القيم الحقيقية للأفعال 1 و2 في
أي خطوة زمنية هي</span>:

- 0.1 <span dir="rtl">و 0.2 مع احتمال 0.5</span>
  (<span dir="rtl">الحالة</span> A)

- 0.9 <span dir="rtl">و 0.8 مع احتمال 0.5</span>
  (<span dir="rtl">الحالة</span> B)

<span dir="rtl">إذا لم تتمكن من معرفة أي حالة تواجهها في أي خطوة، ما هو
أفضل توقع للنجاح الذي يمكنك تحقيقه وكيف يجب أن تتصرف لتحقيقه؟</span>
<span dir="rtl">الآن افترض أنه في كل خطوة يُخبرك بما إذا كنت تواجه
الحالة</span> **A** <span dir="rtl">أو الحالة</span> **B**
<span dir="rtl">(على الرغم من أنك لا تزال لا تعرف القيم الحقيقية
للأفعال). هذه مهمة بحث توصيفي. ما هو أفضل توقع للنجاح الذي يمكنك تحقيقه
في هذه المهمة، وكيف يجب أن تتصرف لتحقيقه؟</span>

**<u>2.10 <span dir="rtl">الملخص</span> (Summary)</u>**

<span dir="rtl">لقد قدمنا في هذا الفصل عدة طرق بسيطة لتحقيق التوازن بين
الاستكشاف</span> ("**exploration**") <span dir="rtl">و"الاستغلال</span>"
("**exploitation**”) <span dir="rtl">تختار طرق</span> "**ε-greedy**"
<span dir="rtl">(الإجابة العشوائية) عشوائيًا بنسبة صغيرة من الوقت، في حين
أن طرق</span> "**UCB**" <span dir="rtl">(الحدود العليا للثقة) تختار بشكل
محدد ولكنها تحقق الاستكشاف</span> (**exploration**) <span dir="rtl">من
خلال تفضيل الإجراءات التي تلقت عينات أقل حتى الآن بشكل غير ملحوظ في كل
خطوة. تقدر خوارزميات</span> **"gradient bandit"**
<span dir="rtl">(المنحدر) تفضيلات الإجراءات بدلاً من قيم الإجراءات، وتفضل
الإجراءات الأكثر تفضيلاً بطريقة متدرجة واحتمالية باستخدام توزيع</span>
"**soft-max**"<span dir="rtl">. التبسيط البسيط لتقدير القيم بشكل متفائل
يتسبب في استكشاف كبير حتى بالنسبة لطرق</span> "**greedy**"
<span dir="rtl">(الجشعة)</span>.

<span dir="rtl">من الطبيعي أن نسأل أي من هذه الطرق هو الأفضل. على الرغم
من أن هذا سؤال صعب الإجابة عليه بشكل عام، يمكننا بالتأكيد تشغيلها جميعًا
على</span> -armed testbed<span dir="rtl">10</span>
<span dir="rtl">(اختبار العشر أذرع) الذي استخدمناه طوال هذا الفصل
ومقارنة أدائها. تعقيد الأمر هو أن كل منها لديه بارامتر؛ للحصول على
مقارنة ذات مغزى، يجب أن نأخذ في الاعتبار أدائها كدالة لمعلماتها. لقد
أظهرت الرسوم البيانية الخاصة بنا حتى الآن مسار التعليم بمرور الوقت لكل
خوارزمية وإعداد بارامتر، لإنتاج منحنى تعلم لتلك الخوارزمية وإعداد
البارامترية. إذا رسمنا منحنيات تعلم لجميع الخوارزميات وجميع إعدادات
المعلمات، فإن الرسم البياني سيكون معقدًا ومزدحمًا للغاية لإجراء مقارنات
واضحة. بدلاً من ذلك، نلخص منحنى التعليم الكامل بقيمته المتوسطة على مدى
1000 خطوة؛ هذه القيمة متناسبة مع المساحة تحت منحنى التعليم. يوضح الشكل
2.6 هذه القياسات لأنواع مختلفة من خوارزميات</span> "bandit"
<span dir="rtl">(الماكينات ذات الأذرع المتعددة) من هذا الفصل، كل منها
كدالة لمعناته الخاصة المعروضة على مقياس واحد على المحور السيني. يُطلق على
هذا النوع من الرسوم البيانية اسم "دراسة المعلمات (</span>parameter
study<span dir="rtl">).</span> <span dir="rtl">لاحظ أن قيم المعلمات
تتغير بعوامل اثنين وتُعرض على مقياس لوغاريتمي. لاحظ أيضًا الأشكال المميزة
على شكل</span> "U" <span dir="rtl">مقلوبة لأداء كل خوارزمية؛ جميع
الخوارزميات تحقق أفضل أداء عند قيمة متوسطة لمعناتها، لا كبيرة جدًا ولا
صغيرة جدًا</span>.

<img src="./media/image9.png" style="width:6.5in;height:3.02083in" />

<span dir="rtl">الشكل 2.6: دراسة للمعلمات</span> (parameter study)
<span dir="rtl">لمختلف خوارزميات</span> "bandit"
<span dir="rtl">(الماكينات ذات الأذرع المتعددة) المقدمة في هذا الفصل. كل
نقطة تمثل متوسط المكافأة</span> (reward) <span dir="rtl">المحصلة على مدى
1000 خطوة باستخدام خوارزمية معينة وإعداد معين لمعناتها</span>.

<span dir="rtl">لقد قدمنا في هذا الفصل عدة طرق بسيطة لتحقيق توازن بين
الاستكشاف</span> (exploration) <span dir="rtl">والاستغلال</span>
(exploitation)<span dir="rtl">.</span> <span dir="rtl">تستخدم طرق</span>
**ε <span dir="rtl">-الجشعة</span>** (ε-greedy methods)
<span dir="rtl">الاختيار العشوائي لنسبة صغيرة من الوقت، بينما تختار
طرق</span> **UCB** <span dir="rtl">بشكل حتمي ولكنها تحقق الاستكشاف من
خلال تفضيل الأفعال التي تلقت عينات أقل حتى الآن. تقدّر **خوارزميات**
**الربط** **التدرجي**</span> (**gradient** **bandit** **algorithms**)
<span dir="rtl">تفضيلات الأفعال بدلاً من قيم الأفعال، ويفضلون الأفعال
الأكثر تفضيلاً بطريقة تدرجية واحتمالية باستخدام توزيع</span> "soft-max".
<span dir="rtl">التقدير المتفائل في البداية يجعل حتى الطرق الجشعة تستكشف
بشكل كبير. من الطبيعي أن نسأل أي من هذه الطرق هو الأفضل. على الرغم من أن
هذه مسألة صعبة للإجابة عليها بشكل عام، إلا أنه يمكننا بالتأكيد تشغيل
جميعها على **اختبار** **الماكينات** **ذات** **الأذرع** **المتعددة**  
(</span>**10**-**armed** **testbed**<span dir="rtl">)</span>
<span dir="rtl">الذي استخدمناه طوال هذا الفصل ومقارنة أدائها. تعقيد
الأمر هو أن جميعها تحتوي على بارامتر؛ للحصول على مقارنة ذات مغزى، يجب
علينا النظر في أدائها كدالة لمعناتها. أظهرت الرسوم البيانية لدينا حتى
الآن مسار التعليم مع مرور الوقت لكل خوارزمية وإعداد للبارامتر، لإنتاج
منحنى تعلم لتلك الخوارزمية وإعداد البارامترية. إذا قمنا برسم منحنيات
التعليم لجميع الخوارزميات وجميع إعدادات المعلمات، فسيكون الرسم البياني
معقدًا للغاية ومزدحمًا لجعل المقارنات واضحة. بدلاً من ذلك، نلخص منحنى
التعليم الكامل بقيمته المتوسطة على مدى 1000 خطوة؛ وهذه القيمة تتناسب مع
المساحة تحت منحنى التعليم. يوضح الشكل 2.6 هذا القياس للخوارزميات
المختلفة في هذا الفصل، كل منها كدالة لمعناتها الخاصة المعروضة على مقياس
واحد على المحور الأفقي</span> (x-axis)<span dir="rtl">.</span>
<span dir="rtl">تُسمى هذه الأنواع من الرسوم البيانية **دراسة**
**للبارامترات**</span> (**parameter** **study**)<span dir="rtl">.</span>
<span dir="rtl">لاحظ أن قيم المعلمات تتنوع بعوامل اثنين وتُعرض على مقياس
لوغاريتمي. لاحظ أيضًا الأشكال المميزة المقلوبة على شكل</span> "U"
<span dir="rtl">لأداء كل خوارزمية؛ حيث أن جميع الخوارزميات تؤدي بشكل
أفضل عند قيمة متوسطة لمعناتها، لا تكون كبيرة جدًا ولا صغيرة جدًا</span>.

<span dir="rtl">لقد أظهرنا للتو أن التحديث المتوقع لخوارزمية "**الربط**
**التدرجي**" يساوي تدرج المكافأة المتوقعة، وبالتالي فإن الخوارزمية هي
حالة من حالات الصعود التدريجي العشوائي. هذا يضمن أن الخوارزمية لها خصائص
تلاقي قوية</span>.

<span dir="rtl">لاحظ أننا لم نطلب أي خصائص من **خط** **الأساس**
**للمكافأة**</span> (**reward** **baseline**) <span dir="rtl">بخلاف أنه
لا يعتمد على الفعل المحدد. على سبيل المثال، كان يمكننا تعيينه إلى صفر،
أو إلى 1000، وكانت الخوارزمية ستظل حالة من حالات **الصعود** **التدريجي**
**العشوائي**. اختيار خط الأساس لا يؤثر على التحديث المتوقع للخوارزمية،
ولكنه يؤثر على تباين التحديث وبالتالي على معدل التلاقي (كما هو موضح، على
سبيل المثال، في الشكل 2.5). اختيار خط الأساس كمتوسط المكافآت قد لا يكون
الأفضل، لكنه بسيط ويعمل جيدًا في الممارسة العملية</span>.

<span dir="rtl">على الرغم من بساطتها، فإن الطرق المقدمة في هذا الفصل
يمكن اعتبارها **حالة** **الفن**</span> <span dir="rtl">(</span>**state**
**of** **the** <span dir="rtl"></span>**art**<span dir="rtl">).</span>
<span dir="rtl">هناك طرق أكثر تعقيدًا، ولكن تعقيدها وفرضياتها يجعلها غير
عملية لمشكلة التعليم المعزز الكامل التي هي تركيزنا الحقيقي. بدءًا من
الفصل 5، نقدم طرق التعليم لحل مشكلة التعليم المعزز الكامل التي تستخدم
جزئيًا الطرق البسيطة المستكشفة في هذا الفصل</span>.

<span dir="rtl">على الرغم من أن الطرق البسيطة المستكشفة في هذا الفصل قد
تكون الأفضل التي يمكننا القيام بها حاليًا، إلا أنها بعيدة عن كونها حلاً
مرضيًا بالكامل لمشكلة تحقيق التوازن بين **الاستكشاف**</span>
(**exploration**) <span dir="rtl">**والاستغلال**</span>
(**exploitation**)<span dir="rtl">.</span>

<span dir="rtl">واحدة من الطرق المدروسة جيدًا لتحقيق التوازن بين
**الاستكشاف**</span> (**exploration**)
<span dir="rtl">**والاستغلال**</span> (**exploitation**)
<span dir="rtl">في **مشاكل الماكينات ذات الأذرع المتعددة**</span>
**(k-armed bandit problems**) <span dir="rtl">هي حساب نوع خاص من قيمة
الفعل يسمى **"مؤشر جيتينز (**</span>**Gittins
index**<span dir="rtl">).</span> <span dir="rtl">في بعض الحالات الخاصة
المهمة، يكون هذا الحساب قابلاً للتنفيذ ويؤدي مباشرة إلى حلول مثلى، على
الرغم من أنه يتطلب معرفة كاملة بالتوزيع السابق للمشاكل المحتملة، وهو ما
نفترض عمومًا أنه غير متوفر. بالإضافة إلى ذلك، لا يبدو أن النظرية أو
القابلية الحسابية لهذا النهج تتعمم على مشكلة التعليم المعزز الكامل التي
نعتبرها في بقية الكتاب</span>.

<span dir="rtl">نهج مؤشر جيتينز هو حالة من **طرق بايزي**
</span>**(Bayesian methods)**<span dir="rtl">، التي تفترض توزيعًا أوليًا
معروفًا على قيم الأفعال ثم تقوم بتحديث التوزيع بدقة بعد كل خطوة (افتراضًا
أن قيم الأفعال الحقيقية ثابتة). بشكل عام، يمكن أن تكون حسابات التحديث
معقدة للغاية، ولكن لبعض التوزيعات الخاصة</span> (<span dir="rtl">تسمى
"المسبقات المتجانسة</span>" (conjugate priors)) <span dir="rtl">تكون
سهلة. إحدى الإمكانيات هي اختيار الأفعال في كل خطوة وفقًا لاحتمالها
اللاحق</span> (**posterior** probability) <span dir="rtl">في كونها أفضل
فعل. هذه الطريقة، التي تُسمى أحيانًا **أخذ عينات لاحقة**</span>
(**posterior** **sampling**) <span dir="rtl">أو **أخذ عينات
تومسون**</span> <span dir="rtl">(</span>**Thompson**
<span dir="rtl"></span>**sampling**<span dir="rtl">)، تؤدي غالبًا بشكل
مشابه لأفضل الطرق الخالية من التوزيع التي قدمناها في هذا الفصل</span>.

<span dir="rtl">في **الإعداد البايزي**، من الممكن حتى حساب التوازن
الأمثل بين **الاستكشاف**</span> (**exploration**)
<span dir="rtl">**والاستغلال**</span>
(**exploitation**)<span dir="rtl">.</span> <span dir="rtl">يمكن حساب
الاحتمال لكل فعل محتمل من المكافآت الفورية الممكنة والتوزيعات اللاحقة
الناتجة على قيم الأفعال. يصبح هذا التوزيع المتطور حالة المعلومات
للمشكلة. بالنظر إلى أفق، لنقل 1000 خطوة، يمكن للمرء أن يأخذ في اعتباره
جميع الأفعال المحتملة، وجميع المكافآت الناتجة الممكنة، وجميع الأفعال
التالية الممكنة، وجميع المكافآت التالية، وهكذا لجميع 1000 خطوة. بالنظر
إلى الافتراضات، يمكن تحديد المكافآت والاحتمالات لكل سلسلة محتملة من
الأحداث، ويجب فقط اختيار الأفضل. ولكن شجرة الإمكانيات تنمو بسرعة شديدة؛
حتى لو كانت هناك فقط فعلين ومكافأتين، فإن الشجرة ستحتوي على</span>
$`2^{2000}`$ <span dir="rtl">ورقة. من غير الممكن بشكل عام إجراء هذا
الحساب الهائل بدقة، ولكن ربما يمكن تقريبه بكفاءة. من المحتمل أن يتحول
هذا النهج إلى مشكلة التعليم المعزز الكامل. في النهاية، قد نتمكن من
استخدام طرق التعليم المعزز التقريبية مثل تلك المقدمة في الجزء الثاني من
هذا الكتاب للاقتراب من هذا الحل الأمثل. لكن ذلك موضوع للبحث ويتجاوز نطاق
هذا الكتاب التمهيدي</span>.

**<span dir="rtl">التمرين 2.11 (برمجي)</span>**: <span dir="rtl">أنشئ
شكلًا مشابهًا للشكل **2.6** للحالة غير الثابتة الموضحة في التمرين **2.5**.
قم بتضمين خوارزمية</span> **ε <span dir="rtl">-الجشع</span>**
(**ε**-**greedy**) <span dir="rtl">ذات حجم الخطوة الثابت مع</span>
α=0.1<span dir="rtl">.</span> <span dir="rtl">استخدم تجارب تتضمن 200,000
خطوة، وكمعيار أداء لكل خوارزمية وإعداد بارامتر، استخدم المكافأة المتوسطة
على مدى آخر 100,000 خطوة</span>.

**<span dir="rtl">الفصل الثالث:</span>**

<span dir="rtl">**العمليات القرار ماركوف المنتهية** (</span>Finite
Markov <span dir="rtl"></span>Decision Processes<span dir="rtl">)</span>

<span dir="rtl">في هذا الفصل نقدم المشكلة الرسمية لعمليات اتخاذ القرار
ماركوف المنتهية، أو الـ</span> **MDPs** <span dir="rtl">المنتهية، التي
نحاول حلها في باقي الكتاب. تتضمن هذه المشكلة تغذية راجعة تقييمية، كما في
مشاكل "الماكينات ذات الأذرع المتعددة"، ولكنها تتضمن أيضًا جانبًا
ارتباطيًا—اختيار إجراءات مختلفة في حالات مختلفة. تعد الـ</span> **MDPs**
<span dir="rtl">تجسيدًا كلاسيكيًا لصنع القرار المتسلسل، حيث تؤثر الأفعال
ليس فقط على المكافآت الفورية، ولكن أيضًا على الحالات أو الوضعيات التالية،
ومن خلالها على المكافآت المستقبلية. وبالتالي، تتضمن الـ</span> **MDPs**
<span dir="rtl">**مكافآت** **مؤجلة** والحاجة إلى الموازنة بين
**المكافآت** **الفورية** **والمكافآت** **المؤجلة**. بينما في **"مشاكل
الماكينات ذات الأذرع المتعددة"** قدرنا قيمة كل إجراء</span>
$`\mathbf{q*}\left( \mathbf{a} \right)`$<span dir="rtl">، في الـ</span>
**MDPs** <span dir="rtl">نحن نقوم بتقدير قيمة كل إجراء</span>
$`\mathbf{\ \ q*(s,a)\ }`$<span dir="rtl">في كل حالة</span>
s<span dir="rtl">، أو نقوم بتقدير قيمة كل حالة</span> $`\mathbf{v*(s)}`$
<span dir="rtl">بناءً على اختيارات الإجراءات المثلى. هذه الكميات المعتمدة
على الحالة أساسية لتحديد الفضل بدقة في العواقب طويلة الأمد لاختيارات
الأفعال الفردية</span>.

<span dir="rtl">تُعد الـ</span> **MDPs** <span dir="rtl">شكلاً مثاليًا
رياضيًا لمشكلة التعليم المعزز يمكن تقديم بيانات نظرية دقيقة لها. نقدم
العناصر الرئيسية لهيكل المشكلة الرياضي، مثل العوائد، دوال القيمة،
ومعادلات بيلمان. نحاول توضيح نطاق التطبيقات الواسع الذي يمكن صياغته
كـ</span> **MDPs** <span dir="rtl">منتهية. كما هو الحال في جميع الذكاء
الاصطناعي، هناك توتر بين **نطاق القابلية للتطبيق والقابلية الرياضية**.
في هذا الفصل، نقدم هذا التوتر ونناقش بعض التبادلات والتحديات التي ينطوي
عليها. يتم تناول بعض الطرق التي يمكن من خلالها تجاوز الـ</span> **MDPs**
<span dir="rtl">في الفصل **17**</span>.

**<u>3.1 <span dir="rtl">واجهة الوكيل والبيئة</span> (The
Agent–Environment Interface)</u>**

<span dir="rtl">تم تصميم</span> **MDPs** <span dir="rtl">لتكون إطارًا
بسيطًا لمشكلة التعليم من التفاعل لتحقيق هدف ما. المتعلم وصانع القرار يُطلق
عليه اسم الوكيل</span> (**agent**)<span dir="rtl">.</span>
<span dir="rtl">الشيء الذي يتفاعل معه، والذي يشمل كل شيء خارج الوكيل،
يُسمى البيئة</span> (**environment**)<span dir="rtl">.</span>
<span dir="rtl">يتفاعل الوكيل والبيئة بشكل مستمر، حيث يختار الوكيل
إجراءات، والبيئة تستجيب لهذه الإجراءات وتقدم مواقف جديدة للوكيل. كما أن
البيئة تولد مكافآت، وهي قيم عددية خاصة يسعى الوكيل لتعظيمها بمرور الوقت
من خلال اختياره للإجراءات</span>.

<img src="./media/image10.png" style="width:6.5in;height:2.27404in" />

**<span dir="rtl">الشكل 3.1: تفاعل الوكيل والبيئة في عملية اتخاذ القرار
ماركوف</span> (MDP)<span dir="rtl">.</span>**

<span dir="rtl">بشكل أكثر تحديدًا، يتفاعل الوكيل والبيئة في كل من سلسلة
من الخطوات الزمنية المنفصلة،</span> $`\mathbf{t = 0,1,2,3,\ldots}`$
<span dir="rtl">في كل خطوة زمنية</span> $`\mathbf{t}`$<span dir="rtl">،
يتلقى الوكيل تمثيلًا لحالة البيئة،</span>
$`\mathbf{S}_{\mathbf{t}}\mathbf{\in s}`$ <span dir="rtl">، وعلى أساس
ذلك يختار إجراءً،</span>
$`\mathbf{A}_{\mathbf{t}}\mathbf{ \in A(s)}`$<span dir="rtl">.</span>
<span dir="rtl">بعد خطوة زمنية واحدة، كنتيجة جزئية لإجرائه، يتلقى الوكيل
مكافأة عددية،</span>
$`\mathbf{R}_{\mathbf{t + 1}}\mathbf{\in}\mathbf{R}\mathbf{\subset}\mathbb{R}`$
<span dir="rtl">، ويجد نفسه في حالة جديدة،</span>
$`\mathbf{S}_{\mathbf{t + 1}}`$ <span dir="rtl">وبالتالي، تؤدي عملية
اتخاذ القرار ماركوف</span> (**MDP**) <span dir="rtl">والوكيل إلى سلسلة
أو مسار يبدأ على النحو التالي</span>:

<span dir="rtl">  
</span>
``` math
\mathbf{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ S}_{\mathbf{0}}\mathbf{,}\mathbf{A}_{\mathbf{0}}\mathbf{,}\mathbf{R}_{\mathbf{1}}\mathbf{,}\mathbf{S}_{\mathbf{1}}\mathbf{,}\mathbf{A}_{\mathbf{1}}\mathbf{,}\mathbf{R}_{\mathbf{2}}\mathbf{,}\mathbf{S}_{\mathbf{2}}\mathbf{,}\mathbf{A}_{\mathbf{2}}\mathbf{,}\mathbf{R}_{\mathbf{3}}\mathbf{\ldots\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.1)}
```

<span dir="rtl">في عملية اتخاذ القرار ماركوف المحدودة</span> (**finite**
**MDP**)<span dir="rtl">، تحتوي مجموعات الحالات والأفعال
والمكافآت</span> $`\mathbf{(S،\ A،و\ R)}`$ <span dir="rtl">على عدد محدود
من العناصر. في هذه الحالة، المتغيرات العشوائية</span>
$`\mathbf{R}_{\mathbf{t}}`$ <span dir="rtl"></span>
<span dir="rtl">و</span>$`\mathbf{S}_{\mathbf{t}}`$ <span dir="rtl">لها
توزيعات احتمالية متقطعة محددة تعتمد فقط على الحالة والإجراء السابقين.
أي، لقيم معينة لهذه المتغيرات العشوائية،</span>
$`\mathbf{s}^{\mathbf{'}}\mathbf{\in S\ \ }\text{and  }\mathbf{r}\mathbf{\in}\mathbf{R}`$
<span dir="rtl">، هناك احتمال حدوث تلك القيم في الوقت</span>
$`t`$<span dir="rtl">، بناءً على قيم معينة للحالة والإجراء
السابقين</span>:

``` math
\mathbf{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ p}\left( \mathbf{s}^{\mathbf{'}}\mathbf{,r}\mid\mathbf{s,a} \right)\mathbf{=}\mathbf{\Pr}\textit{\textbf{\{}}\mathbf{S}_{\mathbf{t}}\mathbf{=}\mathbf{s}^{\mathbf{'}}\mathbf{,}\mathbf{R}_{\mathbf{t}}\mathbf{= r}\mathbf{\mid}\mathbf{S}_{\mathbf{t - 1}}\mathbf{= s,}\mathbf{A}_{\mathbf{t - 1}}\mathbf{= a}\textit{\textbf{\}}}\mathbf{\ \ \ \ \ \ \ \ \ (3.2)}
```

<span dir="rtl">لكل</span>
$`\mathbf{s}^{\mathbf{'}}\mathbf{\in S\ \ و\ \ r \in R\ و\ a \in A}\left( \mathbf{s} \right)\mathbf{\ }`$
<span dir="rtl">الدالة</span> p <span dir="rtl">تحدد ديناميات عملية
اتخاذ القرار</span> **Markov**<span dir="rtl">.</span>
<span dir="rtl">النقطة فوق علامة المساواة في المعادلة تذكرنا بأنها تعريف
(في هذه الحالة للدالة</span> $`\mathbf{p}`$ <span dir="rtl">)</span>
<span dir="rtl">وليست حقيقة تتبع من التعريفات السابقة. دالة
الديناميات</span>
$`\mathbf{p:S \times R \times S \times A \rightarrow}\left\lbrack \mathbf{1,0} \right\rbrack`$
<span dir="rtl">هي دالة محددة عادية بأربعة متغيرات. الرمز ‘**\|**’ في
وسطها يأتي من التدوين للفرصة الشرطية</span>.

<span dir="rtl">نستخدم المصطلحات **وكيل**</span> (**agent**)
**<span dir="rtl">وبيئة</span>** (**environment**)
**<span dir="rtl">وإجراء</span>** (**action**) <span dir="rtl">بدلاً من
المصطلحات الهندسية **تحكم**</span> (**controller**)
<span dir="rtl">ونظام مُتحكم به (أو مصنع)</span>
<span dir="rtl">(</span>**controlled system or**
<span dir="rtl"></span>**plant**<span dir="rtl">)</span>
<span dir="rtl">**وإشارة** **تحكم**</span> (**control** **signal**)
<span dir="rtl">لأنها أكثر دلالة لجمهور أوسع</span>.

<span dir="rtl">نقصر الانتباه على **الوقت** **المنفصل**</span>
(**discrete** **time**) <span dir="rtl">للحفاظ على البساطة قدر الإمكان،
على الرغم من أن العديد من الأفكار يمكن تمديدها إلى الحالة
المستمرة</span> (**continuous** **time**) <span dir="rtl">انظر، على سبيل
المثال،</span> Bertsekas and Tsitsiklis, 1996; Doya,
1996)<span dir="rtl">).</span>

<span dir="rtl">لتبسيط التدوين، نفترض أحياناً الحالة الخاصة التي يكون
فيها مجموعة الإجراءات هي نفسها في جميع الحالات ونكتبها ببساطة كـ</span>
($`A`$)<span dir="rtl">.</span>

<span dir="rtl">نستخدم</span> $`\mathbf{R}_{\mathbf{t + 1}}`$
<span dir="rtl">بدلاً من</span> $`\mathbf{R}_{\mathbf{t}}`$
<span dir="rtl">للإشارة إلى المكافأة الناتجة عن</span>
$`\mathbf{A}_{\mathbf{t}}`$ <span dir="rtl">لأنها توضح أن المكافأة
التالية والحالة التالية،</span> $`\mathbf{R}_{\mathbf{t + 1}}`$
<span dir="rtl">و</span>
$`\mathbf{S}_{\mathbf{t + 1}}`$<span dir="rtl">، يتم تحديدهما بشكل
مشترك. للأسف، كلا التدوينين يُستخدمان على نطاق واسع في الأدبيات</span>.

<span dir="rtl">لكن هنا تذكرنا فقط أن</span> $`\mathbf{p}`$
<span dir="rtl">يحدد توزيع احتمالي لكل اختيار لـ</span> **s
<span dir="rtl">و</span>a**<span dir="rtl">، أي أن</span>:

``` math
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \sum_{\mathbf{s}^{\mathbf{'}}\mathbf{\in}\mathcal{S}}^{}{\sum_{\mathbf{r}\mathbf{\in}\mathcal{R}}^{}{\mathbf{p}\left( \mathbf{s}^{\mathbf{'}}\mathbf{,r} \middle| \mathbf{s,a} \right)}}\mathbf{= 1},\text{   for all   }\mathbf{s}\mathbf{\in}\mathcal{S,}\mathbf{a}\mathbf{\in}\mathcal{A}\left( \mathbf{s} \right).\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.3)
```

<span dir="rtl">في عملية اتخاذ القرار ماركوفية</span> (**Markov Decision
Process**)<span dir="rtl">، فإن الاحتمالات التي يحددها الدالة</span>
$`\mathbf{p}`$ <span dir="rtl"></span> <span dir="rtl">تمثل وصفًا كاملاً
لديناميكية البيئة</span> (**environment's
dynamics**)<span dir="rtl">.</span> <span dir="rtl">بمعنى أن احتمال كل
قيمة ممكنة للحالة</span> $`\mathbf{S}_{\mathbf{t}}`$
<span dir="rtl">والمكافأة</span> $`\mathbf{R}_{\mathbf{t}}`$​
<span dir="rtl">يعتمد فقط على الحالة والفعل في اللحظة السابقة مباشرة،
أي</span> $`\mathbf{S}_{\mathbf{t - 1}}`$​ <span dir="rtl">و</span>
$`\mathbf{A}_{\mathbf{t - 1}}`$​<span dir="rtl">، ولا يعتمد إطلاقًا على
الحالات والأفعال السابقة لهما</span>.

<span dir="rtl">ويُفضّل اعتبار هذا ليس كقيد على عملية اتخاذ القرار</span>
(**decision process**) <span dir="rtl">نفسها، بل كقيد على الحالة</span>
(**state**)<span dir="rtl">.</span> <span dir="rtl">إذ يجب أن تحتوي
الحالة على جميع المعلومات المتعلقة بتفاعل الوكيل مع البيئة في الماضي،
والتي تؤثر على المستقبل. وإذا كانت الحالة تحتوي على هذه المعلومات، فإننا
نقول إن لها خاصية ماركوف</span> (**Markov
Property**)<span dir="rtl">.</span>

<span dir="rtl">سوف نفترض خاصية ماركوف طوال هذا الكتاب، رغم أننا، بدءًا
من الجزء الثاني</span> (**Part II**)<span dir="rtl">، سنتناول طرق
تقريب</span> (**approximation methods**) <span dir="rtl">لا تعتمد على
هذه الخاصية. وفي الفصل **17**، سنناقش كيف يمكن تعلم وبناء حالة
ماركوف</span> (**Markov state**) <span dir="rtl">انطلاقًا من ملاحظات غير
ماركوفية  
</span>(**non-Markov observations**)<span dir="rtl">.</span>

<span dir="rtl">انطلاقًا من دالة الديناميكا ذات الأربع مدخلات</span>
$`\mathbf{p}`$<span dir="rtl">، يمكن حساب أي شيء آخر قد يرغب المرء
بمعرفته عن البيئة، مثل احتمالات انتقال الحالة</span> (**state-transition
probabilities**)<span dir="rtl">، والتي نرمز لها، مع بعض التجاوز في
التدوين، كدالة ذات ثلاث مدخلات بالشكل</span>
$`.p:S \times S \times A \rightarrow \lbrack 0,1\rbrack`$

``` math
\mathbf{p}\left( \mathbf{s}^{\mathbf{'}}\mid\mathbf{s,a} \right)\mathbf{=}\mathbf{Pr}\textit{\textbf{\{}}\mathbf{S}_{\mathbf{t}}\mathbf{=}\mathbf{s}^{\mathbf{'}}\mathbf{\mid}\mathbf{S}_{\mathbf{t - 1}}\mathbf{= s,}\mathbf{A}_{\mathbf{t - 1}}\mathbf{= a}\textit{\textbf{\}}}\mathbf{=}\sum_{\mathbf{r}\mathbf{\in}\mathbf{R}}^{}{\mathbf{p}\left( \mathbf{s}^{\mathbf{'}}\mathbf{,r}\mid\mathbf{s,a} \right)}\mathbf{\ \ \ (3.4)}
```

<span dir="rtl">يمكننا أيضًا حساب المكافآت المتوقعة لأزواج الحالة-الإجراء
كدالة ذات متغيرين</span>
$`\mathbf{r:\ S\ }\mathbf{\times}\mathbf{A\ }\mathbf{\rightarrow}\mathbf{R}`$

``` math
\mathbf{r}\left( \mathbf{s,a} \right)\mathbf{= E}\left\lbrack \mathbf{R}_{\mathbf{t}}\mid\mathbf{S}_{\mathbf{t - 1}}\mathbf{= s,}\mathbf{A}_{\mathbf{t - 1}}\mathbf{= a} \right\rbrack\mathbf{=}\sum_{\mathbf{r}\mathbf{\in}\mathbf{R}}^{}\mathbf{r}\sum_{\mathbf{s}^{\mathbf{'}}\mathbf{\in}\mathbf{S}}^{}{\mathbf{p}\left( \mathbf{s}^{\mathbf{'}}\mathbf{,r}\mid\mathbf{s,a} \right)\mathbf{\ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.5)}}
```

<span dir="rtl">وكذلك القيم المتوقعة للمكافآت</span> (**expected
rewards**) <span dir="rtl">بالنسبة لثلاثيات (الحالة – الفعل – الحالة
التالية</span>) (**state–action–next-state triples**)<span dir="rtl">،
والتي تُعبّر عنها كدالة ذات ثلاث مدخلات بالشكل</span>
$`\mathbf{r:S \times A \times S \rightarrow R}`$<span dir="rtl">.</span>

``` math
r\left( s,a,s' \right) \doteq E\left\lbrack R_{t} \middle| S_{t - 1} = s,A_{t - 1} = a,S_{t} = s' \right\rbrack = \sum_{r \in \mathcal{R}}^{}r\frac{p\left( s',r \middle| s,a \right)}{p\left( s' \middle| s,a \right)}.\ \ \ \ \ \ \ \ \ (\mathbf{3.6)}
```

<span dir="rtl">في هذا الكتاب، نستخدم عادةً دالة</span> $`p`$
<span dir="rtl">ذات الأربعة متغيرات **(3.2)**، ولكن قد تكون هذه الرموز
الأخرى مفيدة في بعض الأحيان أيضًا</span>.

<span dir="rtl">إطار عمل الـ</span> **MDP** <span dir="rtl">هو إطار
تجريدي ومرن ويمكن تطبيقه على العديد من المشكلات بطرق مختلفة. على سبيل
المثال، لا يلزم أن تشير خطوات الزمن إلى فترات زمنية ثابتة؛ يمكن أن تشير
إلى مراحل متعاقبة من اتخاذ القرارات والعمل. يمكن أن تكون الأفعال تحكمات
منخفضة المستوى، مثل الفولتية المطبقة على محركات ذراع روبوت، أو قرارات
عالية المستوى، مثل ما إذا كان يجب تناول الغداء أم الذهاب إلى المدرسة
العليا. وبالمثل، يمكن أن تأخذ الحالات أشكالاً متنوعة. يمكن أن تكون الحالة
محددة تماماً من خلال الأحاسيس المنخفضة المستوى، مثل قراءات الحساسات
المباشرة، أو يمكن أن تكون أكثر تجريدية وعالية المستوى، مثل الوصف الرمزي
للأشياء في غرفة. قد يتكون جزء من الحالة من ذاكرة للأحاسيس الماضية أو حتى
يكون ذهنياً أو ذاتياً بالكامل. على سبيل المثال، يمكن أن يكون الوكيل في
حالة عدم التأكد من مكان شيء ما، أو من الصدمة التي تعرض لها حديثاً بطريقة
واضحة. وبالمثل، قد تكون بعض الأفعال ذهنية أو حسابية بالكامل. على سبيل
المثال، قد تتحكم بعض الأفعال فيما يختار الوكيل التفكير فيه، أو في مكان
تركيز انتباهه. بشكل عام، يمكن أن تكون الأفعال أي قرارات نريد أن نتعلم
كيفية اتخاذها، والحالات يمكن أن تكون أي شيء نعرفه قد يكون مفيداً في
اتخاذها</span>.

<span dir="rtl">على وجه الخصوص، فإن الحدود بين الوكيل والبيئة عادةً لا
تكون هي نفسها الحدود الفيزيائية لجسم الروبوت أو الحيوان. عادةً ما يتم رسم
الحدود أقرب إلى الوكيل من ذلك. على سبيل المثال، يجب أن تُعتبر المحركات
والروابط الميكانيكية لجهاز الروبوت والأجهزة الحسّية جزءاً من البيئة بدلاً
من كونها جزءاً من الوكيل. بالمثل، إذا طبقنا إطار عمل الـ</span> **MDP**
<span dir="rtl">على شخص أو حيوان، يجب أن تُعتبر العضلات والهيكل العظمي
والأعضاء الحسية جزءاً من البيئة. كذلك، يُفترض أن المكافآت تُحسب داخل
الأجسام الفيزيائية للأنظمة الطبيعية والاصطناعية المتعلمة، ولكن تُعتبر
خارجية بالنسبة للوكيل</span>.

<span dir="rtl">القاعدة العامة التي نتبعها هي أن أي شيء لا يمكن تغييره
بشكل عشوائي من قِبل الوكيل يُعتبر خارجاً عنه، وبالتالي جزءاً من بيئته. لا
نفترض أن كل شيء في البيئة مجهول للوكيل. على سبيل المثال، غالباً ما يعرف
الوكيل الكثير عن كيفية حساب مكافآته كدالة على أفعاله والحالات التي تُؤخذ
فيها. ولكننا دائماً نعتبر حساب المكافآت خارجياً بالنسبة للوكيل لأنه يحدد
المهمة التي يواجهها الوكيل وبالتالي يجب أن يكون خارج قدرته على التغيير
العشوائي. في الواقع، في بعض الحالات قد يعرف الوكيل كل شيء عن كيفية عمل
بيئته ومع ذلك يواجه مهمة تعلم تعزيز صعبة، تماماً كما قد نعرف تماماً كيفية
عمل لغز مثل مكعب روبيك، ولكن لا نتمكن من حله. تمثل حدود الوكيل–البيئة حد
السيطرة المطلقة للوكيل، وليس معرفته</span>.

<span dir="rtl">يمكن تحديد حدود الوكيل–البيئة في أماكن مختلفة لأغراض
مختلفة. في الروبوتات المعقدة، قد يكون هناك العديد من الوكلاء المختلفين
يعملون في نفس الوقت، كل منهم له حدوده الخاصة. على سبيل المثال، قد يقوم
وكيل واحد باتخاذ قرارات عالية المستوى تشكل جزءاً من الحالات التي يواجهها
وكيل آخر أقل مستوى ينفذ القرارات عالية المستوى. في الممارسة العملية، يتم
تحديد حدود الوكيل–البيئة بمجرد اختيار حالات وأفعال ومكافآت معينة،
وبالتالي تحديد مهمة اتخاذ القرارات المحددة ذات الاهتمام</span>.

<span dir="rtl">إطار عمل</span> **MDP** <span dir="rtl">هو تجريد كبير
لمشكلة التعليم الموجه نحو الهدف من التفاعل. يقترح أن تفاصيل الأجهزة
الحسية والذاكرة والتحكم، وأيًا كان الهدف الذي يسعى المرء لتحقيقه، يمكن
تقليل أي مشكلة في تعلم السلوك الموجه نحو الهدف إلى ثلاث إشارات تتنقل
ذهابًا وإيابًا بين الوكيل وبيئته: إشارة لتمثيل الخيارات التي يتخذها الوكيل
(**الأفعال**)، وإشارة لتمثيل الأساس الذي يتم بناءً عليه اتخاذ الخيارات
(**الحالات**)، وإشارة لتعريف هدف الوكيل (**المكافآت**). قد لا يكون هذا
الإطار كافيًا لتمثيل جميع مشاكل التعليم لاتخاذ القرار بشكل مفيد، لكنه
أثبت كفاءته وعموميته في العديد من التطبيقات</span>.

<span dir="rtl">بالطبع، الحالات والأفعال المحددة تختلف كثيرًا من مهمة إلى
أخرى، وكيفية تمثيلها يمكن أن تؤثر بشكل كبير على الأداء. في التعليم
المعزز، كما هو الحال في أنواع أخرى من التعليم، فإن هذه الخيارات
التمثيلية هي في الوقت الحالي أكثر فنًا من كونها علمًا</span>.

<span dir="rtl">في هذا الكتاب، نقدم بعض النصائح والأمثلة حول طرق جيدة
لتمثيل الحالات والأفعال، لكن تركيزنا الأساسي هو على المبادئ العامة لتعلم
كيفية التصرف بعد اختيار التمثيلات</span>.

<span dir="rtl"><u>**مثال** **3.1**:</u> **المفاعل** **الحيوي**</span>
(**Bioreactor**) <span dir="rtl">افترض أن **التعليم** **المعزز**</span>
<span dir="rtl">(</span>reinforcement learning<span dir="rtl">) يُستخدم
لتحديد درجات الحرارة ومعدلات التحريك لحظة بلحظة **لمفاعل**
**حيوي**</span>
<span dir="rtl">(</span>**bioreactor**<span dir="rtl">)</span>
<span dir="rtl">(خزان كبير من المغذيات والبكتيريا يُستخدم لإنتاج مواد
كيميائية مفيدة). قد تكون **الإجراءات**</span> (**actions**)
<span dir="rtl">في هذا التطبيق هي درجات الحرارة المستهدفة ومعدلات
التحريك المستهدفة التي تُمرر إلى أنظمة التحكم منخفضة المستوى التي،
بدورها، تقوم بتفعيل عناصر التسخين والمحركات للوصول إلى الأهداف. من
المحتمل أن تكون **الحالات**</span> (**states**) <span dir="rtl">عبارة عن
قراءات من ثيرموكبل وأجهزة استشعار أخرى، قد تكون مفلترة ومتأخرة، بالإضافة
إلى مدخلات رمزية تمثل المكونات في الخزان والمادة الكيميائية المستهدفة.
قد تكون **المكافآت**</span> (**rewards**) <span dir="rtl">هي قياسات لحظة
بلحظة لمعدل إنتاج المادة الكيميائية المفيدة بواسطة المفاعل الحيوي. لاحظ
أن كل **حالة**</span> (**state**) <span dir="rtl">هنا هي قائمة، أو متجه،
من قراءات المستشعرات والمدخلات الرمزية، وكل **إجراء**</span>
(**action**) <span dir="rtl">هو متجه يتكون من درجة حرارة مستهدفة ومعدل
تحريك. من المعتاد في مهام **التعليم** **المعزز**</span>"
(**reinforcement** **learning**) <span dir="rtl">أن تكون **الحالات**
**والإجراءات** لها مثل هذه التمثيلات المنظمة. من ناحية أخرى، تكون
**المكافآت**</span> (**rewards**) <span dir="rtl">دائمًا أرقامًا
فردية</span>.

<span dir="rtl">**<u>مثال 3.2:</u>** **روبوت** **الالتقاط**
**والتوصيل**</span> (**Pick-and-Place Robot**) <span dir="rtl">افترض
استخدام **التعليم** **المعزز**</span> (**reinforcement** **learning**)
<span dir="rtl">للتحكم في حركة ذراع روبوت في مهمة متكررة لالتقاط الأشياء
ونقلها. إذا كنا نريد تعلم الحركات التي تكون سريعة وسلسة، فسيتعين على
**العميل** **المتعلم**</span>**learning** **agent**)<span dir="rtl">)
التحكم في المحركات مباشرةً والحصول على معلومات ذات تأخير منخفض حول
المواقع والسرعات الحالية للروابط الميكانيكية. قد تكون
**الإجراءات**</span> (**actions**) <span dir="rtl">في هذه الحالة هي
الفولتات المطبقة على كل محرك في كل وصلة، **والحالات**</span>
(**states**) <span dir="rtl">قد تكون أحدث قراءات لزوايا المفاصل
والسرعات. قد تكون **المكافأة**</span> (**reward**)
<span dir="rtl"></span>+1 <span dir="rtl">لكل جسم يتم التقاطه بنجاح
ووضعه. لتشجيع الحركات السلسة، يمكن منح مكافأة صغيرة سلبية في كل خطوة
زمنية كدالة للـ **اهتزازية**</span> **(jerkiness)
<span dir="rtl">اللحظية للحركة</span>.**

<span dir="rtl">**<u>تمرين 3.1:</u>** صمم ثلاث مهام مثال بنفسك تتناسب مع
إطار عمل الـ</span> **MDP**<span dir="rtl">، مع تحديد الحالات والإجراءات
والمكافآت لكل منها. اجعل الأمثلة الثلاثة مختلفة قدر الإمكان عن بعضها
البعض. الإطار عمل مجرد ومرن ويمكن تطبيقه بطرق مختلفة. قم بتوسيع حدوده
بطريقة ما في واحدة على الأقل من أمثلتك</span>.

<span dir="rtl">**<u>تمرين 3.2:</u>** هل إطار عمل الـ</span> **MDP**
<span dir="rtl">كافٍ لتمثيل جميع مهام التعليم الموجه نحو الهدف بشكل مفيد؟
هل يمكنك التفكير في أي استثناءات واضحة؟</span>

### <span dir="rtl">**<u>التمرين 3.3:</u>** اعتبر مسألة القيادة. يمكنك تعريف الإجراءات من حيث **دواسة الوقود** </span>**(accelerator**)<span dir="rtl">، **عجلة القيادة**</span> (**steering** **wheel**)<span dir="rtl">، **والمكابح**</span> (**brake**)<span dir="rtl">، أي حيث يلتقي جسمك بالآلة. أو يمكنك تعريفها بشكل أبعد—مثلاً، حيث يلتقي **الإطار** **بالطريق**</span> (the rubber meets the road)<span dir="rtl">، مع اعتبار إجراءاتك عزم **دوران** **الإطارات**</span> (**tire** **torques**)<span dir="rtl">.</span> <span dir="rtl">أو يمكنك تعريفها بشكل أقرب—مثلاً، حيث يلتقي **دماغك** **بجسمك**</span> (your brain meets your body)<span dir="rtl">، وتكون الإجراءات عبارة عن تقلصات عضلية للتحكم في أطرافك</span> (muscle twitches to control your limbs)<span dir="rtl">.</span> <span dir="rtl">أو يمكنك الذهاب إلى مستوى عالٍ جداً والقول بأن **إجراءاتك** هي **خياراتك** حول أين **تقود** **السيارة**</span> (your choices of where to drive)<span dir="rtl">. ما هو المستوى الصحيح، والمكان الصحيح لرسم الخط بين **الوكيل**</span> (**agent**) <span dir="rtl">**والبيئة**</span> (**environment**)<span dir="rtl">؟ بناءً على ماذا يتم تفضيل موقع معين على آخر؟ هل هناك سبب أساسي لتفضيل موقع معين على آخر، أم أنه اختيار حر؟</span>

**<span dir="rtl"><u>المثال 3.3:</u> الروبوت المخصص لإعادة
التدوير</span>**

<span dir="rtl">يعمل الروبوت المتحرك على جمع علب الصودا الفارغة في بيئة
مكتب. يحتوي على مستشعرات لاكتشاف العلب، وذراع وملقط يمكنهما التقاط العلب
ووضعها في صندوق على متنه؛ ويعمل على بطارية قابلة لإعادة الشحن. يحتوي
نظام التحكم في الروبوت على مكونات لتفسير المعلومات الحسية، وللتنقل،
ولتحكم في الذراع والملقط. يتم اتخاذ القرارات عالية المستوى حول كيفية
البحث عن العلب بواسطة وكيل تعلم معزز استنادًا إلى مستوى شحن البطارية
الحالي. لنأخذ مثالاً بسيطاً، نفترض أنه يمكن تمييز مستويين فقط من الشحن،
مما يتكون مجموعة حالات صغيرة</span> <span dir="rtl">{</span>high ,
low<span dir="rtl">}</span>S=<span dir="rtl">. في كل حالة، يمكن للوكيل
أن يقرر ما إذا كان (1) يبحث بنشاط عن علبة لفترة معينة من الوقت، (2) يظل
ثابتًا وينتظر شخصًا ليحضر له علبة، أو (3) يعود إلى قاعدته المنزلية لشحن
بطاريته. عندما يكون مستوى الطاقة مرتفعًا، فإن الشحن سيكون دائماً غير
منطقي، لذلك لا ندرجه في مجموعة الإجراءات  
لهذه الحالة. لذلك، فإن مجموعات الإجراءات هي</span> A(high)
<span dir="rtl">= {بحث، انتظر}</span>  
<span dir="rtl">و</span>A(low) <span dir="rtl">=</span>}
<span dir="rtl">بحث، انتظر، شحن</span> {

<span dir="rtl">المكافآت تكون صفرًا معظم الوقت، ولكنها تصبح إيجابية عندما
ينجح الروبوت في تأمين علبة فارغة، أو تكون كبيرة وسلبية إذا نفدت البطارية
بالكامل. أفضل طريقة للعثور على العلب هي البحث النشط عنها، ولكن ذلك
يستنفد بطارية الروبوت، في حين أن الانتظار لا يؤدي إلى ذلك. في كل مرة
يكون فيها الروبوت في وضع البحث، هناك احتمال أن تنفد بطاريته. في هذه
الحالة، يجب على الروبوت إيقاف التشغيل وانتظار إنقاذه (مما يؤدي إلى
مكافأة منخفضة). إذا كان مستوى الطاقة مرتفعًا، فإن فترة البحث النشطة يمكن
إكمالها دائمًا دون خطر استنزاف البطارية. فترة البحث التي تبدأ بمستوى طاقة
مرتفع تترك مستوى الطاقة مرتفعًا باحتمالية</span> $`\alpha`$
<span dir="rtl">وتخفضه إلى منخفض باحتمالية</span> – α
<span dir="rtl">1.</span> <span dir="rtl">من ناحية أخرى، فترة البحث التي
تتم عندما يكون مستوى الطاقة منخفضًا تتركه منخفضًا باحتمالية</span>
$`\beta`$ <span dir="rtl">وتستنفد البطارية باحتمالية</span>
–β<span dir="rtl">1.</span> <span dir="rtl">في الحالة الأخيرة، يجب إنقاذ
الروبوت، ثم يتم شحن البطارية مرة أخرى إلى مستوى مرتفع. كل علبة يتم جمعها
بواسطة الروبوت تُعد مكافأة وحدة، في حين أن مكافأة قدرها 3- تنتج عندما
يحتاج الروبوت إلى إنقاذه. دعنا نعتبر أن</span> $`r_{search} > r_{wait}`$
<span dir="rtl"> يرمزان على التوالي إلى العدد المتوقع من العلب التي
سيجمعها الروبوت (وبالتالي المكافأة المتوقعة) أثناء البحث وأثناء
الانتظار. أخيرًا، افترض أنه لا يمكن جمع أي علب خلال العودة إلى المنزل
لشحن البطارية، وأنه لا يمكن جمع أي علب في خطوة تنفد فيها البطارية.  
يكون هذا النظام بعد ذلك عملية ماركوفية محدودة، ويمكننا كتابة احتمالات
الانتقال والمكافآت المتوقعة، مع الديناميات كما هو موضح في الجدول على
اليسار.
َ</span><img src="./media/image11.png" style="width:6.5in;height:1.95764in" />

<span dir="rtl">تلخيص ديناميات عملية ماركوفية محدودة كـ "مخطط
انتقال</span>"

<span dir="rtl">يوجد نوعان من العقد في مخطط الانتقال</span>:

1.  **<span dir="rtl">عقد الحالة</span> (State
    Nodes)**<span dir="rtl">:</span> <span dir="rtl">تمثل كل حالة محتملة
    بعقدة حالة كبيرة مفتوحة الدائرة تحمل اسم الحالة</span>.

2.  **<span dir="rtl">عقد الإجراء</span> (Action
    Nodes)**<span dir="rtl">:</span> <span dir="rtl">تمثل كل زوج
    حالة-إجراء بعقدة إجراء صغيرة صلبة الدائرة تحمل اسم الإجراء ومتصلة
    بخط بعقدة الحالة</span>.

<span dir="rtl">عند البدء من الحالة</span> s <span dir="rtl">واتخاذ
الإجراء</span> $`a`$<span dir="rtl">، يتم الانتقال على طول الخط من
**عقدة الحالة**</span> $`s`$ <span dir="rtl">إلى **عقدة الإجراء**</span>
$`(s,a)`$<span dir="rtl">.</span> <span dir="rtl">ثم تستجيب البيئة
بانتقال إلى **عقدة الحالة** التالية عبر واحدة من الأسهم الخارجة من
**عقدة الإجراء**</span> $`(s,a)`$ <span dir="rtl">كل سهم يمثل
ثلاثي</span> $`(s,\ s',\ a)`$ <span dir="rtl">حيث</span> $`s'`$
<span dir="rtl">هي الحالة التالية، ونعلم السهم باحتمال الانتقال،</span>
$`p(s' \mid s,a)`$<span dir="rtl">، والمكافأة المتوقعة لهذا
الانتقال،</span> $`r(s,a,s')`$ <span dir="rtl">لاحظ أن احتمالات الانتقال
التي تعلم الأسهم الخارجة من **عقدة الإجراء** دائمًا ما يكون مجموعها
1</span>.

**<span dir="rtl"><u>التمرين 3.4:</u></span>**

<span dir="rtl">قدّم جدولًا مشابهًا لذلك في المثال 3.3، ولكن للـ</span>
$`\mathbf{p}\left( \mathbf{s}^{\mathbf{'}}\mathbf{,r}\mid\mathbf{s,a} \right)`$
<span dir="rtl">يجب أن يحتوي على أعمدة لـ</span>
$`\text{s}\mathbf{,}\text{a}\mathbf{,}\mathbf{s}^{\mathbf{'}}\mathbf{,}\text{r}`$
<span dir="rtl">و</span>
$`\mathbf{p}\left( \mathbf{s}^{\mathbf{'}}\mathbf{,r}\mid\mathbf{s,a} \right)`$
<span dir="rtl">وصفًا لكل رباعية</span> (4-tuple) <span dir="rtl">حيث
يكون</span>
$`\mathbf{p}\left( \mathbf{s}^{\mathbf{'}}\mathbf{,r}\mid\mathbf{s,a} \right)\mathbf{> 0}`$

**<u>3.2 <span dir="rtl">الأهداف والمكافآت</span> (Goals and
Rewards)</u>**

<span dir="rtl">في التعليم المعزز، يتم تحديد غرض أو هدف الوكيل من خلال
إشارة خاصة تُسمى المكافأة، والتي تنتقل من البيئة إلى الوكيل. في كل خطوة
زمنية، تكون المكافأة عددًا بسيطًا،</span>
$`\mathbf{R}_{\mathbf{t}}\mathbb{\in R}`$<span dir="rtl">.</span>
<span dir="rtl">بشكل غير رسمي، هدف الوكيل هو تعظيم إجمالي المكافآت التي
يتلقاها. وهذا يعني تعظيم المكافأة ليس بشكل فوري، بل المكافأة التراكمية
على المدى الطويل. يمكننا أن نحدد هذه الفكرة غير الرسمية بوضوح كما
يلي</span>:

**<span dir="rtl">فرضية المكافأة</span> (Reward Hypothesis)**

"<span dir="rtl">أن كل ما نعنيه بالأهداف والغرض يمكن اعتباره بشكل جيد
كتعظيم للقيمة المتوقعة لمجموع المكافآت التراكمية التي يتم تلقيها (والتي
تُسمى المكافأة)</span>."

<span dir="rtl">استخدام إشارة المكافأة لتحديد فكرة الهدف هو أحد أبرز
ميزات التعليم المعزز</span>.

<span dir="rtl">على الرغم من أن صياغة الأهداف من حيث إشارات المكافأة قد
تبدو في البداية محدودة، إلا أنها أثبتت في الممارسة العملية أنها مرنة
وقابلة للتطبيق على نطاق واسع. أفضل طريقة لرؤية ذلك هي من خلال النظر في
أمثلة حول كيفية استخدامها أو كيفية إمكانية استخدامها. على سبيل المثال،
لجعل روبوت يتعلم المشي، قدم الباحثون مكافأة في كل خطوة زمنية تتناسب مع
حركة الروبوت للأمام. في حال جعل روبوت يتعلم كيفية الهروب من متاهة، تكون
المكافأة غالبًا **1−** لكل خطوة زمنية تمر قبل الهروب؛ وهذا يشجع الوكيل
على الهروب بأسرع وقت ممكن. لجعل روبوت يتعلم العثور على علب الصودا
الفارغة وجمعها لإعادة التدوير، يمكن إعطاؤه مكافأة صفرية في معظم الأوقات،
ثم مكافأة **1+** لكل علبة يتم جمعها. قد يرغب المرء أيضًا في منح الروبوت
مكافآت سلبية عندما يصطدم بالأشياء أو عندما يصرخ أحدهم في وجهه. لتعلم
الوكيل لعب الداما أو الشطرنج، المكافآت الطبيعية هي **1+** للفوز، **1−**
للخسارة، و**0** للتعادل ولكل المواقف غير النهائية</span>.

<span dir="rtl">يمكنك ملاحظة ما يحدث في جميع هذه الأمثلة. الوكيل دائمًا
يتعلم تعظيم مكافآته. إذا أردنا منه أن يقوم بشيء ما لنا، يجب علينا تقديم
المكافآت له بطريقة تجعل تعظيمها يؤدي إلى تحقيق أهدافنا أيضًا</span>.

<span dir="rtl">لذا، من الضروري أن تشير المكافآت التي نحددها بدقة إلى ما
نريد إنجازه. بشكل خاص، إشارة المكافأة ليست المكان المناسب لنقل المعرفة
السابقة للوكيل حول كيفية تحقيق ما نريده منه أن يفعله. على سبيل المثال،
يجب أن يُكافأ الوكيل الذي يلعب الشطرنج فقط على فوزه الفعلي، وليس على
تحقيق أهداف فرعية مثل أخذ قطع خصمه أو السيطرة على وسط اللوحة. إذا تم
مكافأة تحقيق هذه الأنواع من الأهداف الفرعية، فقد يجد الوكيل طريقة
لتحقيقها دون الوصول إلى الهدف الحقيقي. على سبيل المثال، قد يجد طريقة
لأخذ قطع الخصم حتى لو كان ذلك على حساب خسارة اللعبة. إشارة المكافأة هي
طريقتك في التواصل مع الروبوت بشأن ما تريد منه تحقيقه، وليس كيفية تحقيق
ذلك</span>.

**<u>3.3 <span dir="rtl">العوائد والحلقات</span> (Returns and
Episodes)</u>**

<span dir="rtl">حتى الآن، ناقشنا هدف التعليم بشكل غير رسمي. قلنا إن هدف
الوكيل هو تعظيم المكافآت التراكمية التي يتلقاها على المدى الطويل. كيف
يمكن تعريف ذلك بشكل دقيق؟ إذا كانت سلسلة المكافآت التي يتم تلقيها بعد
الخطوة الزمنية</span> $`t`$ <span dir="rtl">تُرمز بـ</span>
$`\mathbf{R}_{\mathbf{t + 1}}`$**​,**$`\mathbf{R}_{\mathbf{t + 2}}`$**​,**$`\mathbf{R}_{\mathbf{t + 3}}`$**​,…**<span dir="rtl">،
فما هو الجانب الدقيق من هذه السلسلة الذي نريد تعظيمه؟ بوجه عام، نسعى
لتعظيم</span> "<span dir="rtl">العائد المتوقع (</span>**expected**
**return**<span dir="rtl">)، حيث يُعرّف العائد، الذي يُرمز إليه بـ</span>
$`G_{t}`$​<span dir="rtl">، كوظيفة محددة من سلسلة المكافآت. في أبسط
الحالات، يكون العائد هو مجموع المكافآت</span>:

``` math
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ G_{t} \doteq R_{t + 1} + R_{t + 2} + R_{t + 3} + \cdot \cdot \cdot + R_{T}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.7)
```

<span dir="rtl">حيث</span> $`T`$ <span dir="rtl">هو آخر خطوة زمنية. هذه
الطريقة تكون منطقية في التطبيقات التي يكون فيها مفهوم الخطوة الزمنية
النهائية طبيعيًا، أي عندما يتجزأ التفاعل بين الوكيل والبيئة بشكل طبيعي
إلى تسلسلات فرعية، والتي نسميها **الحلقات**</span>
(**episodes**)<span dir="rtl">، مثل جولات لعبة، أو رحلات عبر متاهة، أو
أي نوع من التفاعلات المتكررة. تنتهي كل حلقة في حالة خاصة تُسمى **الحالة**
**النهائية**</span> (**terminal** **state**)<span dir="rtl">، تليها
إعادة تعيين إلى حالة بدء قياسية أو إلى عينة من توزيع قياسي لحالات البدء.
حتى إذا كنت تعتبر الحلقات تنتهي بطرق مختلفة، مثل الفوز أو الخسارة في
لعبة، تبدأ الحلقة التالية بشكل مستقل عن كيفية انتهاء السابقة. وبالتالي،
يمكن اعتبار جميع الحلقات تنتهي في نفس الحالة النهائية، مع مكافآت مختلفة
للنتائج المختلفة. المهام التي تتضمن حلقات من هذا النوع تُسمى **مهام**
**حلقية**</span> (**episodic** **tasks**)<span dir="rtl">. في المهام
الحلقية، أحيانًا نحتاج إلى تمييز مجموعة من جميع الحالات غير النهائية،
التي تُرمز بـ</span> $`S`$<span dir="rtl">، عن مجموعة جميع الحالات
بالإضافة إلى الحالة النهائية، التي تُرمز بـ</span>
$`S +`$<span dir="rtl">.</span> <span dir="rtl">وقت الانتهاء،</span>
$`T`$<span dir="rtl">، هو متغير عشوائي يتغير عادة من حلقة إلى
أخرى.</span>

<span dir="rtl">من ناحية أخرى، في العديد من الحالات، لا يتجزأ التفاعل
بين الوكيل والبيئة بشكل طبيعي إلى حلقات يمكن تحديدها، بل يستمر بشكل
مستمر بدون حد. على سبيل المثال، سيكون هذا هو الأسلوب الطبيعي لصياغة مهمة
التحكم في عملية مستمرة، أو تطبيق لروبوت ذو عمر طويل. نطلق على هذه المهام
اسم **المهام** **المستمرة**</span> (**continuing**
**tasks**)<span dir="rtl">.</span> <span dir="rtl">صياغة العائد (3.7)
تكون مشكلة في المهام المستمرة لأن خطوة الوقت النهائية ستكون</span>
$`\mathbf{T\  = \infty}`$<span dir="rtl">، والعائد، الذي نحاول تعظيمه،
يمكن أن يكون بدوره غير محدود. (على سبيل المثال، افترض أن الوكيل يتلقى
مكافأة قدرها **1**+ في كل خطوة زمنية.) لذا، في هذا الكتاب، نستخدم عادةً
تعريفًا للعائد يكون أكثر تعقيدًا من الناحية المفاهيمية ولكنه أبسط
رياضيًا</span>.

<span dir="rtl">المفهوم الإضافي الذي نحتاجه هو **الخصم**</span>
(**discounting**)<span dir="rtl">.</span> <span dir="rtl">وفقًا لهذا
النهج، يحاول الوكيل اختيار الإجراءات بحيث يتم تعظيم مجموع المكافآت
المخصومة التي يتلقاها في المستقبل. على وجه الخصوص، يختار</span>
$`\mathbf{A}_{\mathbf{t}}`$ <span dir="rtl"></span>
<span dir="rtl">لتعظيم العائد المتوقع المخصوم</span>:

``` math
G_{t} = R_{t + 1} + \gamma R_{t + 2} + \gamma^{2}R_{t + 3} + \cdot \cdot \cdot
```

``` math
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  = \sum_{k = 0}^{\infty}{\gamma^{k}R_{t + k + 1}}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.8)
```

<span dir="rtl">حيث أن</span> $`\gamma`$ <span dir="rtl">هو
بارامتر،</span> $`0 \leq \gamma \leq 1`$ <span dir="rtl">ويُسمى **معدل
الخصم** </span>**(discount rate)**

**<span dir="rtl">معدل الخصم</span> (discount rate)**
<span dir="rtl">يحدد القيمة الحالية للمكافآت المستقبلية: مكافأة تُستلم
بعد</span> $`k`$ <span dir="rtl">من خطوات الزمن تساوي فقط</span>
$`\mathbf{\gamma}^{\mathbf{k - 1}}`$ <span dir="rtl">من قيمتها إذا كانت
قد استُلمت فورًا. إذا كانت</span> $`\gamma < 1`$ <span dir="rtl">، فإن
المجموع اللانهائي في (3.8) له قيمة محدودة طالما أن تسلسل المكافآت</span>
$`\mathbf{\{}\mathbf{R}_{\mathbf{k}}\mathbf{\}}`$ <span dir="rtl">محدود.
إذا كانت</span> $`\gamma = 0`$<span dir="rtl">، فإن الوكيل يكون قصير
النظر</span> (**myopic**) <span dir="rtl">بحيث يهتم فقط بتعظيم المكافآت
الفورية: هدفه في هذه الحالة هو تعلم كيفية اختيار</span>
$`\mathbf{A}_{\mathbf{t}}`$**​
<span dir="rtl"></span>**<span dir="rtl">بحيث يتم تعظيم فقط</span>
$`\mathbf{R}_{\mathbf{t + 1}}`$ <span dir="rtl">إذا صادف أن كل إجراء
للوكيل يؤثر فقط على المكافأة الفورية وليس على المكافآت المستقبلية أيضًا،
فإن الوكيل قصير النظر يمكن أن يُعظم **(3.8)** من خلال تعظيم كل مكافأة
فورية على حدة. ولكن بشكل عام، التصرف لتعظيم المكافأة الفورية يمكن أن
يقلل من الوصول إلى المكافآت المستقبلية مما يقلل العائد. عندما
يقترب</span> $`\mathbf{\gamma}`$ <span dir="rtl">من 1، فإن هدف العائد
يأخذ المكافآت المستقبلية بعين الاعتبار بشكل أقوى؛ يصبح الوكيل أكثر بعد
نظر. العوائد في خطوات الزمن المتتالية مرتبطة ببعضها البعض بطريقة مهمة
لنظرية وخوارزميات **التعليم المعزز (**</span>**reinforcement
<span dir="rtl"></span>learning<span dir="rtl">).</span>**

``` math
\text{[}G_{t} = R_{t + 1} + \gamma R_{t + 2} + \gamma^{2}R_{t + 3} + \gamma^{3}R_{t + 4} + \cdots\text{]}
```

``` math
= R_{t + 1} + \gamma R_{t + 2} + \gamma^{2}R_{t + 3} + \gamma^{3}R_{t + 4} +
```

``` math
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  = R_{t + 1} + \gamma G_{t + 1}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.9)
```

<span dir="rtl">لاحظ أن هذا ينطبق على جميع خطوات الزمن</span>
$`t < T`$<span dir="rtl">، حتى إذا حدث التوقف في</span>
$`t + 1`$<span dir="rtl">، إذا قمنا بتعريف</span>
$`G_{T} = 0`$<span dir="rtl">.</span> <span dir="rtl">هذا يجعل من السهل
غالبًا حساب العوائد من تسلسلات المكافآت. لاحظ أنه على الرغم من أن العائد
(3.8) هو مجموع عدد لا نهائي من الحدود، إلا أنه لا يزال محدودًا إذا كانت
المكافأة غير صفرية وثابتة— إذا كانت</span> $`\gamma < 1`$.
<span dir="rtl">على سبيل المثال، إذا كانت المكافأة ثابتة 1+، فإن العائد
هو</span>

``` math
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ G_{t} = \sum_{k = 0}^{\infty}\gamma^{k} = \frac{1}{1 - \gamma}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.10)
```

<span dir="rtl">**<u>التمرين 3.5</u>**: المعادلات في القسم 3.1 تتعلق
بالحالة المستمرة وتحتاج إلى تعديل (بشكل طفيف جدًا) لتطبيقها على المهام
الحلقية. أظهر أنك تعرف التعديلات اللازمة من خلال تقديم النسخة المعدلة
من</span> (3.3)<span dir="rtl">.</span>

<img src="./media/image12.png"
style="width:3.45486in;height:1.97431in" /><span dir="rtl">**<u>مثال
3.4:</u>** موازنة العمود</span> (Pole_Balancing)

<span dir="rtl">الهدف في هذه المهمة هو تطبيق قوى على عربة تتحرك على طول
مسار للحفاظ على عمود موصول بالعربة من السقوط: يُقال إن الفشل يحدث إذا سقط
العمود بعد زاوية معينة من الوضع العمودي أو إذا خرجت العربة عن المسار.
يُعاد ضبط العمود إلى الوضع العمودي بعد كل فشل. يمكن معالجة هذه المهمة
كحالة حلقية، حيث تكون الحلقات الطبيعية هي المحاولات المتكررة لموازنة
العمود. المكافأة في هذه الحالة يمكن أن تكون 1+ لكل خطوة زمنية لا يحدث
فيها فشل، بحيث يكون العائد في كل مرة هو عدد الخطوات حتى الفشل. في هذه
الحالة، تعني الموازنة الناجحة للأبد عائدًا لانهائيًا. بدلاً من ذلك، يمكننا
معالجة موازنة العمود كمهمة مستمرة، باستخدام الخصم. في هذه الحالة، ستكون
المكافأة 1− في كل فشل وصفر في جميع الأوقات الأخرى. سيكون العائد في كل
مرة مرتبطًا بـ</span> $`- \gamma^{k}`$<span dir="rtl">، حيث</span> $`k`$
<span dir="rtl">هو عدد خطوات الزمن قبل الفشل. في كلتا الحالتين، يتم
تعظيم العائد من خلال الحفاظ على توازن العمود لأطول فترة ممكنة</span>.

<span dir="rtl">**<u>التمرين 3.6</u>**: افترض أنك عالجت موازنة العمود
كمهمة حلقية ولكنك استخدمت أيضًا الخصم، مع كون جميع المكافآت صفرًا باستثناء
1</span>- <span dir="rtl">عند الفشل. ما هو العائد في كل مرة في هذه
الحالة؟ كيف يختلف هذا العائد عن العائد في صيغة الخصم المستمرة لهذه
المهمة؟</span>

<span dir="rtl">**<u>التمرين 3.7:</u>** تخيل أنك تصمم روبوتًا لتشغيل
متاهة. قررت منح المكافأة 1+ لهروب الروبوت من المتاهة ومكافأة صفرية في
جميع الأوقات الأخرى. يبدو أن المهمة تنقسم بشكل طبيعي إلى حلقات الجولات
المتعاقبة عبر المتاهة لذلك قررت معالجتها كمهمة حلقية، حيث الهدف هو تعظيم
المكافأة الإجمالية المتوقعة (3.7). بعد تشغيل وكيل التعليم لفترة من
الوقت، تجد أنه لا يظهر أي تحسين في الهروب من المتاهة. ما الذي يحدث؟ هل
قمت بتوصيل ما تريده للوكيل بشكل فعال؟</span>

<span dir="rtl"><u>التمرين 3.8</u>: افترض أن</span> $`\gamma = 0.5\ `$
<span dir="rtl">وتسلسل المكافآت التالي يتم تلقيه</span>:$`,`$

$`R_{2} = 2,R_{3} = 6,R_{4} = 3,و{\ \ R}_{5} = 2،\ مع\ T = 5.\ \ ماهي{\ \ G}_{0}،G_{1}،\ldots ،G_{5}`$
<span dir="rtl">تلميح: اعمل إلى الوراء</span>.

<span dir="rtl">**<u>التمرين 3.9:</u>** افترض أن</span>
$`\gamma = 0.9\ `$ <span dir="rtl">وتسلسل المكافآت هو</span>
$`R_{1} = 2`$ <span dir="rtl"></span> <span dir="rtl">يليه تسلسل لانهائي
من المكافآت بقيمة 7. ما هي</span> $`G_{1}`$ <span dir="rtl">و</span>
$`G_{0}`$

<span dir="rtl">**<u>التمرين 3.10:</u>** أثبت المساواة الثانية في</span>
(3.10)<span dir="rtl">.</span>

**<u>3.4 <span dir="rtl">التدوين الموحد للمهام الحلقية والمستمرة</span>
<span dir="rtl">(</span>Unified Notation for Episodic and
<span dir="rtl"></span>Continuing Tasks<span dir="rtl">)</span></u>**

<span dir="rtl">في القسم السابق وصفنا نوعين من مهام التعليم المعزز،
أحدهما حيث يتفاعل الوكيل مع البيئة بطريقة تنكسر بشكل طبيعي إلى تسلسل من
الحلقات المنفصلة</span> (episodic tasks)<span dir="rtl">، والآخر حيث لا
يحدث ذلك</span> <span dir="rtl">الحلقات المستمرة</span> (continuing
tasks)<span dir="rtl">.</span> <span dir="rtl">الحالة الأولى هي أسهل
رياضيًا لأن كل إجراء يؤثر فقط على عدد محدود من المكافآت التي تُستلم بعد
ذلك خلال الحلقة. في هذا الكتاب نعتبر أحيانًا نوعًا واحدًا من المشكلات
وأحيانًا الأخرى، ولكن غالبًا ما نستخدم كلا النوعين. لذا من المفيد وضع
تدوين واحد يمكننا من التحدث بدقة عن كلا الحالتين في وقت واحد</span>.

<span dir="rtl">للتحدث بدقة عن المهام الحلقية يتطلب بعض التدوين الإضافي.
بدلاً من تسلسل طويل واحد من خطوات الزمن، نحتاج إلى النظر في سلسلة من
الحلقات، كل منها يتكون من تسلسل محدود من خطوات الزمن. نقوم بترقيم خطوات
الزمن في كل حلقة بدءًا من الصفر. لذلك، نحتاج إلى الإشارة ليس فقط
إلى</span> $`S_{t}`$​<span dir="rtl">، التمثيل الحال في الوقت</span>
t<span dir="rtl">، ولكن أيضًا إلى</span> $`S_{t,i}`$
<span dir="rtl">التمثيل الحال في الوقت</span> $`t`$ <span dir="rtl">من
الحلقة</span> $`i`$ <span dir="rtl"></span>) <span dir="rtl">وبالمثل
لـ</span>
$`A_{t,i},R_{t,i},\pi_{t,i},T_{i},\text{etc.}`$<span dir="rtl">،
وغيرها). ومع ذلك، يتضح أنه عندما نناقش المهام الحلقية، نادراً ما نحتاج
إلى التمييز بين الحلقات المختلفة. نحن نعتبر تقريبًا حلقة معينة، أو نذكر
شيئًا صحيحًا لجميع الحلقات. بناءً على ذلك، في الممارسة العملية، نميل تقريبًا
دائمًا إلى تجاوز التدوين قليلًا بإسقاط الإشارة الصريحة إلى رقم الحلقة. أي
أننا نكتب</span> $`{\ \ S}_{t}`$<span dir="rtl">للإشارة إلى</span>
$`S_{t,i}`$ <span dir="rtl">، وهكذا</span>.

<span dir="rtl">نحتاج إلى قاعدة أخرى للحصول على تدوين واحد يغطي كل من
المهام الحلقية والمستمرة. لقد عرفنا العائد كجمع لعدد محدود من المصطلحات
في حالة واحدة (**3.7**) وكجمع لعدد غير محدود من المصطلحات في الحالة
الأخرى (**3.8**). يمكن توحيد هاتين الحالتين من خلال اعتبار انتهاء الحلقة
كدخول إلى حالة خاصة ماصة تنتقل فقط إلى نفسها وتولد فقط مكافآت بقيمة صفر.
على سبيل المثال، انظر إلى مخطط انتقال الحالة</span>:

<img src="./media/image13.png" style="width:6.5in;height:1.04167in" />

<span dir="rtl">هنا، يمثل المربع الصلب الحالة الخاصة الماصة التي تتوافق
مع نهاية الحلقة. بدءًا من</span> $`S_{0}`$​<span dir="rtl">، نحصل على
تسلسل المكافآت</span> $`+ 1، + 1، + 1،\ 0،\ 0،\ 0،\ ....`$
<span dir="rtl">بجمع هذه المكافآت، نحصل على نفس العائد سواء جمعنا
المكافآت على مدار أول</span> $`T`$ <span dir="rtl">مكافآت (هنا</span>
$`T = 3`$<span dir="rtl">) أو على مدار التسلسل الكامل غير المحدود. وهذا
يظل صحيحًا حتى إذا أدخلنا خاصية الخصم. وبالتالي، يمكننا تعريف العائد،
بشكل عام، وفقًا لـ (3.8)، باستخدام قاعدة إسقاط أرقام الحلقات عندما لا
تكون ضرورية، وإدراج إمكانية أن يكون</span>
$`\ \gamma = 1`$<span dir="rtl">إذا ظل المجموع محددًا (على سبيل المثال،
لأن جميع الحلقات تنتهي). بدلاً من ذلك، يمكننا كتابة</span>

``` math
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ G_{t} = \sum_{k = t + 1}^{T}{\gamma^{k - t - 1}R_{k}}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.11)
```

<span dir="rtl">بما في ذلك إمكانية أن يكون</span>
$`T = 1`$<span dir="rtl">أو</span> $`\gamma = 1`$<span dir="rtl">(لكن
ليس كلاهما معًا). نستخدم هذه القواعد طوال بقية الكتاب لتبسيط التدوين
وللتعبير عن التوازي الوثيق بين المهام الحلقية والمستمرة. (في وقت لاحق،
في الفصل 10، سنقدم صيغة تكون مستمرة وغير مخفضة في الوقت نفسه).</span>

**<u>3.5 <span dir="rtl">السياسات ودوال القيمة</span> (Policies and
Value Functions)</u>**

<span dir="rtl">تتضمن معظم خوارزميات التعليم المعزز</span>
(Reinforcement Learning) <span dir="rtl">تقدير دوال القيمة—دوال
للحالات</span> (States) (<span dir="rtl">أو للحالات–الإجراءات</span>
(State–Action Pairs)) <span dir="rtl">التي تقدر مدى جودة كون الوكيل في
حالة معينة (أو مدى جودة تنفيذ إجراء معين في حالة معينة). يُعرَف مفهوم "مدى
الجودة" هنا من حيث المكافآت المستقبلية</span> (Future Rewards)
<span dir="rtl">التي يمكن توقعها، أو بعبارة أدق، من حيث العائد
المتوقع</span> (Expected Return). <span dir="rtl">بالطبع، المكافآت التي
يمكن أن يتوقع الوكيل تلقيها في المستقبل تعتمد على الإجراءات التي
سيتخذها. بناءً على ذلك، تُعرَف دوال القيمة بالنسبة لطرق معينة في التصرف،
تُسمى السياسات</span> (Policies)<span dir="rtl">.</span>

<span dir="rtl">بشكل رسمي، السياسة هي تعيين من الحالات إلى احتمالات
اختيار كل إجراء ممكن. إذا كان الوكيل يتبع السياسة</span> $`\pi`$
<span dir="rtl">في الوقت</span> $`t`$<span dir="rtl">، فإن</span>
$`\pi(a \mid s)`$ <span dir="rtl">هي احتمال أن يكون</span> $`At = a`$
<span dir="rtl">إذا كانت</span> $`S_{t = s}`$ <span dir="rtl">.</span>
<span dir="rtl">مثل</span> $`p،\ \pi`$ <span dir="rtl">هي دالة عادية؛
العلامة "</span>$`\ |\ `$<span dir="rtl">" في وسط</span>
$`\mathbf{\ \pi(a \mid s)}`$<span dir="rtl">تذكر فقط بأنها تعرف توزيع
احتمالات على  
</span>$`\mathbf{a \in A(s)}`$ <span dir="rtl">لكل</span>
$`\mathbf{s \in S}`$ <span dir="rtl">تحدد طرق التعليم المعزز كيفية تغيير
سياسة الوكيل نتيجة لتجربته</span>.

<span dir="rtl">**التمرين 3.11** إذا كانت الحالة الحالية هي</span>
$`S_{t}`$ <span dir="rtl"></span>​<span dir="rtl">، وتم اختيار الإجراءات
وفقًا لسياسة احتمالية</span> $`\mathbf{\pi}`$<span dir="rtl">، فما هو
تو</span> $`\mathbf{R}_{\mathbf{t + 1}}`$​ <span dir="rtl">من حيث</span>
$`\mathbf{\pi}`$ <span dir="rtl">والدالة ذات الأربع معطيات</span>
$`p\ (3.2)`$<span dir="rtl">؟</span>

<span dir="rtl">دالة القيمة</span> (Value Function)
<span dir="rtl">لحالة</span> s <span dir="rtl">تحت سياسة</span>
π<span dir="rtl">، والتي يُرمز لها بـ</span> $`v_{\pi}`$
<span dir="rtl"></span>(s)<span dir="rtl">، هي العائد المتوقع عند البدء
في</span> s <span dir="rtl">واتباع</span> π <span dir="rtl">بعدها.
بالنسبة لمشاكل اتخاذ القرار</span> Markovian (MDPs)<span dir="rtl">،
يمكن تعريف</span> $`v_{\pi}`$ <span dir="rtl"></span>
<span dir="rtl">رسميًا بواسطة</span>

``` math
v^{\pi}(s) = E^{\pi}\left\lbrack G_{t}\mid S_{t} = s \right\rbrack = E^{\pi}\left\lbrack \sum_{k = 0}^{\infty}{\gamma^{k}R_{t + k + 1}}\mid S_{t} = s \right\rbrack,\text{ for all }s \in S\ 
```

``` math
\mathbf{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.12)}
```

<span dir="rtl">حيث
يُشير</span>$`\ E\pi\ \lbrack \cdot \rbrack\ `$<span dir="rtl">إلى القيمة
المتوقعة لمتغير عشوائي بشرط أن يتبع الوكيل السياسة</span>
$`\pi`$<span dir="rtl">، و</span>$`t`$ <span dir="rtl">هو أي خطوة زمنية.
لاحظ أن قيمة الحالة النهائية، إن وجدت، تكون دائمًا صفرًا. نُسمِّي
الدالة</span> $`v_{\pi}`$ <span dir="rtl"></span> <span dir="rtl">دالة
قيمة الحالة</span> (State-Value Function) <span dir="rtl">للسياسة</span>
$`\pi`$

<span dir="rtl">بالمثل، نُعرِّف قيمة اتخاذ إجراء</span> $`a`$
<span dir="rtl">في حالة</span> $`s`$ <span dir="rtl">تحت سياسة</span>
$`\pi`$<span dir="rtl">، والتي يُرمز لها بـ</span> $`q_{\pi}`$
$`(s,a)`$<span dir="rtl">، كالعائد المتوقع عند البدء من</span>
$`s`$<span dir="rtl">، اتخاذ الإجراء</span> $`a`$<span dir="rtl">، ومن
ثم اتباع السياسة</span> $`\pi`$<span dir="rtl">:</span>

``` math
q_{\pi}(s,a) \doteq E_{\pi}\left\lbrack G_{t} \middle| S_{t} = s,A_{t} = a \right\rbrack = E_{\pi}\left\lbrack \sum_{k = 0}^{\infty}{\gamma^{k}R_{t + k + 1}} \middle| S_{t} = s,A_{t} = a \right\rbrack.
```
(3.13)

<span dir="rtl">نُسمِّي</span> $`q_{\pi}`$ <span dir="rtl"></span>
<span dir="rtl">دالة قيمة الإجراء</span> (Action-Value Function)
<span dir="rtl">للسياسة</span> $`\pi`$<span dir="rtl">.</span>

**<u><span dir="rtl">التمرين 3.12</span>:</u>** <span dir="rtl">اعطِ
معادلة لدالة</span> $`v_{\pi}`$ <span dir="rtl"></span>
<span dir="rtl">من حيث</span> $`q_{\pi}`$ <span dir="rtl"></span>
<span dir="rtl">و</span> $`\pi`$<span dir="rtl">.</span>

**<span dir="rtl"><u>التمرين 3.13</u></span>**: <span dir="rtl">اعطِ
معادلة لدالة</span> $`q_{\pi}`$ <span dir="rtl"></span>
<span dir="rtl">من حيث</span> $`v_{\pi}`$ <span dir="rtl"></span>
<span dir="rtl">والدالة ذات الأربع معطيات</span>
$`p`$<span dir="rtl">.</span>

<span dir="rtl">يمكن تقدير دوال القيمة</span> $`v_{\pi}`$
<span dir="rtl"></span> <span dir="rtl">و</span> $`q_{\pi}`$
<span dir="rtl"></span> <span dir="rtl">من التجربة. على سبيل المثال، إذا
كان الوكيل يتبع السياسة</span> $`\pi`$ <span dir="rtl">ويحافظ على متوسط،
لكل حالة يتم مواجهتها، للعوائد الفعلية التي تلت تلك الحالة، فإن المتوسط
سيتقارب إلى قيمة الحالة</span> $`v_{\pi}`$ (s) <span dir="rtl">كلما
اقترب عدد المرات التي يتم فيها مواجهة تلك الحالة من اللانهاية. إذا تم
الاحتفاظ بمتوسطات منفصلة لكل إجراء يتم اتخاذه في كل حالة، فإن هذه
المتوسطات ستتقارب أيضًا إلى قيم الإجراءات</span>(s,a)
<span dir="rtl"></span>$`q_{\pi}`$<span dir="rtl">.</span>
<span dir="rtl">نُسمِّي طرق التقدير من هذا النوع "طرق مونت كارلو
(</span>Monte Carlo Methods<span dir="rtl">)</span>
<span dir="rtl">لأنها تشمل التوسيع عبر العديد من العينات العشوائية
للعوائد الفعلية</span>.

<span dir="rtl">تُقدَّم هذه الأنواع من الطرق في الفصل الخامس. بالطبع، إذا
كانت هناك العديد من الحالات، فقد لا يكون من العملي الاحتفاظ بمتوسطات
منفصلة لكل حالة على حدة. بدلاً من ذلك، سيتعين على الوكيل الحفاظ
على</span> $`v_{\pi}`$ <span dir="rtl"></span> <span dir="rtl">و</span>
$`q_{\pi}`$ <span dir="rtl"></span> <span dir="rtl">كدوال بارامتر
(بمعلمات أقل من عدد الحالات) وضبط المعلمات لتتوافق بشكل أفضل مع العوائد
الملاحظة. يمكن أن يُنتِج ذلك أيضًا تقديرات دقيقة، على الرغم من أن الكثير
يعتمد على طبيعة مُقَرِّب الدالة البارامترية. تُناقش هذه الاحتمالات في الجزء
الثاني من الكتاب</span>.

<span dir="rtl">الخاصية الأساسية لدوال القيمة المستخدمة في التعليم
المعزز وبرمجة الديناميات هي أنها تلبي علاقات تكرارية مشابهة لتلك التي
حددناها بالفعل للعودة (3.9). بالنسبة لأي سياسة</span> π
<span dir="rtl">وأي حالة</span> s<span dir="rtl">، فإن شرط التناسق
التالي ينطبق بين قيمة</span> s <span dir="rtl">وقيمة حالاتها التالية
الممكنة</span>:

``` math
v\pi(s) = E\pi\left\lbrack Gt \middle| St = s \right\rbrack
```

``` math
v\pi(s) = E\pi\left\lbrack Rt + 1 + \gamma Gt + 1 \middle| St = s \right\rbrack
```

``` math
= \sum_{}^{}a\pi\left( a \middle| s \right)\sum_{}^{}{s'\sum_{}^{}rp\left( s',r \middle| s,a \right)\left\lbrack r + \gamma E\pi\left\lbrack Gt + 1 \middle| St + 1 = s' \right\rbrack \right\rbrack}
```

``` math
= \sum_{}^{}a\pi\left( a \middle| s \right)\sum_{}^{}s',rp\left( s',r \middle| s,a \right)\left\lbrack r + \gamma v\pi\left( s' \right) \right\rbrack,\ \ \ forall\ \ \ \ s \in S
```
(3.14)

<img src="./media/image14.png"
style="width:1.80833in;height:1.58194in" /><span dir="rtl">حيث من
المفهوم ضمنيًا أن الأفعال</span> a <span dir="rtl">يتم اختيارها من
المجموعة</span> A(s)<span dir="rtl">، وأن الحالات التالية</span> s′
<span dir="rtl">يتم اختيارها من المجموعة</span> S<span dir="rtl">  
(أو من</span> S+ <span dir="rtl">في حالة مشكلة مَقطعية)، وأن
المكافآت</span> r <span dir="rtl">يتم اختيارها من المجموعة</span>
R<span dir="rtl">.</span> <span dir="rtl">لاحظ أيضًا كيف في المعادلة
الأخيرة قمنا بدمج المجموعتين، واحدة على جميع قيم</span> s′
<span dir="rtl">والأخرى على جميع قيم</span> r<span dir="rtl">، في مجموعة
واحدة على جميع القيم الممكنة لكليهما. نستخدم هذا النوع من الدمج غالبًا
لتبسيط الصيغ. لاحظ كيف يمكن قراءة التعبير النهائي بسهولة كقيمة متوقعة.
فهو في الواقع عبارة عن مجموع لجميع القيم الثلاثة،</span> a
<span dir="rtl">و</span>s′ <span dir="rtl">و</span>
r<span dir="rtl">.</span> <span dir="rtl">لكل مجموعة، نقوم بحساب
احتمالها،</span> π(a∣s) p(s′,r∣s,a)<span dir="rtl">، ونوزن الكمية بين
الأقواس بواسطة هذا الاحتمال، ثم نجمع على جميع الاحتمالات للحصول على قيمة
متوقعة</span>. <span dir="rtl">المعادلة (3.14) هي معادلة بيلمان
لـ</span> $`v_{\pi}`$<span dir="rtl">.</span> <span dir="rtl">إنها تعبر
عن علاقة بين قيمة حالة ما وقيم حالاتها التالية. فكر في النظر إلى الأمام
من حالة إلى حالاتها التالية الممكنة، كما هو موضح في الرسم البياني على
اليمين. كل دائرة مفتوحة تمثل حالة، وكل دائرة صلبة تمثل زوج حالة-إجراء.
بدءًا من الحالة</span> s<span dir="rtl">، العقدة الجذرية في الأعلى، يمكن
للوكيل أن يتخذ أيًا من مجموعة من الأفعال—ثلاثة منها موضحة في الرسم
البياني—بناءً على سياسته</span> $`\mathbf{\pi}`$<span dir="rtl">.</span>
<span dir="rtl">من كل من هذه الأفعال، يمكن للبيئة أن تستجيب بإحدى عدة
حالات تالية،</span> $`\mathbf{s'}`$ <span dir="rtl">(تم عرض اثنين منها
في الشكل)، بالإضافة إلى مكافأة،</span>$`\mathbf{r}`$<span dir="rtl">،
اعتمادًا على دينامياتها التي تحددها الدالة</span>
$`\mathbf{p}`$<span dir="rtl">.</span> <span dir="rtl">معادلة بيلمان
(3.14) تأخذ المتوسط على جميع الاحتمالات، مع وزن كل منها بحسب احتمال
حدوثه. تنص على أن قيمة الحالة البداية يجب أن تكون مساوية لقيمة الحالة
التالية المتوقعة (بعد التخصيم)، بالإضافة إلى المكافأة المتوقعة على طول
الطريق. دالة القيمة</span> $`\mathbf{v}_{\mathbf{\pi}}`$
<span dir="rtl">هي الحل الفريد لمعادلة بيلمان هذه. سنعرض في الفصول
القادمة كيف تشكل هذه المعادلة أساسًا لعدد من الطرق لحساب أو تقريبه أو
تعلمه. نسمي الرسوم البيانية مثل تلك الموضحة أعلاه "رسوم بيانية للنسخ
الاحتياطي" لأنها تصور العلاقات التي تشكل أساس عمليات التحديث أو النسخ
الاحتياطي التي هي جوهر طرق التعليم المعزز. تقوم هذه العمليات بنقل
معلومات القيمة إلى الحالة (أو زوج حالة-إجراء) من حالاتها التالية (أو
أزواج حالة-إجراء). نستخدم الرسوم البيانية للنسخ الاحتياطي في جميع أنحاء
الكتاب لتوفير ملخصات رسومية للخوارزميات التي نناقشها. (لاحظ أنه، على عكس
الرسوم البيانية الانتقالية، فإن العقد الحالة في الرسوم البيانية للنسخ
الاحتياطي لا تمثل بالضرورة حالات مميزة؛ على سبيل المثال، قد تكون الحالة
هي نفسها حالتها التالية)</span>.

**<span dir="rtl">مثال</span> 3.5<span dir="rtl">:</span>
<span dir="rtl">جريدورلد</span>
<span dir="rtl">(</span>Gridworld<span dir="rtl">) </span>**

<span dir="rtl">يوضح الشكل 3.2 (يسار) تمثيلاً لشبكة</span> **gridworld**
<span dir="rtl">(جريدورلد) لنموذج</span> **MDP** <span dir="rtl">(نموذج
ماركوف القرار النهائي) بسيط ونهائي. تمثل الخلايا في الشبكة</span>
**states** <span dir="rtl">(حالات) البيئة. في كل خلية، هناك أربع</span>
**actions <span dir="rtl"></span>**<span dir="rtl">(إجراءات) ممكنة:
الشمال، الجنوب، الشرق، والغرب، والتي تؤدي بشكل حتمي إلى تحريك</span>
**agent <span dir="rtl"></span>**<span dir="rtl">(الوكيل) خطوة واحدة في
الاتجاه المناسب على الشبكة</span> **actions**
<span dir="rtl">(الإجراءات) التي قد تُخرج</span> **agent
<span dir="rtl"></span>**<span dir="rtl">(الوكيل) خارج الشبكة تترك موقعه
دون تغيير، ولكنها تمنحه أيضًا</span> **reward**
<span dir="rtl">(مكافأة)  
قدرها 1-. أما</span> **actions** <span dir="rtl">(الإجراءات) الأخرى
فتنتج</span> **reward** <span dir="rtl">(مكافأة) قدرها 0، باستثناء تلك
التي تحرك</span> **agent** <span dir="rtl">(الوكيل) خارج</span>
**special** **states** (<span dir="rtl">الحالات الخاصة</span>
$`\mathbf{A}^{\mathbf{'}}`$<span dir="rtl">و</span>$`\mathbf{\ \ B}^{\mathbf{'}}`$<span dir="rtl">من</span>
**state**
<span dir="rtl">الحالة</span>$`\mathbf{A}^{\mathbf{'}}`$<span dir="rtl">،
تؤدي جميع</span> **actions** <span dir="rtl">(الإجراءات) إلى</span>
**reward** <span dir="rtl">(**مكافأة**) قدرها 10+ وتأخذ</span> **agent**
<span dir="rtl">(الوكيل) إل</span>$`\mathbf{A}^{\mathbf{'}}`$
<span dir="rtl">من</span> **state**
<span dir="rtl">**الحالة**</span>$`\mathbf{B}^{\mathbf{'}}\ `$<span dir="rtl">،
تؤدي جميع</span> **actions** <span dir="rtl">(**الإجراءات**) إلى</span>
**reward** <span dir="rtl">(**مكافأة**) قدرها 5+ وتأخذ</span> **agent
<span dir="rtl"></span>**<span dir="rtl">(**الوكيل**) إلى</span>
$`\mathbf{B}^{\mathbf{'}}`$

<img src="./media/image15.png"
style="width:6.26806in;height:1.97847in" />

**3.2<span dir="rtl">:</span> Gridworld example <span dir="rtl">(مثال
جريدورلد)</span>**<span dir="rtl">:</span> "exceptional reward dynamics
<span dir="rtl">(ديناميات المكافأة الاستثنائية) (يسار)
و</span>state-value function <span dir="rtl">(دالة قيمة الحالة)
لـ</span> equiprobable <span dir="rtl"></span>**random** **policy**
<span dir="rtl">(السياسة العشوائية المتساوية الاحتمال) (يمين)</span>.

<span dir="rtl">افترض أن</span> agent <span dir="rtl">(الوكيل) يختار
جميع</span> actions <span dir="rtl">(الإجراءات) الأربع باحتمال متساوٍ في
جميع</span> **states** <span dir="rtl">(الحالات). يوضح الشكل 3.2 (يمين)
"دالة القيمة</span> (value function) $`\mathbf{v}_{\mathbf{\pi}}`$
<span dir="rtl">لهذه **السياسة**</span> (policy) <span dir="rtl">في حالة
**المكافأة** **المخفضة**</span> (discounted reward)
<span dir="rtl">حيث</span> $`\gamma = 0.9`$ <span dir="rtl">تم حساب هذه
**دالة** **القيمة**</span> <span dir="rtl">(</span>value
<span dir="rtl"></span>function<span dir="rtl">) عن طريق حل نظام
المعادلات الخطية</span> (linear equations)"
(3.14)<span dir="rtl">.</span> <span dir="rtl">لاحظ القيم السلبية بالقرب
من الحافة السفلية؛ هذه نتيجة للاحتمالية العالية للوصول إلى حافة الشبكة
هناك في ظل **السياسة** **العشوائية**</span> (random
policy)<span dir="rtl">.</span> <span dir="rtl">تُعتبر</span> **state**
<span dir="rtl">(الحالة)</span> A <span dir="rtl">أفضل</span> **state**
<span dir="rtl">(حالة) تحت هذه **السياسة**</span>
(policy)<span dir="rtl">، ولكن العائد المتوقع لها أقل من 10، وهو
"مكافأتها الفورية</span> <span dir="rtl">(</span>immediate
reward<span dir="rtl">)، لأن</span> **agent** <span dir="rtl">(الوكيل)
ينتقل من</span> $`A`$ <span dir="rtl">إلى</span> $`A'`$<span dir="rtl">،
ومن هناك من المحتمل أن يصطدم بحافة الشبكة. أما</span> **state**
<span dir="rtl">(الحالة)، فهي ذات قيمة أكبر من 5، مكافأتها
الفورية</span> (immediate reward)<span dir="rtl">، لأن</span> agent
<span dir="rtl">(الوكيل) ينتقل من</span> $`B`$
<span dir="rtl">إلى</span> $`B'`$ <span dir="rtl">، والتي لديها قيمة
إيجابية. ومن</span> $`{\ \ B}'\ `$<span dir="rtl">يتم تعويض **العقوبة**
**المتوقعة**</span> <span dir="rtl">(</span>expected
<span dir="rtl"></span>penalty<span dir="rtl">) (المكافأة السلبية)
لاحتمالية الاصطدام بحافة الشبكة بشكل أكبر من خلال العائد المتوقع</span>
(expected gain) <span dir="rtl">لاحتمالية الانتقال إلى</span> "$`A`$"
<span dir="rtl">أو</span> "$`B`$"<span dir="rtl">.</span>

**<span dir="rtl">(تمرين3.14)</span>**  
<span dir="rtl">يجب أن تكون "معادلة بيلمان</span> (Bellman equation)"
(3.14) <span dir="rtl">صحيحة لكل</span> "state <span dir="rtl">(حالة)"
من أجل "دالة القيمة</span> (value function)" $`v_{\pi}`$
<span dir="rtl">الموضحة في الشكل 3.2 (يمين) من مثال 3.5. أظهر بشكل عددي
أن هذه المعادلة تنطبق على "الحالة المركزية</span> (center
state)"<span dir="rtl">، والتي تُقدر قيمتها بـ 0.7+، بالنسبة إلى "جيرانها
الأربعة</span> (four neighboring states)"<span dir="rtl">، الذين تقدر
قيمهم بـ 2.3+، 0.4+،</span>  
<span dir="rtl">0.4-، و0.7+. (هذه الأرقام دقيقة إلى منزلة عشرية واحدة
فقط)</span>.

**<span dir="rtl">(تمرين 3.15)</span>**

<span dir="rtl">في مثال **الجيدور**</span>
(**gridworld**)<span dir="rtl">، تكون "المكافآت</span> (rewards)"
<span dir="rtl">إيجابية عند تحقيق الأهداف، وسلبية عند الاصطدام بحافة
العالم، وصفرًا في باقي الأوقات. هل تعتبر إشارات هذه "المكافآت</span>
(rewards) <span dir="rtl">مهمة، أم أن الفواصل الزمنية بينها هي المهمة
فقط؟ أثبت باستخدام المعادلة (3.8) أن إضافة ثابت</span> $`c`$
<span dir="rtl">لجميع "المكافآت</span> (rewards) <span dir="rtl">تضيف
ثابتًا</span> $`\mathbf{v}_{\mathbf{c}}`$ <span dir="rtl"></span>​
<span dir="rtl">إلى قيم جميع</span> states <span dir="rtl">(الحالات)،
وبالتالي لا تؤثر على القيم النسبية لأي</span> states
<span dir="rtl">(حالات)" تحت أي سياسات</span>
(policies)<span dir="rtl">.</span> <span dir="rtl">ما هو</span>
$`\mathbf{v}_{\mathbf{c}}`$ <span dir="rtl">من حيث</span> $`c`$
<span dir="rtl">و</span> $`\gamma`$<span dir="rtl">؟</span>

**<span dir="rtl">(تمرين 3.16)</span>**  
<span dir="rtl">الآن فكر في إضافة ثابت</span> $`c`$
<span dir="rtl">لجميع المكافآت</span> (rewards) <span dir="rtl">في مهمة
إيبسودية</span> (episodic task)<span dir="rtl">، مثل الجري في المتاهة.
هل سيكون لهذا أي تأثير، أم أنه سيترك المهمة دون تغيير كما في المهمة
المستمرة المذكورة أعلاه؟ لماذا أو لماذا لا؟ قدم مثالاً</span>.

<img src="./media/image16.png"
style="width:2.80556in;height:3.55833in" />**<span dir="rtl">مثال3.6:
الغولف (</span>Golf<span dir="rtl">)</span>**

<span dir="rtl">لتصميم لعب حفرة من الجولف كمهمة تعليم معزز</span>
(reinforcement learning)<span dir="rtl">، نحتسب عقوبة (مكافأة سلبية)
قدرها 1- لكل ضربة حتى نضع الكرة في الحفرة. الحالة هي موقع الكرة. قيمة
الحالة هي السالب لعدد الضربات اللازمة للوصول إلى الحفرة من ذلك الموقع.
"إجراءاتنا</span> (actions) <span dir="rtl">هي كيفية توجيه وضرب الكرة،
بالطبع، وأي</span> club <span dir="rtl">(عصا) نختار. لنفترض أننا نأخذ
الأولى كمعطى وننظر فقط في اختيار</span> club <span dir="rtl">(العصا)،
والتي نفترض أنها إما</span> putter <span dir="rtl">(عصا البوت) أو</span>
driver <span dir="rtl">(عصا القيادة). الجزء العلوي من الشكل 3.3 يظهر
دالة القيمة</span> <span dir="rtl">(</span>state-value
<span dir="rtl"></span>function<span dir="rtl">)</span>
<span dir="rtl">المحتملة،</span> $`v_{putt}(s)`$ <span dir="rtl">،
للـ</span> policy <span dir="rtl">(السياسة) التي تستخدم دائمًا عصا البوت.
الحالة النهائية</span> **in-the-hole** <span dir="rtl">(**في**
**الحفرة**) لها قيمة 0. من أي مكان على **المنطقة** **الخضراء**</span>
(**green** **zone**)<span dir="rtl">، نفترض أننا نستطيع القيام بضربة
بوت؛ هذه الحالات لها قيمة 1-. خارج **المنطقة** **الخضراء**</span>
(**green** **zone**) <span dir="rtl">لا يمكننا الوصول إلى الحفرة بالبوت،
والقيمة تكون أكبر. إذا كان بإمكاننا الوصول إلى "المنطقة الخضراء</span>
(**green**) <span dir="rtl">من حالة ما بواسطة البوت، فإن تلك الحالة يجب
أن يكون لها قيمة أقل بمقدار واحد من قيمة **المنطقة** **الخضراء**</span>
(**green**)<span dir="rtl">، أي 2-. لتبسيط الأمر، دعنا نفترض أننا نستطيع
القيام بضربات بوت بدقة وبشكل حتمي، ولكن بنطاق محدود. هذا يعطينا الخط
المحدد بوضوح والمسمى 2- في الشكل؛ جميع المواقع بين هذا الخط و**المنطقة**
**الخضراء**</span> (**green**) <span dir="rtl">تتطلب بالضبط ضربتين
لإكمال الحفرة. وبالمثل، أي موقع ضمن نطاق البوت لخط 2- يجب أن يكون له
قيمة 3-، وهكذا للحصول على جميع الخطوط المحددة الموضحة في الشكل. البوت لا
يخرجنا من **الفخاخ** **الرملية**</span> (**sand**
**traps**)<span dir="rtl">، لذا لها قيمة 1-. بشكل عام، يتطلب الأمر ست
ضربات للوصول من نقطة البداية إلى الحفرة بواسطة البوت</span>.

<img src="./media/image17.png" style="width:2.2in;height:1.60833in" />

**<span dir="rtl">تمرين 3.17</span>**

<span dir="rtl">ما هي معادلة بيلمان لقيم الإجراءات</span>
<span dir="rtl">(</span>Bellman equation for
<span dir="rtl"></span>action values<span dir="rtl">)، أي لـ</span>
$`q_{\pi}`$ <span dir="rtl"></span>​<span dir="rtl">؟ يجب أن تعطي قيمة
الإجراء</span>  
$`q_{\pi}`$ $`(s,a)`$ <span dir="rtl">من حيث قيم الإجراءات</span>
$`q_{\pi}`$ ($`s,a'`$) <span dir="rtl">للخلفاء المحتملين للزوج
"الحالة-الإجراء</span> <span dir="rtl">  
(</span>state–action pair) ($`s,a`$<span dir="rtl">).</span>

<img src="./media/image18.png" style="width:6.26806in;height:1.125in" />**<span dir="rtl">تمرين
3.18:</span>**  
<span dir="rtl">تعتمد قيمة الحالة على قيم "الإجراءات</span> (actions)"
<span dir="rtl">الممكنة في تلك الحالة وكيفية احتمال اتخاذ كل إجراء تحت
السياسة الحالية</span> (current policy)<span dir="rtl">.</span>
<span dir="rtl">يمكننا التفكير في هذا من حيث رسم احتياطي صغير مؤصل عند
الحالة ويأخذ في الاعتبار كل إجراء ممكن</span>:

<span dir="rtl">قدم المعادلة التي تتوافق مع هذه الفكرة والرسم البياني
لقيمة العقدة الجذرية،</span> $`v_{\pi}`$ ($`s`$)<span dir="rtl">، من حيث
القيمة عند العقدة الطرفية المتوقعة،</span> $`q_{\pi}`$
($`s,a`$)<span dir="rtl">، بشرط أن يكون</span>$`= s\ `$
<span dir="rtl"></span>$`S_{t}`$. <span dir="rtl">يجب أن تتضمن هذه
المعادلة توقعًا مشروطًا باتباع السياسة</span> π
(policy)<span dir="rtl">.</span> <span dir="rtl">ثم قدم معادلة ثانية حيث
يتم كتابة القيمة المتوقعة بوضوح من حيث</span> $`\pi(a \mid s)`$
<span dir="rtl">بحيث لا يظهر أي تدوين للقيمة المتوقعة في
المعادلة</span>.

<img src="./media/image19.png"
style="width:3.42222in;height:1.16667in" />**<span dir="rtl">تمرين
3.19:</span>**

<span dir="rtl">تُعتمد قيمة الإجراء</span> $`q_{\pi}`$ ($`s,a`$)
<span dir="rtl">على المكافأة المتوقعة التالية والمجموع المتوقع للمكافآت
المتبقية. يمكننا التفكير في هذا من حيث رسم احتياطي صغير، هذا المرة مؤصل
عند إجراء</span> <span dir="rtl">(</span>state–action
pair<span dir="rtl">) والتفرع إلى **الحالات** التالية الممكنة</span>
(possible next states)"<span dir="rtl">:</span>

<span dir="rtl">قدم المعادلة التي تتوافق مع هذه الفكرة والرسم البياني
لقيمة الإجراء</span> $`q_{\pi}`$ ($`s,a`$) <span dir="rtl">من حيث
المكافأة المتوقعة التالية ومجموع المكافآت المتبقية. يجب أن تتضمن هذه
المعادلة توقعًا مشروطًا باتباع السياسة</span> (policy)
$`\pi`$<span dir="rtl">.</span>

<u>**3**.**6 <span dir="rtl">السياسات المثلى ودوال القيمة المثلى</span>
<span dir="rtl">(</span>Optimal Policies and Optimal Value
<span dir="rtl"></span>Functions**<span dir="rtl">)</span></u>

<span dir="rtl">حل مهمة التعليم المعزز يعني، بشكل عام، العثور على سياسة
تحقق الكثير من المكافآت على المدى الطويل. بالنسبة لمهام "تخطيط ماركوف
المحدودة</span> (finite MDPs)<span dir="rtl">، يمكننا تعريف السياسة
المثلى بدقة على النحو التالي. تحدد دوال القيمة ترتيبًا جزئيًا بين
السياسات. يتم تعريف **سياسة**</span> $`\pi`$ <span dir="rtl">بأنها أفضل
من أو تساوي سياسة</span> $`\pi'`$ <span dir="rtl">إذا كانت عائداتها
المتوقعة أكبر من أو تساوي عائدات</span>$`{\ \pi}'`$<span dir="rtl">لجميع
الحالات. بعبارة أخرى،</span> $`\pi \geq \pi'`$ <span dir="rtl">إذا وفقط
إذا كان</span>
$`\mathbf{v}_{\mathbf{\pi}}\left( \mathbf{s} \right)\mathbf{\geq}\mathbf{v}_{\mathbf{\pi}^{\mathbf{'}}}\left( \mathbf{s} \right)`$
<span dir="rtl">لجميع</span>
$`\mathbf{s \in S}`$<span dir="rtl">.</span> <span dir="rtl">هناك دائمًا
على الأقل سياسة واحدة تكون أفضل من أو تساوي جميع السياسات الأخرى. وهذه
هي السياسة المثلى. على الرغم من أنه قد يكون هناك أكثر من واحدة، نُشير إلى
جميع السياسات المثلى بـ</span> $`\pi*`$<span dir="rtl">.</span>
<span dir="rtl">تشترك جميعها في نفس دالة القيمة الحالة، والتي تُسمى دالة
القيمة الحالة المثلى، وتُرمز بـ</span> $`v*`$<span dir="rtl">، وتُعرّف على
النحو التالي</span>:

``` math
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ v^{*}(s) = \max_{\pi}v_{\pi}(s)\quad\text{for all }s \in S\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.15)
```

<span dir="rtl">السياسات المثلى تشترك أيضًا في نفس دالة قيمة الإجراء
المثلى، والتي تُرمز بـ</span> q∗<span dir="rtl">، وتُعرّف على النحو
التالي</span>:

``` math
{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ q}^{*}(s,a) = \max_{\pi}q_{\pi}(s,a)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.16)
```

<span dir="rtl">لجميع</span> $`s \in S`$
<span dir="rtl">و</span>$`a \in A(s)`$<span dir="rtl">.</span>
<span dir="rtl">بالنسبة لزوج
الحالة–الإجراء</span>$`(s,a)`$<span dir="rtl">، توفر هذه الدالة العائد
المتوقع لأخذ الإجراء</span> $`a`$ <span dir="rtl">في الحالة</span> $`s`$
<span dir="rtl">ومن ثم متابعة سياسة مثلى. وبالتالي، يمكننا كتابة</span>
$`q*`$ <span dir="rtl">من حيث</span> $`v*`$ <span dir="rtl">على  
النحو التالي</span>:

``` math
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ q^{*}(s,a) = E\left\lbrack R_{t + 1} + \gamma v^{*}\left( S_{t + 1} \right)\mid S_{t} = s,A_{t} = a \right\rbrack\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.17)
```

**<span dir="rtl"><u>مثال 3.7:</u> دوال القيمة المثلى للغولف</span>**

<span dir="rtl">الجزء السفلي من الشكل **3.3** يُظهر خطوط التساوي لدالة
قيمة الإجراء المثلى المحتملة</span>  
<span dir="rtl"></span>$`q*(s,driver)`$ <span dir="rtl">هذه هي قيم كل
حالة إذا قمنا أولاً بضرب الكرة باستخدام السائق</span> ($`driver`$)
<span dir="rtl">ثم نختار إما السائق أو الملعقة</span>
($`putter`$)<span dir="rtl">، حسب ما هو أفضل. يتيح لنا السائق ضرب الكرة
لمسافة أبعد، ولكن بدقة أقل. يمكننا الوصول إلى الحفرة في ضربة واحدة
باستخدام السائق فقط إذا كنا بالفعل قريبين جدًا؛ لذلك، تغطي خط التساوي 1-
لـ</span> $`\ q*(s,driver)`$<span dir="rtl">جزءًا صغيرًا فقط من العشب. إذا
كان لدينا ضربة واحدة فقط، يمكننا الوصول إلى الحفرة من مسافة أبعد بكثير،
كما هو موضح بخط التساوي 2-. في هذه الحالة، لا يتعين علينا القيادة إلى
داخل خط التساوي الصغير 1-، ولكن فقط إلى أي مكان على العشب؛ من هناك
يمكننا استخدام الملعقة. تعطي دالة قيمة الإجراء المثلى القيم بعد الالتزام
بإجراء أول معين، في هذه الحالة، إلى السائق، ولكن بعد ذلك استخدام أي
إجراءات تكون الأفضل. خط التساوي 3- هو أبعد من ذلك ويشمل نقطة البداية. من
نقطة البداية، فإن أفضل تسلسل من الإجراءات هو ضربة سائقين وضربة ملعقة،
لإدخال الكرة في ثلاث ضربات</span>.

<span dir="rtl">لأن</span> $`\ v*`$<span dir="rtl">هي دالة القيمة لسياسة
ما، يجب أن تفي بشرط التوافق الذاتي المقدم بواسطة معادلة بيلمان لقيم
الحالة (3.14). ومع ذلك، بما أنها دالة القيمة المثلى، يمكن كتابة شرط
التوافق الخاص بـ</span> $`v*`$ <span dir="rtl">بشكل خاص دون الإشارة إلى
أي سياسة معينة. هذه هي معادلة بيلمان لـ</span> $`\ v*`$<span dir="rtl">،
أو معادلة بيلمان المثلى. من الناحية البديهية، تعبر معادلة بيلمان المثلى
عن حقيقة أن قيمة حالة تحت سياسة مثلى يجب أن تساوي العائد المتوقع لأفضل
إجراء من تلك الحالة</span>:

``` math
v^{*}(s) = \max_{a \in A(s)}q^{*}(s,a)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
```

``` math
\ \ \ \ \ \ \ \ \ \ \ \ \  = \max_{a}E_{\pi^{*}}\left\lbrack G_{t}\mid S_{t} = s,A_{t} = a \right\rbrack\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
```

``` math
\ \ \ \ \ \ \ \ \ \ \ \  = \max_{a}E_{\pi^{*}}\left\lbrack R_{t + 1} + \gamma G_{t + 1}\mid S_{t} = s,A_{t} = a \right\rbrack\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
```

``` math
\ \ \ \ \ \ \ \ \ \ \ \ \  = \max_{a}E\left\lbrack R_{t + 1} + \gamma v^{*}\left( S_{t + 1} \right)\mid S_{t} = s,A_{t} = a \right\rbrack\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.18)
```

``` math
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  = \max_{a}{\sum_{s',r}^{}{p\left( s',r\mid s,a \right)\left\lbrack r + \gamma v^{*}\left( s' \right) \right\rbrack\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.19)}}
```

<span dir="rtl">المعادلتان الأخيرتان هما شكلان من معادلة بيلمان المثلى
لـ</span> v∗<span dir="rtl">.</span> <span dir="rtl">معادلة بيلمان
المثلى لـ</span> $`q*`$ <span dir="rtl">هي</span>

``` math
q^{*}(s,a) = E\left\lbrack R_{t + 1} + \gamma\max_{a'}q^{*}\left( S_{t + 1},a' \right)\mid S_{t} = s,A_{t} = a \right\rbrack
```

``` math
q^{*}(s,a) = \sum_{s',r}^{}{p\left( s',r\mid s,a \right)\left\lbrack r + \gamma\max_{a'}q^{*}\left( s',a' \right) \right\rbrack}
```

<span dir="rtl">تُظهر المخططات التوضيحية في الشكل أدناه بشكل بياني نطاقات
الحالات المستقبلية والإجراءات التي يتم أخذها في الاعتبار في معادلات
بيلمان المثلى لـ</span> $`v*`$
<span dir="rtl">و</span>$`q*`$<span dir="rtl">.</span>
<span dir="rtl">وهذه هي نفسها المخططات التوضيحية لـ</span> $`v_{\pi}`$
<span dir="rtl"></span> <span dir="rtl">و</span> $`q_{\pi}`$
<span dir="rtl"></span> <span dir="rtl">التي تم تقديمها سابقًا، باستثناء
أنه قد تمت إضافة أقواس في نقاط اختيار الوكيل لتمثيل أن الحد الأقصى يتم
أخذه عبر ذلك الاختيار بدلاً من القيمة المتوقعة وفقًا لسياسة معينة. يمثل
المخطط التوضيحي على اليسار معادلة بيلمان المثلى (3.19) بشكل بياني، بينما
يمثل المخطط التوضيحي على اليمين معادلة (3.20) بشكل بياني</span>.

<img src="./media/image20.png"
style="width:6.26806in;height:2.50694in" />

*<span dir="rtl">بالنسبة لـ</span>* **MDPs** *<span dir="rtl">المحدودة،
تحتوي معادلة بيلمان المثلى لـ</span>* $`v*`$ (3.19) <span dir="rtl">على
حل فريد. معادلة بيلمان المثلى هي في الواقع نظام من المعادلات، واحدة لكل
حالة، لذا إذا كان هناك</span> $`n`$ <span dir="rtl">حالة، فإن
هناك</span> $`n`$ <span dir="rtl">معادلات في</span> $`n`$
<span dir="rtl">مجهول. إذا كانت ديناميات البيئة</span> $`p`$
<span dir="rtl">معروفة، فيمكن من حيث المبدأ حل هذا النظام من المعادلات
لـ</span> $`v*`$ *<span dir="rtl">باستخدام أي من الطرق المتنوعة لحل
أنظمة المعادلات غير الخطية. يمكن أيضًا حل مجموعة ذات صلة من المعادلات
ل</span>* $`q*`$

*<span dir="rtl">بمجرد أن يكون لديك</span>* $`v*`$ <span dir="rtl">يصبح
من السهل نسبيًا تحديد سياسة مثلى. لكل حالة</span> $`s`$<span dir="rtl">،
سيكون هناك واحد أو أكثر من الإجراءات التي تحقق الحد الأقصى في معادلة
بيلمان المثلى. أي سياسة تعين احتمالية غير صفرية فقط لهذه الإجراءات هي
سياسة مثلى. يمكنك التفكير في هذا على أنه</span> one-step search
<span dir="rtl">(بحث من خطوة واحدة). إذا كنت تمتلك دالة القيمة
المثلى</span> $`v*`$<span dir="rtl">فإن الإجراءات التي تبدو الأفضل
بعد</span> one-step search <span dir="rtl">(بحث من خطوة واحدة) ستكون هي
الإجراءات المثلى. بعبارة أخرى، أي سياسة تكون</span> greedy
<span dir="rtl">(جشعة) بالنسبة لدالة التقييم المثلى</span>
$`v*\ `$<span dir="rtl">*هي سياسة مثلى. يُستخدم* مصطلح</span>greedy
<span dir="rtl">(جشع) في علوم الكمبيوتر لوصف أي إجراء بحثي أو قرار يختار
البدائل بناءً على اعتبارات محلية أو فورية فقط، دون النظر إلى إمكانية أن
مثل هذا الاختيار قد يمنع الوصول إلى بدائل أفضل في المستقبل. ومن ثم، يصف
السياسات التي تختار الإجراءات بناءً فقط على عواقبها قصيرة الأجل.
جمال</span> $`v*`$<span dir="rtl">هو أنه إذا تم استخدامه لتقييم العواقب
قصيرة الأجل للإجراءات—على وجه التحديد، العواقب من خطوة واحدة فإن
سياسة</span> greedy <span dir="rtl">(جشعة) في هذه الحالة تكون فعليًا مثلى
على المدى الطويل الذي نهتم *به لأن*</span>$`\ v*`$<span dir="rtl">تأخذ
بالفعل في اعتبارها عواقب المكافآت لجميع السلوكيات المستقبلية الممكنة. من
خلال</span>$`v*`$<span dir="rtl">، يتم تحويل العائد المثلى المتوقع على
المدى الطويل إلى كمية متاحة محليًا وفوريًا لكل حالة. وبالتالي،
فإن</span>"one-step-ahead search <span dir="rtl">(بحث من خطوة واحدة)
يحقق الإجراءات المثلى على المدى الطويل</span>.

<span dir="rtl">امتلاك</span> $`q*`$ <span dir="rtl">يجعل اختيار
الإجراءات المثلى أسهل بكثير. مع</span> $`q*`$<span dir="rtl">، لا يحتاج
الوكيل حتى إلى إجراء بحث من خطوة واحدة: لكل حالة</span>
s<span dir="rtl">، يمكنه ببساطة العثور على أي إجراء يحقق الحد الأقصى
لـ</span> $`q*(s,a)`$<span dir="rtl">. دالة قيمة الإجراء</span> $`q*`$
<span dir="rtl">تقوم فعليًا بتخزين نتائج جميع الأبحاث من خطوة واحدة. إنها
توفر العائد المثلى المتوقع على المدى الطويل كقيمة متاحة محليًا وفوريًا لكل
زوج من الحالة والإجراء. وبالتالي، بتكلفة تمثيل دالة من أزواج الحالة
والإجراء بدلاً من مجرد الحالات، يسمح دالة قيمة الإجراء المثلى باختيار
الإجراءات المثلى دون الحاجة إلى معرفة أي شيء عن الحالات التالية المحتملة
وقيمها، أي دون الحاجة إلى معرفة أي شيء عن ديناميات البيئة</span>.

<span dir="rtl">**مثال 3.8:** حل شبكة العالم</span>

<span dir="rtl">افترض أننا قمنا بحل معادلة بيلمان لـ</span>
$`\ v*\ `$<span dir="rtl">لمهمة الشبكة البسيطة المقدمة في المثال 3.5
والمبينة مرة أخرى في الشكل 3.5 (يسار). تذكر أن الحالة</span> $`A`$
<span dir="rtl">تُتبع بمكافأة قدرها 10+ وانتقال إلى الحالة</span>
$`A_{0}`$<span dir="rtl">، بينما الحالة</span> $`B`$
<span dir="rtl">تُتبع بمكافأة قدرها +5 وانتقال إلى الحالة</span>
$`B_{0}`$<span dir="rtl">.</span> <span dir="rtl">يظهر الشكل 3.5 (وسط)
دالة القيمة المثلى، ويظهر الشكل 3.5 (يمين) السياسات المثلى المقابلة.
حيثما توجد أسهم متعددة في خلية، تكون جميع الإجراءات المقابلة
مثلى</span>.

<img src="./media/image21.png"
style="width:6.26806in;height:2.72014in" />

**<span dir="rtl">مثال 3.9: معادلات "بيلمان" المثلى</span> (Bellman
Optimality Equations) <span dir="rtl">للروبوت المعاد تدويره</span>**  
<span dir="rtl">باستخدام (3.19)، يمكننا إعطاء معادلة "بيلمان" المثلى
بشكل صريح في مثال الروبوت المعاد تدويره. لجعل الأمور أكثر اختصارًا،
سنستخدم الرموز التالية:</span> $`h`$
<span dir="rtl">و"</span>$`l`$<span dir="rtl">" للحالات العالية
والمنخفضة، و</span>$`s`$<span dir="rtl">، و</span>$`w`$<span dir="rtl">،
و</span>$`re`$ <span dir="rtl">للإجراءات البحث</span>
(search)<span dir="rtl">، و"الانتظار</span> (wait)"<span dir="rtl">،
وإعادة الشحن</span> (recharge) <span dir="rtl">على التوالي. نظرًا لأن
هناك حالتين فقط، فإن معادلة "بيلمان" المثلى تتألف من معادلتين. يمكن
كتابة المعادلة الخاصة بـ</span> $`\ v*(h)`$<span dir="rtl">كما
يلي</span>:

``` math
\begin{matrix}
v_{*}(\text{ }h) & \  = max\begin{Bmatrix}
p(\text{ }h \mid h,s)\left\lbrack r(\text{ }h,\text{ }s,\text{ }h) + \gamma v_{*}(\text{ }h) \right\rbrack + p(l \mid h,s)\left\lbrack r(\text{ }h,\text{ }s,l) + \gamma v_{*}(l) \right\rbrack \\
p(\text{ }h \mid h,w)\left\lbrack r(\text{ }h,w,h) + \gamma v_{*}(\text{ }h) \right\rbrack + p(l \mid h,w)\left\lbrack r(\text{ }h,w,l) + \gamma v_{*}(l) \right\rbrack
\end{Bmatrix} \\
 & \  = max\begin{Bmatrix}
\alpha\left\lbrack r_{s} + \gamma v_{*}(\text{ }h) \right\rbrack + (1 - \alpha)\left\lbrack r_{s} + \gamma v_{*}(l) \right\rbrack \\
1\left\lbrack r_{w} + \gamma v_{*}(\text{ }h) \right\rbrack + 0\left\lbrack r_{w} + \gamma v_{*}(l) \right\rbrack
\end{Bmatrix} \\
 & \  = max\begin{Bmatrix}
r_{s} + \gamma\left\lbrack \alpha v_{*}(\text{ }h) + (1 - \alpha)v_{*}(l) \right\rbrack \\
r_{w} + \gamma v_{*}(\text{ }h)
\end{Bmatrix}
\end{matrix}
```

<span dir="rtl">وفقًا لنفس الإجراء لـ</span> $`v*(l)`$<span dir="rtl">،
ينتج عن ذلك المعادلة</span>

``` math
p_{*}(1) = max\begin{Bmatrix}
\beta r_{s} - 3(1 - \beta) + \gamma\left\lbrack (1 - \beta)v_{*}(\text{ }h) + \beta v_{*}(1) \right\rbrack, \\
r_{w} + \gamma v_{*}(1), \\
\gamma v_{*}(\text{ }h)
\end{Bmatrix}
```

<span dir="rtl">بالنسبة لأي اختيار لـ</span> rs​<span dir="rtl">،</span>
rw​<span dir="rtl">،</span> α<span dir="rtl">،</span> β<span dir="rtl">،
و</span> $`\gamma`$ <span dir="rtl">، حيث</span>
$`0\  \leq \gamma < \ 1`$ <span dir="rtl">و</span>
$`0\  \leq \alpha,\ \beta \leq 1\ `$ <span dir="rtl">يوجد زوج واحد فقط
من الأرقام،</span> $`v*(h)`$
<span dir="rtl">و</span>$`v*(l)`$<span dir="rtl">، اللذان يحققان هاتين
المعادلتين غير الخطيتين في نفس الوقت</span>.

<span dir="rtl">حل معادلة</span> "Bellman optimality"
<span dir="rtl">("معادلة بيلمان المثلى") بشكل صريح يوفر طريقة واحدة
لإيجاد سياسة مثلى، وبالتالي حل مشكلة</span> "reinforcement learning"
<span dir="rtl">("التعليم المعزز"). ومع ذلك، نادرًا ما يكون هذا الحل
مفيدًا بشكل مباشر. فهو يشبه البحث الشامل، حيث يتم النظر في جميع
الاحتمالات، وحساب احتمالية حدوثها ومدى استحسانها من حيث المكافآت
المتوقعة. يعتمد هذا الحل على ثلاث افتراضات على الأقل نادرًا ما تكون صحيحة
في الممارسة العملية: (1) أننا نعرف بدقة ديناميكيات البيئة؛ (2) لدينا
موارد حسابية كافية لإكمال حساب الحل؛ و(3) خاصية</span> "Markov"
<span dir="rtl">("ماركوف"). بالنسبة للمهام التي نهتم بها، لا يمكن تنفيذ
هذا الحل بشكل دقيق عمومًا بسبب انتهاك بعض هذه الافتراضات. على سبيل
المثال، على الرغم من أن الافتراضين الأول والثالث لا يشكلان مشكلة في
لعبة</span> "backgammon" <span dir="rtl">("الطاولة")، إلا أن الافتراض
الثاني يمثل عقبة كبيرة. نظرًا لأن اللعبة تحتوي على حوالي</span> 102010^
{20}1020 <span dir="rtl">حالة، فإن حل معادلة</span> "Bellman"
<span dir="rtl">("بيلمان") لـ</span> $`v*`$ <span dir="rtl">سيستغرق آلاف
السنين على أسرع الحواسيب الحالية، وينطبق الأمر نفسه على إيجاد</span>
$`q*`$<span dir="rtl">.</span> <span dir="rtl">في</span> reinforcement
learning <span dir="rtl">(التعليم المعزز)، يجب عادةً الاكتفاء بحلول
تقريبية</span>.

<span dir="rtl">يمكن اعتبار العديد من طرق اتخاذ القرار المختلفة كوسائل
لحل معادلة</span> Bellman optimality <span dir="rtl">(معادلة بيلمان
المثلى) تقريبياً. على سبيل المثال، يمكن اعتبار طرق البحث التجريبي كوسائل
لتوسيع الطرف الأيمن من المعادلة (3.19) عدة مرات، حتى عمق معين، وتشكيل
شجرة</span> (tree) <span dir="rtl">من الاحتمالات، ثم استخدام دالة تقييم
تجريبية لتقريب</span> v∗ <span dir="rtl">في العقد الأوراق</span>
<span dir="rtl">(</span>leaf<span dir="rtl">).</span>
<span dir="rtl">عادة ما تستند طرق البحث التجريبي مثل</span> ($`A*`$)
<span dir="rtl">إلى الحالة العرضية.</span> <span dir="rtl">يمكن ربط
أساليب البرمجة الديناميكية بشكل وثيق حتى بمعادلة</span> **Bellman**
**optimality** <span dir="rtl">(معادلة بيلمان المثلى). يمكن فهم العديد
من أساليب</span> reinforcement learning <span dir="rtl">(التعليم المعزز)
بوضوح على أنها وسائل لحل معادلة</span> Bellman optimality
<span dir="rtl">(معادلة بيلمان المثلى) تقريبياً، باستخدام التحولات
الفعلية التي تم تجربتها بدلاً من المعرفة بالتحولات المتوقعة. سنناقش
مجموعة متنوعة من هذه الأساليب في الفصول التالية</span>.

<span dir="rtl">**<u>تمرين 3.20</u>**:</span> <span dir="rtl">ارسم أو
وصف دالة القيمة المثلى للحالة لمثال الجولف</span>.

<span dir="rtl">**<u>تمرين 3.21</u>**:</span> <span dir="rtl">ارسم أو
وصف خطوط المستوى لدالة القيمة المثلى للإجراء للتسديد،</span>
$`q^{*}\left( s,\text{putter} \right)`$<span dir="rtl">، لمثال
الجولف</span>.

<span dir="rtl">**<u>تمرين 3.22</u>**:</span>
<span dir="rtl">اعتبر</span> "MDP" <span dir="rtl">المستمر الموضح على
اليمين. القرار الوحيد الذي يجب اتخاذه هو في الحالة العليا، حيث يتوفر
إجراءان، يسار ويمين. الأرقام تظهر المكافآت التي يتم تلقيها بشكل حتمي بعد
كل إجراء. هناك بالضبط سياستان حتميتان،</span> $`\pi left`$
<span dir="rtl">  
</span>$`و\ \pi right`$<span dir="rtl">.</span> <span dir="rtl">ما هي
السياسة المثلى إذا كانت</span> $`\gamma`$ <span dir="rtl"></span>0
=<span dir="rtl">؟ إذا كانت</span> $`\gamma`$ <span dir="rtl">= 0.9؟ إذا
كانت</span>$`\ 0.5 = \gamma\ `$<span dir="rtl">؟</span>

<span dir="rtl">**<u>تمرين 3.23</u>**:</span> <span dir="rtl">قدم
معادلة</span> "Bellman" <span dir="rtl">لدالة القيمة المثلى
للإجراء</span> $`q*`$ <span dir="rtl">لروبوت إعادة التدوير</span>.

<span dir="rtl">**<u>تمرين 3.24</u>**:</span> <span dir="rtl">الشكل
**3.5** يعرض القيمة المثلى لأفضل حالة في</span> "$`gridworld`$"
<span dir="rtl">على أنها **24.4**، لرقم عشري واحد. استخدم معرفتك
بالسياسة المثلى ومعادلة (**3.8**) للتعبير عن هذه القيمة رمزياً، ثم احسبها
لثلاثة أرقام عشرية</span>.

<span dir="rtl">**<u>تمرين 3.25</u>**:</span> <span dir="rtl">قدم معادلة
لدالة القيمة المثلى</span> $`v*`$ <span dir="rtl">من حيث دالة القيمة
المثلى للإجراء</span> $`q*`$<span dir="rtl">.</span>

<span dir="rtl">**<u>تمرين 3.26</u>**:</span> <span dir="rtl">قدم معادلة
لدالة القيمة المثلى للإجراء</span> $`q*`$ <span dir="rtl">من حيث دالة
القيمة المثلى</span> $`v*`$ <span dir="rtl">و</span>”$`p`$”
<span dir="rtl">ذات الأربعة وسائط</span>.

<span dir="rtl">**<u>تمرين 3.27</u>**:</span> <span dir="rtl">قدم معادلة
للسياسة المثلى</span> $`\pi*`$ <span dir="rtl">من حيث دالة القيمة المثلى
للإجراء</span> $`q*`$<span dir="rtl">.</span>

<span dir="rtl">**<u>تمرين 3.28</u>**:</span> <span dir="rtl">قدم معادلة
للسياسة المثلى</span> $`\pi*`$ <span dir="rtl">من حيث دالة القيمة
المثلى</span> $`v*`$ <span dir="rtl">و</span>$`p`$ <span dir="rtl">ذات
الأربعة وسائط</span>.

<span dir="rtl">**<u>تمرين 3.29</u>**:</span> <span dir="rtl">أعد كتابة
معادلات</span> Bellman <span dir="rtl">الأربع لدوال القيمة الأربع</span>
$`v_{\pi}`$ <span dir="rtl">،</span> v∗<span dir="rtl">،</span>
$`q_{\pi}`$ <span dir="rtl">، و</span>$`q*`$ <span dir="rtl">من حيث دالة
الثلاث وسائط</span> p (3.4) <span dir="rtl">ودالة الوسيطين</span> (3.5)
r<span dir="rtl">.</span>

**<u>3.7 <span dir="rtl">المثالية والتقريب</span> (Optimality and
Approximation)</u>**

<span dir="rtl">لقد قمنا بتعريف دوال القيمة المثلى والسياسات المثلى. من
الواضح أن الوكيل الذي يتعلم سياسة مثلى قد حقق إنجازًا كبيرًا، ولكن في
الممارسة العملية، نادرًا ما يحدث ذلك. بالنسبة لأنواع المهام التي نهتم
بها، يمكن توليد السياسات المثلى فقط بتكلفة حسابية هائلة. توفر فكرة
المثالية بشكل جيد تنظيمًا للطريقة التي نصفها في هذا الكتاب وتوفر وسيلة
لفهم الخصائص النظرية لمختلف خوارزميات التعليم، لكنها تعتبر مثالًا يمكن
للوكلاء الاقتراب منه بدرجات متفاوتة. كما ناقشنا أعلاه، حتى إذا كان لدينا
نموذج كامل ودقيق لديناميكيات البيئة، فعادةً لا يكون من الممكن ببساطة حساب
سياسة مثلى من خلال حل معادلة</span> Bellman <span dir="rtl">المثلى. على
سبيل المثال، ألعاب اللوح مثل الشطرنج هي جزء صغير من التجربة البشرية، ومع
ذلك لا تستطيع الحواسيب الكبيرة المصممة خصيصًا حساب الحركات المثلى. جانب
حاسم من المشكلة التي يواجهها الوكيل هو دائمًا القوة الحسابية المتاحة له،
وبشكل خاص، كمية الحساب التي يمكنه إجراؤها في خطوة زمنية واحدة</span>.

<span dir="rtl">الذاكرة المتاحة هي أيضًا قيد مهم. غالبًا ما يتطلب الأمر
مقدارًا كبيرًا من الذاكرة لبناء تقريبات لدوال القيمة، والسياسات، والنماذج.
في المهام ذات المجموعات الصغيرة من الحالات النهائية، من الممكن تشكيل هذه
التقريبات باستخدام مصفوفات أو جداول تحتوي على إدخال واحد لكل حالة (أو
زوج حالة-إجراء). هذا ما نطلق عليه الحالة الجدولية، ونسمي الأساليب
الموافقة بأساليب الجدول. ومع ذلك، في العديد من الحالات العملية، هناك
حالات أكثر بكثير من أن تكون إدخالات في جدول. في هذه الحالات، يجب تقريب
الدوال، باستخدام نوع من التمثيل الدال المتميز</span>.

<span dir="rtl">إطارنا لمشكلة التعليم المعزز يجبرنا على الاكتفاء
بالتقريبات. ومع ذلك، فإنه يقدم لنا أيضًا بعض الفرص الفريدة لتحقيق تقريبات
مفيدة. على سبيل المثال، عند تقريب السلوك المثلى، قد تكون هناك حالات
يواجهها الوكيل باحتمالية منخفضة جدًا بحيث يكون اختيار الإجراءات غير
المثلى لها تأثير ضئيل على مقدار المكافأة التي يحصل عليها الوكيل. على
سبيل المثال، يلعب لاعب</span> "backgammon" <span dir="rtl">الذي
صممه</span> "Tesauro" <span dir="rtl">بمهارة استثنائية على الرغم من أنه
قد يتخذ قرارات سيئة جدًا على تكوينات اللوح التي لا تحدث أبدًا في مباريات
ضد خبراء. في الواقع، من الممكن أن يتخذ</span> "TD-Gammon"
<span dir="rtl">قرارات سيئة لجزء كبير من مجموعة حالات اللعبة. تجعل
الطبيعة التفاعلية للتعليم المعزز من الممكن تقريب السياسات المثلى بطرق
تركز المزيد من الجهد على تعلم اتخاذ قرارات جيدة للحالات التي يتم
مواجهتها بشكل متكرر، على حساب بذل جهد أقل للحالات التي يتم مواجهتها بشكل
نادر. هذه واحدة من الخصائص الرئيسية التي تميز التعليم المعزز عن غيره من
الأساليب لحل</span> "MDPs" <span dir="rtl">بشكل تقريبي</span>.

**<u>3.8 <span dir="rtl">ملخص</span> (Summary)</u>**

<span dir="rtl">لنلخص عناصر مشكلة التعليم المعزز التي قدمناها في هذا
الفصل. التعليم المعزز يتعلق بتعلم كيفية التصرف من خلال التفاعل لتحقيق
هدف معين. يتفاعل وكيل التعليم المعزز وبيئته عبر سلسلة من الخطوات الزمنية
المتقطعة. تحديد واجهتهما يعرف مهمة معينة: الأفعال هي الخيارات التي
يتخذها الوكيل؛ والحالات هي الأساس لاتخاذ هذه الخيارات؛ والمكافآت هي
الأساس لتقييم الخيارات. كل شيء داخل الوكيل معروف تمامًا وقابل للتحكم من
قبل الوكيل؛ وكل شيء خارج الوكيل غير قابل للتحكم بالكامل ولكن قد يكون
معروفًا بالكامل أو لا. السياسة هي قاعدة احتمالية يتم بموجبها اختيار
الوكيل للأفعال كدالة للحالات. هدف الوكيل هو تعظيم مقدار المكافأة التي
يحصل عليها على مدى الزمن</span>.

<span dir="rtl">عندما يتم صياغة إعداد التعليم المعزز الموضح أعلاه مع
احتمالات الانتقال المحددة بدقة، فإنه يشكل عملية اتخاذ القرار
ماركوفية</span> (MDP)<span dir="rtl">.</span> <span dir="rtl">ال</span>
MDP<span dir="rtl">المحدودة هي</span> MDP <span dir="rtl">مع مجموعات
حالة، إجراء، و(كما نصيغها هنا) مكافآت محدودة. الكثير من النظرية الحالية
للتعليم المعزز مقيد بـ</span> MDPs <span dir="rtl">المحدودة، لكن
الأساليب والأفكار تنطبق بشكل عام</span>.

<span dir="rtl">العودة هي دالة المكافآت المستقبلية التي يسعى الوكيل
لتكبيرها (من حيث القيمة المتوقعة). لها تعريفات مختلفة اعتمادًا على طبيعة
المهمة وما إذا كان يرغب في خصم المكافأة المتأخرة. الصيغة غير المخصومة
مناسبة للمهام العرضية، حيث يتقطع التفاعل بين الوكيل والبيئة بشكل طبيعي
إلى حلقات؛ والصيغة المخصومة مناسبة للمهام المستمرة، حيث لا يتقطع التفاعل
بشكل طبيعي إلى حلقات ولكن يستمر بلا حدود. نحاول تعريف العوائد لأنواع
المهام المختلفة بحيث يمكن تطبيق مجموعة واحدة من المعادلات على كل من
الحالات العرضية والمستمرة</span>.

<span dir="rtl">دوال قيمة السياسة تعين لكل حالة، أو زوج حالة-إجراء،
العائد المتوقع من تلك الحالة، أو زوج الحالة-الإجراء، بالنظر إلى أن
الوكيل يستخدم السياسة. دوال القيمة المثلى تعين لكل حالة، أو زوج
حالة-إجراء، أكبر عائد متوقع يمكن تحقيقه بواسطة أي سياسة. السياسة التي
تكون دوال قيمتها مثلى هي سياسة مثلى. في حين أن دوال القيمة المثلى
للحالات وزوج الحالة-الإجراء فريدة لـ</span> MDP <span dir="rtl">معين،
يمكن أن تكون هناك العديد من السياسات المثلى. أي سياسة تكون جشعة بالنسبة
لدوال القيمة المثلى يجب أن تكون سياسة مثلى. معادلات</span> Bellman
<span dir="rtl">المثلى هي شروط اتساق خاصة يجب أن تفي بها دوال القيمة
المثلى والتي يمكن، من حيث المبدأ، حلها للحصول على دوال القيمة المثلى،
والتي يمكن من خلالها تحديد سياسة مثلى بسهولة نسبية</span>.

<span dir="rtl">يمكن طرح مشكلة التعليم المعزز بطرق مختلفة اعتمادًا على
الافتراضات حول مستوى المعرفة المتاحة للوكيل في البداية. في مشكلات
المعرفة الكاملة، يمتلك الوكيل نموذجًا كاملًا ودقيقًا لديناميات البيئة. إذا
كانت البيئة هي</span> MDP<span dir="rtl">، فإن هذا النموذج يتكون من دالة
الديناميات ذات الأربعة وسائط الكاملة (3.2). في مشكلات المعرفة غير
الكاملة، لا يتوفر نموذج كامل ومثالي للبيئة</span>.

<span dir="rtl">حتى إذا كان الوكيل يمتلك نموذجًا كاملًا ودقيقًا للبيئة، فإن
الوكيل عادةً لا يكون قادرًا على إجراء حسابات كافية لكل خطوة زمنية
لاستخدامه بالكامل. الذاكرة المتاحة هي أيضًا قيد مهم. قد تكون الذاكرة
مطلوبة لبناء تقريبات دقيقة لدوال القيمة، والسياسات، والنماذج. في معظم
الحالات العملية هناك حالات أكثر بكثير من أن تكون إدخالات في جدول، ويجب
عمل تقريبات</span>.

<span dir="rtl">تُعَرف فكرة المثالية بشكل جيد لتنظيم النهج نحو التعليم
الذي نصفه في هذا الكتاب وتوفر وسيلة لفهم الخصائص النظرية لمختلف
خوارزميات التعليم، لكنها تعتبر مثالًا يمكن للوكلاء في التعليم المعزز
الاقتراب منه بدرجات متفاوتة. في التعليم المعزز، نحن مهتمون جدًا بالحالات
التي لا يمكن العثور فيها على حلول مثلى ولكن يجب تقريبيها بطريقة
ما</span>. <span dir="rtl"></span>

<span dir="rtl">الفصل الرابع:</span>

<span dir="rtl">البرمجة الديناميكية</span> (Dynamic Programming)

<span dir="rtl">تشير مصطلحات البرمجة الديناميكية</span> (**DP**)
<span dir="rtl">إلى مجموعة من الخوارزميات التي يمكن استخدامها لحساب
**السياسات**</span> **(policies)** <span dir="rtl">المثلى بناءً على نموذج
مثالي **للبيئة**</span> **(environment)** <span dir="rtl">كعملية اتخاذ
قرار **ماركوفية**.</span> (**MDP**) <span dir="rtl">تكون خوارزميات
البرمجة الديناميكية التقليدية ذات فائدة محدودة في التعليم المعزز، نظرًا
لافتراضها بوجود نموذج مثالي ولتكلفتها الحاسوبية الكبيرة، ولكنها لا تزال
مهمة من الناحية النظرية. توفر البرمجة الديناميكية أساسًا أساسيًا لفهم
الأساليب المقدمة في بقية هذا الكتاب. في الواقع، يمكن اعتبار جميع هذه
الأساليب محاولات لتحقيق تأثير مشابه لتأثير البرمجة الديناميكية، ولكن
بتكلفة حسابية أقل ودون افتراض نموذج مثالي **للبيئة**</span>
**<span dir="rtl">(</span>environment<span dir="rtl">).</span>**

**<span dir="rtl">عادةً ما نفترض أن البيئة</span> (environment)
<span dir="rtl">هي</span> MDP <span dir="rtl">محدودة. أي، نفترض أن
مجموعات الحالات</span> (states) <span dir="rtl">والأفعال</span>**
**(actions**) <span dir="rtl">و**المكافآت**
</span>**(rewards)**<span dir="rtl">،</span>
$`S`$<span dir="rtl">،</span> $`A`$<span dir="rtl">،
و</span>$`R`$<span dir="rtl">، هي مجموعات محدودة، وأن دينامياتها يتم
تحديدها بمجموعة من الاحتمالات</span>
$`p(s',r \mid s,a)`$<span dir="rtl">، لجميع (</span>
$`s \in S,a \in A(s),r \in R,s' \in S +`$ <span dir="rtl">حيث</span>
$`S +`$ <span dir="rtl">هو</span> $`S`$ <span dir="rtl">بالإضافة إلى
حالة نهائية إذا كانت المشكلة عرضية). على الرغم من أن أفكار **البرمجة
الديناميكية**</span> **(DP)** <span dir="rtl">يمكن تطبيقها على المشكلات
ذات **الحالات**</span> **(states)** <span dir="rtl">والأفعال المستمرة،
إلا أن الحلول الدقيقة ممكنة فقط في حالات خاصة. طريقة شائعة للحصول على
حلول تقريبية للمهام ذات **الحالات**</span> **(states)**
<span dir="rtl">والأفعال المستمرة هي تقسيم **فضاءات الحالات**</span>
**(state)** <span dir="rtl">و**الأفعال** </span>**(actions)
<span dir="rtl"></span>**<span dir="rtl">ثم تطبيق طرق **البرمجة
الديناميكية**</span> **(DP)** <span dir="rtl">ذات الحالات المحدودة.
الأساليب التي نستكشفها في الفصل 9 تنطبق على المشكلات المستمرة وهي امتداد
كبير لتلك الطريقة</span>.

<span dir="rtl">الفكرة الرئيسية للبرمجة الديناميكية</span>
(DP)<span dir="rtl">، وللتعليم المعزز بشكل عام، هي استخدام **دوال
القيمة**</span> **<span dir="rtl">(</span>value
<span dir="rtl"></span>functions<span dir="rtl">)</span>**
<span dir="rtl">لتنظيم وهيكلة البحث عن السياسات الجيدة. في هذا الفصل
نوضح كيف يمكن استخدام **البرمجة الديناميكية**</span> **(DP)**
<span dir="rtl">لحساب **دوال القيمة**</span> **(value functions)**
<span dir="rtl">المعرفة في الفصل 3. كما نوقش هناك، يمكننا بسهولة الحصول
على **السياسات المثلى**</span> **(optimal policies)**
<span dir="rtl">بمجرد أن نجد **دوال القيمة المثلى** </span>**(optimal
value functions)**<span dir="rtl">،</span> $`v*`$
<span dir="rtl">أو</span> $`q*`$<span dir="rtl">، التي تحقق معادلات
**بيلمان المثلى**</span> **<span dir="rtl">(</span>Bellman
<span dir="rtl"></span>optimality equations<span dir="rtl">).</span>**

``` math
v^{*}(s) = \max_{a}E\left\lbrack R_{t + 1} + \gamma v^{*}\left( S_{t + 1} \right)\mid S_{t} = s,A_{t} = a \right\rbrack
```

``` math
= \max_{a}{\sum_{s',r}^{}{p\left( s',r\mid s,a \right)\left\lbrack r + \gamma v^{*}\left( s' \right) \right\rbrack}},
```

<span dir="rtl">او</span>

``` math
q^{*}(s,a) = E\left\lbrack R_{t + 1} + \gamma\max_{a'}q^{*}\left( S_{t + 1},a' \right)\mid S_{t} = s,A_{t} = a \right\rbrack
```

``` math
= \sum_{s',r}^{}{p\left( s',r\mid s,a \right)\left\lbrack r + \gamma\max_{a'}q^{*}\left( s',a' \right) \right\rbrack},
```

<span dir="rtl">لجميع</span> $`s \in S`$<span dir="rtl">،
و</span>$`a \in A(s)`$<span dir="rtl">،
و</span>$`\ .s' \in S + \ `$<span dir="rtl">كما سنرى، يتم الحصول على
خوارزميات البرمجة الديناميكية</span> (DP) <span dir="rtl">عن طريق تحويل
معادلات بيلمان مثل هذه إلى تعيينات، أي إلى قواعد تحديث لتحسين تقريبات
دوال القيمة المطلوبة</span>.

**<u>4.1 <span dir="rtl">(تقييم السياسة</span> (Policy Evaluation)
(<span dir="rtl">(التنبؤ)</span></u>**

<span dir="rtl">أولاً، نناقش كيفية حساب دالة القيمة للحالة</span>
$`v_{\pi}`$ <span dir="rtl"></span> <span dir="rtl">لسياسة
عشوائية</span> $`\pi`$<span dir="rtl">.</span> <span dir="rtl">يُطلق على
هذا اسم</span> Policy Evaluation <span dir="rtl">(تقييم السياسة) في
أدبيات البرمجة الديناميكية</span> (DP)<span dir="rtl">.</span>
<span dir="rtl">نطلق عليه أيضًا</span> prediction problem
<span dir="rtl">(مشكلة التنبؤ). تذكر من الفصل 3 أنه، لجميع</span>
$`s \in S`$ <span dir="rtl">،</span>

``` math
v^{\pi}(s) = E_{\pi}\left\lbrack G_{t}\mid S_{t} = s \right\rbrack
```

``` math
= E_{\pi}\left\lbrack R_{t + 1} + \gamma G_{t + 1}\mid S_{t} = s \right\rbrack
```

> 
> ``` math
> = E_{\pi}\left\lbrack R_{t + 1} + \gamma v^{\pi}\left( S_{t + 1} \right)\mid S_{t} = s \right\rbrack
> ```

``` math
= \sum_{a}^{}{\pi\left( a\mid s \right)\sum_{s',r}^{}{p\left( s',r\mid s,a \right)\left\lbrack r + \gamma v^{\pi}\left( s' \right) \right\rbrack}},
```

<span dir="rtl">حيث</span> $`\pi(a \mid s)`$ <span dir="rtl">هو احتمال
اتخاذ الإجراء</span> a <span dir="rtl">في الحالة</span> s
<span dir="rtl">تحت السياسة</span> $`\pi`$<span dir="rtl">، والتوقعات
مشروطة بـ</span> $`\pi`$ <span dir="rtl">للدلالة على أنها تعتمد على
اتباع السياسة</span> $`\pi`$<span dir="rtl">.</span> <span dir="rtl">يتم
ضمان وجود وتفرد</span> $`v_{\pi}`$ <span dir="rtl"></span>
<span dir="rtl">طالما أن</span> γ\<1 <span dir="rtl">أو التوقف النهائي
مضمون من جميع الحالات تحت السياسة</span> $`\pi`$<span dir="rtl">.</span>

<span dir="rtl">إذا كانت ديناميات البيئة معروفة تمامًا، فإن (4.4) هو نظام
من</span> ∣S∣ <span dir="rtl">معادلات خطية متزامنة في</span> ∣S∣
<span dir="rtl">مجهولات (الـ</span> $`v_{\pi}`$
(s)<span dir="rtl">،</span> $`s \in S`$<span dir="rtl">.</span>
<span dir="rtl">من حيث المبدأ، فإن حلها هو حساب مباشر، وإن كان مملًا.
لأغراضنا، تكون طرق الحل التكرارية هي الأنسب. اعتبر سلسلة من دوال القيمة
التقريبية</span> $`v_{0},v_{1},v_{2},`$ <span dir="rtl">...، كل منها
يحدد</span> S+ <span dir="rtl">الى</span> R <span dir="rtl">(الأعداد
الحقيقية). يتم اختيار التقدير الأولي</span> $`v_{0}`$
<span dir="rtl">بشكل عشوائي (باستثناء أنه يجب تعيين القيمة 0 للحالة
النهائية، إذا كانت موجودة)، ويتم الحصول على كل تقدير تالي باستخدام
معادلة بيلمان لـ</span> $`v_{\pi}`$ <span dir="rtl"></span> (4.4)
<span dir="rtl">كقاعدة تحديث</span>:

``` math
v_{k + 1}(s) = E_{\pi}\left\lbrack R_{t + 1} + \gamma v_{k}\left( S_{t + 1} \right)\mid S_{t} = s \right\rbrack
```

``` math
= \sum_{a}^{}{\pi\left( a\mid s \right)\sum_{s',r}^{}{p\left( s',r\mid s,a \right)\left\lbrack r + \gamma v_{k}\left( s' \right) \right\rbrack}},
```

<span dir="rtl">لـكل</span> $`s \in S`$<span dir="rtl">. من الواضح
أن</span> vk=$`v_{\pi}`$ <span dir="rtl"></span> <span dir="rtl">هو نقطة
ثابتة لهذه القاعدة التحديثية لأن معادلة بيلمان لـ</span> $`v_{\pi}`$
<span dir="rtl"> تضمن لنا التساوي في هذه الحالة. في الواقع، يمكن إظهار
أن السلسلة</span> {$`vk`$} <span dir="rtl">تتقارب عمومًا إلى</span>
$`v_{\pi}`$ <span dir="rtl"> عندما</span> $`k \rightarrow \infty`$
<span dir="rtl">تحت نفس الظروف التي تضمن وجود</span>
$`v_{\pi}`$<span dir="rtl">.</span> <span dir="rtl">يُطلق على هذا
الخوارزمية اسم تقييم السياسة التكراري</span> (Iterative Policy
Evaluation)<span dir="rtl">.</span>

<span dir="rtl">لإنتاج كل تقدير تالي،</span> $`vk + 1\ `$​
<span dir="rtl">من</span> $`vk`$​<span dir="rtl">، يقوم تقييم السياسة
التكراري بتطبيق نفس العملية على كل حالة</span>
$`s`$<span dir="rtl">:</span> <span dir="rtl">حيث يستبدل القيمة القديمة
لـ</span> $`s`$ <span dir="rtl">بقيمة جديدة يتم الحصول عليها من القيم
القديمة لحالات الخلفية لـ</span> s<span dir="rtl">، ومن المكافآت الفورية
المتوقعة، عبر جميع الانتقالات ذات الخطوة الواحدة الممكنة تحت السياسة
التي يتم تقييمها. نطلق على هذا النوع من العمليات اسم التحديث المتوقع. كل
تكرار من تقييم السياسة التكراري يقوم بتحديث قيمة كل حالة مرة واحدة
لإنتاج دالة القيمة التقديرية الجديدة</span> $`vk + 1`$

<span dir="rtl">هُناك أنواع مختلفة من التحديثات المتوقعة، بناءً على ما إذا
كان يتم تحديث حالة (كما هو الحال هنا) أو زوج حالة–إجراء، وأيضاً بناءً على
الطريقة الدقيقة التي يتم بها دمج القيم المقدرة للحالات التالية. جميع
التحديثات التي تُجرى في خوارزميات البرمجة الديناميكية تُسمى تحديثات متوقعة
لأنها تستند إلى توقع حول جميع الحالات التالية الممكنة بدلاً من حالة
عشوائية. يمكن التعبير عن طبيعة التحديث في معادلة، كما هو موضح أعلاه، أو
في مخطط التراجع كما تم تقديمه في الفصل الثالث. على سبيل المثال، يُعرض
مخطط التراجع المتعلق بالتحديث المتوقع المستخدم في تقييم السياسات
التكراري في الصفحة 59</span>.

<span dir="rtl">لتنفيذ برنامج حاسوبي تسلسلي لتقييم السياسات التكراري كما
هو موضح في (4.5)، تحتاج إلى استخدام مصفوفتين، واحدة للقيم
القديمة،</span> $`vk(s)`$<span dir="rtl">، وأخرى للقيم الجديدة،</span>
vk+1(s)<span dir="rtl">.</span> <span dir="rtl">باستخدام مصفوفتين، يمكن
حساب القيم الجديدة واحدة تلو الأخرى من القيم القديمة دون تغيير القيم
القديمة. بالطبع، من الأسهل استخدام مصفوفة واحدة وتحديث القيم "في
مكانها"، أي، مع كل قيمة جديدة تحل محل القديمة مباشرةً. ثم، اعتماداً على
ترتيب تحديث الحالات، قد تُستخدم أحياناً القيم الجديدة بدلاً من القديمة في
الجهة اليمنى من (4.5). يتقارب هذا الخوارزم في المكان أيضاً إلى</span>
$`v_{\pi}`$ <span dir="rtl"></span>​<span dir="rtl">؛ في الواقع، يتقارب
عادةً بشكل أسرع من النسخة ذات المصفوفتين، كما قد تتوقع، لأنه يستخدم
البيانات الجديدة بمجرد توفرها. نعتبر التحديثات كما لو كانت تتم في جولة
عبر مساحة الحالة. بالنسبة لخوارزمية التحديث في المكان، فإن ترتيب تحديث
الحالات خلال الجولة له تأثير كبير على معدل التقارب. عادةً ما يكون لدينا
النسخة في المكان في أذهاننا عندما نفكر في خوارزميات البرمجة
الديناميكية</span>.

<span dir="rtl">نسخة كاملة من "تقييم السياسات التكراري</span>"
(Iterative Policy Evaluation) <span dir="rtl">في المكان موضحة في الشيفرة
الزائفة في الصندوق أدناه. لاحظ كيف يتعامل مع الإنهاء. من الناحية
النظرية، يتقارب "تقييم السياسات التكراري" فقط في النهاية، ولكن في
الممارسة العملية يجب إيقافه قبل الوصول إلى هذه النهاية. تختبر الشيفرة
الزائفة الكمية</span>

$`\max\left( \left| v_{k + 1}(s) - v_{k}(s) \right| \right)`$
<span dir="rtl"></span> <span dir="rtl">بعد كل جولة وتتوقف عندما تصبح
هذه الكمية صغيرة بما فيه الكفاية</span>.

<span dir="rtl">تقييم السياسات التكراري، لتقدير</span> *  *
``` math
```

<span dir="rtl">مثال 4.1: اعتبر شبكة</span> 4×44 <span dir="rtl">كما هو
موضح أدناه</span>.

<img src="./media/image23.png"
style="width:6.26806in;height:2.22292in" />

<span dir="rtl">الحالات غير النهائية هي</span>
$`S = \ \{ 1,2,\ldots,14\}`$<span dir="rtl">.</span>
<span dir="rtl">هناك أربع إجراءات ممكنة في كل حالة،  
</span>$`A = \{ up,down,right,left\}`$<span dir="rtl">، والتي تؤدي بشكل
حتمي إلى الانتقالات بين الحالات المقابلة، باستثناء الإجراءات التي قد
تخرج الوكيل عن الشبكة، حيث تظل الحالة كما هي. على سبيل المثال،</span>
$`p(6, - 1 \mid 5,right)\  = 1`$<span dir="rtl">،</span>
$`p(7, - 1 \mid 7,right)\  = 1`$<span dir="rtl">،
و</span>$`p(10,r \mid 5,right)\  = 0`$ <span dir="rtl">لجميع</span>
$`r \in R`$<span dir="rtl">.</span> <span dir="rtl">هذه مهمة غير مخصومة
وذات طابع حلقي. المكافأة هي</span> −1 <span dir="rtl">في جميع الانتقالات
حتى يتم الوصول إلى الحالة النهائية. الحالة النهائية مميزة في الشكل (رغم
أنها موضحة في مكانين، إلا أنها تعتبر حالة واحدة رسميًا). لذلك، فإن دالة
المكافأة المتوقعة هي</span> $`r(s,a,s')\  = - 1`$ <span dir="rtl">لجميع
الحالات</span> $`s`$<span dir="rtl">،</span> $`s'`$
<span dir="rtl">والإجراءات</span> $`a`$<span dir="rtl">.</span>
<span dir="rtl">لنفترض أن الوكيل يتبع سياسة عشوائية متساوية الاحتمالات
(جميع الإجراءات متساوية الاحتمال). يعرض الجانب الأيسر من الشكل 4.1 تسلسل
دوال القيمة</span> {$`vk`$} <span dir="rtl">التي تم حسابها بواسطة تقييم
السياسات التكراري. التقدير النهائي هو في الواقع</span> $`v\pi`$
<span dir="rtl">، والذي في هذه الحالة يعطي لكل حالة نفي عدد الخطوات
المتوقع من تلك الحالة حتى الوصول إلى النهاية</span>.

**<span dir="rtl"><u>تمرين 4.1</u></span>:** <span dir="rtl">في المثال
4.1، إذا كانت</span> $`\pi`$ <span dir="rtl">هي السياسة العشوائية
المتساوية الاحتمال، ما هو</span> $`q_{\pi}(11,down)`$<span dir="rtl">؟
وما هو</span> $`q_{\pi}`$
<span dir="rtl"></span>($`7,down`$)<span dir="rtl">؟</span>

**<span dir="rtl"><u>تمرين 4.2</u>:</span>** <span dir="rtl">في المثال
4.1، لنفترض أنه تم إضافة حالة جديدة 15 إلى شبكة العالم، أسفل الحالة 13
مباشرةً، وأن إجراءاتها: اليسار، الأعلى، اليمين، والأسفل، تأخذ الوكيل إلى
الحالات 12، 13، 14، و15 على التوالي. افترض أن الانتقالات من الحالات
الأصلية لم تتغير. ما هي قيمة</span> $`v\pi\ (15)`$
<span dir="rtl">للسياسة العشوائية المتساوية الاحتمال؟ الآن، لنفترض أن
ديناميات الحالة 13 قد تغيرت أيضًا، بحيث أن الإجراء "أسفل" من الحالة 13
يأخذ الوكيل إلى الحالة الجديدة 15. ما هي قيمة</span> $`v_{\pi}`$(15)
<span dir="rtl">للسياسة العشوائية المتساوية الاحتمال في هذه
الحالة؟</span>

**<span dir="rtl"><u>تمرين 4.3</u>:</span>** <span dir="rtl">ما هي
المعادلات النظيرة للمعادلات (4.3)، (4.4)، و (4.5) لدالة قيمة
الإجراء</span> $`q_{\pi}`$ <span dir="rtl">وتقريبها المتتابع بواسطة
تسلسل من الدوال</span>
$`q0\  ،\ q1\  ،\ q2\  ،\ ...`$<span dir="rtl">؟</span>

**<u>4.2 <span dir="rtl">تحسين السياسات</span> <span dir="rtl">-</span>
Policy Improvement <span dir="rtl"></span></u>**

<span dir="rtl">سبب حساب دالة القيمة لسياسة معينة هو مساعدة في العثور
على سياسات أفضل. لنفترض أننا حددنا دالة القيمة</span> $`v_{\pi}`$
<span dir="rtl"></span> <span dir="rtl">لسياسة حتمية معينة</span>
$`\pi`$<span dir="rtl">.</span> <span dir="rtl">بالنسبة لحالة
معينة</span> $`s`$<span dir="rtl">، نود أن نعرف ما إذا كان ينبغي علينا
تغيير السياسة لاختيار إجراء</span> $`a \neq \pi(s)`$
<span dir="rtl">بشكل حتمي. نحن نعرف مدى جودة اتباع السياسة الحالية
من</span> $`s`$ <span dir="rtl">أي</span> $`v_{\pi}`$ $`(s)`$
<span dir="rtl">ولكن هل سيكون من الأفضل أو الأسوأ التبديل إلى السياسة
الجديدة؟ إحدى الطرق للإجابة على هذا السؤال هي النظر في اختيار</span>
$`a`$ <span dir="rtl">في</span> $`s`$ <span dir="rtl">ومن ثم</span>

<img src="./media/image24.png"
style="width:6.1125in;height:3.77569in" /><img src="./media/image25.png"
style="width:6.26806in;height:4.43819in" />

**<span dir="rtl">الشكل 4.1:</span>** **<span dir="rtl">تلاقي تقييم
السياسة التكراري على شبكة صغيرة</span>.** <span dir="rtl">العمود الأيسر
هو تسلسل تقريبات دالة قيمة الحالة للسياسة العشوائية (جميع الإجراءات
متساوية الاحتمال). العمود الأيمن هو تسلسل السياسات الجشعة المقابلة
لتقديرات دالة القيمة (يتم عرض الأسهم لجميع الإجراءات التي تحقق الحد
الأقصى، والأرقام المعروضة مقربة إلى رقمين معنويين). السياسة الأخيرة
مضمونة فقط أن تكون تحسينًا على السياسة العشوائية، ولكن في هذه الحالة،
وكذلك جميع السياسات بعد التكرار الثالث، تكون مثالية</span>.

<span dir="rtl">اتباع السياسة الحالية،</span>
$`\pi`$<span dir="rtl">.</span> <span dir="rtl">قيمة هذه الطريقة في
التصرف هي</span>

``` math
q_{\pi}(s,a) = E\left\lbrack R_{t + 1} + \gamma v_{\pi}\left( S_{t + 1} \right)\mid S_{t} = s,A_{t} = a \right\rbrack
```

``` math
q_{\pi}(s,a) = \sum_{s',r}^{}{p\left( s',r\mid s,a \right)\left( r + \gamma v_{\pi}\left( s' \right) \right)}
```

<span dir="rtl">المعيار الأساسي هو ما إذا كان هذا أكبر من أو أقل
من</span> $`v_{\pi}(S)`$<span dir="rtl">إذا كان أكبر—أي، إذا كان من
الأفضل اختيار</span> "$`a`$" <span dir="rtl">مرة واحدة في</span> $`s`$
<span dir="rtl">ومن ثم متابعة</span> "$`\pi`$" <span dir="rtl">بدلاً من
متابعة</span> $`\pi`$ <span dir="rtl">طوال الوقت—فمن المتوقع أن يكون من
الأفضل أيضًا اختيار</span> $`a`$ <span dir="rtl">في كل مرة يتم فيها
الوصول إلى</span> $`s`$<span dir="rtl">، وأن السياسة الجديدة ستكون في
الواقع أفضل بشكل عام</span>.

<span dir="rtl">أن هذا صحيح هو حالة خاصة من نتيجة عامة تُسمى "نظرية تحسين
السياسة (</span>policy improvement
<span dir="rtl"></span>theorem<span dir="rtl">).</span>
<span dir="rtl">لنفترض أن</span> $`\pi`$
<span dir="rtl">و</span>$`\pi'`$ <span dir="rtl">هما أي زوج من السياسات
الحتمية بحيث، لجميع</span> s ∈ S<span dir="rtl">،</span>

``` math
q_{\pi}\left( s,\pi'(s) \right) \geq v_{\pi}(s)
```

<span dir="rtl">ثم يجب أن تكون السياسة</span> $`\pi'`$
<span dir="rtl">بقدر ما هي جيدة أو أفضل من</span>
$`\pi`$<span dir="rtl">.</span> <span dir="rtl">أي أنها يجب أن تحقق
عائدًا متوقعًا أكبر أو يساوي من جميع الحالات</span> $`s \in S`$
<span dir="rtl"></span>

``` math
v_{\pi'}(s) \geq v_{\pi}(s)
```

<span dir="rtl">علاوة على ذلك، إذا كان هناك عدم تساوي صارم في المعادلة
(4.7) في أي حالة، فيجب أن يكون هناك عدم تساوي صارم في المعادلة (4.8) في
تلك الحالة. تنطبق هذه النتيجة بشكل خاص على السياسات التي نظرنا فيها في
الفقرة السابقة، وهي السياسة الحتمية الأصلية</span>
$`\pi`$<span dir="rtl">، والسياسة المعدلة</span>
$`\pi'`$<span dir="rtl">، التي تكون متماثلة مع</span> $`\pi`$
<span dir="rtl">باستثناء أن</span> $`\pi'(s) = a \neq \pi(s)`$
<span dir="rtl">من الواضح أن المعادلة (4.7) صحيحة في جميع الحالات الأخرى
بخلاف</span> $`s`$<span dir="rtl">.</span> <span dir="rtl">لذا، إذا
كانت</span> $`{q_{\pi}(s,a) > v}_{\pi}`$(s)<span dir="rtl">، فإن السياسة
المعدلة تكون بالفعل أفضل من</span> $`\pi`$ <span dir="rtl">فكرة إثبات
نظرية تحسين السياسة سهلة الفهم. بدءًا من المعادلة (4.7)، نواصل توسيع
جانب</span> $`q_{\pi}`$ <span dir="rtl"></span> <span dir="rtl">باستخدام
المعادلة (4.6) وإعادة تطبيق المعادلة (4.7) حتى نحصل على</span>
$`v_{\pi}'(s)`$

``` math
\begin{matrix}
v_{\pi}(s) & \  \leq q_{\pi}\left( s,\pi'(s) \right) \\
 & \  = \mathbb{E}\left\lbrack R_{t + 1} + \gamma v_{\pi}\left( S_{t + 1} \right) \mid S_{t} = s,A_{t} = \pi'(s) \right\rbrack \\
 & \  = \mathbb{E}_{\pi'}\left\lbrack R_{t + 1} + \gamma v_{\pi}\left( S_{t + 1} \right) \mid S_{t} = s \right\rbrack \\
 & \  \leq \mathbb{E}_{\pi'}\left\lbrack R_{t + 1} + \gamma q_{\pi}\left( S_{t + 1},\pi'\left( S_{t + 1} \right) \right) \mid S_{t} = s \right\rbrack \\
 & \  = \mathbb{E}_{\pi'}\left\lbrack R_{t + 1} + \gamma\mathbb{E}_{\pi'}\left( R_{t + 2} + \gamma v_{\pi}\left( S_{t + 2} \right) \mid S_{t + 1},A_{t + 1} = \pi'\left( S_{t + 1} \right) \right\rbrack \mid S_{t} = s \right\rbrack \\
 & \  = \mathbb{E}_{\pi'}\left\lbrack R_{t + 1} + \gamma R_{t + 2} + \gamma^{2}v_{\pi}\left( S_{t + 2} \right) \mid S_{t} = s \right\rbrack \\
 & \  \leq \mathbb{E}_{\pi'}\left\lbrack R_{t + 1} + \gamma R_{t + 2} + \gamma^{2}R_{t + 3} + \gamma^{3}v_{\pi}\left( S_{t + 3} \right) \mid S_{t} = s \right\rbrack \\
 & \  \vdots \\
 & \  \leq \mathbb{E}_{\pi'}\left\lbrack R_{t + 1} + \gamma R_{t + 2} + \gamma^{2}R_{t + 3} + \gamma^{3}R_{t + 4} + \cdots \mid S_{t} = s \right\rbrack \\
 & \  = v_{\pi'}(s)
\end{matrix}
```

<span dir="rtl">حتى الآن، رأينا كيف يمكن، بالنظر إلى سياسة ودالة قيمتها،
تقييم تغيير في السياسة عند حالة واحدة إلى إجراء معين. من الطبيعي أن
نعتبر التغييرات في جميع الحالات وجميع الإجراءات الممكنة، باختيار في كل
حالة الإجراء الذي يبدو الأفضل وفقًا لـ</span>
$`q_{\pi}(s,a)`$<span dir="rtl">. بمعنى آخر، لتقدير السياسة الجشعة
الجديدة،</span> $`\pi'`$<span dir="rtl">، المعطاة بواسطة</span>

``` math
\pi'(s) = \text{argmax}_{a}\ q_{\pi}(s,a)
```

``` math
\pi'(s) = \text{argmax}_{a}\ E\left\lbrack R_{t + 1} + \gamma v_{\pi}\left( S_{t + 1} \right)\mid S_{t} = s,A_{t} = a \right\rbrack
```

``` math
\pi'(s) = \text{argmax}_{a}\ \sum_{s',r}^{}{p\left( s',r\mid s,a \right)\left( r + \gamma v_{\pi}\left( s' \right) \right)}
```

<span dir="rtl">حيث يشير</span> $`argmax\_ a\ `$<span dir="rtl">إلى
قيمة</span> $`a`$ <span dir="rtl">التي يتم فيها تعظيم التعبير الذي يليها
(مع كسر التعادل بشكل تعسفي). تأخذ السياسة الجشعة الإجراء الذي يبدو
الأفضل على المدى القصير—بعد خطوة واحدة من التطلع—وفقًا
لـ</span>"$`v_{\pi}`$"<span dir="rtl">.</span> <span dir="rtl">من خلال
البناء، تلتقي السياسة الجشعة بشروط نظرية تحسين السياسة (4.7)، لذا نعلم
أنها تكون بقدر ما هي جيدة أو أفضل من السياسة الأصلية. تُسمى عملية إنشاء
سياسة جديدة تحسن من السياسة الأصلية، من خلال جعلها جشعة بالنسبة لدالة
قيمة السياسة الأصلية، "تحسين السياسة</span> (policy
improvement)<span dir="rtl">.</span>

<span dir="rtl">افترض أن السياسة الجشعة الجديدة،</span>
$`\pi'`$<span dir="rtl">، تكون جيدة كما هي، ولكن ليست أفضل من السياسة
القديمة</span> $`\pi`$<span dir="rtl">.</span> <span dir="rtl">عندها
تكون</span> $`v_{\pi} = v_{\pi'}`$​<span dir="rtl">، ومن المعادلة (4.9)
يتبع أنه لجميع</span> $`s \in S`$ <span dir="rtl"></span>:

``` math
v_{\pi'}(s) = \max_{a}\ E\left\lbrack R_{t + 1} + \gamma v_{\pi'}\left( S_{t + 1} \right)\mid S_{t} = s,A_{t} = a \right\rbrack
```

``` math
v_{\pi'}(s) = \max_{a}\ \ \sum_{s',r}^{}{p\left( s',r\mid s,a \right)\left( r + \gamma v_{\pi'}\left( s' \right) \right)}
```

<span dir="rtl">ولكن هذا هو نفس معادلة الأمثلى لبيلمان</span> (Bellman
optimality equation) (4.1)<span dir="rtl">، وبالتالي، يجب أن يكون</span>
$`v\pi'`$ <span dir="rtl">هو</span> $`v*`$<span dir="rtl">، ويجب أن تكون
كل من</span> $`\pi`$ <span dir="rtl">و</span>$`\pi'`$
<span dir="rtl">سياسات مثلى. لذلك، يجب أن يوفر **تحسين**
**السياسة**</span> **policy improvement** <span dir="rtl">سياسة أفضل
بشكل صارم باستثناء الحالات التي تكون فيها السياسة الأصلية بالفعل
مثلى</span>.

<span dir="rtl">حتى الآن في هذا القسم، نظرنا في الحالة الخاصة للسياسات
الحتمية. في الحالة العامة، تحدد السياسة العشوائية</span> $`\pi`$
<span dir="rtl">احتمالات،</span> $`\pi(a \mid s)`$<span dir="rtl">، لأخذ
كل إجراء،</span> $`a`$<span dir="rtl">، في كل حالة،</span>
$`s`$<span dir="rtl">.</span> <span dir="rtl">لن نتطرق إلى التفاصيل،
ولكن في الواقع، تمتد جميع أفكار هذا القسم بسهولة إلى السياسات العشوائية.
على وجه الخصوص، تنطبق **نظرية تحسين السياسة**</span>
**<span dir="rtl">(</span>policy improvement
theorem<span dir="rtl">)</span>** <span dir="rtl">كما هو مذكور للحالة
العشوائية. بالإضافة إلى ذلك، إذا كانت هناك تعادلات في خطوات تحسين
السياسة مثل "(4.9)"—أي، إذا كانت هناك عدة إجراءات يتم تحقيق الحد الأقصى
عندها—ففي الحالة العشوائية لا نحتاج إلى اختيار إجراء واحد من بينها. بدلاً
من ذلك، يمكن إعطاء كل إجراء محقق للحد الأقصى جزءًا من احتمالية اختياره في
السياسة **الجشعة الجديدة**</span> **<span dir="rtl">(</span>greedy
policy<span dir="rtl">)</span>.** <span dir="rtl">يُسمح بأي نظام تقسيم
طالما أن جميع الإجراءات غير المثلى تُعطى احتمالية صفرية</span>.

<span dir="rtl">الصف الأخير من **الشكل 4.1** يظهر مثالاً على **تحسين
السياسة**</span> **policy improvement** <span dir="rtl">للسياسات
العشوائية. هنا، السياسة الأصلية</span> $`\pi`$ <span dir="rtl">هي سياسة
عشوائية متساوية الاحتمالات</span> **equiprobable random
policy**<span dir="rtl">، والسياسة الجديدة</span> $`\pi'`$
<span dir="rtl">هي سياسة جشعة</span> greedy policy
<span dir="rtl">بالنسبة إلى</span> $`v\pi`$<span dir="rtl">.</span>
<span dir="rtl">تُظهر دالة القيمة</span> $`v\pi`$ <span dir="rtl">في
الرسم البياني السفلي الأيسر، ومجموعة السياسات الممكنة</span> $`\pi'`$
<span dir="rtl">تُظهر في الرسم البياني السفلي الأيمن. الحالات التي تحتوي
على عدة أسهم في مخطط</span> $`\pi'`$ <span dir="rtl">هي تلك التي تحقق
فيها عدة إجراءات الحد الأقصى في "(4.9)"؛ يُسمح بأي تقسيم للاحتمالات بين
هذه الإجراءات. يمكن ملاحظة أن دالة القيمة لأي سياسة من هذا النوع،</span>
$`v\pi'(s)`$<span dir="rtl">، هي إما 1-، 2-، أو 3- في جميع
الحالات،</span> $`s \in S`$<span dir="rtl">، في حين أن</span>
$`v\pi(s)`$ <span dir="rtl">هي على الأكثر 14-. وبالتالي  
</span>$`\ v\pi'(s) \geq v\pi(s)v`$<span dir="rtl">، في جميع
الحالات</span> $`s \in S`$<span dir="rtl">، مما يوضح تحسين
السياسة</span> policy improvement<span dir="rtl">.</span>
<span dir="rtl">على الرغم من أن السياسة الجديدة</span> π′
<span dir="rtl">في هذه الحالة قد تكون مثلى، فإن التحسين فقط هو ما يضمن
بشكل عام</span>.

<u>4.3 **<span dir="rtl">تكرار السياسة</span> (Policy Iteration)**</u>

<span dir="rtl">بمجرد تحسين سياسة ما،</span> $`\pi`$<span dir="rtl">،
باستخدام</span> $`v\pi`$ <span dir="rtl">للحصول على سياسة أفضل،</span>
$`\pi'`$<span dir="rtl">، يمكننا بعد ذلك حساب</span>
$`\ v\pi'`$<span dir="rtl">وتحسينها مرة أخرى للحصول على</span> $`\pi''`$
<span dir="rtl">أفضل. وبهذا يمكننا الحصول على تسلسل من السياسات ودوال
القيمة التي تتحسن بشكل رتيب</span>:

``` math
\pi_{0}\overset{\phantom{E}}{\rightarrow}v_{\pi_{0}}\overset{\phantom{I}}{\rightarrow}\pi_{1}\overset{\phantom{E}}{\rightarrow}v_{\pi_{1}}\overset{\phantom{I}}{\rightarrow}\pi_{2}\overset{\phantom{E}}{\rightarrow}\cdots\overset{\phantom{I}}{\rightarrow}\pi_{*}\overset{\phantom{E}}{\rightarrow}v_{*},
```

<span dir="rtl">حيث يشير الرمز</span>
$`\overset{\phantom{E}}{\rightarrow}`$ <span dir="rtl"></span>
<span dir="rtl">إلى **تقييم السياسة** </span>**(policy
evaluation)**<span dir="rtl">، ويشير الرمز</span>
$`\overset{\phantom{I}}{\rightarrow}`$ <span dir="rtl">إلى  
**تحسين السياسة**</span> **<span dir="rtl">(</span>policy
improvement<span dir="rtl">).</span>** <span dir="rtl"></span>
<span dir="rtl">كل سياسة تكون تحسينًا صارمًا عن السابقة (إلا إذا كانت
السياسة بالفعل مثالية). وبما أن **عملية اتخاذ القرار ماركوف**</span>
**(MDP)** <span dir="rtl">لها عدد محدود من السياسات، يجب أن يتقارب هذا
العملية إلى سياسة مثالية ودالة قيمة مثالية في عدد محدود من التكرارات.
تُسمى هذه الطريقة لإيجاد السياسة المثالية **تكرار السياسة**
</span>**(policy iteration)**<span dir="rtl">.</span>
<span dir="rtl">يتم تقديم خوارزمية كاملة في المربع أدناه. لاحظ أن كل
تقييم للسياسة، وهو بحد ذاته حساب تكراري، يبدأ بدالة القيمة للسياسة
السابقة. يؤدي ذلك عادةً إلى زيادة كبيرة في سرعة تقارب تقييم السياسة (على
الأرجح لأن دالة القيمة تتغير قليلاً من سياسة إلى أخرى)</span>.

<span dir="rtl">تكرار السياسة</span> (Policy Iteration)
<span dir="rtl">باستخدام تقييم السياسة التكراري لتقدير</span> π
<span dir="rtl">و</span>π∗

### <img src="./media/image26.png"
style="width:6.26806in;height:4.27083in" />

### 

### <span dir="rtl"><u>مثال 4.2</u>: تأجير سيارات جاك</span>

<span dir="rtl">يدير جاك موقعين لشركة تأجير سيارات على مستوى البلاد. في
كل يوم، يصل عدد من العملاء إلى كل موقع لاستئجار سيارات. إذا كان لدى جاك
سيارة متاحة، يقوم بتأجيرها ويحصل على 10 دولارات من الشركة الوطنية. إذا
كان لا يوجد لديه سيارات في ذلك الموقع، فإن العمل يكون ضائعًا. تصبح
السيارات متاحة للإيجار في اليوم التالي بعد إعادتها. لضمان توفر السيارات
في الأماكن التي تحتاجها، يمكن لجاك نقلها بين الموقعين خلال الليل، بتكلفة
قدرها 2 دولار لكل سيارة يتم نقلها</span>.

<span dir="rtl">نفترض أن عدد السيارات المطلوبة والمُعادة في كل موقع هي
متغيرات عشوائية بواسون</span> (Poisson)<span dir="rtl">، مما يعني أن
احتمال أن يكون العدد</span> n <span dir="rtl">هو</span>
$`\frac{\leftthreetimes^{n}}{n\mathbb{i}}\mathbb{e}^{- \leftthreetimes}`$
<span dir="rtl"></span>​<span dir="rtl">، حيث</span> $`\leftthreetimes`$
<span dir="rtl">هو العدد المتوقع. لنفترض أن</span> $`\leftthreetimes`$
<span dir="rtl">هو 3 و4 لطلبات التأجير في الموقعين الأول والثاني، و3 و2
للإعادات. لتبسيط المشكلة قليلاً، نفترض أنه لا يمكن أن يكون هناك أكثر من
20 سيارة في كل موقع (وأي سيارات إضافية تُعاد إلى الشركة الوطنية، وبالتالي
تختفي من المشكلة) ويمكن نقل ما يصل إلى خمس سيارات من موقع إلى آخر في
ليلة واحدة. نأخذ معدل الخصم</span> γ=0.9 <span dir="rtl">ونعمل على صياغة
هذه المشكلة كمشكلة</span> MDP <span dir="rtl">منتهية مستمرة، حيث تكون
الخطوات الزمنية هي الأيام، والحالة هي عدد السيارات في كل موقع في نهاية
اليوم، والإجراءات هي الأعداد الصافية للسيارات التي يتم نقلها بين
الموقعين خلال الليل</span>.

<span dir="rtl">تظهر الشكل 4.2 تسلسل السياسات التي تم العثور عليها
باستخدام **تكرار السياسة**</span> **<span dir="rtl">(</span>Policy
<span dir="rtl"></span>Iteration<span dir="rtl">)</span>**
<span dir="rtl">بدءًا من السياسة التي لا تنقل أي سيارات</span>.

<img src="./media/image27.png"
style="width:6.26806in;height:3.66597in" />

<span dir="rtl">الشكل 4.2: تسلسل السياسات التي تم العثور عليها باستخدام
**تكرار السياسة**</span> **(Policy Iteration)** <span dir="rtl">في مشكلة
تأجير سيارات جاك، ودالة القيمة النهائية للحالة. تُظهر الرسوم البيانية
الخمسة الأولى، لكل عدد من السيارات في كل موقع في نهاية اليوم، عدد
السيارات التي يجب نقلها من الموقع الأول إلى الموقع الثاني (الأرقام
السلبية تشير إلى التحويلات من الموقع الثاني إلى الأول). كل سياسة تالية
هي تحسين صارم على السياسة السابقة، والسياسة الأخيرة هي المثلى</span>.

**<span dir="rtl">تكرار السياسة</span> (Policy Iteration)**
<span dir="rtl">غالبًا ما يتقارب في عدد قليل من التكرارات بشكل مدهش، كما
يوضح مثال تأجير سيارات جاك، وأيضًا كما هو موضح في المثال في الشكل 4.1.
يُظهر الرسم البياني في الأسفل الأيسر من الشكل 4.1 دالة القيمة للسياسة
العشوائية المتساوية الاحتمال، ويعرض الرسم البياني في الأسفل الأيمن سياسة
جشعة لهذه الدالة. يؤكد **نظرية تحسين السياسة**</span>
**<span dir="rtl">(</span>Policy <span dir="rtl"></span>Improvement
Theorem<span dir="rtl">)</span>** <span dir="rtl">أن هذه السياسات أفضل
من السياسة العشوائية الأصلية. في هذه الحالة، ومع ذلك، فإن هذه السياسات
ليست أفضل فقط، بل مثلى، حيث تنتقل إلى الحالات النهائية في أقل عدد من
الخطوات. في هذا المثال، سيجد تكرار السياسة المثلى بعد تكرار واحد
فقط</span>.

### <span dir="rtl"><u>تمرين 4.4:</u></span>

### <span dir="rtl">يحتوي خوارزمية **تكرار السياسة**</span> **(Policy Iteration)** <span dir="rtl">في الصفحة 80 على خطأ خفي حيث قد لا تنتهي أبدًا إذا كانت السياسة تتبدل باستمرار بين سياسات أو أكثر التي تكون جيدة بنفس القدر. هذا مقبول لأغراض التعليم، ولكن ليس للاستخدام الفعلي. قم بتعديل **الشيفرة**</span> **pseudo-code** <span dir="rtl">بحيث يتم ضمان التقارب</span>.

### 

### <span dir="rtl"><u>تمرين4.5</u> َ كيف سيتم تعريف **تكرار السياسة**</span> **(Policy Iteration)** <span dir="rtl">لقيم الإجراءات؟ قدم خوارزمية كاملة لحساب</span> $`q*`$<span dir="rtl">، مماثلة لتلك الموجودة في الصفحة 80 لحساب</span> $`v*`$<span dir="rtl">.</span> <span dir="rtl">يرجى إيلاء اهتمام خاص لهذا التمرين، لأن الأفكار المعنية ستُستخدم في جميع أنحاء الكتاب</span>.

### <span dir="rtl"><u>تمرين 4.6</u> َ َ افترض أنك مقيد بالنظر فقط في السياسات التي تكون</span> ($`\varepsilon - soft`$)<span dir="rtl">، مما يعني أن احتمالية اختيار كل إجراء في كل حالة</span> s <span dir="rtl">هي على الأقل</span> $`\epsilon/ \mid A(s) \mid`$<span dir="rtl">.</span> <span dir="rtl">صف كيف ستتغير بشكل نوعي كل من الخطوات 3 و2 و1، بالتتابع، في خوارزمية **تكرار السياسة**</span> **(Policy Iteration)** <span dir="rtl">لدالة القيمة</span> $`v*`$ <span dir="rtl">الموجودة في الصفحة 80</span>.

### <span dir="rtl">تمرين 4.7 (برمجة)</span>

<span dir="rtl">اكتب برنامجًا لتكرار السياسة وأعد حل مشكلة تأجير سيارات
جاك مع التعديلات التالية</span>:

- <span dir="rtl">أحد موظفي جاك في الموقع الأول يستقل الحافلة إلى منزله
  كل ليلة ويعيش بالقرب من الموقع الثاني. هو سعيد بنقل سيارة واحدة إلى
  الموقع الثاني مجانًا. كل سيارة إضافية لا تزال تكلف 2 دولار، وكذلك جميع
  السيارات التي يتم نقلها في الاتجاه الآخر</span>.

- <span dir="rtl">بالإضافة إلى ذلك، لدى جاك مساحة محدودة لوقوف السيارات
  في كل موقع. إذا تم الاحتفاظ بأكثر من 10 سيارات في موقع واحد خلال الليل
  (بعد أي نقل للسيارات)، فيجب تكبد تكلفة إضافية قدرها 4 دولارات لاستخدام
  موقف سيارات ثاني (بغض النظر عن عدد السيارات التي تُحتفظ بها
  هناك)</span>.

<span dir="rtl">تحدث هذه الأنواع من غير الخطيات والديناميات العشوائية في
المشاكل الحقيقية ولا يمكن التعامل معها بسهولة بواسطة طرق التحسين بخلاف
البرمجة الديناميكية. للتحقق من برنامجك، قم أولاً بتكرار النتائج المعطاة
للمشكلة الأصلية</span>.

<u>**4**.**4** **<span dir="rtl">تكرار القيمة</span> (Value
Iteration)**</u>

<span dir="rtl">إحدى عيوب **تكرار السياسة**</span> **(Policy
Iteration)** <span dir="rtl">هي أن كل تكرار من تكراراته يتطلب تقييم
السياسة، والذي قد يكون بدوره عملية تكرارية طويلة تتطلب عدة مرات مرور عبر
مجموعة الحالات. إذا تم إجراء تقييم السياسة بشكل تكراري، فإن التقارب
الدقيق إلى</span> $`v\pi`$ <span dir="rtl">يحدث فقط في النهاية. هل يجب
علينا الانتظار لتحقيق التقارب الكامل، أم يمكننا التوقف قبل ذلك؟</span>

<span dir="rtl">يوضح المثال في الشكل 4.1 بالتأكيد أنه قد يكون من الممكن
تقليص تقييم السياسة. في هذا المثال، فإن التكرارات الإضافية لتقييم
السياسة بعد الثلاثة الأولى ليس لها تأثير على السياسة الجشعة
المقابلة</span>.

<span dir="rtl">في الواقع، يمكن تقليص خطوة تقييم السياسة في **تكرار
السياسة**</span> **(Policy Iteration)** <span dir="rtl">بطرق متعددة دون
فقدان ضمانات التقارب لتكرار السياسة. إحدى الحالات الخاصة المهمة هي عندما
يتم إيقاف تقييم السياسة بعد مرور واحد فقط (تحديث واحد لكل حالة). تُسمى
هذه الخوارزمية **تكرار القيمة**</span> **<span dir="rtl">(</span>Value
<span dir="rtl"></span>Iteration<span dir="rtl">).</span>**
<span dir="rtl">يمكن كتابتها كعملية تحديث بسيطة للغاية تجمع بين تحسين
السياسة وخطوات تقييم السياسة المقطوعة</span>:

``` math
v_{k + 1}(s) = \max_{a}E\left\lbrack R_{t + 1} + \gamma v_{k}\left( S_{t + 1} \right)\mid S_{t} = s,A_{t} = a \right\rbrack
```

``` math
v_{k + 1}(s) = \max_{a}{\sum_{s',r}^{}{p\left( s',r\mid s,a \right)\left\lbrack r + \gamma v_{k}\left( s' \right) \right\rbrack}}
```

<span dir="rtl">لكل</span> $`s \in S`$ <span dir="rtl"></span>.
<span dir="rtl">بالنسبة لأي</span> $`v0`$
<span dir="rtl"></span>​<span dir="rtl">، يمكن إثبات أن التسلسل</span>
{vk} <span dir="rtl">يتقارب إلى</span> $`v*`$ <span dir="rtl">تحت نفس
الشروط التي تضمن وجود</span> $`v*`$<span dir="rtl">.</span>

<span dir="rtl">طريقة أخرى لفهم **تكرار القيمة**</span> **(Value
Iteration)** <span dir="rtl">هي بالإشارة إلى معادلة **بيلمان المثلى
(**</span>**Bellman <span dir="rtl"></span>Optimality
Equation<span dir="rtl">)</span>**
<span dir="rtl"></span>(4.1)<span dir="rtl">.</span>
<span dir="rtl">لاحظ أن **تكرار القيمة** يتم الحصول عليه ببساطة عن طريق
تحويل معادلة بيلمان المثلى إلى قاعدة تحديث. كما يمكنك ملاحظة أن تحديث
**تكرار القيمة** مطابق لتحديث **تقييم السياسة**</span> **(Policy
Evaluation)** (4.5) <span dir="rtl">باستثناء أنه يتطلب أخذ الحد الأقصى
على جميع الإجراءات. طريقة أخرى لرؤية العلاقة الوثيقة هي مقارنة مخططات
الاحتياطي لهذه الخوارزميات في الصفحة 59 (تقييم السياسة) وعلى اليسار في
الشكل 3.4 (تكرار القيمة). هذان هما عمليتا الاحتياطي الطبيعيان
لحساب</span> $`v\pi`$
<span dir="rtl">و</span>$`v*`$<span dir="rtl">.</span>

<span dir="rtl">أخيرًا، دعنا نعتبر كيف ينتهي **تكرار القيمة**
</span>**(Value Iteration)**<span dir="rtl">.</span> <span dir="rtl">مثل
**تقييم السياسة**، يتطلب **تكرار القيمة** بشكل رسمي عددًا لا نهائيًا من
التكرارات للتقارب بدقة إلى</span> $`v*`$<span dir="rtl">.</span>
<span dir="rtl">عمليًا، نتوقف بمجرد أن يتغير دالة القيمة بمقدار صغير فقط
في مرور واحد. تعرض الصندوق أدناه خوارزمية كاملة مع هذا النوع من شرط
التوقف</span>.

**<span dir="rtl">تكرار القيمة</span> (Value
Iteration)**<span dir="rtl">، لتقدير</span>$`\pi \approx \pi_{*}`$

<img src="./media/image28.png"
style="width:6.26806in;height:2.87986in" />

``` math
\pi(s) = arg\max_{a}\mspace{2mu}\sum_{s',r}^{}\mspace{2mu} p\left( s',r \mid s,a \right)\left\lbrack r + \gamma V\left( s' \right) \right\rbrack
```

<span dir="rtl">**تكرار القيمة**</span> ("Value Iteration”)
<span dir="rtl">يجمع بفعالية، في كل عملية تمرير، بين تمرير واحد لتقييم
السياسة وتمريرة واحدة لتحسين السياسة. يتم تحقيق تقارب أسرع غالبًا عن طريق
إدراج عدة تمريرات لتقييم السياسة بين كل تمريرة لتحسين السياسة. بشكل عام،
يمكن اعتبار فئة كاملة من خوارزميات **تكرار السياسة المقتطع**</span>
("Truncated Policy Iteration”) <span dir="rtl">كأنها تسلسلات من
التمريرات، بعضها يستخدم تحديثات لتقييم السياسة وبعضها يستخدم تحديثات
لتكرار القيمة. نظرًا لأن عملية</span> "$`\max`$” <span dir="rtl">في
(4.10) هي الفرق الوحيد بين هذه التحديثات، فهذا يعني ببساطة أنه يتم إضافة
عملية</span> "$`\max`$” <span dir="rtl">إلى بعض تمريرات تقييم السياسة.
جميع هذه الخوارزميات تتقارب إلى سياسة مثلى لمشاكل اتخاذ القرار المحدودة
ذات الخصم</span>.

<img src="./media/image29.png"
style="width:2.92222in;height:2.92639in" />**<span dir="rtl"><u>مثال
4.3</u>: مشكلة المقامر</span>**

<span dir="rtl">المقامر لديه فرصة لوضع رهانات على نتائج سلسلة من رميات
العملة. إذا جاءت العملة "وجه</span> ("$`heads`$")<span dir="rtl">، يربح
المبلغ الذي راهن عليه في هذه الرمية؛ وإذا جاءت "كتف</span>
("$`tails`$")<span dir="rtl">، يخسر رهانه. تنتهي اللعبة عندما يربح
المقامر عن طريق الوصول إلى هدفه المتمثل في 100 دولار، أو يخسر عندما ينفد
المال. في كل رمية، يجب على المقامر أن يقرر ما هو مقدار المال الذي يراهن
به، بأعداد صحيحة من الدولارات. يمكن صياغة هذه المشكلة كمشكلة اتخاذ قرار
غير مخفضة، حلقية، ومحدودة. الحالة هي رأس المال للمقامر،</span>
$`s\  \in \text{\{}1,\ 2,\ \ldots,\ 99\text{\}}`$
<span dir="rtl">والإجراءات هي الرهانات،  
</span>$`a \in \text{\{}0,1,\ldots,\min(s,100 - s)\text{\}}`$
<span dir="rtl">المكافأة هي صفر في جميع الانتقالات باستثناء تلك التي يصل
فيها المقامر إلى هدفه، حيث تكون 1+</span>.

<span dir="rtl">تُعطي دالة قيمة الحالة بعد ذلك احتمال الفو ز من كل حالة.
السياسة هي تحويل من مستويات رأس المال إلى الرهانات. السياسة المثلى تعظم
احتمال الوصول إلى الهدف. دع</span> $`p_{h}`$ <span dir="rtl">تشير إلى
احتمال ظهور "وجه</span>" ("$`heads`$") <span dir="rtl">للعملة. إذا
كان</span> $`p_{h}`$ <span dir="rtl">معروفًا، فإن المشكلة بالكامل معروفة
ويمكن حلها، على سبيل المثال، باستخدام تكرار القيمة</span> ("Value
Iteration")<span dir="rtl">.</span> <span dir="rtl">يوضح الشكل 4.3
التغير في دالة القيمة عبر التمريرات المتعاقبة لتكرار القيمة، والسياسة
النهائية التي تم العثور عليها، في حالة</span> $`p_{h} = 0.4`$.
<span dir="rtl">هذه السياسة مثلى، ولكنها ليست فريدة. في الواقع، هناك
عائلة كاملة من السياسات المثلى، جميعها تتوافق مع التعادل في اختيار
إجراء</span> "$`argmax`$" <span dir="rtl">بالنسبة لدالة القيمة المثلى.
هل يمكنك تخمين شكل العائلة الكاملة؟</span>

**<span dir="rtl">الشكل 4.3</span>**: <span dir="rtl">حل مشكلة المقامر
بالنسبة لـ.</span> $`p_{h} = 0.4`$. <span dir="rtl">يُظهر الرسم البياني
العلوي دالة القيمة التي تم العثور عليها من خلال التكرارات المتتالية
لتكرار القيمة. يُظهر الرسم البياني السفلي السياسة النهائية</span>.

**<span dir="rtl"><u>التمرين 4.8</u>:</span>** <span dir="rtl">لماذا
تمتلك السياسة المثلى لمشكلة المقامر شكلاً غريباً؟ بشكل خاص، بالنسبة لرأس
المال الذي يبلغ 50، يقوم بالمقامرة بكل شيء في رهان واحد، ولكن بالنسبة
لرأس المال الذي يبلغ 51، لا يفعل ذلك. لماذا تُعتبر هذه سياسة جيدة؟</span>

**<span dir="rtl"><u>التمرين 4.9 (برمجة)</u>:</span>**
<span dir="rtl">نفذ تكرار القيمة لمشكلة المقامر وحلها لقيم</span>
$`ph = 0.25`$
<span dir="rtl">و</span>p$`h = 0.55`$<span dir="rtl">.</span>
<span dir="rtl">في البرمجة، قد تجد أنه من المناسب إدخال حالتين وهميتين
تتعلقان بالإنهاء برأس المال 0 و100، ومنحهما القيم 0 و1 على التوالي. اعرض
نتائجك بشكل رسومي، كما في الشكل 4.3. هل نتائجك مستقرة عندما يقترب</span>
θ <span dir="rtl">من 0؟</span>

**<span dir="rtl"><u>التمرين 4.10:</u></span>** <span dir="rtl">ما هو
النظير لتحديث تكرار القيمة (4.10) بالنسبة لقيم الإجراءات،</span>
$`qk + 1(s,a)`$<span dir="rtl">؟</span>

**<u>4.5 <span dir="rtl">البرمجة الديناميكية غير المتزامنة</span>
(Asynchronous Dynamic Programming)</u>**

<span dir="rtl">أحد العيوب الكبيرة لطرق البرمجة الديناميكية التي
ناقشناها حتى الآن هو أنها تتضمن عمليات على مجموعة الحالات بالكامل في
عملية الـ</span> **MDP**<span dir="rtl">، أي أنها تتطلب تنفيذ التحديثات
على مجموعة الحالات. إذا كانت مجموعة الحالات كبيرة جدًا، فإن حتى تنفيذ
واحد يمكن أن يكون مكلفًا بشكل غير معقول. على سبيل المثال، تحتوي لعبة
الطاولة على أكثر من</span> $`\ \ 10^{20}`$<span dir="rtl">حالة. حتى إذا
كان بإمكاننا تنفيذ تحديث قيمة على مليون حالة في الثانية، فسوف يستغرق
الأمر أكثر من ألف سنة لإكمال تنفيذ واحد</span>.

<span dir="rtl">تعتبر خوارزميات البرمجة الديناميكية غير المتزامنة
خوارزميات تكرارية تنفذ التحديثات بدون الحاجة إلى تنظيم النظام في شكل
تنفيذات منظمة لمجموعة الحالات. تقوم هذه الخوارزميات بتحديث قيم الحالات
بأي ترتيب كان، باستخدام القيم المتاحة للحالات الأخرى. قد يتم تحديث قيم
بعض الحالات عدة مرات قبل تحديث قيم حالات أخرى مرة واحدة. ومع ذلك، لضمان
التقارب بشكل صحيح، يجب على خوارزمية غير متزامنة أن تستمر في تحديث قيم
جميع الحالات؛ لا يمكنها تجاهل أي حالة بعد نقطة معينة في الحساب. تتيح
خوارزميات البرمجة الديناميكية غير المتزامنة مرونة كبيرة في اختيار
الحالات لتحديثها</span>.

<span dir="rtl">على سبيل المثال، تقوم إحدى نسخ تنفيذ قيمة غير المتزامنة
بتحديث قيمة حالة واحدة فقط،</span> $`s_{k}`$ <span dir="rtl">، في كل
خطوة،</span> “$`k`$”<span dir="rtl">، باستخدام تحديث قيمة (“4.10”). إذا
كان</span> “$`0 \leq \gamma < 1`$”<span dir="rtl">، فإن التقارب الأسِّي
إلى</span> $`v*`$ <span dir="rtl">مضمون إذا تكررت جميع الحالات في
التسلسل</span> $`s_{k}`$ <span dir="rtl">عددًا لا نهائيًا من المرات (يمكن
أن يكون التسلسل حتى عشوائيًا). (في حالة الأحداث غير المخصومة، من الممكن
أن تكون هناك بعض الترتيبات للتحديثات التي لا تؤدي إلى التقارب، لكن من
السهل نسبيًا تجنبها). بشكل مشابه، من الممكن مزج تقييم السياسة وتحديث
القيمة لإنتاج نوع من تكرار السياسة المقطوع غير المتزامن. على الرغم من أن
تفاصيل هذه الخوارزميات وغيرها من خوارزميات البرمجة الديناميكية غير
المعتادة تتجاوز نطاق هذا الكتاب، إلا أنه من الواضح أن عددًا قليلاً من
التحديثات المختلفة تشكل لبنات البناء التي يمكن استخدامها بمرونة في
مجموعة واسعة من خوارزميات البرمجة الديناميكية بدون تنفيذات منظمة</span>.

<span dir="rtl">بالطبع، لا يعني تجنب تنفيذات المنظمة بالضرورة أننا
يمكننا الاستغناء عن الحسابات. يعني ذلك فقط أن الخوارزمية لا تحتاج إلى أن
تعلق في تنفيذ طويل بشكل ميؤوس منه قبل أن تتمكن من إحراز تقدم في تحسين
السياسة. يمكننا محاولة الاستفادة من هذه المرونة من خلال اختيار الحالات
التي نطبق عليها التحديثات بحيث نحسن معدل تقدم الخوارزمية. يمكننا محاولة
ترتيب التحديثات للسماح للمعلومات القيمة بالانتشار من حالة إلى أخرى
بطريقة فعالة. قد لا تحتاج بعض الحالات إلى تحديث قيمها بقدر ما تحتاج إليه
حالات أخرى. قد نحاول حتى تخطي تحديث بعض الحالات تمامًا إذا لم تكن ذات صلة
بالسلوك الأمثل. تمت مناقشة بعض الأفكار حول كيفية القيام بذلك في الفصل
8</span>.

**<span dir="rtl">البرمجة الديناميكية غير المتزامنة</span> (Asynchronous
Dynamic Programming)** <span dir="rtl">تسهّل أيضًا دمج الحوسبة مع التفاعل
في الوقت الحقيقي. لحل مشكلة قرار ماركوف المعطاة</span>
(MDP)<span dir="rtl">، يمكننا تشغيل خوارزمية برمجة ديناميكية تكرارية في
نفس الوقت الذي يختبر فيه وكيل</span> ($`agent`$) <span dir="rtl">فعليًا
الـ</span> MDP<span dir="rtl">.</span> <span dir="rtl">يمكن استخدام
تجربة الوكيل لتحديد الحالات التي تطبق عليها خوارزمية البرمجة الديناميكية
تحديثاتها. في الوقت نفسه، يمكن لمعلومات القيمة والسياسة الأخيرة من
خوارزمية البرمجة الديناميكية توجيه اتخاذ القرارات للوكيل. على سبيل
المثال، يمكننا تطبيق التحديثات على الحالات أثناء زيارة الوكيل لها. هذا
يجعل من الممكن تركيز تحديثات خوارزمية البرمجة الديناميكية على أجزاء من
مجموعة الحالات الأكثر صلة بالوكيل. هذا النوع من التركيز هو موضوع متكرر
في التعليم المعزز</span> <span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>.

**<u>4.6 <span dir="rtl">التكرار العام للسياسات</span> (Generalized
Policy Iteration)</u>**

<img src="./media/image30.png"
style="width:2.30903in;height:3.62986in" />**<span dir="rtl">تتكون عملية
تحسين السياسة</span> (Policy Iteration) <span dir="rtl">من عمليتين
متزامنتين ومتفاعلتين، الأولى تجعل وظيفة القيمة</span> (value function)
<span dir="rtl">متوافقة مع السياسة الحالية</span> (policy)
<span dir="rtl">تقييم السياسة</span> (policy
evaluation)<span dir="rtl">، والأخرى تجعل السياسة</span> (policy)
<span dir="rtl">جشعة بالنسبة لوظيفة القيمة الحالية</span> (value
function) <span dir="rtl">تحسين السياسة</span> (policy)
<span dir="rtl"></span>improvement)<span dir="rtl">).</span>**
<span dir="rtl">في تحسين السياسة، تتناوب هاتان العمليتان، حيث تنتهي كل
واحدة قبل بدء الأخرى، ولكن هذا ليس ضروريًا في الواقع. في **تكرار القيمة**
</span>**(Value Iteration)**<span dir="rtl">، على سبيل المثال، يتم تنفيذ
تكرار واحد فقط من تقييم السياسة</span> (policy evaluation)
<span dir="rtl">بين كل تحسين للسياسة</span> (policy improvement).
<span dir="rtl">في **طرق البرمجة الديناميكية غير المتزامنة**
</span>**(Asynchronous DP Methods)**<span dir="rtl">، يتم دمج عمليتي
التقييم</span> (evaluation) <span dir="rtl">والتحسين</span>
(improvement) <span dir="rtl">بشكل أكثر دقة. في بعض الحالات، يتم تحديث
حالة واحدة في عملية واحدة قبل العودة إلى الأخرى. طالما تستمر كلا
العمليتين في تحديث جميع الحالات، فإن النتيجة النهائية تكون عادةً هي
نفسها—الاقتراب من وظيفة القيمة المثلى</span>
<span dir="rtl">(</span>optimal <span dir="rtl"></span>value
function<span dir="rtl">)</span> <span dir="rtl">وسياسة مثلى</span>
(optimal policy)<span dir="rtl">.</span>

<span dir="rtl">نستخدم مصطلح **التحسين العام للسياسات**</span>
**(Generalized Policy Iteration)** <span dir="rtl">للإشارة إلى الفكرة
العامة التي تتمثل في السماح لعمليات تقييم السياسة</span> (policy)
<span dir="rtl">وتحسين السياسة</span> (policy) <span dir="rtl">بالتفاعل،
بغض النظر عن التفاصيل الدقيقة ودرجة التفاصيل الخاصة بالعمليتين. يمكن وصف
معظم طرق **التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">بشكل جيد بأنها</span> **GPI**<span dir="rtl">.</span>
<span dir="rtl">أي أن جميعها تحتوي على سياسات</span> (policy)
<span dir="rtl">ووظائف قيمة</span> (value functions)
<span dir="rtl">يمكن تحديدها، حيث يتم دائمًا تحسين السياسة</span>
(policy) <span dir="rtl">بالنسبة لوظيفة القيمة</span> (value function)
<span dir="rtl">وتدفع وظيفة القيمة</span> (value function)
<span dir="rtl">دائمًا نحو وظيفة القيمة</span> (value function)
<span dir="rtl">للسياسة</span> (policy)<span dir="rtl">، كما هو مقترح في
الرسم البياني على اليمين. إذا استقرت كل من عملية التقييم</span>
(evaluation process) <span dir="rtl">وعملية التحسين</span> (improvement
process)<span dir="rtl">، أي أنهما لم تنتجا تغييرات بعد الآن، فإن وظيفة
القيمة</span> (value function) <span dir="rtl">والسياسة</span> (policy)
<span dir="rtl">يجب أن تكونا مثليتين. تستقر وظيفة القيمة</span> (value
function) <span dir="rtl">فقط عندما تكون متوافقة مع السياسة</span>
(policy) <span dir="rtl">الحالية، وتستقر السياسة</span> (policy)
<span dir="rtl">فقط عندما تكون جشعة بالنسبة لوظيفة القيمة</span>
**(value function)** <span dir="rtl">الحالية. وبالتالي، تستقر كلا
العمليتين فقط عندما يتم العثور على سياسة</span> (**policy**)
<span dir="rtl">جشعة بالنسبة **لوظيفتها التقييمية**</span> **(evaluation
function)** <span dir="rtl">الخاصة. وهذا يعني أن معادلة **مثالية
بيلمان** </span>**(Bellman Optimality)** (4.1) <span dir="rtl">تكون
صحيحة، وبالتالي فإن **السياسة** </span>**(policy) <span dir="rtl">ووظيفة
القيمة </span>**(**value function**) <span dir="rtl">هما
مثاليتان</span>.

<img src="./media/image31.png"
style="width:2.65417in;height:1.80903in" />**<span dir="rtl">يمكن أيضًا
التفكير في التفاعل بين عمليات التقييم</span> (evaluation)
<span dir="rtl">والتحسين</span> (improvement) <span dir="rtl">في</span>
Generalized Policy Iteration <span dir="rtl">(التحسين العام
للسياسات)</span> <span dir="rtl">من حيث وجود قيدين أو هدفين—على سبيل
المثال، كخطين في الفضاء ثنائي الأبعاد كما هو موضح في الرسم البياني على
اليمين.</span>** <span dir="rtl">على الرغم من أن الهندسة الحقيقية أكثر
تعقيدًا بكثير من هذا، فإن الرسم البياني يشير إلى ما يحدث في الحالة
الواقعية. تدفع كل عملية وظيفة القيمة</span> (value function)
<span dir="rtl">أو السياسة</span> (policy) <span dir="rtl">نحو أحد
الخطين اللذين يمثلان حلاً لأحد الهدفين. يتفاعل الهدفان لأن الخطين ليسا
متعامدين. الدفع مباشرة نحو أحد الأهداف يتسبب في بعض الحركة بعيدًا عن
الهدف الآخر. ومع ذلك، فإن العملية المشتركة تقرب النظام بشكل أكبر نحو
الهدف العام للمثالية. تتوافق الأسهم في هذا الرسم البياني مع سلوك تحسين
السياسة</span> (policy iteration) <span dir="rtl">حيث تأخذ كل واحدة
النظام لتحقيق أحد الهدفين بالكامل. في</span> **GPI**<span dir="rtl">،
يمكن أيضًا اتخاذ خطوات أصغر وغير مكتملة نحو كل هدف. في كلتا الحالتين،
تحقق العمليتان معًا الهدف العام للمثالية حتى وإن لم يكن أي منهما يحاول
تحقيقه مباشرة</span>.

**<u>4.7 <span dir="rtl">كفاءة البرمجة الديناميكية</span> (Efficiency of
Dynamic Programming)</u>**

**<span dir="rtl">قد لا تكون البرمجة الديناميكية</span> (Dynamic
Programming) <span dir="rtl">عملية لمشاكل كبيرة جدًا، لكن مقارنة بالطرق
الأخرى لحل مشاكل قرارات ماركوف</span> (MDPs)<span dir="rtl">، فإن طرق
البرمجة الديناميكية</span> (DP) <span dir="rtl">فعالة جدًا</span>.**
<span dir="rtl">إذا تجاهلنا بعض التفاصيل التقنية، فإن الوقت الذي تستغرقه
طرق البرمجة الديناميكية</span> (DP) <span dir="rtl">للعثور على سياسة
مثلى</span> (optimal policy) <span dir="rtl">يكون عادةً متعدد الحدود
بالنسبة لعدد الحالات</span> (states) <span dir="rtl">والإجراءات</span>
(actions)<span dir="rtl">.</span> <span dir="rtl">إذا كانت</span> $`n`$
<span dir="rtl">و</span>$`k`$ <span dir="rtl">هما عدد الحالات</span>
(states) <span dir="rtl">والإجراءات</span> (actions) <span dir="rtl">على
التوالي، فإن طريقة البرمجة الديناميكية</span> (DP) <span dir="rtl">تحتاج
إلى عدد من العمليات الحسابية أقل من بعض دوال متعددة الحدود لـ</span>
$`n`$ <span dir="rtl">و</span>$`k`$<span dir="rtl">.</span>
<span dir="rtl">تضمن طريقة البرمجة الديناميكية</span> (DP)
<span dir="rtl">العثور على سياسة مثلى</span>
<span dir="rtl">(</span>optimal
<span dir="rtl"></span>policy<span dir="rtl">) في وقت متعدد الحدود حتى
وإن كان العدد الإجمالي للسياسات</span> (deterministic policies)
<span dir="rtl">هو</span> $`kn`$<span dir="rtl">.</span>
<span dir="rtl">من هذه الناحية، تكون البرمجة الديناميكية</span> (DP)
<span dir="rtl">أسرع بشكل كبير من أي بحث مباشر في فضاء السياسات</span>
(policy space)<span dir="rtl">، لأن البحث المباشر يتطلب فحص كل سياسة
بشكل شامل لتقديم نفس الضمان</span>.

<span dir="rtl">يمكن أيضًا استخدام طرق البرمجة الخطية</span> (linear
programming) <span dir="rtl">لحل مشاكل قرارات ماركوف</span>
(MDPs)<span dir="rtl">، وفي بعض الحالات تكون ضمانات التقارب في أسوأ
الحالات</span> <span dir="rtl">(</span>worst-case
<span dir="rtl"></span>convergence guarantees<span dir="rtl">)</span>
<span dir="rtl">أفضل من تلك التي تقدمها طرق البرمجة الديناميكية</span>
(DP)<span dir="rtl">.</span> <span dir="rtl">ولكن تصبح طرق البرمجة
الخطية</span> (linear programming) <span dir="rtl">غير عملية عند عدد أقل
بكثير من الحالات</span> (states) <span dir="rtl">مقارنة بطرق البرمجة
الديناميكية</span> (DP) <span dir="rtl">(بنسبة حوالي 100). بالنسبة لأكبر
المشاكل، تكون طرق البرمجة الديناميكية</span> (DP) <span dir="rtl">هي
الوحيدة القابلة للتطبيق</span>.

<span dir="rtl">في بعض الأحيان يُعتقد أن البرمجة الديناميكية</span> (DP)
<span dir="rtl">لها تطبيقات محدودة بسبب لعنة الأبعاد</span>
<span dir="rtl">(</span>curse <span dir="rtl"></span>of
dimensionality<span dir="rtl">)، وهي الحقيقة التي تجعل عدد
الحالات</span> (states) <span dir="rtl">ينمو بشكل أسي مع عدد متغيرات
الحالة. تخلق مجموعات الحالات الكبيرة صعوبات، لكن هذه الصعوبات هي صعوبات
متأصلة في المشكلة، وليست في البرمجة الديناميكية</span> (DP)
<span dir="rtl">كطريقة حل. في الواقع، تكون البرمجة الديناميكية</span>
(DP) <span dir="rtl">أكثر ملاءمة للتعامل مع فضاءات الحالات الكبيرة
مقارنة بالطرق المتنافسة مثل البحث المباشر</span> (direct search)
<span dir="rtl">والبرمجة الخطية</span> (linear
programming)<span dir="rtl">.</span>

<span dir="rtl">في الممارسة العملية، يمكن استخدام طرق البرمجة
الديناميكية</span> (DP) <span dir="rtl">مع الحواسيب الحديثة لحل مشاكل
قرارات ماركوف</span> (MDPs) <span dir="rtl">التي تحتوي على ملايين
الحالات</span> (states)<span dir="rtl">.</span> <span dir="rtl">تُستخدم
كل من تحسين السياسة</span> (policy iteration) <span dir="rtl">وتكرار
القيمة</span> (value iteration) <span dir="rtl">على نطاق واسع، وليس من
الواضح أي منهما، إن وجد، هو الأفضل بشكل عام. في الممارسة العملية، عادةً
ما تتقارب هذه الطرق بشكل أسرع بكثير من أوقات التشغيل النظرية في أسوأ
الحالات، خاصة إذا بدأت بقيم أولية جيدة لوظائف القيمة</span> (value
functions) <span dir="rtl">أو السياسات</span>
(policies)<span dir="rtl">.</span>

**<span dir="rtl">في المشاكل التي تحتوي على فضاءات حالات كبيرة، تُفضل
عادةً طرق البرمجة الديناميكية غير المتزامنة</span> (Asynchronous DP
methods)<span dir="rtl">.</span>** <span dir="rtl">لإكمال حتى دورة واحدة
من طريقة متزامنة</span> (synchronous method)<span dir="rtl">، يتطلب
الأمر حسابًا وذاكرة لكل حالة</span> (state)<span dir="rtl">.</span>
<span dir="rtl">بالنسبة لبعض المشاكل، حتى هذا القدر من الذاكرة والحساب
يكون غير عملي، ومع ذلك، قد تظل المشكلة قابلة للحل لأن عددًا نسبيًا قليلًا
من الحالات يحدث على طول مسارات الحلول المثلى</span> (optimal solution
trajectories)<span dir="rtl">. يمكن تطبيق الطرق غير المتزامنة</span>
(asynchronous methods) <span dir="rtl">وطرق التحسين العام
للسياسات</span> (GPI) <span dir="rtl">الأخرى في مثل هذه الحالات وقد تجد
سياسات جيدة أو مثلى</span> (good or optimal policies)
<span dir="rtl">بشكل أسرع بكثير من الطرق المتزامنة</span> (synchronous
methods)<span dir="rtl">.</span>

**<u>4.8 <span dir="rtl">ملخص</span> (Summary)</u>**

**<span dir="rtl">في هذا الفصل، أصبحنا على دراية بالأفكار الأساسية
والخوارزميات للبرمجة الديناميكية</span> <span dir="rtl">(</span>Dynamic
<span dir="rtl"></span>Programming<span dir="rtl">)</span>
<span dir="rtl">كما تتعلق بحل مشاكل قرارات ماركوف المنتهية</span>
(finite MDPs)<span dir="rtl">.</span>**

<span dir="rtl">تتمثل تقييم السياسة</span> (Policy Evaluation)
<span dir="rtl">في حساب تكراري لقيم الوظائف للسياسة المحددة. بينما يشير
تحسين السياسة</span> (Policy Improvement) <span dir="rtl">إلى حساب سياسة
محسنة بناءً على وظيفة القيمة لتلك السياسة. من خلال دمج هاتين العمليتين،
نحصل على تحسين السياسة</span> (Policy Iteration) <span dir="rtl">وتكرار
القيمة</span> (Value Iteration)<span dir="rtl">، وهما أكثر طرق البرمجة
الديناميكية شيوعًا. يمكن استخدام أي منهما لحساب السياسات المثلى</span>
(optimal policies) <span dir="rtl">ووظائف القيمة</span> (value
functions) <span dir="rtl">لمشاكل قرارات ماركوف المنتهية</span> (finite
MDPs) <span dir="rtl">بدقة عند توفر معرفة كاملة بالمشكلة</span>.

<span dir="rtl">تعمل الطرق الكلاسيكية للبرمجة الديناميكية</span> (DP)
<span dir="rtl">على عمليات تحديث عبر مجموعة الحالات</span>
<span dir="rtl">(</span>state set<span dir="rtl">)، حيث يتم تنفيذ عملية
تحديث متوقعة على كل حالة. كل عملية من هذه العمليات تقوم بتحديث قيمة حالة
واحدة بناءً على قيم جميع الحالات اللاحقة الممكنة واحتمالات حدوثها.
التحديثات المتوقعة مرتبطة ارتباطًا وثيقًا بمعادلات بيلمان</span> (Bellman
equations)<span dir="rtl">:</span> <span dir="rtl">فهي ليست أكثر من هذه
المعادلات محولة إلى تعليمات تعيين. عندما لا تؤدي التحديثات إلى أي
تغييرات في القيمة، يكون قد حدث التقارب إلى القيم التي تلبي معادلة بيلمان
المقابلة. تمامًا كما توجد أربع وظائف قيمة أساسية</span>
$`(v⇡،\ v ⇤ ،\ q⇡،\ وq ⇤ )`$<span dir="rtl">, توجد أربع معادلات بيلمان
مقابلة وأربع تحديثات متوقعة مقابلة. تُعطى رؤية بديهية لعملية تحديث
البرمجة الديناميكية</span> (DP) <span dir="rtl">من خلال مخططات النسخ
الاحتياطي (</span>backup
<span dir="rtl"></span>diagrams<span dir="rtl">).</span>

<span dir="rtl">يمكن فهم طرق البرمجة
الديناميكية</span>DP)<span dir="rtl">)، وفي الواقع، معظم طرق التعليم
المعزز</span> <span dir="rtl">(</span>reinforcement
<span dir="rtl"></span>learning methods)<span dir="rtl">، من خلال النظر
إليها كمفهوم التحسين العام للسياسات</span>
<span dir="rtl">(</span>Generalized <span dir="rtl"></span>Policy
Iteration, GPI<span dir="rtl">)</span> GPI <span dir="rtl">هو الفكرة
العامة لعمليتين متفاعلتين تدوران حول سياسة تقريبية</span> (approximate
policy) <span dir="rtl">ووظيفة قيمة تقريبية</span> (approximate value
function)<span dir="rtl">.</span> <span dir="rtl">تقوم إحدى العمليات
بأخذ السياسة كما هي وتنفيذ شكل ما من تقييم السياسة</span> (policy
evaluation)<span dir="rtl">، مما يغير وظيفة القيمة لتكون أقرب إلى وظيفة
القيمة الحقيقية للسياسة. بينما تقوم العملية الأخرى بأخذ وظيفة القيمة كما
هي وتنفيذ شكل ما من تحسين السياسة</span> (policy
improvement)<span dir="rtl">، مما يغير السياسة لجعلها أفضل، بافتراض أن
وظيفة القيمة هي وظيفتها. على الرغم من أن كل عملية تغير الأساس للآخر، إلا
أنهما يعملان معًا بشكل عام لإيجاد حل مشترك: سياسة ووظيفة قيمة لا تتغير
بواسطة أي من العمليتين، وبالتالي تكون مثلى</span>
(optimal)<span dir="rtl">.</span> <span dir="rtl">في بعض الحالات، يمكن
إثبات تقارب</span> **GPI**<span dir="rtl">، خاصة للطرق الكلاسيكية
للبرمجة الديناميكية</span> (DP) <span dir="rtl">التي قدمناها في هذا
الفصل. في حالات أخرى لم يتم إثبات التقارب، ولكن فكرة</span> **GPI**
<span dir="rtl">تحسن فهمنا للطرق</span>.

<span dir="rtl">ليس من الضروري تنفيذ طرق البرمجة الديناميكية</span> (DP)
<span dir="rtl">في دورات كاملة عبر مجموعة الحالات</span> (state set)
<span dir="rtl">الطرق غير المتزامنة للبرمجة الديناميكية</span>
(Asynchronous DP methods) <span dir="rtl">هي طرق تكرارية في المكان تقوم
بتحديث الحالات بترتيب عشوائي، قد يكون محددًا عشوائيًا ويستخدم معلومات
قديمة</span>. **<span dir="rtl">يمكن النظر إلى العديد من هذه الطرق
كأشكال دقيقة من التحسين العام للسياسات</span>
(GPI)<span dir="rtl">.</span>**

<span dir="rtl">أخيرًا، نلاحظ خاصية خاصة أخرى لطرق البرمجة
الديناميكية</span> (DP)<span dir="rtl">.</span> <span dir="rtl">جميعها
تقوم بتحديث تقديرات قيم الحالات استنادًا إلى تقديرات قيم الحالات اللاحقة.
أي أنها تقوم بتحديث التقديرات على أساس تقديرات أخرى. نحن نسمي هذه الفكرة
العامة التمهيد</span> (bootstrapping) <span dir="rtl">العديد من طرق
التعليم المعزز</span> (reinforcement learning methods)
<span dir="rtl">تقوم بعملية التمهيد، حتى تلك التي لا تتطلب، كما تتطلب
البرمجة الديناميكية</span> (DP)<span dir="rtl">، نموذجًا كاملاً ودقيقًا
للبيئة. في الفصل التالي، نستكشف طرق التعليم المعزز التي لا تتطلب نموذجًا
ولا تقوم بالتمهيد. وفي الفصل الذي يليه، نستكشف طرقًا لا تتطلب نموذجًا
ولكنها تقوم بالتمهيد. هذه الخصائص والميزات الأساسية قابلة للفصل، لكنها
يمكن أن تُخلط بطرق مثيرة للاهتمام</span>.

<span dir="rtl">الفصل الخامس:</span>

<span dir="rtl">طرق مونت كارلو</span> <span dir="rtl">(</span>Monte
Carlo Methods<span dir="rtl">)</span>

<span dir="rtl">في هذا الفصل، نبحث في طرق التعليم الأولى لتقدير وظائف
القيمة واكتشاف السياسات المثلى. على عكس الفصل السابق، لا نفترض هنا معرفة
كاملة بالبيئة. تتطلب طرق مونت كارلو</span> <span dir="rtl">(</span>Monte
Carlo <span dir="rtl"></span>methods<span dir="rtl">)</span>
<span dir="rtl">فقط الخبرة — سلاسل من الحالات والإجراءات والمكافآت من
التفاعل الفعلي أو المحاكي مع البيئة</span>.

<span dir="rtl">التعليم من الخبرة الفعلية</span> (actual experience)
<span dir="rtl">ملحوظ لأنه لا يتطلب معرفة مسبقة بديناميات البيئة</span>
(environment’s dynamics)<span dir="rtl">، ومع ذلك يمكنه تحقيق سلوك
مثالي. كما أن التعليم من الخبرة المحاكية</span> (simulated experience)
<span dir="rtl">قوي أيضًا. على الرغم من أنه يتطلب نموذجًا</span>
(model)<span dir="rtl">، فإن النموذج يحتاج فقط إلى توليد الانتقالات
النموذجية</span> (sample transitions)<span dir="rtl">، وليس التوزيعات
الاحتمالية الكاملة لجميع الانتقالات الممكنة المطلوبة للبرمجة
الديناميكية</span> (DP) <span dir="rtl">في العديد من الحالات المفاجئة،
يكون من السهل توليد خبرة يتم أخذ عينات منها وفقًا للتوزيعات الاحتمالية
المطلوبة، ولكن من غير الممكن الحصول على التوزيعات بشكل صريح</span>.

<span dir="rtl">تعتبر طرق مونت كارلو</span> (Monte Carlo methods)
<span dir="rtl">طرقًا لحل مشكلة التعليم المعزز</span> (reinforcement
learning problem) <span dir="rtl">بناءً على متوسط العوائد
النموذجية</span> (sample returns) <span dir="rtl">لضمان توفر عوائد محددة
جيدًا، نحدد هنا طرق مونت كارلو فقط للمهام الحلقية</span> (episodic tasks)
<span dir="rtl">أي أننا نفترض أن الخبرة مقسمة إلى حلقات</span>
(episodes)<span dir="rtl">، وأن جميع الحلقات تنتهي في النهاية بغض النظر
عن الإجراءات المتخذة. فقط عند إتمام الحلقة يتم تغيير تقديرات القيمة
والسياسات. وبالتالي، يمكن أن تكون طرق مونت كارلو تدريجية من حيث
الحلقات</span> <span dir="rtl">(</span>incremental in an
episode-by-episode sense<span dir="rtl">)، ولكن ليس من حيث الخطوات
الفردية</span> (online)<span dir="rtl">.</span> <span dir="rtl">يُستخدم
مصطلح **"مونت كارلو**</span>**"** <span dir="rtl"></span>(Monte Carlo)
<span dir="rtl">غالبًا بشكل أوسع لأي طريقة تقدير تشمل عنصرًا عشوائيًا
كبيرًا. هنا نستخدمه تحديدًا للطرق المعتمدة على متوسط العوائد
الكاملة</span> (average complete returns) <span dir="rtl">بالمقارنة مع
الطرق التي تتعلم من العوائد الجزئية</span> (partial
returns)<span dir="rtl">، والتي ستتم مناقشتها في الفصل التالي</span>.

\*<span dir="rtl">تقوم طرق مونت كارلو بأخذ عينات ومتوسط العوائد لكل زوج
من الحالة–الإجراء</span> <span dir="rtl">(</span>state–action
<span dir="rtl"></span>pair<span dir="rtl">)</span> <span dir="rtl">بشكل
مشابه للطرق التي استكشفناها في الفصل الثاني والتي تأخذ عينات ومتوسط
المكافآت لكل إجراء. الفرق الرئيسي هو أنه الآن توجد حالات متعددة، كل منها
يعمل مثل مشكلة كازينو مختلفة</span> (different bandit problem)
<span dir="rtl">مثل كازينو البحث التوصيفي</span> (associative-search)
<span dir="rtl">أو كازينو السياق</span> (contextual bandit)
<span dir="rtl">والمشاكل المختلفة مترابطة. أي أن العائد بعد اتخاذ إجراء
في حالة واحدة يعتمد على الإجراءات المتخذة في الحالات اللاحقة في نفس
الحلقة. نظرًا لأن جميع اختيارات الإجراءات تخضع للتعليم، تصبح المشكلة غير
ثابتة من وجهة نظر الحالة السابقة</span>.

<span dir="rtl">للتعامل مع عدم الثبات</span>
(nonstationary)<span dir="rtl">، نتكيف مع فكرة التحسين العام
للسياسات</span> <span dir="rtl">(</span>Generalized
<span dir="rtl"></span>Policy Iteration<span dir="rtl">)</span>
<span dir="rtl">التي تم تطويرها في الفصل 4 للبرمجة الديناميكية</span>
(DP) <span dir="rtl">بينما قمنا هناك بحساب وظائف القيمة من معرفة
الـ</span> MDP<span dir="rtl">، هنا نتعلم وظائف القيمة من العوائد
النموذجية</span> <span dir="rtl">(</span>sample
<span dir="rtl"></span>returns<span dir="rtl">)</span>
<span dir="rtl">مع الـ</span> MDP<span dir="rtl">.</span>
<span dir="rtl">تظل وظائف القيمة والسياسات المقابلة تتفاعل لتحقيق
المثالية بنفس الطريقة الأساسية</span>
**<span dir="rtl">(</span>GPI<span dir="rtl">).</span>**

<span dir="rtl">كما في فصل البرمجة الديناميكية، نبدأ أولاً بمشكلة
التنبؤ</span> (prediction problem) (<span dir="rtl">حساب</span> $`v\pi`$
<span dir="rtl">و</span> $`{\ \ q}_{\pi}`$ <span dir="rtl">لسياسة ثابتة
عشوائية</span> $`\pi\ `$<span dir="rtl">، ثم تحسين السياسة</span>
(policy improvement)<span dir="rtl">، وأخيراً، مشكلة التحكم</span>
(control problem) <span dir="rtl">وحلها بواسطة التحسين العام
للسياسات</span> (GPI) <span dir="rtl">يتم توسيع كل من هذه الأفكار
المأخوذة من البرمجة الديناميكية إلى حالة مونت كارلو</span> (Monte Carlo)
<span dir="rtl">التي تتوفر فيها فقط تجربة النموذج</span>
<span dir="rtl">(</span>sample experience<span dir="rtl">).</span>

**<u>5.1 <span dir="rtl">التنبؤ بواسطة مونت كارلو</span> (Monte Carlo
Prediction)</u>**

<span dir="rtl">نبدأ بالنظر في طرق مونت كارلو</span> (Monte Carlo
methods) <span dir="rtl">لتعلم دالة قيمة الحالة (</span>state-value
function<span dir="rtl">)</span> <span dir="rtl">لسياسة معينة. تذكر أن
قيمة الحالة هي العائد المتوقع—العائد التراكمي المستقبلي المخصوم
المتوقع—الذي يبدأ من تلك الحالة</span>.

<span dir="rtl">طريقة واضحة لتقديرها من التجربة هي ببساطة حساب متوسط
العوائد الملاحظة بعد الزيارات لتلك الحالة. مع ملاحظة المزيد من العوائد،
يجب أن يتقارب المتوسط إلى القيمة المتوقعة. هذه الفكرة تشكل أساس جميع طرق
مونت كارلو</span> (Monte Carlo methods)<span dir="rtl">.</span>

<span dir="rtl">على وجه الخصوص، افترض أننا نرغب في تقدير</span>
$`v\pi(s)`$<span dir="rtl">، قيمة حالة</span> $`s`$ <span dir="rtl">تحت
سياسة</span> $`\pi`$<span dir="rtl">، بناءً على مجموعة من الحلقات التي تم
الحصول عليها من خلال اتباع</span> $`\pi`$ <span dir="rtl">والمرور
عبر</span> $`s`$<span dir="rtl">.</span> <span dir="rtl">كل ظهور
للحالة</span> $`s`$ <span dir="rtl">في حلقة يُسمى زيارة لـ</span>
$`s`$<span dir="rtl">.</span> <span dir="rtl">بالطبع، يمكن أن يتم
زيارة</span> $`s`$ <span dir="rtl">عدة مرات في نفس الحلقة؛ دعنا نسمّي أول
مرة يتم فيها زيارة</span> $`s`$ <span dir="rtl">في حلقة بالزيارة الأولى
لـ</span> $`s`$<span dir="rtl">.</span>

<span dir="rtl">تقدّر طريقة مونت كارلو للزيارة الأولى</span> (First-Visit
MC) $`v\pi(s)`$ <span dir="rtl">كمتوسط للعوائد التي تلي الزيارات الأولى
لـ</span> $`s`$<span dir="rtl">، بينما تقدّر طريقة مونت كارلو للزيارة
الكلية</span> (Every-Visit MC) <span dir="rtl">العوائد التي تلي جميع
الزيارات لـ</span> $`s`$<span dir="rtl">.</span> <span dir="rtl">هاتان
الطريقتان في مونت كارلو</span> (MC) <span dir="rtl">متشابهتان جدًا ولكن
لهما خصائص نظرية مختلفة قليلاً. لقد تم دراسة طريقة الزيارة الأولى بشكل
واسع، وتعود إلى الأربعينيات من القرن الماضي، وهي التي نركز عليها في هذا
الفصل. تمتد طريقة الزيارة الكلية بشكل طبيعي أكثر إلى **تقريب
الدوال**</span> (**function approximation**) **<span dir="rtl">وتتبع
الأهلية</span> <span dir="rtl">(</span>eligibility
traces<span dir="rtl">)</span>**<span dir="rtl">، كما نناقش في الفصول
9  
و12. تظهر طريقة الزيارة الأولى في شكل إجرائي في الصندوق. ستكون طريقة
الزيارة الكلية هي نفسها باستثناء عدم التحقق من أن</span> s
<span dir="rtl">قد حدثت في وقت سابق في الحلقة</span>.

**<span dir="rtl">توقع مونت كارلو للزيارة الأولى، لتقدير</span>**
$`\mathbf{V =}\mathbf{v}_{\mathbf{\pi}}`$

<img src="./media/image32.png"
style="width:6.26806in;height:2.76181in" />

<span dir="rtl">كلا من طريقة مونت كارلو للزيارة الأولى</span>
(First-Visit MC) <span dir="rtl">وطريقة مونت كارلو للزيارة الكلية</span>
(Every-Visit MC) <span dir="rtl">يتقاربون إلى</span> $`v\pi(s)`$
<span dir="rtl">مع تزايد عدد الزيارات (أو الزيارات الأولى) إلى</span>
$`s`$ <span dir="rtl">إلى اللانهاية. من السهل رؤية ذلك في حالة طريقة
الزيارة الأولى. في هذه الحالة، كل عائد هو تقدير مستقل وموزع بشكل متطابق
لـ</span> $`v\pi(s)`$ <span dir="rtl">وله تباين محدود. وفقًا لقانون
الأعداد الكبيرة، يتقارب تسلسل المتوسطات لهذه التقديرات إلى قيمتها
المتوقعة. كل متوسط هو نفسه تقدير غير متحيز، والانحراف المعياري لخطأه
ينخفض كـ</span> $`1 \slash \sqrt{n}`$​<span dir="rtl">، حيث</span> $`n`$
<span dir="rtl"></span> <span dir="rtl">هو عدد العوائد التي تم متوسطها.
طريقة الزيارة الكلية أقل مباشرة، ولكن تقديراتها تتقارب أيضًا بشكل تربيعي
إلى</span> $`v\pi(s)`$<span dir="rtl">  
</span> (Singh and Sutton, 1996)<span dir="rtl">.</span>

<span dir="rtl">يتم توضيح استخدام طرق مونت كارلو بشكل أفضل من خلال
مثال</span>.

**<span dir="rtl"><u>مثال 5.1</u>: لعبة بلاك جاك</span>**

<span dir="rtl">الهدف من لعبة البلاك جاك الشهيرة في الكازينوهات هو
الحصول على بطاقات تكون قيمتها العددية الإجمالية أقرب ما يمكن إلى 21 دون
تجاوزها. تُحسب جميع بطاقات الوجه كـ 10، ويمكن أن يحسب الآس كـ 1 أو كـ 11.
نحن نعتبر النسخة التي يتنافس فيها كل لاعب بشكل مستقل ضد الموزع. تبدأ
اللعبة بتوزيع بطاقتين لكل من الموزع واللاعب. تكون إحدى بطاقات الموزع
مكشوفة والأخرى مقلوبة. إذا كان لدى اللاعب 21 على الفور (آس وبطاقة بقيمة
10)، يُطلق على ذلك اسم "الطبيعي". يفوز حينئذٍ ما لم يكن لدى الموزع أيضًا
طبيعي، وفي هذه الحالة تنتهي اللعبة بالتعادل. إذا لم يكن لدى اللاعب
طبيعي، فيمكنه طلب بطاقات إضافية واحدة تلو الأخرى (ضربات)، حتى يتوقف
(يثبت) أو يتجاوز 21 (يتعثر). إذا تعثر، يخسر؛ إذا ثبت، يصبح دور الموزع.
يقوم الموزع بالضرب أو الثبات وفقًا لاستراتيجية ثابتة دون خيار: يثبت على
أي مجموع من 17 أو أكثر، ويضرب خلاف ذلك. إذا تعثر الموزع، يفوز اللاعب؛
وإلا، يتم تحديد النتيجة—فوز، خسارة، أو تعادل—بناءً على من تكون مجموعته
النهائية أقرب إلى 21</span>.

<span dir="rtl">لعب البلاك جاك يتم صياغته بشكل طبيعي كمسألة قرار</span>
Markovian <span dir="rtl">متناهية الإجرائيات</span> (MDP)
<span dir="rtl">الحلقية. كل لعبة من البلاك جاك تعتبر حلقة. يتم منح
المكافآت 1+ و1− و0 للفوز والخسارة والتعادل، على التوالي. جميع المكافآت
داخل اللعبة تكون صفرًا، ولا نقوم بخصم</span>
$`(\#\  = \ 1)`$<span dir="rtl">؛ لذا فإن هذه المكافآت النهائية هي أيضًا
العوائد. تصرفات اللاعب هي إما الضرب أو الثبات. تعتمد الحالات على بطاقات
اللاعب وبطاقة الموزع المكشوفة. نفترض أن البطاقات يتم سحبها من مجموعة لا
نهائية</span>

<span dir="rtl">(أي مع الاستبدال) بحيث لا يوجد فائدة من تتبع البطاقات
التي تم سحبها بالفعل. إذا كان لدى اللاعب آس يمكنه حسابه كـ 11 دون أن
يتعثر، فيقال إن الآس قابل للاستخدام. في هذه الحالة، يُحتسب دائمًا كـ 11
لأن احتسابه كـ 1 يجعل المجموع 11 أو أقل، وفي هذه الحالة لا يوجد قرار يجب
اتخاذه لأن اللاعب يجب دائمًا أن يضرب. وبالتالي، يتخذ اللاعب قراراته بناءً
على ثلاث متغيرات: المجموع الحالي (12-21)، بطاقة الموزع المكشوفة (آس10-)،
وما إذا كان لديه آس قابل للاستخدام. وهذا يجعل العدد الإجمالي للحالات 200
حالة</span>.

<span dir="rtl">افترض سياسة تقوم بالثبات إذا كان مجموع اللاعب 20 أو 21،
وإلا يقوم بالضرب. للعثور على دالة قيمة الحالة لهذه السياسة باستخدام
طريقة مونت كارلو، يتم محاكاة العديد من ألعاب البلاك جاك باستخدام السياسة
ومتوسط العوائد التي تلي كل حالة. بهذه الطريقة، حصلنا على تقديرات دالة
قيمة الحالة كما هو موضح في **الشكل** **5.1**. تقديرات الحالات التي تحتوي
على آس قابل للاستخدام أقل يقينًا وأقل انتظامًا لأن هذه الحالات أقل شيوعًا.
على أي حال، بعد 500,000 لعبة، تكون دالة القيمة تقريبية جدًا</span>.

<img src="./media/image33.png"
style="width:6.26806in;height:3.22083in" /> <span dir="rtl">**الشكل
5.1**: دوال قيمة الحالة التقريبية لسياسة البلاك جاك التي تثبت فقط عند 20
أو 21، والتي تم حسابها بواسطة تقييم السياسة باستخدام طريقة مونت
كارلو</span>.

<span dir="rtl">**<u>تمرین 5.1</u>**:</span> <span dir="rtl">لاحظ الرسوم
البيانية على اليمين في الشكل 5.1. لماذا تزداد دالة القيمة المقدرة للصفين
الأخيرين في الجزء الخلفي؟ لماذا تنخفض دالة القيمة للصف الأخير بالكامل
على اليسار؟ لماذا تكون القيم في المقدمة أعلى في الرسوم البيانية العليا
مقارنة بالسفلى؟</span>

<span dir="rtl">**<u>تمرین 5.2</u>**:</span> <span dir="rtl">افترض أنه
تم استخدام طريقة مونت كارلو لكل زيارة بدلاً من طريقة مونت كارلو لأول
زيارة في مهمة البلاك جاك. هل تتوقع أن تكون النتائج مختلفة بشكل كبير؟
لماذا أو لماذا لا؟</span>

<span dir="rtl">على الرغم من أننا نملك معرفة كاملة بالبيئة في مهمة
البلاك جاك، إلا أنه لن يكون من السهل تطبيق طرق البرمجة
الديناميكية</span> (DP) <span dir="rtl">لحساب دالة القيمة. تتطلب طرق
البرمجة الديناميكية توزيع الأحداث التالية—وبشكل خاص، تحتاج إلى ديناميات
البيئة كما تحددها دالة الأربعة متغيرات</span> p <span dir="rtl"></span>—
<span dir="rtl">ومن الصعب تحديد ذلك للبلاك جاك. على سبيل المثال، افترض
أن مجموع اللاعب هو 14 واختار أن يلتزم</span>
(stick)<span dir="rtl">.</span> <span dir="rtl">ما هو احتمال انتهاء
اللعبة بجائزة 1+ بناءً على بطاقة الموزع المكشوفة؟ يجب حساب جميع
الاحتمالات قبل أن يمكن تطبيق</span> DP<span dir="rtl">، وهذه الحسابات
غالبًا ما تكون معقدة وعرضة للأخطاء. بالمقابل، فإن توليد الألعاب النموذجية
المطلوبة من قبل طرق مونت كارلو سهل. وهذا يحدث بشكل مفاجئ كثيرًا؛ القدرة
على استخدام طرق مونت كارلو للعمل فقط مع الحلقات التجريبية يمكن أن تكون
ميزة كبيرة حتى عندما يكون لديك معرفة كاملة بديناميات البيئة</span>.

<span dir="rtl">هل يمكننا تعميم فكرة الرسوم البيانية الاحتياطية على
خوارزميات مونت كارلو؟ الفكرة العامة للرسوم البيانية الاحتياطية هي إظهار
العقدة الجذرية التي سيتم تحديثها في الأعلى وإظهار جميع الانتقالات والعقد
الورقية أدناه التي تسهم مكافآتها والقيم المقدرة في التحديث. بالنسبة
لتقدير</span> $`v\pi`$ <span dir="rtl">باستخدام مونت كارلو، الجذر هو
عقدة الحالة، وأدناه توجد المسار الكامل من الانتقالات على طول حلقة واحدة
معينة، والتي تنتهي في الحالة النهائية، كما هو موضح على اليمين. بينما
يظهر رسم</span> DP <span dir="rtl">جميع الانتقالات الممكنة، يظهر رسم
مونت كارلو فقط تلك التي تم أخذ عينات منها في الحلقة الواحدة. بينما يتضمن
رسم</span> DP <span dir="rtl">فقط الانتقالات ذات الخطوة الواحدة، يمتد
رسم مونت كارلو حتى نهاية الحلقة. تعكس هذه الاختلافات في الرسوم البيانية
بدقة الفروقات الأساسية بين الخوارزميات</span>.

<span dir="rtl">حقيقة هامة حول طرق مونت كارلو هي أن التقديرات لكل حالة
مستقلة. التقدير لحالة واحدة لا يبني على تقدير أي حالة أخرى، كما هو الحال
في</span> DP<span dir="rtl">.</span> <span dir="rtl">بعبارة أخرى، لا
تقوم طرق مونت كارلو بالتمهيد كما عرّفناه في الفصل السابق</span>.

<span dir="rtl">على وجه الخصوص، لاحظ أن تكلفة حساب قيمة حالة واحدة
مستقلة عن عدد الحالات. هذا يمكن أن يجعل طرق مونت كارلو جذابة بشكل خاص
عندما تحتاج إلى قيمة حالة واحدة فقط أو مجموعة من الحالات. يمكنك توليد
العديد من الحلقات النموذجية بدءًا من الحالات ذات الاهتمام، وتقدير
المكافآت من هذه الحالات فقط، متجاهلاً جميع الحالات الأخرى. هذه ميزة ثالثة
يمكن أن تتمتع بها طرق مونت كارلو مقارنة بطرق</span> DP
<span dir="rtl">(بعد القدرة على التعليم من التجربة الفعلية ومن التجربة
المحاكاة)</span>.

**<span dir="rtl"><u>مثال 5.2</u>: الفقاعة الصابونية</span>**

<img src="./media/image34.png"
style="width:2.30069in;height:1.83333in" /><span dir="rtl">افترض أن
إطارًا سلكيًا يشكل حلقة مغلقة يتم غمسه في الماء الصابوني لتشكيل سطح صابوني
أو فقاعة تتوافق في حوافها مع الإطار السلكي. إذا كانت هندسة الإطار السلكي
غير منتظمة ولكن معروفة، كيف يمكنك حساب شكل السطح؟</span>

<span dir="rtl">السطح له خاصية أن القوة الإجمالية على كل نقطة تؤثر بها
النقاط المجاورة تساوي صفرًا (وإلا لكان الشكل سيتغير). وهذا يعني أن ارتفاع
السطح عند أي نقطة هو متوسط ارتفاعاته عند النقاط في دائرة صغيرة حول تلك
النقطة. بالإضافة إلى ذلك، يجب أن يتقابل السطح عند حدوده مع الإطار
السلكي</span>.

<span dir="rtl">النهج المعتاد لمشاكل من هذا النوع هو وضع شبكة فوق
المنطقة المغطاة بالسطح وحساب ارتفاعه عند نقاط الشبكة من خلال حساب
تكراري. يتم إجبار نقاط الشبكة على الحدود لتتوافق مع الإطار السلكي، ويتم
تعديل جميع النقاط الأخرى نحو متوسط ارتفاعات أقرب أربع جيران لها. ثم
تتكرر هذه العملية، تمامًا مثل تقييم السياسة التكراري في</span>
DP<span dir="rtl">، وفي النهاية تتقارب إلى تقريبي قريب للسطح
المطلوب</span>.

<span dir="rtl">هذا مشابه لنوع المشكلة التي صممت من أجلها طرق مونت كارلو
في الأصل. بدلاً من الحساب التكراري الموصوف أعلاه، تخيل أنك واقف على السطح
وتأخذ مسيرة عشوائية، تنتقل عشوائيًا من نقطة شبكة إلى النقاط المجاورة، مع
احتمال متساوي، حتى تصل إلى الحدود. يتبين أن القيمة المتوقعة للارتفاع عند
الحدود هي تقريب قريب لارتفاع السطح المطلوب عند النقطة التي بدأت منها (في
الواقع، هي القيمة بالضبط التي تحسبها الطريقة التكرارية الموصوفة أعلاه).
وبالتالي، يمكن تقريبيًا ارتفاع السطح عند نقطة معينة ببساطة عن طريق حساب
متوسط ارتفاعات الحدود للعديد من المسيرات التي تبدأ من تلك النقطة. إذا
كنت مهتمًا فقط بالقيمة عند نقطة واحدة، أو أي مجموعة صغيرة ثابتة من
النقاط، فإن طريقة مونت كارلو يمكن أن تكون أكثر كفاءة بكثير من الطريقة
التكرارية المستندة إلى التناسق المحلي</span>.

**<u>5.2 <span dir="rtl">تقدير قيم الإجراءات باستخدام مونت كارلو</span>
<span dir="rtl">(</span>Monte Carlo Estimation of Action
Values<span dir="rtl">)</span></u>**

<span dir="rtl">إذا لم يكن النموذج متاحًا، فإن تقدير قيم الأفعال (قيم
أزواج الحالة-الإجراء) بدلاً من قيم الحالة يكون مفيدًا بشكل خاص. مع وجود
نموذج، تكون قيم الحالة وحدها كافية لتحديد سياسة؛ حيث يمكنك ببساطة النظر
إلى الخطوة التالية واختيار الإجراء الذي يؤدي إلى أفضل مزيج من المكافأة
والحالة التالية، كما فعلنا في الفصل الخاص بطرق البرمجة الديناميكية. ومع
ذلك، بدون نموذج، لا تكفي قيم الحالة وحدها. يجب عليك تقدير قيمة كل إجراء
بشكل صريح لجعل القيم مفيدة في اقتراح سياسة. وبالتالي، أحد أهدافنا
الرئيسية لطرق مونت كارلو هو تقدير</span> $`q*`$<span dir="rtl">.</span>

<span dir="rtl">للقيام بذلك، نبدأ بمشكلة تقييم السياسة لقيم الأفعال.
المشكلة هي تقدير</span> $`{,q}_{\pi}`$(s, a) <span dir="rtl">وهو العائد
المتوقع عند بدء التنفيذ من الحالة</span> $`s`$<span dir="rtl">، واتخاذ
الإجراء</span> $`a`$<span dir="rtl">، ومن ثم اتباع السياسة</span>
$`⇡`$<span dir="rtl">. طرق مونت كارلو لهذا الغرض تشبه إلى حد كبير تلك
المقدمة لقيم الحالة، باستثناء أننا نتحدث الآن عن الزيارات إلى زوج  
الحالة-الإجراء بدلاً من الحالة فقط. يُقال إن زوج الحالة-الإجراء</span>
$`s,\ a`$ <span dir="rtl">تم زيارته في حلقة إذا تم زيارة الحالة</span>
$`s`$ <span dir="rtl">وأخذ الإجراء</span> $`a`$ <span dir="rtl">في تلك
الحالة. طريقة كل الزيارات في مونت كارلو تقدر قيمة زوج  
الحالة-الإجراء على أنها متوسط العوائد التي تلت جميع الزيارات إليه. بينما
طريقة أول زيارة في مونت كارلو تقدر متوسط العوائد التي تلت أول مرة في كل
حلقة تم فيها زيارة الحالة واختيار الإجراء. هذه الطرق تتقارب تربيعيًا، كما
هو الحال من قبل، إلى القيم المتوقعة الحقيقية مع اقتراب عدد الزيارات إلى
كل زوج حالة-إجراء من اللانهاية</span>.

<span dir="rtl">التعقيد الوحيد هو أن العديد من أزواج الحالة-الإجراء قد
لا يتم زيارتها أبدًا. إذا كانت السياسة</span> $`\pi`$
<span dir="rtl">سياسة حتمية، فإنه عند اتباع</span> $`\pi`$
<span dir="rtl">ستلاحظ العوائد فقط لإجراء واحد من كل حالة. بدون عوائد
لمتوسطها، لن تتحسن تقديرات مونت كارلو للإجراءات الأخرى مع الخبرة. هذه
مشكلة خطيرة لأن الهدف من تعلم قيم الأفعال هو المساعدة في اختيار الأفعال
المتاحة في كل حالة. لمقارنة البدائل، نحتاج إلى تقدير قيمة جميع الأفعال
من كل حالة، وليس فقط تلك التي نفضلها حاليًا</span>.

<span dir="rtl">هذه هي المشكلة العامة في الحفاظ على الاستكشاف، كما تمت
مناقشته في سياق مشكلة الماكينات ذات الأذرع المتعددة في الفصل 2. لكي يعمل
تقييم السياسة لقيم الأفعال، يجب علينا ضمان الاستكشاف المستمر. إحدى الطرق
للقيام بذلك هي تحديد أن الحلقات تبدأ من زوج حالة-إجراء، وأن كل زوج له
احتمال غير صفري لأن يتم اختياره كبداية. وهذا يضمن أن جميع أزواج
الحالة-الإجراء سيتم زيارتها عددًا لا نهائيًا من المرات في حدود عدد لا
نهائي من الحلقات. نحن نطلق على هذه الفرضية اسم فرضية الاستكشاف من
البداية</span>.

<span dir="rtl">**الافتراض الخاص بالبدء بالاستكشاف**</span> (assumption
of exploring starts) <span dir="rtl">يكون مفيدًا في بعض الحالات، ولكن لا
يمكن الاعتماد عليه بشكل عام، خاصة عند التعليم مباشرةً من التفاعل الفعلي
مع البيئة. في مثل هذه الحالات، من غير المحتمل أن تكون شروط البداية مفيدة
بالقدر الكافي. البديل الأكثر شيوعًا لضمان تغطية جميع أزواج الحالة-الإجراء
هو التركيز فقط على السياسات التي تكون عشوائية وتضمن احتمالًا غير صفري
لاختيار جميع الإجراءات في كل حالة. سنناقش نوعين رئيسيين من هذه النهج في
الأقسام القادمة. في الوقت الحالي، سنواصل استخدام افتراض البدء بالاستكشاف
لاستكمال عرض طريقة تحكم مونت كارلو الكاملة</span>.

**<span dir="rtl"><u>تمرين 5.3</u>:</span>** <span dir="rtl">ما هو مخطط
التحديث</span> (backup diagram) <span dir="rtl">لتقدير</span>
$`q_{\pi}`$ <span dir="rtl"></span> <span dir="rtl">باستخدام مونت
كارلو؟</span>

**<u>5.3 <span dir="rtl">التحكم بواسطة مونت كارلو</span> (Monte Carlo
Control)</u>**

<img src="./media/image35.png"
style="width:1.34514in;height:1.38681in" /><span dir="rtl">نحن الآن
جاهزون لبحث كيفية استخدام تقدير مونت كارلو في التحكم، أي لتقريب السياسات
المثلى. الفكرة العامة هي اتباع نفس النمط كما في فصل البرمجة
الديناميكية</span> (**DP**)<span dir="rtl">، أي وفقًا لفكرة التكرار العام
للسياسة</span> (**GPI**)<span dir="rtl">.</span>
<span dir="rtl">في</span> **GPI**<span dir="rtl">، يتم الحفاظ على سياسة
تقريبية وقيمة تقريبية. يتم تعديل وظيفة القيمة مرارًا لتقريبها بشكل أفضل
من وظيفة القيمة للسياسة الحالية، ويتم تحسين السياسة بشكل متكرر بالنسبة
لوظيفة القيمة الحالية، كما هو موضح في الرسم البياني على اليمين. تعمل
هذان النوعان من التغييرات ضد بعضهما البعض إلى حد ما، حيث يخلق كل منهما
هدفًا متحركًا للآخر، ولكن معًا يتسببان في اقتراب كل من السياسة ووظيفة
القيمة من المثالية</span>.

<span dir="rtl">للبدء، دعنا نأخذ بعين الاعتبار نسخة مونت كارلو من تكرار
السياسات الكلاسيكي. في هذه الطريقة، نقوم بإجراء خطوات كاملة متناوبة من
تقييم السياسة وتحسين السياسة، بدءًا من سياسة عشوائية</span> $`⇡₀`$
<span dir="rtl">وانتهاءً بالسياسة المثلى ووظيفة قيمة الإجراء
المثلى</span>:

``` math
\pi_{0}\overset{\phantom{E}}{\rightarrow}q_{\pi_{0}}\overset{\phantom{I}}{\rightarrow}\pi_{1}\overset{\phantom{E}}{\rightarrow}q_{\pi_{1}}\overset{\phantom{I}}{\rightarrow}\pi_{2}\overset{\phantom{E}}{\rightarrow}\cdots\overset{\phantom{I}}{\rightarrow}\pi_{*}\overset{\phantom{E}}{\rightarrow}q_{*},
```

<span dir="rtl">حيث يُشير الرمز</span>
$`\overset{\phantom{E}}{\rightarrow}`$ <span dir="rtl">إلى تقييم السياسة
الكامل، ويُشير الرمز</span> $`\overset{\phantom{I}}{\rightarrow}`$
<span dir="rtl">إلى تحسين السياسة الكامل. يتم تقييم السياسة تمامًا كما تم
وصفه في القسم السابق. يتم تجربة العديد من الحلقات، مع اقتراب وظيفة قيمة
الإجراء التقريبية من الوظيفة الحقيقية بشكل تقاربي. في الوقت الحالي، دعنا
نفترض أننا نلاحظ بالفعل عددًا لا نهائيًا من الحلقات، وأن الحلقات يتم
توليدها ببداية استكشافية. تحت هذه الافتراضات، ستقوم طرق مونت كارلو بحساب
كل</span> $`q_{\pi}k`$ <span dir="rtl">بدقة، لأي</span> $`\pi k`$
<span dir="rtl"></span>​.

<span dir="rtl">يتم تحسين السياسة من خلال جعل السياسة جشعة بالنسبة
لوظيفة القيمة الحالية. في هذه الحالة، لدينا وظيفة قيمة الإجراء، وبالتالي
لا حاجة لنموذج لبناء السياسة الجشعة. بالنسبة لأي وظيفة قيمة إجراء</span>
$`q`$ <span dir="rtl">لا، تكون السياسة الجشعة المقابلة هي التي، لكل
حالة</span> $`s \in S`$ <span dir="rtl">، تختار بشكل حتمي الإجراء الذي
له أقصى قيمة إجراء</span>:

``` math
\pi(s) = \text{argmax}_{a}\ q(s,a)
```

<span dir="rtl">يمكن تحسين السياسة بعد ذلك عن طريق بناء كل سياسة</span>
$`\pi k + 1`$ <span dir="rtl"></span>​ <span dir="rtl">لتكون جشعة بالنسبة
إلى</span> $`q_{\pi k}`$<span dir="rtl">.</span> <span dir="rtl">عندها،
تنطبق نظرية تحسين السياسة (التي تم تناولها في القسم 4.2) على</span>
$`\pi k`$ <span dir="rtl">و</span>$`\ \pi k + 1\ `$<span dir="rtl">لأن،
لكل حالة</span> $`s \in S`$ <span dir="rtl">،</span>

``` math
\begin{aligned}
q_{\pi_{k}}\left( s,\pi_{k + 1}(s) \right) & = q_{\pi_{k}}\left( s,\underset{a}{\arg\max}q_{\pi_{k}}(s,a) \right) \\
 & = \max_{a}\mspace{2mu} q_{\pi_{k}}(s,a) \\
 & \geq q_{\pi_{k}}\left( s,\pi_{k}(s) \right) \\
 & \geq v_{\pi_{k}}(s)
\end{aligned}
```

<span dir="rtl">كما ناقشنا في الفصل السابق، فإن النظرية تضمن لنا أن كل
سياسة</span> $`{\ \ \pi}_{k + 1}`$(policy) <span dir="rtl">تكون أفضل
بشكل موحد من السياسة</span> $`\pi_{k}`$
<span dir="rtl"></span>(policy)<span dir="rtl">، أو تكون بنفس جودة
السياسة</span> $`\pi_{k}`$
<span dir="rtl"></span>(policy)<span dir="rtl">، وفي هذه الحالة تكون
كلاهما سياسات مثالية. وهذا بدوره يضمن لنا أن العملية العامة تتقارب إلى
السياسة المثلى (</span>optimal policy<span dir="rtl">)</span>
<span dir="rtl">ودالة القيمة المثلى</span> (optimal value
function)<span dir="rtl">.</span> <span dir="rtl">بهذه الطريقة، يمكن
استخدام طرق مونت كارلو</span> (Monte Carlo methods)
<span dir="rtl">لإيجاد السياسات المثلى</span> (optimal policies)
<span dir="rtl">بناءً على عينات الحلقات فقط وبدون أي معرفة أخرى بديناميات
البيئة</span> (environment)<span dir="rtl">.</span>

<span dir="rtl">لقد قمنا بعمل افتراضين غير مرجحين أعلاه للحصول على ضمانة
سهلة للتقارب لطريقة مونت كارلو</span> (Monte Carlo method)
<span dir="rtl">أحدهما هو أن الحلقات تبدأ بشكل استكشافي</span>
<span dir="rtl">(</span>exploring starts<span dir="rtl">)، والآخر هو أن
تقييم السياسة</span> (policy evaluation) <span dir="rtl">يمكن أن يتم
بعدد غير محدود من الحلقات. للحصول على خوارزمية عملية، سنحتاج إلى إزالة
كلا الافتراضين. سنؤجل النظر في الافتراض الأول حتى وقت لاحق في هذا
الفصل</span>.

<span dir="rtl">في الوقت الحالي، نركز على الافتراض بأن تقييم
السياسة</span> (policy evaluation) <span dir="rtl">يتم على عدد غير محدود
من الحلقات. هذا الافتراض من السهل نسبيًا إزالته. في الواقع، نفس المشكلة
تظهر حتى في طرق البرمجة الديناميكية التقليدية مثل تقييم السياسة
التكراري</span> (iterative policy evaluation)<span dir="rtl">، التي
تتقارب أيضًا فقط بشكل تقاربي إلى دالة القيمة الحقيقية</span> (true value
function) <span dir="rtl">في كل من حالات البرمجة الديناميكية</span> (DP)
<span dir="rtl">ومونت كارلو</span> (Monte Carlo)<span dir="rtl">، هناك
طريقتان لحل المشكلة. واحدة هي التمسك بفكرة تقريب</span> $`{q\pi}_{k}`$
<span dir="rtl"></span>(action-value function) <span dir="rtl">في كل
تقييم للسياسة</span> (policy evaluation)<span dir="rtl">.</span>
<span dir="rtl">يتم إجراء قياسات وافتراضات للحصول على حدود على حجم
واحتمالية الخطأ في التقديرات، ثم يتم اتخاذ خطوات كافية خلال كل تقييم
للسياسة</span> (policy evaluation) <span dir="rtl">لضمان أن تكون هذه
الحدود صغيرة بما فيه الكفاية. يمكن جعل هذا النهج مرضيًا تمامًا من حيث ضمان
التقارب الصحيح حتى مستوى معين من التقريب. ومع ذلك، من المحتمل أن يتطلب
هذا أيضًا عددًا كبيرًا جدًا من الحلقات ليكون مفيدًا عمليًا في أي مشكلة بخلاف
أصغر المشكلات</span>.

<span dir="rtl">هناك نهج ثانٍ لتجنب العدد غير المحدود من الحلقات الذي
يُفترض عادةً لتقييم السياسة</span> <span dir="rtl">(</span>policy
<span dir="rtl"></span>evaluation<span dir="rtl">)، حيث نتخلى عن محاولة
إتمام تقييم السياسة</span> (policy evaluation) <span dir="rtl">قبل
العودة إلى تحسين السياسة</span> (policy
improvement)<span dir="rtl">.</span> <span dir="rtl">في كل خطوة تقييم،
نقوم بتحريك دالة القيمة نحو</span> $`{q\pi}_{k}`$ (action-value
function)<span dir="rtl">، ولكننا لا نتوقع أن نقترب فعليًا إلا بعد العديد
من الخطوات. استخدمنا هذه الفكرة عندما قدمنا لأول مرة فكرة</span> GPI
(generalized policy iteration) <span dir="rtl">في القسم 4.6. شكل متطرف
من هذه الفكرة هو تكرار القيمة</span> (value iteration)<span dir="rtl">،
حيث يتم تنفيذ تكرار واحد فقط من تقييم السياسة التكراري</span> (iterative
policy evaluation) <span dir="rtl">بين كل خطوة من خطوات تحسين
السياسة</span> (policy improvement)<span dir="rtl">.</span>
<span dir="rtl">النسخة في المكان من تكرار القيمة</span> (in-place value
iteration) <span dir="rtl">أكثر تطرفًا؛ حيث نتناوب بين خطوات التحسين
والتقييم لحالات فردية</span>.

<span dir="rtl">بالنسبة لتكرار السياسة باستخدام مونت كارلو</span> (Monte
Carlo policy iteration)<span dir="rtl">، من الطبيعي التناوب بين
التقييم</span> (evaluation) <span dir="rtl">والتحسين</span>
(improvement) <span dir="rtl">على أساس حلقة بحلقة. بعد كل حلقة، تُستخدم
العوائد الملاحظة لتقييم السياسة</span> (policy
evaluation)<span dir="rtl">، ثم يتم تحسين السياسة</span> (policy)
<span dir="rtl">في جميع الحالات التي تمت زيارتها في الحلقة. خوارزمية
كاملة بسيطة على هذه الأسس، والتي نسميها مونت كارلو مع بدايات
استكشافية</span> (Monte Carlo ES)<span dir="rtl">، يتم تقديمها في
الشيفرة الكاذبة في الصندوق في الصفحة التالية.</span>

<span dir="rtl">مونت كارلو</span> ES <span dir="rtl">(البدايات
الاستكشافية)</span> Monte Carlo ES <span dir="rtl"></span>(Exploring
Starts) <span dir="rtl">لتقدير</span> $`\pi \approx \pi_{*}`$

<img src="./media/image36.png"
style="width:6.26806in;height:3.35486in" />

**<u><span dir="rtl">تمرين 5.4</span>:</u>**

<span dir="rtl">الشيفرة الزائفة لـ</span> Monte Carlo ES
<span dir="rtl">غير فعّالة لأنه، لكل زوج من الحالة–الإجراء، تحتفظ بقائمة
بكل العوائد وتحسب متوسطها بشكل متكرر. سيكون من الأكثر كفاءة استخدام
تقنيات مماثلة لتلك الموضحة في القسم 2.4 للحفاظ على المتوسط وعدد
التحديثات (لكل زوج من الحالة–الإجراء) وتحديثهما بشكل تدريجي. وصف كيف
سيتم تعديل الشيفرة الزائفة لتحقيق ذلك</span>.

<span dir="rtl">في</span> Monte Carlo ES<span dir="rtl">، يتم تجميع
وتقدير جميع العوائد لكل زوج من الحالة–الإجراء، بغض النظر عن السياسة التي
كانت سارية عند ملاحظتها. من السهل أن نرى أن</span> Monte Carlo ES
<span dir="rtl">لا يمكن أن يتقارب إلى أي سياسة غير مثالية. إذا حدث ذلك،
فإن دالة القيمة ستتقارب في النهاية إلى دالة القيمة لتلك السياسة، مما
سيؤدي بدوره إلى تغيير السياسة. الاستقرار يتحقق فقط عندما تكون كل من
السياسة ودالة القيمة مثالية. يبدو أن التقارب إلى هذه النقطة الثابتة
المثالية لا مفر منه حيث تقل التغييرات في دالة القيمة الخاصة بالإجراء
بمرور الوقت، لكنه لم يُثبت بعد بشكل رسمي. في رأينا، هذه واحدة من الأسئلة
النظرية الأساسية المفتوحة في التعليم المعزز</span> <span dir="rtl">(لحل
جزئي، راجع</span> Tsitsiklis<span dir="rtl">، 2002)</span>.

**<span dir="rtl"><u>مثال 5.3</u>: حل لعبة البلاك جاك</span>**

<span dir="rtl">من السهل تطبيق</span> Monte Carlo ES <span dir="rtl">على
البلاك جاك. نظرًا لأن الحلقات هي ألعاب محاكية، فمن السهل ترتيب بدايات
استكشافية تشمل جميع الاحتمالات. في هذه الحالة، يختار المرء بطاقات
التاجر، ومجموع اللاعب، وما إذا كان لدى اللاعب آس قابل للاستخدام أم لا،
جميعها بشكل عشوائي باحتمال متساوٍ. كسياسة ابتدائية، نستخدم السياسة التي
تم تقييمها في المثال السابق للبلاك جاك، وهي السياسة التي تتوقف فقط على
20 أو 21. يمكن أن تكون دالة القيمة الخاصة بالإجراء صفرًا لجميع أزواج
الحالة–الإجراء. يوضح الشكل 5.2 السياسة المثلى للبلاك جاك التي
وجدها</span> Monte Carlo ES<span dir="rtl">.</span> <span dir="rtl">هذه
السياسة هي نفسها "الاستراتيجية الأساسية" لـ</span> Thorp (1966)
<span dir="rtl">مع الاستثناء الوحيد للعلامة اليسرى الأكثر في السياسة لآس
قابل للاستخدام، والتي ليست موجودة في استراتيجية</span>
Thorp<span dir="rtl">.</span> <span dir="rtl">نحن غير متأكدين من سبب هذه
الفجوة، ولكننا واثقون من أن ما هو معروض هنا هو بالفعل السياسة المثلى
للإصدار الذي وصفناه من البلاك جاك</span>.

<img src="./media/image37.png"
style="width:4.79097in;height:3.53403in" />

<span dir="rtl">لشكل 5.2: السياسة المثلى ودالة القيمة للحالة للبلاك جاك،
التي وجدها</span> Monte Carlo ES<span dir="rtl">.</span>
<span dir="rtl">دالة القيمة للحالة المعروضة تم حسابها من دالة القيمة
الخاصة بالإجراء التي وجدها</span> Monte Carlo ES <span dir="rtl"></span>

**<u>5.4 <span dir="rtl">التحكم باستخدام</span> <span dir="rtl">مونت
كارلو</span> <span dir="rtl">بدون بدايات استكشافية  
(</span>Monte Carlo Control without Exploring
Starts<span dir="rtl">)</span></u>**

<span dir="rtl">كيف يمكننا تجنب الافتراض غير المحتمل للبدايات
الاستكشافية؟ الطريقة العامة الوحيدة لضمان اختيار جميع الإجراءات بشكل
لانهائي هي أن يواصل الوكيل اختيارها. هناك نهجان لضمان ذلك، مما يؤدي إلى
ما نسميه "طرق السياسة الحالية</span> (on-policy methods)
<span dir="rtl">وطرق السياسة المختلفة</span>
<span dir="rtl">(</span>off-policy methods<span dir="rtl">).</span>
<span dir="rtl">تحاول "طرق السياسة الحالية</span>" (on-policy methods)
<span dir="rtl">تقييم أو تحسين السياسة التي تُستخدم لاتخاذ القرارات،
بينما تقيم "طرق السياسة المختلفة</span> (off-policy methods)
<span dir="rtl">أو تحسن سياسة مختلفة عن تلك التي تُستخدم لتوليد البيانات.
طريقة</span> Monte Carlo ES <span dir="rtl">التي تم تطويرها أعلاه هي
مثال على "طريقة السياسة الحالية</span> (on-policy
method)<span dir="rtl">.</span> <span dir="rtl">في هذا القسم نوضح كيف
يمكن تصميم "طريقة تحكم</span> Monte Carlo <span dir="rtl">للسياسة
الحالية (</span>on-policy Monte Carlo control
<span dir="rtl"></span>method<span dir="rtl">) لا تستخدم الافتراض غير
الواقعي للبدايات الاستكشافية. تعتبر "طرق السياسة المختلفة
(</span>off-policy <span dir="rtl"></span>methods<span dir="rtl">) في
القسم التالي</span>.

<span dir="rtl">في طرق التحكم الخاصة بالسياسة الحالية، تكون السياسة
عمومًا لينة</span> (soft)<span dir="rtl">، مما يعني أن  
</span> $`\pi(a|s)\  > \ 0`$ <span dir="rtl">لجميع</span> s ∈ S
<span dir="rtl">وجميع</span> $`a\  \in \ A(s)`$<span dir="rtl">، ولكن
يتم تدريجيًا تحويلها لتكون أقرب وأقرب إلى سياسة مثالية حتمية. توفر العديد
من الطرق التي تمت مناقشتها في الفصل 2 آليات لذلك. الطريقة التي نقدمها في
هذا القسم تستخدم سياسات</span> $`\varepsilon - greedy`$<span dir="rtl">،
مما يعني أنه في معظم الأحيان تختار إجراءً له أعلى قيمة تقديرية للإجراء،
ولكن باحتمالية</span> "ε" <span dir="rtl">تختار بدلاً من ذلك إجراءً بشكل
عشوائي. أي أن جميع الإجراءات غير الجشعة تُعطى الاحتمالية الأدنى
للاختيار،</span>
$`\frac{\varepsilon}{\left| A(s) \right|}`$<span dir="rtl">، والنسبة
المتبقية من الاحتمالية،</span>
$`1 - \varepsilon + \frac{\varepsilon}{\left| A(s) \right|}`$
<span dir="rtl">، تُعطى للإجراء الجشع. تعتبر سياسات</span> "ε-greedy"
<span dir="rtl">أمثلة على السياسات</span> ε-limited
($`\varepsilon - soft`$)<span dir="rtl">، والتي تُعرّف بأنها سياسات يكون
فيها</span>
$`\pi\left( \left. \ a \right|s \right) \geq \frac{\varepsilon}{\left| A(S) \right|}`$
<span dir="rtl">لجميع الحالات والإجراءات، لبعض</span>
$`\varepsilon\  > \ 0`$ <span dir="rtl">من بين السياسات</span>
"$`\varepsilon - soft`$"<span dir="rtl">، تعتبر سياسات</span>
"$`\varepsilon - greedy`$" <span dir="rtl">هي من حيث ما تكون الأقرب إلى
الجشع</span>.

<span dir="rtl">الفكرة العامة للتحكم باستخدام</span> Monte Carlo
<span dir="rtl">للسياسة الحالية هي ما زالت فكرة</span>
**GPI**<span dir="rtl">.</span> <span dir="rtl">كما في</span> Monte
Carlo ES<span dir="rtl">، نستخدم طرق</span> MC <span dir="rtl">للزيارة
الأولى لتقدير دالة القيمة الخاصة بالإجراء للسياسة الحالية. بدون افتراض
بدايات استكشافية، لا يمكننا ببساطة تحسين السياسة من خلال جعلها جشعة
بالنسبة لدالة القيمة الحالية، لأن ذلك سيمنع المزيد من استكشاف الإجراءات
غير الجشعة. لحسن الحظ، لا يتطلب</span> **GPI** <span dir="rtl">أن تكون
السياسة قد وصلت إلى سياسة جشعة بالكامل، فقط أن تكون موجهة نحو سياسة
جشعة. في طريقتنا الخاصة بالسياسة الحالية، سنحركها فقط نحو سياسة</span>
$`\varepsilon - greedy`$<span dir="rtl">.</span> <span dir="rtl">بالنسبة
لأي سياسة</span> $`\varepsilon - soft`$<span dir="rtl">،</span>
$`\pi`$<span dir="rtl">، فإن أي سياسة</span>
$`\varepsilon - greedy\ `$<span dir="rtl">بالنسبة إلى</span> $`q_{\pi}`$
<span dir="rtl">مضمونة أن تكون أفضل من أو مساوية لـ</span>
$`\pi`$<span dir="rtl">.</span> <span dir="rtl">الخوارزمية الكاملة
مذكورة في المربع أدناه</span>.

**<span dir="rtl">التحكم باستخدام</span> Monte Carlo
<span dir="rtl">للزيارة الأولى للسياسة الحالية</span>
<span dir="rtl">(للسياسات</span> ε-soft<span dir="rtl">) تقديرا الى
</span>**$`\mathbf{\pi \approx}\mathbf{\pi}^{\mathbf{*}}`$<img src="./media/image38.png"
style="width:6.26806in;height:3.98403in" /><span dir="rtl">إن أي
سياسة</span> (policy) "$`\varepsilon`$-greedy" <span dir="rtl">بالنسبة
لـ</span> $`q_{\pi}`$ <span dir="rtl"></span> <span dir="rtl">هي تحسين
على أي سياسة</span> (policy) "$`\varepsilon`$-soft"<span dir="rtl">،
وذلك مؤكد من خلال نظرية تحسين السياسات</span> (policy improvement
theorem)<span dir="rtl">.</span> <span dir="rtl">دع</span> $`\pi 0`$
<span dir="rtl">تمثل السياسة</span> (policy)
"-greedy"<span dir="rtl">.</span> <span dir="rtl">تنطبق شروط نظرية تحسين
السياسات</span> <span dir="rtl">(</span>policy improvement
<span dir="rtl"></span>theorem<span dir="rtl">) لأن لأي</span>
$`s \in S`$ <span dir="rtl">:</span>

``` math
q^{\pi}\left( s,\pi_{0}(s) \right) = \sum_{a}^{}{\pi_{0}\left( a \middle| s \right)q^{\pi}(s,a)}
```

``` math
q^{\pi}\left( s,\pi_{0}(s) \right) = \frac{1}{\left| A(s) \right|}\sum_{a}^{}{q^{\pi}(s,a)} + (1 - \epsilon)\max_{a}q^{\pi}(s,a)
```

``` math
q^{\pi}\left( s,\pi_{0}(s) \right) = \epsilon\frac{1}{\left| A(s) \right|}\sum_{a}^{}{q^{\pi}(s,a)} + (1 - \epsilon)\sum_{a}^{}{\pi\left( a \middle| s \right)q^{\pi}(s,a)}
```

<span dir="rtl">(المجموع هو متوسط مرجح بأوزان غير سالبة مجموعها 1،
وبالتالي يجب أن يكون أقل من أو يساوي أكبر رقم متوسط)</span>

``` math
q^{\pi}\left( s,\pi_{0}(s) \right) = \epsilon\frac{1}{\left| A(s) \right|}\sum_{a}^{}{q^{\pi}(s,a)} - \epsilon\frac{1}{\left| A(s) \right|}\sum_{a}^{}{q^{\pi}(s,a)} + \sum_{a}^{}{\pi\left( a \middle| s \right)q^{\pi}(s,a)}
```

<span dir="rtl">بالتالي، وفقًا لنظرية تحسين السياسات</span> (policy
improvement theorem)<span dir="rtl">  
،</span>
$`\pi_{0} \geq \pi\quad\left( \text{i.e., }v^{\pi_{0}}(s) \geq v^{\pi}(s)\text{ for all }s \in S \right)`$
<span dir="rtl"></span> <span dir="rtl">الآن سنثبت أن التساوي يمكن أن
يحدث فقط عندما تكون كل من</span> $`\pi 0`$​
<span dir="rtl">و</span>$`\pi`$ <span dir="rtl">مثالية بين
السياسات</span> "$`\varepsilon`$-soft"<span dir="rtl">، أي عندما تكونان
أفضل أو مساوية لجميع السياسات</span>" $`\varepsilon`$ -soft"
<span dir="rtl">الأخرى.</span>

<span dir="rtl">لنأخذ بعين الاعتبار بيئة جديدة مشابهة تمامًا للبيئة
الأصلية، باستثناء أن هناك شرطًا بأن تكون السياسات</span> "-soft"
"<span dir="rtl">منقولة" إلى داخل البيئة. تحتوي البيئة الجديدة على نفس
مجموعة الإجراءات</span> (action set) <span dir="rtl">والحالات</span>
(state set) <span dir="rtl">كالبيئة الأصلية، وتتصرف على النحو التالي:
إذا كانت في الحالة</span> $`s`$ <span dir="rtl">وتم اتخاذ الإجراء</span>
$`a`$<span dir="rtl">، فإنه باحتمال</span> $`1 - \epsilon`$
<span dir="rtl">تتصرف البيئة الجديدة تمامًا مثل البيئة القديمة.
وباحتمال</span> $`\epsilon`$ <span dir="rtl">تعيد البيئة الجديدة اختيار
الإجراء بشكل عشوائي، باحتمالات متساوية، ثم تتصرف مثل البيئة القديمة مع
الإجراء الجديد والعشوائي</span>.

<span dir="rtl">أفضل ما يمكن القيام به في هذه البيئة الجديدة مع السياسات
العامة هو نفسه أفضل ما يمكن القيام به في البيئة الأصلية مع
السياسات</span> " $`\varepsilon`$ -soft". <span dir="rtl">لنفترض
أن</span>
$`\ v\hat{}*`$<span dir="rtl">و</span>$`\ q*\ `$<span dir="rtl">تمثلان
دوال القيمة المثلى للبيئة الجديدة. إذًا، تكون السياسة</span> $`\pi`$
<span dir="rtl">مثالية بين السياسات</span> " $`\varepsilon`$ -soft"
<span dir="rtl">إذا وفقط إذا كانت</span>
$`v\pi = v`$<span dir="rtl">.</span> <span dir="rtl">من تعريف</span>
$`v\hat{}*`$ <span dir="rtl">نعلم أنه هو الحل الفريد لـ</span> ...

``` math
\widetilde{v^{*}}(s) = (1 - \epsilon)\max_{a}e\widetilde{q}*(s,a) + \epsilon\frac{1}{\left| A(s) \right|}\sum_{a}^{}e\widetilde{q}*(s,a)
```

``` math
\widetilde{v^{*}}(s) = (1 - \epsilon)\max_{a}{\sum_{s',r}^{}{p\left( s',r \middle| s,a \right)\left\lbrack r + \gamma\widetilde{v^{*}}\left( s' \right) \right\rbrack}} + \epsilon\frac{1}{\left| A(s) \right|}\sum_{a}^{}{\sum_{s',r}^{}{p\left( s',r \middle| s,a \right)\left\lbrack r + \gamma\widetilde{v^{*}}\left( s' \right) \right\rbrack}}
```

<span dir="rtl">عندما يحدث التساوي ولا يتم تحسين</span> $`\pi`$
<span dir="rtl"></span> <span dir="rtl">سياسة</span>
"$`\varepsilon`$-soft" <span dir="rtl">بعد الآن، فإننا نعلم أيضًا، من
(5.2)، أن</span>

``` math
v_{\pi}(s) = (1 - \epsilon)\max_{a}q_{\pi}(s,a) + \epsilon\frac{1}{\left| A(s) \right|}\sum_{a}^{}{q_{\pi}(s,a)}
```

``` math
v_{\pi}(s) = (1 - \epsilon)\max_{a}{\sum_{s',r}^{}{p\left( s',r \middle| s,a \right)\left\lbrack r + \gamma v_{\pi}\left( s' \right) \right\rbrack}} + \epsilon\frac{1}{\left| A(s) \right|}\sum_{a}^{}{\sum_{s',r}^{}{p\left( s',r \middle| s,a \right)\left\lbrack r + \gamma v_{\pi}\left( s' \right) \right\rbrack}}
```

<span dir="rtl">ومع ذلك، فإن هذه المعادلة هي نفسها المعادلة السابقة،
باستثناء استبدال</span> $`v\pi`$ <span dir="rtl">بـ</span>
$`\widetilde{{.v}^{*}}`$ <span dir="rtl"></span> <span dir="rtl">بما أن
هو الحل الفريد، يجب أن يكون.</span> $`v\pi = \widetilde{v^{*}}`$
<span dir="rtl">بشكل أساسي، لقد أظهرنا في الصفحات القليلة الماضية أن
تكرار السياسات يعمل بالنسبة للسياسات</span> $`\varepsilon`$
-soft<span dir="rtl">.</span> <span dir="rtl">باستخدام الفهم الطبيعي
للسياسة</span> (policy) <span dir="rtl">الجشعة</span> (greedy)
<span dir="rtl">للسياسات</span> "$`\varepsilon`$-soft"<span dir="rtl">،
يتم ضمان التحسين في كل خطوة، باستثناء عندما يتم العثور على أفضل
سياسة</span> (policy) <span dir="rtl">بين السياسات</span> "
$`\varepsilon`$ -soft"<span dir="rtl">.</span> <span dir="rtl">هذه
التحليلات مستقلة عن كيفية تحديد دوال قيمة الإجراء</span> (action-value
functions) <span dir="rtl">في كل مرحلة، لكنها تفترض أنها تحسب
بدقة</span>.

<span dir="rtl">هذا يعيدنا إلى نفس النقطة تقريبًا كما في القسم السابق.
الآن نحقق فقط أفضل سياسة</span> (policy) <span dir="rtl">بين
السياسات</span>soft"<span dir="rtl">-</span>
$`"\varepsilon`$<span dir="rtl">، ولكن من ناحية أخرى، لقد أزلنا افتراض
بدايات الاستكشاف</span> <span dir="rtl">(</span>exploring
<span dir="rtl"></span>starts<span dir="rtl">)</span>.

**<u>5.5 <span dir="rtl">التنبؤ خارج السياسة</span> (Off-policy)
<span dir="rtl">عبر عينات الأهمية</span> (Importance Sampling)</u>**

<span dir="rtl">تواجه جميع طرق التحكم في التعليم معضلة: فهي تسعى لتعلم
قيم الأفعال</span> (action values) <span dir="rtl">بناءً على سلوك مثالي
لاحق، ولكنها تحتاج إلى التصرف بطرق غير مثالية لاستكشاف جميع
الأفعال</span> <span dir="rtl">(</span>to find
<span dir="rtl"></span>the optimal actions<span dir="rtl">). كيف يمكنها
التعليم عن السياسة المثالية</span> (optimal policy)
<span dir="rtl">بينما تتصرف وفقًا لسياسة استكشافية</span> (exploratory
policy)<span dir="rtl">؟ النهج المعتمد على السياسة</span> (on-policy)
<span dir="rtl">في القسم السابق هو في الواقع حل وسط—إنه يتعلم قيم
الأفعال ليس للسياسة المثالية، بل لسياسة قريبة من المثالية لا تزال
تستكشف. نهج أكثر مباشرة هو استخدام سياستين، واحدة يتم التعليم عنها والتي
تصبح السياسة المثالية، وأخرى أكثر استكشافًا وتستخدم لتوليد السلوك</span>
(behavior)<span dir="rtl">.</span> <span dir="rtl">السياسة التي يتم
التعليم عنها تُسمى</span> target policy <span dir="rtl">(سياسة الهدف)،
والسياسة المستخدمة لتوليد السلوك تُسمى</span> behavior
<span dir="rtl"></span>policy <span dir="rtl">(سياسة السلوك). في هذه
الحالة نقول إن التعليم يتم من البيانات</span> off <span dir="rtl">(خارج)
سياسة الهدف، وعملية التعليم الشاملة تُسمى</span> off-policy learning
<span dir="rtl">(التعليم خارج السياسة)</span>

<span dir="rtl">طوال بقية هذا الكتاب، سنتناول كل من الأساليب المعتمدة
على السياسة (</span>on-policy<span dir="rtl">) والأساليب غير المعتمدة
على السياسة (</span>off-policy<span dir="rtl">). عادةً ما تكون الأساليب
المعتمدة على السياسة أبسط، ويتم تناولها أولاً. بينما تتطلب الأساليب غير
المعتمدة على السياسة مفاهيم واصطلاحات إضافية، ولأن البيانات تأتي من
سياسة مختلفة، فإن هذه الأساليب غالباً ما تكون ذات تباين أكبر وأبطأ في
التلاقي. من ناحية أخرى، تعد الأساليب غير المعتمدة على السياسة أكثر قوة
وعمومية. فهي تشمل الأساليب المعتمدة على السياسة كحالة خاصة عندما تكون
السياسات المستهدفة وسلوكيات السياسة هي نفسها. كما أن للأساليب غير
المعتمدة على السياسة مجموعة متنوعة من الاستخدامات الإضافية في التطبيقات.
على سبيل المثال، يمكن غالباً تطبيقها للتعليم من البيانات التي تم إنشاؤها
بواسطة وحدة تحكم تقليدية غير تعلمية، أو من خبير بشري. يرى البعض أيضاً أن
التعليم غير المعتمد على السياسة هو مفتاح لتعلم نماذج تنبؤية متعددة
الخطوات لديناميات العالم (انظر القسم 17.2؛</span>
Sutton<span dir="rtl">، 2009؛</span> Sutton <span dir="rtl">وآخرون،
2011).</span>

<span dir="rtl">في هذا القسم، نبدأ دراسة الأساليب غير المعتمدة على
السياسة من خلال النظر في مشكلة التنبؤ، حيث تكون كل من السياسة المستهدفة
وسلوك السياسة ثابتة. بمعنى آخر، افترض أننا نرغب في تقدير</span> $`v\pi`$
<span dir="rtl">أو</span> $`q_{\pi}`$<span dir="rtl">، ولكن كل ما لدينا
هو حلقات بيانات تتبع سياسة أخرى</span> $`b`$<span dir="rtl">، حيث</span>
$`b \neq \pi b`$<span dir="rtl">.</span> <span dir="rtl">في هذه الحالة،
تعتبر</span> $`\pi`$ <span dir="rtl">هي السياسة المستهدفة، و</span>$`b`$
<span dir="rtl">هي سياسة السلوك، وكلا السياسات تعتبر ثابتة
ومعطاة</span>.

<span dir="rtl">لكي نستخدم الحلقات من سياسة</span> $`b`$
<span dir="rtl">لتقدير القيم للسياسة</span> $`\pi`$<span dir="rtl">،
نحتاج إلى أن يتم اتخاذ كل إجراء يتم تحت سياسة</span> $`\pi`$
<span dir="rtl">أيضًا، على الأقل بين الحين والآخر، تحت سياسة</span>
$`b`$<span dir="rtl">.</span> <span dir="rtl">بمعنى آخر، نحتاج إلى
أن</span> $`\pi(a \mid s)`$ <span dir="rtl">يعني أن</span>
$`b(a \mid s) > 0`$<span dir="rtl">.</span> <span dir="rtl">يُطلق على هذا
افتراض التغطية. ويترتب على التغطية أن تكون سياسة</span> $`b`$
<span dir="rtl">عشوائية في الحالات التي لا تكون فيها متماثلة مع</span>
$`\pi`$<span dir="rtl">.</span> <span dir="rtl">من ناحية أخرى، قد تكون
السياسة المستهدفة</span> $`\pi`$ <span dir="rtl">محددة، وفي الواقع، هذه
حالة ذات اهتمام خاص في تطبيقات التحكم. في التحكم، تكون السياسة المستهدفة
عادةً هي السياسة الجشعة المحددة بالنسبة للتقدير الحالي لدالة قيمة
الإجراء. تصبح هذه السياسة سياسة مثالية محددة بينما تظل سياسة السلوك
عشوائية وأكثر استكشافية، على سبيل المثال، سياسة</span> $`\epsilon`$
<span dir="rtl">الجشعة. ومع ذلك، في هذا القسم، نركز على مشكلة التنبؤ،
حيث تكون السياسة</span> $`\pi`$ <span dir="rtl">ثابتة ومعطاة</span>.

<span dir="rtl">تقريباً جميع الأساليب غير المعتمدة على السياسة تستخدم
تقنية **أهمية العينة** (</span>importance
<span dir="rtl"></span>sampling<span dir="rtl">)، وهي تقنية عامة لتقدير
القيم المتوقعة تحت توزيع معين بناءً على عينات مأخوذة من توزيع آخر. نقوم
بتطبيق أهمية العينة على التعليم غير المعتمد على السياسة من خلال وزن
العوائد وفقًا لاحتمال حدوث مساراتها تحت السياسات المستهدفة وسلوك السياسة،
والذي يُطلق عليه نسبة أهمية العينة</span>.

<span dir="rtl">بالنظر إلى حالة البداية</span> $`S_{t}`$
<span dir="rtl"></span>​<span dir="rtl">، فإن احتمال حدوث المسار التالي
من الحالة والإجراء،</span>
$`A_{t},S_{t + 1},A_{t + 1},\ldots,S_{T}`$​<span dir="rtl">، تحت أي
سياسة</span> $`\pi`$ <span dir="rtl">هو</span>:

``` math
\Pr\text{\{}A_{t},S_{t + 1},A_{t + 1},\ldots,S_{T} \mid S_{t},A_{t:T - 1} \sim \pi\text{\}}
```

``` math
= \pi\left( A_{t}\mid S_{t} \right) \cdot p\left( S_{t + 1}\mid S_{t},A_{t} \right) \cdot \pi\left( A_{t + 1}\mid S_{t + 1} \right) \cdot \ldots \cdot p\left( S_{T}\mid S_{T - 1},A_{T - 1} \right)
```

``` math
= \prod_{k = t}^{T - 1}{\pi\left( A_{k}\mid S_{k} \right)} \cdot p\left( S_{k + 1}\mid S_{k},A_{k} \right)
```

<span dir="rtl">حيث إن</span> $`p`$ <span dir="rtl">هنا هي دالة احتمال
الانتقال بين الحالات المعرفة في (3.4). وبالتالي، فإن الاحتمال النسبي
للمسار تحت السياسات المستهدفة وسلوك السياسة (نسبة أهمية العينة)
(</span>"importance-sampling
<span dir="rtl"></span>ratio”<span dir="rtl">)</span>
<span dir="rtl">هو</span>:

``` math
\rho_{t}:T - 1 = \frac{\prod_{k = t}^{T - 1}{\pi\left( A_{k}\mid S_{k} \right)} \cdot p\left( S_{k + 1}\mid S_{k},A_{k} \right)}{\prod_{k = t}^{T - 1}{b\left( A_{k}\mid S_{k} \right)} \cdot p\left( S_{k + 1}\mid S_{k},A_{k} \right)} = \prod_{k = t}^{T - 1}\frac{\pi\left( A_{k}\mid S_{k} \right)}{b\left( A_{k}\mid S_{k} \right)}
```

<span dir="rtl">على الرغم من أن احتمالات المسار تعتمد على احتمالات
الانتقال في عملية اتخاذ القرار ماركوف</span> (MDP)<span dir="rtl">،
والتي تكون عادةً غير معروفة، إلا أنها تظهر بشكل متطابق في كل من البسط
والمقام، وبالتالي يتم إلغاؤها. وتنتهي نسبة أهمية العينة</span>
<span dir="rtl">(</span>("importance-sampling ratio"
<span dir="rtl">بالاعتماد فقط على السياسات الاثنين وتسلسل الأفعال، وليس
على الـ</span> **MDP**<span dir="rtl">.</span>

<span dir="rtl">تذكر أننا نرغب في تقدير العوائد المتوقعة (القيم) تحت
السياسة المستهدفة، ولكن كل ما لدينا هو العوائد</span> $`Gt`$
<span dir="rtl">الناتجة عن سياسة السلوك. هذه العوائد لها التوقع
الخطأ</span>
$`E\left\lbrack G_{t}\mid S_{t} = s \right\rbrack = v_{b}(s)`$
<span dir="rtl"></span> <span dir="rtl">ولا يمكن بالتالي حساب متوسطها
للحصول على</span> $`v\pi`$<span dir="rtl">.</span> <span dir="rtl">هنا
يأتي دور أهمية العينة. نسبة</span> $`\rho t:\ T - 1`$
<span dir="rtl"></span>​<span dir="rtl">تحول العوائد لتكون لها التوقع
الصحيح</span>:

``` math
E\left\lbrack \rho_{t:T - 1}G_{t}\mid S_{t} = s \right\rbrack = v^{\pi}(s)
```

<span dir="rtl">الآن نحن جاهزون لتقديم خوارزمية مونت كارلو التي تقوم
بمتوسط العوائد من دفعة من الحلقات الملاحظة التي تتبع السياسة</span>
$`b`$ <span dir="rtl">لتقدير</span> $`v\pi(s)`$<span dir="rtl">.</span>
<span dir="rtl">من المفيد هنا ترقيم خطوات الزمن بطريقة تزيد عبر حدود
الحلقات. أي، إذا انتهت الحلقة الأولى من الدفعة في حالة نهائية في الزمن
100، فإن الحلقة التالية تبدأ في الزمن</span>
$`t = 10`$<span dir="rtl">.</span> <span dir="rtl">هذا يمكّننا من استخدام
أرقام الخطوات الزمنية للإشارة إلى خطوات معينة في حلقات معينة. على وجه
الخصوص، يمكننا تعريف مجموعة جميع خطوات الزمن التي يتم فيها زيارة
الحالة</span> s<span dir="rtl">، والتي نرمز لها بـ</span>
$`T(s)`$<span dir="rtl">.</span> <span dir="rtl">وهذا ينطبق على طريقة
الزيارة في كل مرة؛ أما بالنسبة لطريقة الزيارة الأولى، فإن</span>
$`T(s)`$ <span dir="rtl">ستشمل فقط خطوات الزمن التي كانت زيارات أولى
لـ</span> $`s`$ <span dir="rtl">ضمن حلقاتها. أيضًا، لنمثل</span> $`T(t)`$
<span dir="rtl">بوقت الانتهاء الأول بعد الزمن</span>
$`t`$<span dir="rtl">، و</span>$`Gt`$ <span dir="rtl">بالعائد بعد</span>
$`t`$ <span dir="rtl">حتى</span> $`T(t)`$<span dir="rtl">.</span>
<span dir="rtl">إذن،</span> $`\{ Gt\} t \in T(s)`$ <span dir="rtl">هي
العوائد التي تتعلق بالحالة</span> $`s`$<span dir="rtl">،
و</span>{$`\rho t:T(t) - 1`$} <span dir="rtl">هي نسب أهمية العينة
المقابلة. لتقدير</span> $`v\pi(s)`$ <span dir="rtl">نقوم ببساطة بتوسيع
العوائد بواسطة النسب وأخذ متوسط النتائج</span>:

``` math
V(s) = \frac{\sum_{t \in T(s)}^{}{\rho_{t:T(t) - 1}G_{t}}}{\left| T(s) \right|}
```

<span dir="rtl">عندما يتم تنفيذ أخذ العينات بالأهمية كمعدل بسيط بهذه
الطريقة، يُطلق عليه أخذ العينات بالأهمية العادي</span>.
<span dir="rtl">بديل مهم هو أخذ العينات بالأهمية الموزونة، الذي يستخدم
متوسطًا موزونًا، ويُعرف بأنه</span>

``` math
V(s) = \frac{\sum_{t \in T(s)}^{}{\rho_{t:T(t) - 1}G_{t}}}{\sum_{t \in T(s)}^{}\rho_{t:T(t) - 1}}
```

<span dir="rtl">أو صفر إذا كان المقام يساوي صفرًا. لفهم هذين النوعين من
أخذ العينات بالأهمية، دعنا ننظر إلى تقديرات "أساليب الزيارة
الأولى</span> (first-visit methods) <span dir="rtl">بعد مراقبة عائد واحد
من الحالة</span> $`s`$<span dir="rtl">.</span> <span dir="rtl">في تقدير
"المتوسط الموزون</span> (weighted-average estimate)<span dir="rtl">،
نسبة</span> $`\rho t:\ T(t) - 1`$ <span dir="rtl"></span>​
<span dir="rtl">للعائد الوحيد تُلغى في البسط والمقام، بحيث يصبح التقدير
مساويًا للعائد الملاحظ بغض النظر عن النسبة (بافتراض أن النسبة غير صفرية).
نظرًا لأن هذا العائد كان الوحيد الذي تم ملاحظته، فإن هذا تقدير معقول،
ولكن توقعه هو</span> $`vb(s)`$ <span dir="rtl">وليس</span>
$`v\pi(s)`$<span dir="rtl">، ومن هذا المنظور الإحصائي، هو "منحاز</span>
(biased)<span dir="rtl">.</span> <span dir="rtl">في المقابل، النسخة
الزيارة الأولى</span>" (first-visit version) <span dir="rtl">من "مقدر
أخذ العينات بالأهمية العادي</span>" <span dir="rtl">(</span>ordinary
importance-sampling estimator<span dir="rtl">)</span> **(5.5)**
<span dir="rtl">هو دائمًا</span> $`v\pi(s)`$ <span dir="rtl">في
التوقع</span> <span dir="rtl">(غير منحاز
\_</span>unbiased<span dir="rtl">)، ولكنه قد يكون متطرفًا</span>
(extreme)<span dir="rtl">.</span> <span dir="rtl">افترض أن النسبة كانت
عشرة، مما يشير إلى أن المسار الملاحظ أكثر احتمالاً بعشر مرات تحت "السياسة
الهدف</span>" (target policy) <span dir="rtl">مقارنةً بـ "سياسة
السلوك</span>" <span dir="rtl">(</span>behavior
<span dir="rtl"></span>policy<span dir="rtl">).</span>
<span dir="rtl">في هذه الحالة، يكون تقدير أخذ العينات بالأهمية العادي
عشرة أضعاف العائد الملاحظ. أي أنه سيكون بعيدًا جدًا عن العائد الملاحظ حتى
وإن كان مسار الحلقة يُعتبر ممثلًا جدًا للسياسة الهدف</span>.

<span dir="rtl">بشكل رسمي، يُعبر عن الفرق بين أساليب الزيارة الأولى
للنوعين من أخذ العينات بالأهمية في تحيزاتهما وتبايناتهما. أخذ العينات
بالأهمية العادي غير منحاز</span> (unbiased) <span dir="rtl">في حين أن
أخذ العينات بالأهمية الموزونة منحاز</span> (biased) <span dir="rtl">(على
الرغم من أن التحيز يتناقص تدريجيًا ليصل إلى الصفر). من ناحية أخرى، فإن
تباين أخذ العينات بالأهمية العادي عادةً غير محدود لأن تباين النسب يمكن أن
يكون غير محدود، في حين أن المقدر الموزون لا يتجاوز فيه الوزن الأكبر لأي
عائد واحد عن واحد. في الواقع، بافتراض أن العوائد محدودة، فإن تباين
المقدر بأخذ العينات بالأهمية الموزونة يتناقص ليصل إلى الصفر حتى إذا كان
تباين النسب نفسه غير محدود</span> (Precup, Sutton, and Dasgupta
2001)<span dir="rtl">.</span> <span dir="rtl">في الممارسة العملية، يكون
المقدر الموزون عادةً أقل تباينًا بكثير ويفضل بشكل كبير. ومع ذلك، لن نتخلى
تمامًا عن أخذ العينات بالأهمية العادي لأنه أسهل في التمديد إلى الأساليب
التقريبية باستخدام "تقريب الوظائف</span> (function approximation)
<span dir="rtl">التي نستكشفها في الجزء الثاني من هذا الكتاب</span>.

<span dir="rtl">أساليب كل زيارةد</span> (every-visit methods)
<span dir="rtl">لأخذ العينات بالأهمية العادي والموزون كلاهما
منحاز</span> (biased)<span dir="rtl">، على الرغم من أن التحيز يتناقص
تدريجيًا ليصل إلى الصفر مع زيادة عدد العينات. في الممارسة العملية، غالبًا
ما تفضل أساليب كل زيارة لأنها تلغي الحاجة إلى تتبع أي الحالات التي تمت
زيارتها ولأنها أسهل بكثير في التمديد إلى التقريبات. يتم تقديم
خوارزمية</span> MC <span dir="rtl">كاملة لكل زيارة لتقييم "السياسة خارج
السياسة</span>" (off-policy policy evaluation) <span dir="rtl">باستخدام
أخذ العينات بالأهمية الموزونة في القسم التالي على الصفحة 110</span>.

<span dir="rtl">**<u>تمرين 5.5</u>**: اعتبر عملية اتخاذ القرار</span>
(MDP) Markov <span dir="rtl">تحتوي على حالة غير نهائية واحدة وإجراء واحد
ينتقل بها إلى الحالة غير النهائية مع احتمال</span> p
<span dir="rtl">وينتقل إلى الحالة النهائية مع احتمال</span>
$`1 - p`$<span dir="rtl">. تكون المكافأة 1+ في جميع الانتقالات،
و</span>$`\gamma = 1`$<span dir="rtl">.</span> <span dir="rtl">افترض أنك
لاحظت تجربة واحدة استمرت 10 خطوات، مع عائد قدره 10. ما هي تقديرات
الزيارة الأولى والزيارة الكلية لقيمة الحالة غير النهائية؟</span>

<span dir="rtl">**<u>مثال 5.4</u>**: تقدير قيمة حالة في البلاك جاك
بطريقة خارج السياسة</span> <span dir="rtl">(</span>**Off**-**policy**
Estimation of a <span dir="rtl"></span>**Blackjack State
Value**<span dir="rtl">)</span>Top of Form

*Bottom of Form*

<img src="./media/image39.png"
style="width:6.26806in;height:2.20208in" /><span dir="rtl">قمنا بتطبيق
كل من طرق العينة العادية</span> (**ordinary**
**importance**-**sampling**) <span dir="rtl">وطرق "العينة ذات
الوزن</span> (**weighted** **importance**-**sampling**)
<span dir="rtl">لتقدير قيمة حالة واحدة في لعبة البلاك جاك</span>
(**blackjack**) <span dir="rtl">(مثال 5.1) باستخدام بيانات خارج
السياسة</span> (**off**-**policy**)<span dir="rtl">.</span>
<span dir="rtl">تذكر أن إحدى مزايا طرق مونت كارلو</span> (Monte Carlo)
<span dir="rtl">هي أنها يمكن استخدامها لتقييم حالة واحدة دون الحاجة إلى
تشكيل تقديرات لحالات أخرى. في هذا المثال، قمنا بتقييم الحالة التي يظهر
فيها التاجر بطاقة 2، مجموع بطاقات اللاعب هو 13، ولدى اللاعب "آس قابل
للاستخدام</span> (**usable** **ace**) <span dir="rtl">(أي أن اللاعب يحمل
آس و2، أو ما يعادلها ثلاثة آسات). تم توليد البيانات من خلال البدء في هذه
الحالة ثم اختيار الضرب</span> (**hit**) <span dir="rtl">أو التمسك</span>
(**stick**) <span dir="rtl">عشوائيًا باحتمال متساوٍ (سياسة السلوك)</span>
(**behavior** **policy**)<span dir="rtl">. كانت السياسة المستهدفة هي
التمسك فقط عند مجموع 20 أو 21، كما في مثال 5.1. قيمة هذه الحالة تحت
السياسة المستهدفة تقدر بحوالي 0.27726− (تم تحديد ذلك عن طريق توليد مئة
مليون حلقة باستخدام السياسة المستهدفة وحساب متوسط عوائدها). اقتربت كلتا
طريقتي العينة خارج السياسة من هذه القيمة بعد 1000 حلقة خارج السياسة
باستخدام السياسة العشوائية. للتأكد من أنهم فعلوا ذلك بشكل موثوق، قمنا
بتنفيذ 100 جولة مستقلة، كل منها بدءًا من تقديرات صفرية وتعلمت لمدة 10,000
حلقة. تُظهر **الشكل 5.3** منحنيات التعليم الناتجة—خطأ التربيع لتقديرات كل
طريقة كدالة لعدد الحلقات، مُتوسطًا على 100 جولة. يقترب الخطأ من الصفر لكلا
الخوارزميتين، لكن طريقة العينة ذات الوزن لها خطأ أقل بكثير في البداية،
كما هو معتاد في الممارسة العملية</span>.

(Figure 5.3) <span dir="rtl">العينة ذات الوزن</span> (Weighted
importance sampling) <span dir="rtl">تنتج تقديرات خطأ أقل لقيمة حالة
واحدة في لعبة البلاك جاك</span> (blackjack) <span dir="rtl">من حلقات
خارج السياسة</span> (off-policy)<span dir="rtl">.</span>

<span dir="rtl">تقديرات "العينة العادية</span>" (ordinary importance
sampling) <span dir="rtl">ستحتوي عادةً على تباين لانهائي، وبالتالي خصائص
تقارب غير مرضية، كلما كانت العوائد المقيَّمة تحتوي على تباين لانهائي—ويمكن
أن يحدث ذلك بسهولة في تعلم خارج السياسة</span> (off-policy)
<span dir="rtl">عندما تحتوي المسارات على حلقات. مثال بسيط يظهر في الشكل
5.4. هناك حالة غير نهائية واحدة</span> s <span dir="rtl">واثنان من
الإجراءات: اليمين</span> (right) <span dir="rtl">واليسار</span>
(left)<span dir="rtl">.</span> <span dir="rtl">يسبب الإجراء **اليمين**
انتقالًا حتميًا إلى النهاية، بينما الإجراء **اليسار** ينتقل، باحتمال 0.9،
إلى الحالة</span> $`s`$ <span dir="rtl">أو، باحتمال 0.1، إلى النهاية.
المكافآت هي 1+ على الانتقال الأخير وخلاف ذلك تكون صفرًا. اعتبر السياسة
المستهدفة التي تختار دائمًا **اليسار**. جميع الحلقات تحت هذه السياسة
تتكون من بعض الانتقالات (ربما صفر) إلى الحالة</span> s
<span dir="rtl">تليها النهاية بمكافأة وعائد قدره 1+. وبالتالي،
قيمة</span> $`s`$ <span dir="rtl">تحت السياسة المستهدفة هي 1</span>
(γ=1)<span dir="rtl">.</span> <span dir="rtl">افترض أننا نقوم بتقدير هذه
القيمة باستخدام بيانات **خارج** **السياسة** باستخدام سياسة السلوك التي
تختار "**اليمين**" و"**اليسار**" باحتمال متساوٍ</span>.

<img src="./media/image40.png"
style="width:6.26806in;height:3.39097in" />

Figure 5.4<span dir="rtl">: العينة العادية</span> (Ordinary importance
sampling) <span dir="rtl">تنتج تقديرات غير مستقرة بشكل مفاجئ على "عملية
اتخاذ القرار</span> Markov (MDP) <span dir="rtl">ذات الحالة الواحدة
الموضحة في المثال (مثال 5.5). التقدير الصحيح هنا</span>
($`\gamma = 1`$)<span dir="rtl">، ورغم أن هذا هو القيمة المتوقعة لعائد
العينة (بعد العينة ذات الوزن)، إلا أن تباين العينات لانهائي، والتقديرات
لا تتقارب إلى هذه القيمة. هذه النتائج تتعلق بـ مونت كارلو خارج
السياسة</span> (off-policy first-visit MC)

<span dir="rtl">الجزء السفلي من **الشكل** **5.4** يُظهر عشرة تجارب مستقلة
لخوارزمية "مونت كارلو للزيارة الأولى  
</span>(first-visit MC) <span dir="rtl">باستخدام **العينة
العادية**</span> (ordinary importance sampling)<span dir="rtl">.</span>
<span dir="rtl">حتى بعد ملايين الحلقات، تفشل التقديرات في التقارب إلى
القيمة الصحيحة 1. بالمقابل، ستعطي خوارزمية "العينة ذات
الوزن"</span>(weighted importance-sampling) <span dir="rtl">تقديرًا دقيقًا
بقيمة 1 إلى الأبد بعد الحلقة الأولى التي انتهت بالإجراء **اليسار**. جميع
العوائد التي لا تساوي 1 (أي التي تنتهي بالإجراء "اليمين") ستكون غير
متوافقة مع السياسة المستهدفة وبالتالي سيكون لها</span>
$`\ \rho t:T(t) - 1`$<span dir="rtl">من الصفر ولن تساهم في البسط أو
المقام في المعادلة (5.6). خوارزمية العينة ذات الوزن تنتج متوسطًا مرجحًا
فقط للعوائد المتوافقة مع السياسة المستهدفة، وجميع هذه العوائد ستكون
بالضبط 1</span>.

<span dir="rtl">يمكننا التحقق من أن تباين العوائد المقيَّمة باستخدام
العينة ذات الوزن هو لانهائي في هذا المثال من خلال حساب بسيط. تباين أي
متغير عشوائي</span> $`X`$ <span dir="rtl">هو القيمة المتوقعة للاختلاف عن
متوسطه</span>$`ˉX`$<span dir="rtl">، والذي يمكن كتابته كالتالي</span>

``` math
\text{Var}\lbrack X\rbrack = E\left\lbrack \left( X - \overline{X} \right)^{2} \right\rbrack = E\left\lbrack X^{2} - 2X\overline{X} + \overline{X^{2}} \right\rbrack = E\left\lbrack X^{2} \right\rbrack - \overline{X^{2}}
```

<span dir="rtl">لذا، إذا كان المتوسط محدودًا، كما هو الحال في حالتنا، فإن
التباين يكون لانهائيًا إذا وفقط إذا كانت القيمة المتوقعة لمربع المتغير
العشوائي لانهائية. لذا، نحتاج فقط لإظهار أن القيمة المتوقعة لمربع العائد
المقيَّم باستخدام العينة ذات الوزن هي لانهائية</span>:

``` math
E_{b}\left\lbrack \left( \prod_{t = 0}^{T - 1}{\frac{\pi\left( \left. \ A_{t} \right|s_{t} \right)}{b\left( \left. \ A_{t} \right|S_{t} \right)}G_{0}} \right)^{2} \right\rbrack
```

<span dir="rtl">لحساب هذه القيمة المتوقعة، نقوم بتفكيكها إلى حالات بناءً
على طول الحلقة</span> (episode) <span dir="rtl">والإنهاء. لاحظ أولاً أنه،
بالنسبة لأي حلقة تنتهي بالإجراء اليمين</span> (right
action)<span dir="rtl">، يكون نسبة العينة ذات الوزن</span> (importance
sampling ratio) <span dir="rtl">صفرًا، لأن السياسة المستهدفة</span>
(target policy) <span dir="rtl">لن تأخذ هذا الإجراء؛ وبالتالي، فإن هذه
الحلقات لا تساهم في القيمة المتوقعة</span> (the expectation)
<span dir="rtl">(القيمة في الأقواس ستكون صفرًا) ويمكن تجاهلها. نحتاج فقط
إلى النظر في الحلقات التي تشمل عددًا من الإجراءات اليسار</span> (left
actions) <span dir="rtl">(ربما صفر) التي تنتقل إلى الحالة غير
النهائية</span> <span dir="rtl">(</span>nonterminal
<span dir="rtl"></span>state<span dir="rtl">)، تليها إجراء "اليسار" الذي
ينتقل إلى النهاية. جميع هذه الحلقات لها عائد قدره 1، لذا يمكن تجاهل
عامل</span> $`\mathbf{G0}`$<span dir="rtl">.</span> ​
<span dir="rtl">للحصول على المربع المتوقع، نحتاج فقط إلى النظر في كل طول
حلقة، مضاعفًا احتمال حدوث الحلقة بمربع نسبة العينة ذات الوزن، وجمع هذه
القيم</span>.

``` math
\frac{1}{2} \cdot 0.1 \cdot \left( \frac{1}{0.5} \right)^{2}
```

<span dir="rtl">الحلقة ذات الطول 1</span>

``` math
+ \frac{1}{2} \cdot 0.9 \cdot \frac{1}{2} \cdot 0.1 \cdot \left( \frac{1}{0.5} \cdot \frac{1}{0.5} \right)^{2}
```

<span dir="rtl">الحلقة ذات الطول 2</span>

``` math
\frac{1}{2} \cdot 0.9 \cdot \frac{1}{2} \cdot 0.9 \cdot \frac{1}{2} \cdot 0.1 \cdot \left( \frac{1}{0.5} \cdot \frac{1}{0.5} \cdot \frac{1}{0.5} \right)^{2}
```

<span dir="rtl">الحلقة ذات الطول 3</span>

<span dir="rtl">+...</span>

``` math
= 0.1\sum_{k = 0}^{\infty}{0.9}^{k} \cdot 2^{k} \cdot 2 = 0.2\sum_{k = 0}^{\infty}{1.8}^{k} = 1
```

### **<span dir="rtl"><u>تمرين 5.6</u></span>**

<span dir="rtl">ما هو المعادلة المكافئة لـ (5.6) لقيم الإجراءات</span>
$`Q(s,a)`$ <span dir="rtl">بدلاً من قيم الحالة</span>
$`V(s)`$<span dir="rtl">، مرة أخرى بالنظر إلى العوائد التي تم الحصول
عليها باستخدام</span> $`b`$<span dir="rtl">؟</span>

### **<span dir="rtl"><u>تمرين 5.7</u></span>**

<span dir="rtl">في منحنيات التعليم مثل تلك التي تظهر في الشكل 5.3، عادة
ما يقل الخطأ مع التدريب، كما حدث بالفعل لطريقة العينة العادية. لكن
بالنسبة لطريقة العينة ذات الوزن، زاد الخطأ أولاً ثم انخفض. لماذا تعتقد أن
هذا حدث؟</span>

### **<span dir="rtl"><u>تمرين 5.8</u></span>**

<span dir="rtl">النتائج في المثال 5.5 والمبينة في **الشكل** **5.4**
استخدمت طريقة "مونت كارلو للزيارة الأولى</span>"
<span dir="rtl">(</span>first-visit MC<span dir="rtl">).</span>
<span dir="rtl">افترض أنه بدلاً من ذلك تم استخدام طريقة "مونت كارلو لكل
زيارة</span> (every-visit MC) <span dir="rtl">على نفس المشكلة. هل سيظل
تباين التقدير لانهائيًا؟ لماذا أو لماذا لا؟</span>

**<u>5.6 <span dir="rtl">تنفيذ متزايد</span> (Incremental
Implementation)</u>**

<span dir="rtl">يمكن تنفيذ طرق التنبؤ باستخدام مونت كارلو بشكل متزايد،
على أساس حلقة بحلقة، باستخدام امتدادات للتقنيات الموصوفة في **الفصل**
**2** (**القسم** **2**.**4**). بينما في الفصل 2 كنا نعدل المكافآت، في
طرق مونت كارلو نعدل العوائد. في جميع الجوانب الأخرى، يمكن استخدام نفس
الأساليب تمامًا كما في الفصل 2 لطرق مونت كارلو ذات السياسة</span>
(on-policy)<span dir="rtl">.</span> <span dir="rtl">بالنسبة لطرق مونت
كارلو غير ذات السياسة  
(</span>off-policy<span dir="rtl">)، نحتاج إلى النظر بشكل منفصل في تلك
التي تستخدم العينة ذات الوزن العادي وتلك التي تستخدم العينة ذات الوزن
الموزون</span>.

<span dir="rtl">في العينة ذات الوزن العادي، تُعدل العوائد بواسطة نسبة
العينة ذات الوزن</span> $`\rho t:T(t) - 1`$ <span dir="rtl">
</span>(5.3)<span dir="rtl">، ثم يتم ببساطة متوسطها، كما في (5.5).
بالنسبة لهذه الطرق، يمكننا مرة أخرى استخدام الأساليب المتزايدة في الفصل
2، ولكن باستخدام العوائد المعدلة بدلاً من المكافآت في ذلك الفصل. هذا يترك
حالة طرق غير ذات السياسة التي تستخدم العينة ذات الوزن الموزون. هنا يتعين
علينا تشكيل متوسط موزون للعوائد، ويجب استخدام خوارزمية متزايدة مختلفة
قليلاً</span>.

<span dir="rtl">افترض أن لدينا تسلسلاً من العوائد</span>
$`G1,G2,\ldots,Gn - 1`$​<span dir="rtl">، كلها تبدأ في نفس الحالة وكل
منها مع وزن عشوائي مطابق</span> $`Wi`$
<span dir="rtl"></span>(<span dir="rtl">على سبيل المثال،</span>
$`Wi = \rho ti:T(ti) - 1`$<span dir="rtl">.</span> <span dir="rtl">نريد
تشكيل التقدير</span>

``` math
V_{n} = \frac{\sum_{k = 1}^{n - 1}{W_{k}G_{k}}}{\sum_{k = 1}^{n - 1}W_{k}},\quad n \geq 2
```

<span dir="rtl">واستمر في تحديثها كلما حصلنا على عائد إضافي واحد</span>
$`Gn`$<span dir="rtl">.</span> <span dir="rtl">بالإضافة إلى
متابعة</span> $`Vn`$ <span dir="rtl"></span>​<span dir="rtl">، يجب علينا
الحفاظ على المجموع التراكمي</span> Cn <span dir="rtl"></span>​
<span dir="rtl">للأوزان المعطاة للعوائد الأولى</span>
n<span dir="rtl">.</span> <span dir="rtl">قاعدة التحديث لـ</span> $`Vn`$
<span dir="rtl">هي</span>:

``` math
V_{n + 1} = V_{n} + \frac{W_{n}}{C_{n}}\left( G_{n} - V_{n} \right),\quad n \geq 1
```

<span dir="rtl">و</span>

``` math
C_{n + 1} = C_{n} + W_{n + 1}
```

<span dir="rtl">حيث</span> $`C0 = 0`$ <span dir="rtl">(و</span>$`V1`$
<span dir="rtl"></span>​ <span dir="rtl">يمكن أن يكون عشوائيًا وبالتالي لا
يحتاج إلى تحديده). تحتوي الصندوق في الصفحة التالية على خوارزمية كاملة
للتحديث المتزايد حلقة بحلقة لتقييم سياسة مونت كارلو. الخوارزمية مخصصة
بشكل أساسي للحالة غير ذات السياسة</span> (off-policy)<span dir="rtl">،
باستخدام العينة ذات الوزن الموزون، ولكن يمكن تطبيقها أيضًا للحالة ذات
السياسة</span> (on-policy) <span dir="rtl">ببساطة عن طريق اختيار
السياسات المستهدفة</span> (target) <span dir="rtl">وسلوك</span>
(behavior) <span dir="rtl">على أنها نفس الشيء (في هذه الحالة</span>
$`\pi = b`$<span dir="rtl">، و</span>$`W`$ <span dir="rtl">دائمًا 1).
يقترب التقدير</span> $`Q`$ <span dir="rtl">من</span> $`q_{\pi}`$
<span dir="rtl"></span> <span dir="rtl">(لكل أزواج الحالة-الإجراء التي
يتم مواجهتها) بينما يتم اختيار الإجراءات وفقًا لسياسة قد تكون
مختلفة،</span> $`b`$<span dir="rtl">.</span>

### <span dir="rtl"><u>تمرين 5.9</u></span>

<span dir="rtl">قم بتعديل خوارزمية تقييم سياسة مونت كارلو للزيارة
الأولى</span> (first-visit MC policy evaluation) <span dir="rtl">(التي
توجد في القسم 5.1) لاستخدام التنفيذ المتزايد</span> (incremental
implementation) <span dir="rtl">لمتوسطات العينة</span> (sample averages)
<span dir="rtl">الموصوف في القسم 2.4</span>.

### <span dir="rtl"><u>تمرين 5.10</u></span>

<span dir="rtl">استنبط قاعدة التحديث ذات الوزن المتوسط</span>
(weighted-average update rule) (5.8) <span dir="rtl">من (5.7). اتبع نمط
استنباط القاعدة غير الوزنية</span> (unweighted rule) (2.3).

### <span dir="rtl">التحكم باستخدام</span> "MC" <span dir="rtl">خارج السياسة</span> (Off-policy MC control)<span dir="rtl">الى تقدير</span> $`\pi \approx \pi^{*}`$

<img src="./media/image41.png"
style="width:6.26806in;height:3.44236in" /> <span dir="rtl"></span>

<span dir="rtl">**<u>5.7</u>**: نحن الآن جاهزون لتقديم مثال عن الفئة
الثانية من أساليب التحكم بالتعلّم</span> <span dir="rtl">(</span>learning
control <span dir="rtl"></span>methods<span dir="rtl">) التي نناقشها في
هذا الكتاب، وهي</span>: <span dir="rtl">**الأساليب غير المعتمدة على
السياسة الحالية** (</span>**off-policy
methods**<span dir="rtl">).</span> <span dir="rtl">تذكّر أن السمة المميّزة
لأساليب **المعتمدة على السياسة الحالية** (</span>**on-policy
method<span dir="rtl">) </span>**<span dir="rtl">هي أنها تقدّر قيمة سياسة
معينة أثناء استخدامها للتحكم. في المقابل، تقوم **الأساليب غير المعتمدة
على السياسة الحالية**</span> (**off-policy methods**)
<span dir="rtl">بالفصل بين هاتين الوظيفتين</span>.

<span dir="rtl">السياسة المستخدمة لتوليد السلوك تُسمى **السياسة
السلوكية**</span> (**behavior policy**)<span dir="rtl">، وقد لا تكون
مرتبطة بالسياسة التي يتم تقييمها وتحسينها، والتي تُسمى **السياسة
المستهدفة**</span> (**target policy**)<span dir="rtl">.</span>
<span dir="rtl">ومن مزايا هذا الفصل أن **السياسة المستهدفة**</span>
(**target policy**) <span dir="rtl">يمكن أن تكون **حتمية**</span>
(**deterministic**) <span dir="rtl">على سبيل المثال، سياسة جشعة</span>
(**greedy**) – <span dir="rtl">بينما يمكن لـ **السياسة السلوكية**
(</span>**behavior <span dir="rtl"></span>policy**<span dir="rtl">) أن
تستمر في تجربة كل الأفعال الممكنة</span>.

<span dir="rtl">تستخدم أساليب التحكم</span> **Monte Carlo
<span dir="rtl">غير المعتمدة على السياسة الحالية</span>**
<span dir="rtl">(</span>**off-policy Monte <span dir="rtl"></span>Carlo
control methods**<span dir="rtl">)</span> <span dir="rtl">واحدة من
التقنيات التي تم تقديمها في القسمين السابقين. فهي تتبع **السياسة
السلوكية**</span> (**behavior policy**) <span dir="rtl">أثناء تعلّمها
وتحسينها لـ **السياسة المستهدفة** (</span>**target
<span dir="rtl"></span>policy**<span dir="rtl">).</span>
<span dir="rtl">وتتطلب هذه التقنيات أن تمتلك **السياسة السلوكية**
احتمالًا غير صفري لاختيار كل الأفعال التي قد تختارها **السياسة
المستهدفة**</span> (**coverage**)<span dir="rtl">.</span>
<span dir="rtl">ولكي نستكشف جميع الاحتمالات، نُطالب بأن تكون **السياسة
السلوكية** **مرنة**</span> ($`\mathbf{soft}`$) <span dir="rtl">أي أنها
تختار كل الأفعال في كل الحالات باحتمال غير صفري</span>.
<span dir="rtl"></span>

<span dir="rtl">يعرض الإطار في الصفحة التالية طريقة **تحكم**</span>
**Monte Carlo <span dir="rtl">غير المعتمدة على السياسة الحالية</span>**
<span dir="rtl"></span>(**off-policy Monte Carlo control**)
<span dir="rtl">بالاعتماد على **التحسين السياسي العام**</span> (**GPI**)
<span dir="rtl">و**التقدير المرجّح باستخدام الأهمية**</span> (**weighted
importance sampling**) <span dir="rtl">من أجل تقدير السياسة
المثلى</span> $`\mathbf{\pi*}`$ <span dir="rtl">وقيم الحالة-الفعل
المثلى</span> $`\mathbf{q*}`$<span dir="rtl">.</span>
<span dir="rtl">السياسة المستهدفة</span>
$`\mathbf{\pi}`$<span dir="rtl">، التي نريد أن تقترب من</span>
$`\mathbf{\pi*}`$<span dir="rtl">، هي السياسة الجشعة</span> (**greedy
policy**) <span dir="rtl">بالنسبة لـ</span>
$`\mathbf{Q}`$<span dir="rtl">، وهو تقدير لقيم</span>
$`\mathbf{q\backslash\pi}`$<span dir="rtl">.</span>

<span dir="rtl">يمكن أن تكون **السياسة السلوكية**</span> (**behavior
policy** $`\mathbf{b}`$) <span dir="rtl">أي شيء، ولكن لضمان تقارب</span>
$`\mathbf{\pi}`$ <span dir="rtl">نحو السياسة المثلى، يجب الحصول على عدد
لا نهائي من العوائد</span> (returns) <span dir="rtl">لكل زوج من الحالات
والأفعال. ويمكن ضمان ذلك من خلال اختيار</span> $`\mathbf{b}`$
<span dir="rtl">لتكون</span>
($`\mathbf{\varepsilon - soft}`$)<span dir="rtl">.</span>
<span dir="rtl">وبذلك، تتقارب السياسة</span> $`\mathbf{\pi}`$
<span dir="rtl">نحو المثالية في جميع الحالات التي تم المرور بها، حتى وإن
كانت الأفعال تُختار حسب **سياسة سلوكية مرنة مختلفة**</span> (**soft
behavior policy**) <span dir="rtl">قد تتغير بين الحلقات أو حتى داخل
الحلقة الواحدة</span>.<span dir="rtl">  
</span><img src="./media/image42.png"
style="width:6.26806in;height:3.81597in" /><span dir="rtl">  
قد تكون هناك مشكلة محتملة في أن هذه الطريقة تتعلم فقط من أطراف
الحلقات</span> (episodes) <span dir="rtl">عندما تكون جميع الإجراءات
المتبقية في الحلقة جشعة</span> (greedy)<span dir="rtl">.</span>
<span dir="rtl">إذا كانت الإجراءات غير الجشعة</span> (nongreedy actions)
<span dir="rtl">شائعة، فإن التعليم سيكون بطيئاً، خصوصاً للحالات التي تظهر
في الأجزاء المبكرة من الحلقات الطويلة. من المحتمل أن يؤدي ذلك إلى إبطاء
التعليم بشكل كبير. لم تكن هناك تجربة كافية مع طرق مونت كارلو خارج
السياسة</span> (off-policy Monte Carlo methods) <span dir="rtl">لتقييم
مدى خطورة هذه المشكلة. إذا كانت المشكلة خطيرة، فإن أهم طريقة لمعالجتها
هي على الأرجح من خلال دمج التعليم بالتباين الزمني</span>
(temporal-difference learning)<span dir="rtl">، الفكرة الخوارزمية التي
سيتم تناولها في الفصل التالي. بدلاً من ذلك، إذا كانت</span> $`\gamma`$
<span dir="rtl">(غاما) أقل من 1، فقد تساعد الفكرة التي سيتم تناولها في
القسم التالي أيضاً بشكل كبير</span>.

**<span dir="rtl"><u>تمرين 5.11</u></span>**

<span dir="rtl">في الخوارزمية المربعة لسيطرة مونت كارلو خارج
السياسة</span> (off-policy MC control)<span dir="rtl">، قد كنت تتوقع أن
يكون تحديث</span> W <span dir="rtl">مرتبطًا بنسبة أخذ العينات
المهمة</span>
$`\frac{\pi\left( A_{t} \middle| S_{t} \right)}{b\left( A_{t} \middle| S_{t} \right)}`$
<span dir="rtl"></span>​<span dir="rtl">، ولكن بدلاً من ذلك، يتضمن</span>
​.
<span dir="rtl"></span>$`\frac{1}{b\left( A_{t} \middle| S_{t} \right)}`$
<span dir="rtl"></span> <span dir="rtl">لماذا يكون هذا صحيحًا على الرغم
من ذلك؟</span>

**<span dir="rtl"><u>تمرين 5.12</u>: مضمار السباق</span> (Racetrack)
<span dir="rtl">(برمجة)</span>**

<span dir="rtl">فكر في قيادة سيارة سباق حول منعطفات مشابهة لتلك الموضحة
في الشكل 5.5. تريد أن تسير بأقصى سرعة ممكنة، ولكن ليس بسرعة تجعل السيارة
تخرج عن المضمار</span> (track)<span dir="rtl">.</span>
<span dir="rtl">في مضمار السباق المبسط لدينا، تكون السيارة في واحدة من
مجموعة محددة من مواقع الشبكة</span> (grid positions)<span dir="rtl">،
وهي الخلايا في الرسم البياني. السرعة أيضًا محددة بشكل منفصل، وهي عدد
خلايا الشبكة التي يتم التحرك بها أفقيًا ورأسيًا في كل خطوة زمنية</span>
(time step)<span dir="rtl">.</span> <span dir="rtl">الإجراءات هي زيادات
في مكونات السرعة</span> <span dir="rtl">(</span>velocity
<span dir="rtl"></span>components<span dir="rtl">).</span>
<span dir="rtl">يمكن تغيير كل منها بـ 1+ أو 1- أو 0 في كل خطوة، بإجمالي
تسع (3 × 3) إجراءات. يتم تقييد كل من مكونات السرعة لتكون غير سلبية وأقل
من 5، ولا يمكن أن تكون كلاهما صفرًا إلا عند خط البداية</span> (starting
line)<span dir="rtl">.</span> <span dir="rtl">تبدأ كل حلقة في واحدة من
الحالات العشوائية المختارة مع كلا مكونات السرعة صفرًا وتنتهي عندما تعبر
السيارة خط النهاية</span> (finish line)<span dir="rtl">.</span>
<span dir="rtl">المكافآت هي 1- لكل خطوة حتى تعبر السيارة خط النهاية. إذا
ضربت السيارة حدود المضمار</span> (track boundary)<span dir="rtl">، يتم
نقلها إلى موضع عشوائي على خط البداية، ويتم تقليل كلا مكونات السرعة إلى
صفر، وتستمر الحلقة. قبل تحديث موقع السيارة في كل خطوة زمنية، تحقق لمعرفة
ما إذا كان المسار المتوقع للسيارة يتقاطع مع حدود المضمار. إذا تقاطع مع
خط النهاية، تنتهي الحلقة؛ إذا تقاطع في أي مكان آخر، تعتبر السيارة قد
ضربت حدود المضمار ويتم إرسالها إلى خط البداية. لجعل المهمة أكثر تحديًا،
مع احتمال 0.1 في كل خطوة زمنية، تكون زيادات السرعة كلاهما صفرًا، بشكل
مستقل عن الزيادات المقصودة. استخدم طريقة تحكم مونت كارلو</span> (Monte
Carlo control) <span dir="rtl">لحساب السياسة المثلى</span> (optimal
policy) <span dir="rtl">من كل حالة بدء. اعرض عدة مسارات تتبع السياسة
المثلى</span> (optimal policy) <span dir="rtl">(ولكن قم بإيقاف
الضوضاء</span> (noise) <span dir="rtl">لهذه المسارات)</span>.

<img src="./media/image43.png"
style="width:6.26806in;height:3.48403in" />

**<u>5.8 <span dir="rtl">أخذ *العينات المهمة مع الأخذ في الاعتبار
التخفيف* (</span>Discounting-aware Importance
Sampling<span dir="rtl">)</span></u>**

<span dir="rtl">حتى الآن، تعتمد طرق السياسة خارج السياسة</span>
(off-policy) <span dir="rtl">التي نظرنا فيها على تشكيل أوزان أخذ العينات
المهمة</span> (importance-sampling weights)
<span dir="rtl">للعائدات</span> (returns) <span dir="rtl">التي تعتبر ككل
واحد، دون الأخذ في الاعتبار الهياكل الداخلية للعائدات كجمع للمكافآت
المخفضة</span> <span dir="rtl">(</span>discounted
<span dir="rtl"></span>rewards<span dir="rtl">).</span>
<span dir="rtl">نناقش الآن بإيجاز أفكار البحث الحديثة لاستخدام هذه
الهيكلية لتقليل التباين بشكل كبير في مقدرات السياسة خارج السياسة</span>
(off-policy estimators)<span dir="rtl">.</span>

<span dir="rtl">على سبيل المثال، اعتبر الحالة التي تكون فيها الحلقات
طويلة و</span>$`\gamma`$ <span dir="rtl">(غاما) أقل بكثير من 1</span>.

<span dir="rtl">على سبيل التوضيح، افترض أن الحلقات تستمر 100 خطوة
و</span>$`\gamma\  = \ 0`$<span dir="rtl">. ستكون العائدات من الزمن 0
هي</span> $`G0\  = R1`$​<span dir="rtl">، لكن نسبة أخذ العينات المهمة
ستكون عبارة عن حاصل ضرب 100 عامل،</span>
$`\frac{\pi\left( A_{0} \middle| S_{0} \right)}{b\left( A_{0} \middle| S_{0} \right)} \cdot \frac{\pi\left( A_{1} \middle| S_{1} \right)}{b\left( A_{1} \middle| S_{1} \right)}\cdots\frac{\pi\left( A_{99} \middle| S_{99} \right)}{b\left( A_{99} \middle| S_{99} \right)}`$
<span dir="rtl">في أخذ العينات المهمة العادي، سيتم ضبط العائد بواسطة
المنتج الكامل، ولكن من الضروري فعلاً ضبطه بواسطة العامل الأول فقط،</span>
​. <span dir="rtl"></span> <span dir="rtl">العوامل الـ</span>
$`\frac{\pi\left( A_{0} \middle| S_{0} \right)}{b\left( A_{0} \middle| S_{0} \right)}`$
<span dir="rtl"></span> <span dir="rtl">99 الأخرى</span> ​
$`\cdot \frac{\pi\left( A_{1} \middle| S_{1} \right)}{b\left( A_{1} \middle| S_{1} \right)}\cdots\frac{\pi\left( A_{99} \middle| S_{99} \right)}{b\left( A_{99} \middle| S_{99} \right)}`$
<span dir="rtl">غير ذات صلة لأن العائد قد تم تحديده بالفعل بعد المكافأة
الأولى. هذه العوامل اللاحقة كلها مستقلة عن العائد ولها قيمة متوقعة تساوي
1؛ فهي لا تغير التحديث المتوقع، لكنها تضيف بشكل هائل إلى تباينه</span>

<span dir="rtl">**التباين غير المحدود**</span> (variance
infinite)<span dir="rtl">. دعنا الآن نُفكر في فكرة لتجنب هذا التباين
الكبير غير الضروري. جوهر الفكرة هو التفكير في الخصم على أنه تحديد احتمال
**الإنهاء**</span> (termination) <span dir="rtl">أو، بشكل مكافئ، درجة
**الإنهاء الجزئي**</span> (partial termination)<span dir="rtl">.</span>
<span dir="rtl">لأي</span> $`\gamma \in \lbrack 0,1)`$ <span dir="rtl">،
يمكننا أن نفكر في العائد</span> $`G0`$ <span dir="rtl">على أنه ينتهي
جزئيًا في خطوة واحدة، بدرجة</span> $`1 - \gamma`$<span dir="rtl">، مما
ينتج عائدًا يتضمن المكافأة الأولى فقط،</span> $`R1`$
<span dir="rtl"></span>​<span dir="rtl">، وينتهي جزئيًا بعد خطوتين،
بدرجة</span> ($`1 - \gamma`$)<span dir="rtl">، مما ينتج عائدًا
يتضمن</span> $`R1 + R2\ `$ <span dir="rtl">وهكذا. الدرجة الأخيرة تتوافق
مع الانتهاء في الخطوة الثانية،</span> $`1 - \gamma`$<span dir="rtl">،
وعدم الانتهاء بالفعل في الخطوة الأولى،</span> γ<span dir="rtl">.</span>
<span dir="rtl">درجة الانتهاء في الخطوة الثالثة هي</span>
$`(1 - \gamma)\ \gamma 2،`$ <span dir="rtl">مع انعكاس</span>
$`\gamma 2`$ <span dir="rtl">أن الانتهاء لم يحدث في أي من الخطوتين
الأوليين. العوائد الجزئية هنا تُسمى **العوائد الجزئية المسطحة**
(</span>flat partial returns<span dir="rtl">)</span>.

``` math
{\overline{G}}_{t}:h = R_{t + 1} + R_{t + 2} + \cdots + R_{h},\quad 0 \leq t < h \leq T
```

<span dir="rtl">حيث أن "المسطح</span> ($`flat`$) <span dir="rtl">يُشير
إلى غياب الخصم، والجزئي</span> ($`partial`$) <span dir="rtl">يُشير إلى أن
هذه العوائد لا تمتد حتى النهاية ولكنها تتوقف عند</span>
$`h`$<span dir="rtl">، والذي يُسمى **الأفق**</span> ($`horizon`$)
<span dir="rtl">(و</span>$`T`$ <span dir="rtl">هو وقت انتهاء الحلقة).
يمكن اعتبار العائد الكامل التقليدي</span> $`Gt`$ <span dir="rtl">كمجموعة
من العوائد الجزئية المسطحة كما هو موضح أعلاه على النحو التالي</span>:

``` math
G_{t} = R_{t + 1} + \gamma R_{t + 2} + \gamma^{2}R_{t + 3} + \cdots + \gamma^{T - t - 1}R_{T}
```

``` math
= (1 - \gamma)R_{t + 1}
```

``` math
+ (1 - \gamma)\gamma\left( R_{t + 1} + R_{t + 2} \right) + (1 - \gamma)\gamma^{2}
```

``` math
\left( R_{t + 1} + R_{t + 2} + R_{t + 3} \right)
```

``` math
+ \cdots
```

``` math
+ (1 - \gamma)\gamma^{T - t - 2}\left( R_{t + 1} + R_{t + 2} + \cdots + R_{T - 1} \right)
```

``` math
+ \gamma^{T - t - 1}\left( R_{t + 1} + R_{t + 2} + \cdots + R_{T} \right)
```

``` math
= (1 - \gamma)\sum_{h = t + 1}^{T - 1}{\gamma^{h - t - 1}\overline{G_{t:h}}} + \gamma^{T - t - 1}\overline{G_{t:T}}
```

<span dir="rtl">الآن نحتاج إلى تعديل العوائد الجزئية المسطحة بواسطة
**نسبة العينة المهمة (**</span>**importance
<span dir="rtl"></span>sampling ratio<span dir="rtl">)</span>**
<span dir="rtl">التي تكون مشابهة أيضًا مقطوعة. بما أن</span> $`Gˉt:h`$
<span dir="rtl">تشمل فقط المكافآت حتى أفق</span> $`h`$<span dir="rtl">،
نحتاج فقط إلى نسبة الاحتمالات حتى</span> $`h`$<span dir="rtl">.</span>
<span dir="rtl">نعرّف **مقدّر العينة المهمة** العادي، الذي يشابه (5.5)،
على النحو التالي</span>:

``` math
V(s) = \frac{\sum_{t \in T(s)}^{}\left\lbrack (1 - \gamma)\sum_{h = t + 1}^{T(t) - 1}{\gamma^{h - t - 1}\rho_{t:h - 1}\overline{G_{t:h}}} + \gamma^{T(t) - t - 1}\rho_{t:T(t) - 1}\overline{G_{t:T(t)}} \right\rbrack}{\left| T(s) \right|}
```

<span dir="rtl">ومقدّر **العينة المهمة الموزون**</span> (weighted
importance-sampling estimator)<span dir="rtl">، الذي يشابه (5.6)، كما
يلي</span>:

<span dir="rtl">نُطلق على هذين المقدّرين **مقدّرات العينة المهمة المدركة
للخصم** (</span>discounting-aware <span dir="rtl"></span>importance
sampling estimators<span dir="rtl">).</span> <span dir="rtl">إنها تأخذ
في الاعتبار معدل الخصم ولكن ليس لها تأثير (تكون كما هي مع مقدّرات
السياسات المختلفة من القسم 5.5) إذا كان</span>
$`\gamma = 1`$<span dir="rtl">.</span>

**<u>5.9 <span dir="rtl">عينة مهمة لكل قرار</span> (Per-decision
Importance Sampling)</u>**

<span dir="rtl">هناك طريقة أخرى يمكن من خلالها أخذ هيكل العائد كمجموع من
المكافآت في **عينات الأهمية خارج السياسة**</span> **(off-policy
importance sampling)** <span dir="rtl">في الاعتبار، وهي طريقة قد تكون
قادرة على تقليل التباين حتى في غياب الخصم (أي حتى إذا كان</span>
$`\gamma = 1`$<span dir="rtl">.</span> <span dir="rtl">في **تقديرات
الأهمية خارج السياسة** </span>**(off-policy estimators)
<span dir="rtl">(</span>**5.5<span dir="rtl">) و (5.6)، كل مصطلح في
البسط هو نفسه مجموع</span>:

``` math
\rho_{t:T - 1}G_{t} = \rho_{t:T - 1}\left( R_{t + 1} + \gamma R_{t + 2} + \cdots + \gamma^{T - t - 1}R_{T} \right)
```

``` math
= \rho_{t:T - 1}R_{t + 1} + \gamma\rho_{t:T - 1}R_{t + 2} + \cdots + \gamma^{T - t - 1}\rho_{t:T - 1}R_{T}
```

<span dir="rtl">ستند تقديرات **الأسلوب خارج السياسة**</span>
**(off-policy estimators)** <span dir="rtl">إلى القيم المتوقعة لهذه
المصطلحات، والتي يمكن كتابتها بطريقة أبسط. لاحظ أن كل مصطلح فرعي في
(5.11) هو ناتج ضرب **مكافأة عشوائية**</span> **(random reward)**
<span dir="rtl">ونسبة **عينة الأهمية العشوائية**</span>
**<span dir="rtl">(</span>random
<span dir="rtl"></span>importance-sampling
ratio<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">على سبيل المثال، يمكن كتابة أول مصطلح فرعي، باستخدام
(5.3)، كالتالي</span>:

``` math
\rho_{t:T - 1}R_{t + 1} = \frac{\pi\left( A_{t} \middle| S_{t} \right)}{b\left( A_{t} \middle| S_{t} \right)} \cdot \frac{\pi\left( A_{t + 1} \middle| S_{t + 1} \right)}{b\left( A_{t + 1} \middle| S_{t + 1} \right)} \cdot \frac{\pi\left( A_{t + 2} \middle| S_{t + 2} \right)}{b\left( A_{t + 2} \middle| S_{t + 2} \right)}\cdots\frac{\pi\left( A_{T - 1} \middle| S_{T - 1} \right)}{b\left( A_{T - 1} \middle| S_{T - 1} \right)}R_{t + 1}
```

<span dir="rtl">من بين جميع هذه العوامل، قد يُشتبه في أن الأول والأخير
(المكافأة) فقط هما المتعلقان؛ جميع الآخرين يتعاملون مع أحداث حدثت بعد
المكافأة. علاوة على ذلك، فإن القيمة المتوقعة لجميع هذه العوامل الأخرى هي
واحد</span>:

``` math
E\left\lbrack \frac{\pi\left( A_{k} \middle| S_{k} \right)}{b\left( A_{k} \middle| S_{k} \right)} \right\rbrack = \sum_{a}^{}\frac{b\left( a \middle| S_{k} \right)}{\pi\left( a \middle| S_{k} \right)} \cdot \frac{\pi\left( a \middle| S_{k} \right)}{b\left( a \middle| S_{k} \right)} = \sum_{a}^{}{\pi\left( a \middle| S_{k} \right)} = 1
```

<span dir="rtl">ببضع خطوات إضافية، يمكن إثبات أنه، كما كان يُشتبه، ليس
لهذه العوامل الأخرى أي تأثير في التوقع، بعبارة أخرى، أن</span>

``` math
E\left\lbrack \rho_{t:T - 1}R_{t + 1} \right\rbrack = E\left\lbrack \rho_{t:t}R_{t + 1} \right\rbrack
```

<span dir="rtl">إذا قمنا بتكرار هذه العملية للمصطلح الفرعي</span> k
<span dir="rtl">في (5.11)، نحصل على</span>

``` math
E\left\lbrack \rho_{t:T - 1}R_{t + k} \right\rbrack = E\left\lbrack \rho_{t:t + k - 1}R_{t + k} \right\rbrack
```

<span dir="rtl">يترتب على ذلك أن القيمة المتوقعة لمصطلحنا الأصلي (5.11)
يمكن كتابتها كالتالي</span>

``` math
E\left\lbrack \rho_{t:T - 1}\overline{G_{t}} \right\rbrack = E\left\lbrack \widetilde{G_{t}} \right\rbrack
```

<span dir="rtl">عندما</span>

``` math
\widetilde{G_{t}} = \rho_{t:t}R_{t + 1} + \gamma\rho_{t:t + 1}R_{t + 2} + \gamma^{2}\rho_{t:t + 2}R_{t + 3} + \cdots + \gamma^{T - t - 1}\rho_{t:T - 1}R_{T}
```

<span dir="rtl">نُطلق على هذه الفكرة **عينة الأهمية لكل قرار**
</span>**(per-decision importance sampling)**<span dir="rtl">.</span>
<span dir="rtl">يتبع ذلك مباشرةً أنه يوجد **تقدير بديل لعينات
الأهمية**</span> **<span dir="rtl">(</span>importance-sampling
estimator<span dir="rtl">)</span>**<span dir="rtl">، بنفس القيمة
المتوقعة غير المنحازة (في حالة الزيارة الأولى) مثل تقدير **عينات الأهمية
العادية**</span> **<span dir="rtl">(</span>ordinary-importance-sampling
estimator<span dir="rtl">) </span>**(5.5)<span dir="rtl">،
باستخدام</span> Gt:

``` math
V(s) = \frac{\sum_{t \in T(s)}^{}\widetilde{G_{t}}}{\left| T(s) \right|}
```

<span dir="rtl">التي قد نتوقع أن تكون في بعض الأحيان أقل تبايناً. هل هناك
نسخة لكل قرار من  
**عينات الأهمية الموزونة**</span> **<span dir="rtl">(</span>weighted
importance sampling<span dir="rtl">)</span>**<span dir="rtl">؟ هذا أقل
وضوحاً. حتى الآن، جميع التقديرات التي تم اقتراحها لهذا، التي نعرفها، ليست
متسقة (أي أنها لا تتقارب إلى القيمة الحقيقية مع البيانات
اللامتناهية)</span>.

**<u>5.10 <span dir="rtl">الملخص</span> (Summary)</u>**

<span dir="rtl">تتعلم طرق **مونت كارلو**</span> **(Monte Carlo)**
<span dir="rtl">المقدمة في هذا الفصل دوال القيمة</span> (value
functions) <span dir="rtl">والسياسات المثلى</span> (optimal policies)
<span dir="rtl">من خلال تجربة تجريبية على شكل حلقات عينات</span>
<span dir="rtl">(</span>sample
<span dir="rtl"></span>episodes<span dir="rtl">). تمنحها هذه الطرق ثلاث
مزايا على الأقل مقارنةً بأساليب **البرمجة الديناميكية**
</span>**(DP)**<span dir="rtl">. أولاً، يمكن استخدامها لتعلم السلوك
المثالي</span> (optimal behavior) <span dir="rtl">مباشرةً من
التفاعل</span> (interaction) <span dir="rtl">مع **البيئة**
</span>**(environment)**<span dir="rtl">، دون الحاجة إلى نموذج لديناميات
**البيئة** </span>**(environment)**<span dir="rtl">.</span>
<span dir="rtl">ثانياً، يمكن استخدامها مع نماذج المحاكاة</span>
(simulation) <span dir="rtl">أو العينات</span>
(samples)<span dir="rtl">.</span> <span dir="rtl">في العديد من
التطبيقات، من السهل محاكاة حلقات العينات</span> (sample episodes)
<span dir="rtl">حتى وإن كان من الصعب بناء نموذج صريح لاحتمالات
الانتقال</span> (transition probabilities) <span dir="rtl">المطلوبة من
طرق **البرمجة الديناميكية** </span>**(DP)**<span dir="rtl">.</span>
<span dir="rtl">ثالثاً، من السهل والفعال تركيز طرق **مونت كارلو**</span>
**(Monte Carlo)** <span dir="rtl">على مجموعة صغيرة من الحالات</span>
(states)<span dir="rtl">. يمكن تقييم منطقة ذات اهتمام خاص</span> (region
of special interest) <span dir="rtl">بدقة دون الحاجة إلى تقييم كامل
مجموعة الحالات</span> (states) <span dir="rtl">بدقة (سنستكشف هذا بمزيد
من التفصيل في الفصل 8)</span>.

<span dir="rtl">ميزة رابعة لطرق **مونت كارلو** </span>**(Monte
Carlo)**<span dir="rtl">، والتي نناقشها لاحقاً في الكتاب، هي أنها قد تكون
أقل تأثراً بانتهاك خاصية ماركوف</span> (Markov
property)<span dir="rtl">.</span> <span dir="rtl">وذلك لأنها لا تقوم
بتحديث تقديرات القيمة</span> (value estimates) <span dir="rtl">بناءً على
تقديرات قيمة الحالات التالية. بعبارة أخرى، لأنها لا تقوم بعملية
**التعزيز** </span>**(bootstrapping)**<span dir="rtl">.</span>

<span dir="rtl">في تصميم طرق **تحكم مونت كارلو** </span>**(Monte Carlo
control methods)**<span dir="rtl">، اتبعنا المخطط العام  
للـ **التكرار العام للسياسة**</span> **(generalized policy iteration,
GPI)** <span dir="rtl">المقدمة في الفصل 4. يتضمن</span> GPI
<span dir="rtl">تفاعلات عملية **تقييم السياسة**</span> **(policy
evaluation)** <span dir="rtl">و**تحسين السياسة**</span>
**<span dir="rtl">(</span>policy
<span dir="rtl"></span>improvement<span dir="rtl">)</span>**.
<span dir="rtl">توفر طرق **مونت كارلو**</span> **(Monte Carlo)**
<span dir="rtl">عملية **تقييم سياسة**</span>
**<span dir="rtl">(</span>policy
<span dir="rtl"></span>evaluation<span dir="rtl">)</span>**
<span dir="rtl">بديلة. بدلاً من استخدام نموذج لحساب قيمة كل حالة</span>
(**state**)<span dir="rtl">، تقوم ببساطة بمتوسط العديد من العوائد</span>
(**returns**) <span dir="rtl">التي تبدأ من الحالة</span>
(**state**)<span dir="rtl">.</span> <span dir="rtl">بما أن قيمة
الحالة</span> (**state**) <span dir="rtl">هي العائد المتوقع</span>
(**expected** **return**)<span dir="rtl">، فإن هذا المتوسط يمكن أن يصبح
تقديراً جيداً للقيمة</span> (**value**)<span dir="rtl">.</span>
<span dir="rtl">في طرق التحكم</span> (**control**
**methods**)<span dir="rtl">، نحن مهتمون بشكل خاص بتقريب دوال قيمة
الإجراءات (</span>**action**-**value** **functions**<span dir="rtl">)،
لأنها يمكن أن تُستخدم لتحسين السياسة</span> (**policy**)
<span dir="rtl">دون الحاجة إلى نموذج لديناميات الانتقال في **البيئة**
</span>**(environment)**<span dir="rtl">.</span> <span dir="rtl">تقوم
طرق **مونت كارلو**</span> **(Monte Carlo)** <span dir="rtl">بدمج خطوات
**تقييم السياسة**</span> **(policy evaluation)**
<span dir="rtl">و**تحسين السياسة** </span>**(policy improvement)**
<span dir="rtl">على أساس حلقة تلو الأخرى، ويمكن تنفيذها تدريجياً على أساس
حلقة تلو الأخرى</span>.

<span dir="rtl">تظل مسألة الحفاظ على الاستكشاف</span> (**exploration**)
<span dir="rtl">الكافي قضية في طرق **تحكم مونت كارلو** </span>**(Monte
Carlo control methods)**<span dir="rtl">.</span> <span dir="rtl">ليس
كافياً فقط اختيار الإجراءات</span> (**actions**) <span dir="rtl">التي
يُقدّر أنها الأفضل حالياً، لأن ذلك يعني أنه لن يتم الحصول على عوائد</span>
(**returns**) <span dir="rtl">للإجراءات البديلة، وقد لا يتم تعلم أنها في
الواقع أفضل. إحدى الطرق هي تجاهل هذه المشكلة بالافتراض أن الحلقات تبدأ
بأزواج الحالة–الإجراء</span> (**state**–**action** **pairs**)
<span dir="rtl">التي يتم اختيارها عشوائيًا لتغطية جميع الاحتمالات. يمكن
في بعض الأحيان ترتيب هذه البدايات الاستكشافية</span> (**exploring**
**starts**) <span dir="rtl">في التطبيقات التي تستخدم الحلقات المحاكاة،
ولكنها غير مرجحة في التعليم من التجربة الحقيقية. في الطرق **المعتمدة على
السياسة** </span>**(on-policy)**<span dir="rtl">، يلتزم الوكيل</span>
(**agent**) <span dir="rtl">دائمًا بالاستكشاف</span> (**exploration**)
<span dir="rtl">ويحاول العثور على أفضل سياسة</span> (**policy**)
<span dir="rtl">لا تزال تستكشف</span>
(**explores**)<span dir="rtl">.</span> <span dir="rtl">في الطرق **خارج
السياسة**</span>
**<span dir="rtl">(</span>off-policy<span dir="rtl">)</span>**<span dir="rtl">،
يستمر الوكيل</span> (**agent**) <span dir="rtl">أيضًا في الاستكشاف</span>
(**exploration**)<span dir="rtl">، ولكنه يتعلم سياسة مثالية حتمية قد
تكون غير مرتبطة بالسياسة المتبعة</span>.

<span dir="rtl">يشير **توقع خارج السياسة**</span> **(off-policy
prediction)** <span dir="rtl">إلى تعلم دالة القيمة</span> (**value**
**function**) <span dir="rtl">لسياسة الهدف</span> (**target**
**policy**) <span dir="rtl">من بيانات يتم توليدها بواسطة سياسة
سلوك</span> (**behavior** **policy**) <span dir="rtl">مختلفة. تعتمد مثل
هذه أساليب التعليم على شكل من أشكال **أخذ عينات الأهمية**</span>
**<span dir="rtl">(</span>importance
<span dir="rtl"></span>sampling<span dir="rtl">)</span>**<span dir="rtl">،
أي على وزن العوائد</span> (**returns**) <span dir="rtl">بنسبة احتمالات
اتخاذ الإجراءات</span> (**actions**) <span dir="rtl">الملاحظة بموجب
السياستين، مما يحول توقعاتها من سياسة السلوك</span> (**behavior**
**policy**) <span dir="rtl">إلى سياسة الهدف</span> (**target**
**policy**)<span dir="rtl">.</span> <span dir="rtl">تستخدم **أخذ عينات
الأهمية العادية** </span>**(ordinary importance sampling)**
<span dir="rtl">متوسط بسيط للعوائد الموزونة</span> (**weighted**
**returns**)<span dir="rtl">، بينما تستخدم **أخذ عينات الأهمية
الموزونة** </span>**(weighted importance sampling)
<span dir="rtl"></span>**<span dir="rtl">متوسط موزون. تنتج **أخذ عينات
الأهمية العادية** </span>**(ordinary importance sampling)**
<span dir="rtl">تقديرات غير منحازة، ولكنها تحتوي على تباين أكبر، وربما
لانهائي، بينما تكون **أخذ عينات الأهمية الموزونة (**</span>**weighted
importance <span dir="rtl"></span>sampling<span dir="rtl">)
</span>**<span dir="rtl">دائمًا ذات تباين محدد ويفضل في الممارسة العملية.
على الرغم من بساطتها المفاهيمية، تظل طرق **مونت كارلو**</span> **(Monte
Carlo)** <span dir="rtl">خارج السياسة</span> (**off**-**policy**)
<span dir="rtl">لكل من التنبؤ</span> (**prediction**)
<span dir="rtl">والتحكم</span> (**control**) <span dir="rtl">غير مستقرة
وموضوعًا للبحث المستمر</span>.

<span dir="rtl">تختلف طرق **مونت كارلو**</span> **(Monte Carlo)**
<span dir="rtl">التي تم تناولها في هذا الفصل عن طرق **البرمجة
الديناميكية**</span> **(DP)** <span dir="rtl">التي تم تناولها في الفصل
السابق بطريقتين رئيسيتين. أولاً، تعمل على تجربة العينات</span>
(**sample** **experience**)<span dir="rtl">، وبالتالي يمكن استخدامها
للتعليم المباشر دون نموذج. ثانيًا، لا تقوم بالتعزيز</span>
(**bootstrapping**)<span dir="rtl">.</span> <span dir="rtl">أي أنها لا
تقوم بتحديث تقديرات القيمة</span> (**value** **estimates**)
<span dir="rtl">بناءً على تقديرات قيمة أخرى. هاتان الاختلافات ليست
مترابطة بشكل وثيق، ويمكن فصلها. في الفصل التالي، سننظر في الأساليب التي
تتعلم من التجربة، مثل طرق **مونت كارلو** </span>**(Monte
Carlo)**<span dir="rtl">، ولكنها تقوم أيضًا بالتعزيز</span>
(**bootstrapping**)<span dir="rtl">، مثل طرق **البرمجة الديناميكية**
</span>**(DP)**

**<span dir="rtl">الفصل السادس:</span>**  
**<span dir="rtl">التعليم بالتفرقة الزمنية</span>
<span dir="rtl">(</span>Temporal-Difference
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**

<span dir="rtl">إذا كان هناك مفهوم واحد يمكن اعتباره مركزيًا وجديدًا في
التعليم المعزز، فإنه بلا شك سيكون التعليم بالتفرقة الزمنية</span>
<span dir="rtl">(</span>**TD** **learning**<span dir="rtl">).</span>
<span dir="rtl">يُعد التعليم بالتفرقة الزمنية مزيجًا من أفكار مونت كارلو
وأفكار البرمجة الديناميكية</span>
<span dir="rtl">(</span>**DP**<span dir="rtl">).</span>
<span dir="rtl">مثل طرق مونت كارلو، يمكن لطرق</span> **TD**
<span dir="rtl">التعليم مباشرة من التجارب الخام دون الحاجة إلى نموذج
لديناميكيات البيئة. وكما هو الحال في البرمجة الديناميكية، تقوم
طرق</span> **TD** <span dir="rtl">بتحديث التقديرات استنادًا جزئيًا إلى
تقديرات أخرى تم تعلمها، دون انتظار النتيجة النهائية (أي أنها تعتمد على
نفسها في التحديث). العلاقة بين</span> **TD**
**<span dir="rtl">و</span>DP** <span dir="rtl">وطرق</span> **MC**
<span dir="rtl">هي موضوع متكرر في نظرية التعليم المعزز؛ هذا الفصل هو
بداية استكشافنا لهذه العلاقة. قبل أن ننتهي، سنرى أن هذه الأفكار
والأساليب تندمج مع بعضها البعض ويمكن دمجها بطرق متعددة. على وجه الخصوص،
في الفصل **7**، نقدم خوارزميات</span> **n-step**<span dir="rtl">، التي
توفر جسرًا بين</span> **TD** <span dir="rtl">وطرق مونت كارلو، وفي
**الفصل** **12** نقدم خوارزمية</span> **TD(λ)<span dir="rtl">،
</span>**<span dir="rtl">التي توحدها بسلاسة</span>.

<span dir="rtl">كالعادة، نبدأ بالتركيز على مشكلة تقييم السياسة أو مشكلة
التنبؤ، وهي مشكلة تقدير دالة القيمة</span> $`v\pi`$
<span dir="rtl">لسياسة معينة</span> $`\pi`$<span dir="rtl">.</span>
<span dir="rtl">بالنسبة لمشكلة التحكم (إيجاد سياسة مثلى)، تستخدم
طرق</span> **DP** **<span dir="rtl">و</span>TD** <span dir="rtl">ومونت
كارلو بعض التغيرات من التكرار السياسي العام</span> .(**GPI**)
<span dir="rtl">الاختلافات في هذه الطرق هي في الأساس اختلافات في نهجها
لحل مشكلة التنبؤ</span>.

**<u>6.1 <span dir="rtl">التنبؤ باستخدام التعليم بالتفرقة الزمنية</span>
(TD Prediction)</u>**

<span dir="rtl">كلا من طرق **التعليم بالتفرقة الزمنية**</span>
**(Temporal-Difference - TD)** <span dir="rtl">و**مونت كارلو**</span>
**<span dir="rtl">(</span>Monte
<span dir="rtl"></span>Carlo<span dir="rtl">)</span>**
<span dir="rtl">تستخدم التجربة لحل مشكلة التنبؤ. عند الحصول على بعض
التجارب التي تتبع سياسة</span> $`\pi`$<span dir="rtl">، يقوم كلا
الأسلوبين بتحديث تقديرهم</span> $`V`$ <span dir="rtl">لدالة
القيمة</span> $`v\pi`$ <span dir="rtl">لحالات</span> $`S_{t}`$
<span dir="rtl"></span>​ <span dir="rtl">غير النهائية التي تحدث في تلك
التجربة. بشكل تقريبي، تنتظر طرق **مونت كارلو**</span> **(Monte Carlo)**
<span dir="rtl">حتى تُعرف العائدات بعد الزيارة، ثم تستخدم تلك العائدات
كهدف لـ (</span>$`V(S_{t}`$ <span dir="rtl">.</span>
<span dir="rtl">طريقة **مونت كارلو**</span> **(Monte Carlo)**
<span dir="rtl">البسيطة ذات الزيارة الكاملة مناسبة للبيئات غير المستقرة
هي</span>:

``` math
V\left( S_{t} \right) \leftarrow V\left( S_{t} \right) + \alpha\left\lbrack G_{t} - V\left( S_{t} \right) \right\rbrack
```

<span dir="rtl">حيث أن</span> $`Gt`$ <span dir="rtl">هو العائد الفعلي
بعد الزمن</span> $`t`$<span dir="rtl">، و</span>$`\alpha`$
<span dir="rtl">هو معامل حجم الخطوة الثابت (راجع المعادلة 2.4). دعونا
نسمّي هذه الطريقة **مونت كارلو ذات معامل الخطوة الثابت**
</span>**(Constant-α MC)**<span dir="rtl">.</span> <span dir="rtl">بينما
يجب على طرق **مونت كارلو**</span> **(Monte Carlo)**
<span dir="rtl">الانتظار حتى نهاية الحلقة لتحديد التحديث لـ</span>
$`V(S_{t})`$ <span dir="rtl">حيث أن</span> $`Gt`$ <span dir="rtl">لا
يُعرف إلا عندها)، تحتاج طرق **التعليم بالتفرقة الزمنية**</span> **(TD)**
<span dir="rtl">فقط إلى الانتظار حتى الخطوة الزمنية التالية. عند
الزمن</span> $`t + 1`$<span dir="rtl">، يقومون على الفور بتكوين هدف
وإجراء تحديث مفيد باستخدام المكافأة الملحوظة</span> $`R_{t + 1}`$
<span dir="rtl"></span>​ <span dir="rtl">والتقدير</span>
V($`S_{t}`$+1)<span dir="rtl">.</span> <span dir="rtl">أبسط طريقة</span>
**TD <span dir="rtl"></span>**<span dir="rtl">تقوم بالتحديث
كالتالي</span>:

``` math
V\left( S_{t} \right) \leftarrow V\left( S_{t} \right) + \alpha\left\lbrack R_{t + 1} + \gamma V\left( S_{t + 1} \right) - V\left( S_{t} \right) \right\rbrack
```

<span dir="rtl">يتم التحديث مباشرة عند الانتقال إلى الحالة</span>
$`S_{t + 1}`$ <span dir="rtl"></span>​ <span dir="rtl">واستلام
المكافأة</span> $`R_{t + 1}`$​<span dir="rtl">.</span> <span dir="rtl">في
الواقع، الهدف من تحديث **مونت كارلو**</span> **(Monte Carlo)**
<span dir="rtl">هو</span> $`Gt`$
<span dir="rtl"></span>​<span dir="rtl">، بينما الهدف من تحديث **التعليم
بالتفرقة الزمنية** </span>**(TD)
<span dir="rtl"></span>**<span dir="rtl">هو</span>
$`R_{t + 1} + \gamma V\left( S_{t + 1} \right)`$

<span dir="rtl">تُسمى هذه الطريقة</span> **TD (0)
<span dir="rtl"></span>**<span dir="rtl">أو</span> **TD
<span dir="rtl">ذات الخطوة الواحدة</span> (one-step
TD)**<span dir="rtl">، لأنها حالة خاصة من طرق</span> **TD(λ)**
<span dir="rtl">و</span>**TD <span dir="rtl">متعددة الخطوات</span>
(n-step TD)** <span dir="rtl">التي تم تطويرها في الفصل 12 والفصل 7. يحدد
المربع أدناه طريقة</span> **TD (0)** <span dir="rtl">بشكل كامل في صورة
إجرائية</span>

<span dir="rtl">طريقة</span> TD(0) <span dir="rtl">الجدولية</span>
(Tabular TD(0)) <span dir="rtl">لتقدير</span> vπ <span dir="rtl"></span>

<img src="./media/image44.png"
style="width:6.26806in;height:2.74097in" />

<span dir="rtl">لأن طريقة</span> **TD(0)
<span dir="rtl"></span>**<span dir="rtl">تعتمد في تحديثها جزئيًا على
تقدير موجود بالفعل، نقول إنها طريقة تعتمد على نفسها في التحديث</span>
(**bootstrapping**)<span dir="rtl">، مثل **البرمجة الديناميكية**</span>
**<span dir="rtl">(</span>Dynamic Programming <span dir="rtl"></span>–
DP<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">نعلم من الفصل 3 أن</span>:

``` math
v^{\pi}(s) = E_{\pi}\left\lbrack G_{t} \middle| S_{t} = s \right\rbrack
```

``` math
\ \ \ \ \ \ \ \ \ \ \ \  = E_{\pi}\left\lbrack R_{t + 1} + \gamma G_{t + 1} \middle| S_{t} = s \right\rbrack
```

``` math
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  = E_{\pi}\left\lbrack R_{t + 1} + \gamma v^{\pi}\left( S_{t + 1} \right) \middle| S_{t} = s \right\rbrack
```

<span dir="rtl">بشكل تقريبي، تستخدم طرق **مونت كارلو**</span> **(Monte
Carlo)** <span dir="rtl">تقديرًا للمعادلة (6.3) كهدف، بينما تستخدم طرق
**البرمجة الديناميكية**</span> **(Dynamic Programming - DP)**
<span dir="rtl">تقديرًا للمعادلة (6.4) كهدف. يعتبر هدف  
**مونت كارلو**</span> **(Monte Carlo)** <span dir="rtl">تقديرًا لأن
القيمة المتوقعة في (6.3) غير معروفة؛ حيث يتم استخدام عائد عينة بدلاً من
العائد المتوقع الحقيقي. يعتبر هدف **البرمجة الديناميكية
(**</span>**Dynamic <span dir="rtl"></span>Programming –
DP<span dir="rtl">) </span>**<span dir="rtl">تقديرًا ليس بسبب القيم
المتوقعة، التي يُفترض أنها مقدمة بالكامل من نموذج البيئة، ولكن لأن</span>
$`v\pi(S_{t} + 1)`$ <span dir="rtl">غير معروفة ويُستخدم التقدير
الحالي</span> $`V(S_{t} + 1)`$ <span dir="rtl">بدلاً منها. هدف **التعليم
بالتفرقة الزمنية**</span> **(Temporal-Difference - TD)**
<span dir="rtl">هو تقدير للأسباب نفسها: فهو يقوم بأخذ عينات من القيم
المتوقعة في (6.4) ويستخدم التقدير الحالي</span> $`V`$
<span dir="rtl">بدلاً من</span> $`v\pi`$ <span dir="rtl">الحقيقية. لذلك،
تجمع طرق **التعليم بالتفرقة الزمنية**</span> **(Temporal-Difference -
TD)** <span dir="rtl">بين أخذ العينات من **مونت كارلو**</span>
**<span dir="rtl">(</span>Monte
<span dir="rtl"></span>Carlo<span dir="rtl">)
</span>**<span dir="rtl">والاعتماد على التحديث الذاتي من **البرمجة
الديناميكية**</span> **<span dir="rtl">(</span>Dynamic Programming –
DP<span dir="rtl">)</span>**<span dir="rtl">.</span> <span dir="rtl">كما
سنرى، مع الحذر والإبداع، يمكننا تحقيق الفوائد من كل من طرق **مونت
كارلو**</span> **<span dir="rtl">(</span>Monte
<span dir="rtl"></span>Carlo<span dir="rtl">)
</span>**<span dir="rtl">و**البرمجة الديناميكية** </span>**(Dynamic
Programming - DP)**<span dir="rtl">.</span>

<span dir="rtl">يُظهر الشكل الموجود على اليمين مخطط النسخ الاحتياطي
لطريقة</span> **TD (0)
<span dir="rtl"></span>**<span dir="rtl">الجدولية. يتم تحديث تقدير
القيمة لعقدة الحالة الموجودة في أعلى مخطط النسخ الاحتياطي بناءً على
الانتقال لعينة واحدة من هذه العقدة إلى الحالة التالية مباشرة. نشير إلى
تحديثات **التعليم بالتفرقة الزمنية**</span>
**<span dir="rtl">(</span>Temporal-Difference –
TD<span dir="rtl">)</span>** <span dir="rtl">و**مونت كارلو**</span>
**(Monte Carlo)** <span dir="rtl">بأنها</span>
<img src="./media/image45.png"
style="width:0.80972in;height:1.29861in" /><span dir="rtl">تحديثات عينة
لأنها تتضمن النظر إلى الأمام لعينة من حالة خلفية (أو زوج حالة–إجراء)،
واستخدام قيمة الحالة الخلفية والمكافأة على طول الطريق لحساب قيمة
مسترجعة، ومن ثم تحديث قيمة الحالة الأصلية (أو زوج الحالة–الإجراء) وفقًا
لذلك. تختلف تحديثات العينة عن التحديثات المتوقعة لطرق  
**البرمجة الديناميكية**</span> **(Dynamic Programming - DP)**
<span dir="rtl">في أنها تعتمد على عينة واحدة من الحالة الخلفية بدلاً من
توزيع كامل لكل الخلفاء المحتملين</span>.

<span dir="rtl">أخيرًا، لاحظ أن الكمية الموجودة بين الأقواس المربعة في
تحديث</span> **TD (0) <span dir="rtl"></span>**<span dir="rtl">هي نوع من
الخطأ، تقيس الفرق بين القيمة المقدرة للحالة</span> $`S_{t}`$
<span dir="rtl"></span>​ <span dir="rtl">والتقدير الأفضل</span>
$`R_{t + 1} + \gamma V\left( S_{t + 1} \right)`$ <span dir="rtl">تُسمى
هذه الكمية **خطأ** </span>**TD (TD error)**<span dir="rtl">، وتظهر في
أشكال مختلفة عبر **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">:</span>

``` math
\delta_{t} = R_{t + 1} + \gamma V\left( S_{t + 1} \right) - V\left( S_{t} \right)
```

<span dir="rtl">لاحظ أن **خطأ**</span> **TD (TD error)**
<span dir="rtl">في كل لحظة زمنية هو الخطأ في التقدير الذي تم في تلك
اللحظة. نظرًا لأن **خطأ**</span> **TD** <span dir="rtl">يعتمد على الحالة
التالية والمكافأة التالية، فإنه لا يكون متاحًا فعليًا إلا بعد خطوة زمنية
واحدة. بمعنى آخر،</span> $`\delta t`$ <span dir="rtl">هو الخطأ في</span>
$`V(S_{t})`$<span dir="rtl">، ويكون متاحًا عند الزمن</span>
$`t + 1`$<span dir="rtl">.</span> <span dir="rtl">كما لاحظ أنه إذا لم
يتغير المصفوفة</span> $`V`$ <span dir="rtl">أثناء الحلقة (كما يحدث في
طرق **مونت كارلو** </span>**(Monte Carlo)**)<span dir="rtl">، يمكن كتابة
خطأ **مونت كارلو** كمجموع لأخطاء</span> **TD**<span dir="rtl">:</span>

``` math
G_{t} - V\left( S_{t} \right) = R_{t + 1} + \gamma G_{t + 1} - V\left( S_{t} \right) + \gamma V\left( S_{t + 1} \right) - \gamma V\left( S_{t + 1} \right) = \delta_{t} + \gamma\left( G_{t + 1} - V\left( S_{t + 1} \right) \right) = \delta_{t} + \gamma\left( \delta_{t + 1} + \gamma\left( G_{t + 2} - V\left( S_{t + 2} \right) \right) \right) = \delta_{t} + \gamma\delta_{t + 1} + \gamma^{2}\delta_{t + 2} + \ldots + \gamma^{T - t - 1}\delta_{T - 1} + \gamma^{T - t}(0 - 0) = \Sigma_{k = t}^{T - 1}\gamma^{k - t}\delta_{k}
```

<span dir="rtl">هذه الهوية ليست دقيقة إذا تم تحديث</span> $`V`$
<span dir="rtl">أثناء الحلقة (كما هو الحال في</span> **TD
(0**)<span dir="rtl">)، ولكن إذا كان حجم الخطوة صغيرًا فقد تظل صالحة بشكل
تقريبي. تلعب تعميمات هذه الهوية دورًا مهمًا في نظرية وخوارزميات **التعليم
بالتفرقة الزمنية** </span>**(Temporal-Difference
Learning)**<span dir="rtl">.</span>

<span dir="rtl">**تمرين 6.1**:</span> <span dir="rtl">إذا تغير</span>
$`V`$ <span dir="rtl">أثناء الحلقة، فإن المعادلة (6.6) تنطبق بشكل تقريبي
فقط؛ ما الفرق بين الجانبين؟ دع</span> $`Vt`$ <span dir="rtl">يشير إلى
مصفوفة قيم الحالة المستخدمة في الزمن</span> $`t`$
<span dir="rtl">في</span> **TD <span dir="rtl">خطأ</span> (TD error)**
<span dir="rtl">(المعادلة 6.5) وفي تحديث</span> **TD**
<span dir="rtl">(المعادلة 6.2). أعد اشتقاق الخطوات أعلاه لتحديد الكمية
الإضافية التي يجب إضافتها إلى مجموع</span> **TD
<span dir="rtl">أخطاء</span> (TD errors)** <span dir="rtl">لكي تكون
مساوية **لخطأ مونت كارلو** </span>**(Monte Carlo
error)**<span dir="rtl">.</span>

**<u>6.1 <span dir="rtl">القيادة إلى المنزل</span>
<span dir="rtl">(</span>Driving Home<span dir="rtl">)</span></u>**

<span dir="rtl">في كل يوم تقود فيه سيارتك إلى المنزل من العمل، تحاول
التنبؤ بمدة الوقت الذي ستستغرقه للوصول إلى المنزل. عندما تغادر مكتبك،
تلاحظ الوقت، يوم الأسبوع، الطقس، وأي شيء آخر قد يكون ذا صلة. لنفترض أن
هذا اليوم هو الجمعة، وتغادر في تمام الساعة **6:00** مساءً، وتقدر أن الأمر
سيستغرق **30** دقيقة للوصول إلى المنزل. عندما تصل إلى سيارتك، يكون الوقت
**6:05**، وتلاحظ أن المطر قد بدأ. غالبًا ما تكون حركة المرور</span>
(**Traffic**) <span dir="rtl">أبطأ في المطر، لذا تعيد تقديرك بأنه
سيستغرق 35 دقيقة من ذلك الحين، أي ما مجموعه 40 دقيقة. بعد خمس عشرة
دقيقة، تكون قد أكملت الجزء الخاص بالطرق السريعة</span> (**Highway**)
<span dir="rtl">في وقت جيد. عندما تخرج إلى طريق ثانوي</span>
(**Secondary** **road**)<span dir="rtl">، تخفض تقديرك لإجمالي وقت السفر
إلى 35 دقيقة. لسوء الحظ، في هذه اللحظة تجد نفسك خلف شاحنة بطيئة</span>
(**Slow** **truck**)<span dir="rtl">، والطريق ضيق جدًا للمرور. ينتهي بك
الأمر باتباع الشاحنة حتى تصل إلى الشارع الجانبي الذي تعيش فيه في الساعة
**6:40**. وبعد ثلاث دقائق تكون في المنزل. وبالتالي، فإن تسلسل الحالات،
والأوقات، والتنبؤات هو كما يلي</span>:

<img src="./media/image46.png"
style="width:6.26806in;height:1.86042in" />

<span dir="rtl">المكافآت</span> (**Rewards**) <span dir="rtl">في هذا
المثال هي الأوقات المنقضية في كل جزء من الرحلة. نحن لا نقوم بخصم
المكافآت</span> ($`\gamma = 1`$)<span dir="rtl">، وبالتالي فإن العائد
لكل حالة هو الوقت الفعلي للوصول من تلك الحالة. قيمة</span> (**Value**)
<span dir="rtl">كل حالة هي الوقت المتوقع للوصول. يعطي العمود الثاني من
الأرقام القيمة الحالية المقدرة لكل حالة تمت مواجهتها</span>.

<span dir="rtl">طريقة بسيطة لفهم عمل طرق **مونت كارلو**</span> **(Monte
Carlo)** <span dir="rtl">هي رسم الوقت الكلي المتوقع (العمود الأخير) على
التسلسل، كما في **الشكل** **6.1** (على اليسار). تشير الأسهم الحمراء إلى
التغييرات في التنبؤات التي توصي بها طريقة **مونت كارلو ذات معامل الخطوة
الثابت**</span> **<span dir="rtl">(</span>Constant-α MC
<span dir="rtl"></span>method<span dir="rtl">)</span>**
<span dir="rtl">(**المعادلة** **6.1**)، مع</span>
$`\alpha = 1`$<span dir="rtl">.</span> <span dir="rtl">هذه التغييرات هي
بالضبط الأخطاء بين القيمة المقدرة (الوقت المتوقع للوصول) في كل حالة
والعائد الفعلي (الوقت الفعلي للوصول). على سبيل المثال، عندما خرجت من
الطريق السريع</span> (**Highway**) <span dir="rtl">كنت تظن أنه سيستغرق
15 دقيقة إضافية فقط للوصول إلى المنزل، ولكن في الواقع استغرق الأمر
**23** **دقيقة**. تنطبق المعادلة **6.1** في هذه اللحظة وتحدد زيادة في
تقدير الوقت المتبقي بعد الخروج من الطريق السريع. كان الخطأ،</span>
$`Gt - V(S_{t})`$<span dir="rtl">، في هذا الوقت هو ثماني دقائق. لنفترض
أن معامل حجم الخطوة</span> $`\alpha`$ <span dir="rtl">هو 1/2. عندها سيتم
تعديل الوقت المتوقع للوصول بعد الخروج من الطريق السريع بزيادة أربع دقائق
نتيجة لهذه التجربة. ربما كان هذا التغيير كبيرًا جدًا في هذه الحالة؛
فالشاحنة كانت على الأرجح مجرد حظ سيئ. في أي حال، يمكن إجراء التغيير فقط
خارج الإنترنت، أي بعد وصولك إلى المنزل. فقط في هذه اللحظة تعرف أي من
العوائد الفعلية</span>.

<span dir="rtl">هل من الضروري الانتظار حتى تكون النتيجة النهائية معروفة
قبل أن يبدأ التعليم؟ لنفترض أنه في يوم آخر قدرت مرة أخرى عند مغادرة
مكتبك أن الأمر سيستغرق **30** دقيقة للقيادة إلى المنزل، ولكنك علقت في
ازدحام مروري ضخم. بعد **25** **دقيقة** من مغادرة المكتب، لا تزال تتحرك
ببطء شديد على الطريق السريع</span>.

<img src="./media/image47.png"
style="width:6.26806in;height:2.32569in" />

<span dir="rtl">**الشكل 6.1**: التغييرات الموصى بها في مثال القيادة إلى
المنزل بواسطة طرق مونت كارلو</span> <span dir="rtl">(</span>Monte
<span dir="rtl"></span>Carlo Methods<span dir="rtl">)</span>
<span dir="rtl">على اليسار وطرق الزمن الفعلي</span> (TD Methods)
<span dir="rtl">على اليمين</span>.

<span dir="rtl">تقدّر أنك ستحتاج إلى **25** **دقيقة** أخرى للوصول إلى
المنزل، ليصبح المجموع **50** **دقيقة**. أثناء انتظارك في الزحام
المروري</span> <span dir="rtl">(</span>Traffic<span dir="rtl">)، تعرف
بالفعل أن تقديرك الأولي البالغ **30** **دقيقة** كان متفائلًا جدًا. هل يجب
أن تنتظر حتى تصل إلى المنزل قبل زيادة تقديرك للحالة الأولية؟ وفقًا لنهج
**مونت كارلو** </span>**(Monte Carlo)**<span dir="rtl">، يجب عليك
الانتظار، لأنك لا تعرف بعد العائد الحقيقي</span>.

<span dir="rtl">أما وفقًا لنهج **التعليم بالتفرقة الزمنية**
</span>**(Temporal-Difference - TD)**<span dir="rtl">، فسوف تتعلم فورًا،
وتقوم بتعديل تقديرك الأولي من **30** **دقيقة** نحو **50** **دقيقة**. في
الواقع، سيتم تعديل كل تقدير نحو التقدير الذي يليه مباشرة. بالعودة إلى
يوم القيادة الأول، يوضح **الشكل** **6.1** (على اليمين) التغييرات في
التنبؤات التي توصي بها قاعدة</span> **TD** <span dir="rtl">(المعادلة
6.2) (هذه هي التغييرات التي تم إجراؤها إذا كان</span>
$`\alpha = 1`$<span dir="rtl">) كل خطأ يتناسب مع التغير الزمني في
التنبؤ، أي مع الفروقات الزمنية</span> (**Temporal** **Differences**)
<span dir="rtl">في التنبؤات. بالإضافة إلى إعطائك شيئًا لتفعله أثناء
انتظارك في الزحام، هناك عدة أسباب حسابية تجعل من المفيد التعليم بناءً على
التنبؤات الحالية بدلاً من الانتظار حتى النهاية عندما تعرف العائد الفعلي.
سنناقش بإيجاز بعض هذه الأسباب في القسم التالي</span>.

<span dir="rtl">**<u>تمرين 6.2</u>**:</span> <span dir="rtl">هذا تمرين
لمساعدتك في تطوير حدسك حول سبب كون طرق **التعليم بالتفرقة الزمنية**
</span>**(TD methods) <span dir="rtl"></span>**<span dir="rtl">غالبًا
أكثر كفاءة من طرق **مونت كارلو** </span>**(Monte Carlo
methods)**<span dir="rtl">.</span> <span dir="rtl">فكر في مثال القيادة
إلى المنزل وكيف يتم التعامل معه بواسطة طريقتي</span> **TD
<span dir="rtl">ومونت كارلو</span> (Monte Carlo)**<span dir="rtl">. هل
يمكنك تخيل سيناريو يكون فيه تحديث</span> **TD
<span dir="rtl"></span>**<span dir="rtl">أفضل في المتوسط من تحديث **مونت
كارلو**؟ قدم سيناريو مثالياً—وصفاً لتجربة سابقة وحالة حالية—تتوقع فيه أن
يكون تحديث</span> **TD <span dir="rtl"></span>**<span dir="rtl">أفضل.
إليك تلميحًا: افترض أن لديك الكثير من الخبرة في القيادة إلى المنزل من
العمل. ثم انتقلت إلى مبنى جديد وموقف سيارات جديد (لكن لا تزال تدخل
الطريق السريع من نفس المكان). الآن تبدأ في تعلم التنبؤات للمبنى الجديد.
هل ترى لماذا من المرجح أن تكون تحديثات</span> **TD
<span dir="rtl"></span>**<span dir="rtl">أفضل بكثير، على الأقل في
البداية، في هذه الحالة؟ هل يمكن أن يحدث شيء مماثل في السيناريو
الأصلي؟</span>

**<u>6.2 <span dir="rtl">مزايا طرق التنبؤ بالتعليم بالتفرقة
الزمنية</span> (Advantages of TD Prediction Methods)</u>**

<span dir="rtl">طرق **التعليم بالتفرقة الزمنية**</span> **(TD methods)**
<span dir="rtl">تُحدّث تقديراتها جزئيًا استنادًا إلى تقديرات أخرى. يتعلمون
من تخمين لتخمين آخر—إنهم يعتمدون على التحديث الذاتي</span>
(**Bootstrap**)<span dir="rtl">.</span> <span dir="rtl">هل هذا أمر جيد
للقيام به؟ ما هي المزايا التي تتمتع بها طرق</span> **TD**
<span dir="rtl">على طرق **مونت كارلو**</span> **(Monte Carlo)**
<span dir="rtl">و**البرمجة الديناميكية** </span>**(Dynamic Programming -
DP)**<span dir="rtl">؟ تطوير والإجابة على مثل هذه الأسئلة سيستغرق بقية
هذا الكتاب وربما أكثر. في هذا القسم، نتوقع بإيجاز بعض الإجابات</span>.

<span dir="rtl">من الواضح أن طرق</span> **TD
<span dir="rtl"></span>**<span dir="rtl">تتمتع بميزة على طرق **البرمجة
الديناميكية**</span> **(DP)** <span dir="rtl">في أنها لا تتطلب نموذجًا
للبيئة، ولا توزيعات احتمالية المكافأة</span> (**Reward**)
<span dir="rtl">أو الحالة التالية. الميزة الأخرى الأكثر وضوحًا
لطرق</span> **TD <span dir="rtl"></span>**<span dir="rtl">على طرق **مونت
كارلو**</span> **(Monte Carlo)** <span dir="rtl">هي أنها تنفذ بشكل طبيعي
عبر الإنترنت وبطريقة كاملة متزايدة. مع طرق **مونت كارلو**</span>
**(Monte Carlo)** <span dir="rtl">يجب الانتظار حتى نهاية الحلقة، لأن
العائد لا يُعرف إلا عندها، بينما مع طرق</span> **TD
<span dir="rtl"></span>**<span dir="rtl">يكفي الانتظار خطوة زمنية واحدة
فقط. بشكل مدهش، غالبًا ما يكون هذا اعتبارًا حاسمًا. بعض التطبيقات تتطلب
حلقات طويلة جدًا، بحيث يكون تأخير التعليم حتى نهاية الحلقة بطيئًا جدًا.
تطبيقات أخرى هي مهام مستمرة وليس لديها حلقات على الإطلاق. أخيرًا، كما
لاحظنا في الفصل السابق، يجب على بعض طرق **مونت كارلو**</span> **(Monte
Carlo)** <span dir="rtl">تجاهل أو خصم الحلقات التي يتم فيها اتخاذ
إجراءات تجريبية، مما قد يؤدي إلى بطء كبير في التعليم. طرق</span> **TD
<span dir="rtl"></span>**<span dir="rtl">أقل عرضة بكثير لهذه المشكلات
لأنها تتعلم من كل انتقال بغض النظر عن الإجراءات اللاحقة التي يتم
اتخاذها</span>.

<span dir="rtl">ولكن هل طرق</span> **TD
<span dir="rtl"></span>**<span dir="rtl">سليمة؟ بالتأكيد، من الملائم
التعليم من تخمين لتخمين آخر، دون انتظار النتيجة الفعلية، ولكن هل يمكننا
ضمان الوصول إلى الإجابة الصحيحة؟ لحسن الحظ، الجواب هو نعم. بالنسبة لأي
سياسة ثابتة</span> $`\pi`$<span dir="rtl">، تم إثبات أن</span> **TD (0)
<span dir="rtl"></span>**<span dir="rtl">يتقارب مع</span>
$`v\pi`$<span dir="rtl">، في المتوسط لمعامل حجم خطوة ثابت إذا كان صغيرًا
بما فيه الكفاية، ومع احتمال 1 إذا انخفض معامل حجم الخطوة وفقًا لظروف
التقريب العشوائي المعتادة (المعادلة 2.7). معظم إثباتات التقارب تنطبق فقط
على الحالة القائمة على الجدول للخوارزمية المقدمة أعلاه (المعادلة 6.2)،
ولكن بعضها ينطبق أيضًا على حالة تقريب الدالة الخطية العامة. يتم مناقشة
هذه النتائج في سياق أكثر عمومية في الفصل 9</span>.

<span dir="rtl">إذا كانت طرق</span> **TD <span dir="rtl">ومونت
كارلو</span> (Monte Carlo)** <span dir="rtl">تتقارب بشكل تدريجي نحو
التنبؤات الصحيحة، فإن السؤال الطبيعي التالي هو "أيها يصل إلى هناك أولاً؟"
بعبارة أخرى، أي طريقة تتعلم بشكل أسرع؟ أيها تستفيد بشكل أكثر كفاءة من
البيانات المحدودة؟ في الوقت الحالي، هذا سؤال مفتوح بمعنى أنه لم يتمكن
أحد من إثبات رياضيًا أن طريقة واحدة تتقارب بشكل أسرع من الأخرى. في
الواقع، ليس من الواضح حتى ما هي الطريقة الأكثر ملاءمة لصياغة هذا السؤال
بشكل رسمي! ومع ذلك، في الممارسة العملية، تم العثور على طرق</span> **TD
<span dir="rtl"></span>**<span dir="rtl">عادة لتتقارب بشكل أسرع من طرق
**مونت كارلو ذات معامل الخطوة الثابت**</span> **(Constant-α MC
methods)** <span dir="rtl">في المهام الاحتمالية، كما هو موضح في</span>
**6.2**<span dir="rtl">.</span>

**<span dir="rtl">مثال 6.2 السير العشوائي</span> (Random Walk)**

<span dir="rtl">في هذا المثال، نقارن تجريبيًا قدرات التنبؤ لكل من</span>
**TD(0) <span dir="rtl">ومونت كارلو ذات معامل الخطوة الثابت</span>
(Constant-α MC)** <span dir="rtl">عند تطبيقهما على عملية مكافأة ماركوف
التالية</span>:

<img src="./media/image48.png"
style="width:6.26806in;height:0.75903in" />

<span dir="rtl">عملية مكافأة ماركوف</span> (**Markov** **Reward**
**Process** - **MRP**) <span dir="rtl">هي عملية اتخاذ قرار ماركوف بدون
إجراءات. سنستخدم غالبًا</span> **MRPs
<span dir="rtl"></span>**<span dir="rtl">عندما نركز على مشكلة التنبؤ،
حيث لا يوجد حاجة للتمييز بين الديناميكيات الناتجة عن البيئة وتلك الناتجة
عن الوكيل. في هذه</span> **MRP<span dir="rtl">،</span>**
<span dir="rtl">تبدأ جميع الحلقات في الحالة الوسطى،</span>
$`\mathbf{C}`$<span dir="rtl">، ثم تتجه إما إلى اليسار أو اليمين بحالة
واحدة في كل خطوة، وباحتمالية متساوية. تنتهي الحلقات إما في أقصى اليسار
أو أقصى اليمين. عندما تنتهي حلقة في اليمين، تحدث مكافأة بقيمة 1+؛ أما
جميع المكافآت الأخرى فهي صفر. على سبيل المثال، قد تتكون حلقة نموذجية من
تسلسل الحالات والمكافآت التالي:</span> **C, 0, B, 0, C, 0, D, 0, E,
1<span dir="rtl">.</span>** <span dir="rtl">نظرًا لأن هذه المهمة غير
مخفضة، فإن القيمة الحقيقية لكل حالة هي احتمال الانتهاء في اليمين إذا
بدأت من تلك الحالة. وبالتالي، فإن القيمة الحقيقية للحالة الوسطى
هي</span> $`v\pi(C) = 0.5`$<span dir="rtl">.</span>
<span dir="rtl">القيم الحقيقية لجميع الحالات من</span> **A
<span dir="rtl"></span>**<span dir="rtl">إلى</span> **E
<span dir="rtl"></span>**<span dir="rtl">هي</span>:

``` math
\frac{1}{6},\frac{2}{6},\frac{3}{6},\frac{4}{6},\frac{5}{6}
```

<img src="./media/image49.png"
style="width:6.26806in;height:2.54028in" />

<span dir="rtl">يُظهر الرسم البياني الأيسر أعلاه القيم التي تم تعلمها بعد
عدد مختلف من الحلقات في تشغيل واحد لطريقة</span> **TD
(0)<span dir="rtl">.</span>** <span dir="rtl">التقديرات بعد 100 حلقة
تكون تقريبًا كما تكون قريبة من القيم الحقيقية—مع معامل حجم خطوة
ثابت</span> $`\alpha = 0.1`$ <span dir="rtl">في هذا المثال، تتقلب القيم
بشكل غير محدد استجابةً لنتائج الحلقات الأخيرة. يُظهر الرسم البياني الأيمن
منحنيات التعليم للطريقتين لمختلف قيم</span>
$`\alpha`$<span dir="rtl">.</span> <span dir="rtl">مقياس الأداء المعروض
هو **جذر متوسط مربع الخطأ**</span> **(Root Mean-Squared Error - RMS)**
<span dir="rtl">بين دالة القيمة المكتسبة والدالة القيمة الحقيقية، والذي
يتم حسابه على خمس حالات، ثم يتم متوسطه على 100 تشغيل. في جميع الحالات،
تم تهيئة دالة القيمة التقريبية إلى القيمة الوسيطة</span> $`V(s) = 0.5`$
<span dir="rtl">لجميع الحالات</span> $`s`$<span dir="rtl">.</span>
<span dir="rtl">كانت طريقة</span> **TD** <span dir="rtl">باستمرار أفضل
من طريقة</span> **MC <span dir="rtl"></span>**<span dir="rtl">في هذه
المهمة</span>.

<span dir="rtl">**<u>تمرين 6.3</u>**: من النتائج المعروضة في الرسم
البياني الأيسر في مثال السير العشوائي، يبدو أن الحلقة الأولى نتج عنها
تغيير في</span> $`V(A)`$ <span dir="rtl">فقط. ماذا يخبرك هذا عما حدث في
الحلقة الأولى؟ لماذا تم تغيير التقدير لهذه الحالة فقط؟ بمقدار كم تم
تغييره بالضبط؟</span>

<span dir="rtl">**<u>تمرين 6.4</u>**:</span> <span dir="rtl">النتائج
المحددة المعروضة في الرسم البياني الأيمن في مثال السير العشوائي تعتمد
على قيمة معامل حجم الخطوة</span> $`\alpha`$<span dir="rtl">.</span>
<span dir="rtl">هل تعتقد أن الاستنتاجات حول أي الخوارزميات أفضل ستتأثر
إذا تم استخدام نطاق أوسع من قيم</span> $`\alpha`$<span dir="rtl">؟ هل
هناك قيمة ثابتة مختلفة لـ</span> $`\alpha`$ <span dir="rtl">يمكن أن تحقق
فيها أي من الخوارزميات أداءً أفضل بشكل ملحوظ مما هو موضح؟ لماذا أو لماذا
لا؟</span>

<span dir="rtl">**<u>تمرين 6.5</u>**:</span> <span dir="rtl">في الرسم
البياني الأيمن لمثال السير العشوائي، يبدو أن **جذر متوسط مربع
الخطأ**</span> **<span dir="rtl">(</span>RMS
<span dir="rtl"></span>error<span dir="rtl">)
</span>**<span dir="rtl">لطريقة</span> **TD
<span dir="rtl"></span>**<span dir="rtl">ينخفض ثم يرتفع مرة أخرى، خصوصًا
عند قيم</span> $`\alpha`$ <span dir="rtl">العالية. ما الذي يمكن أن يكون
السبب وراء ذلك؟ هل تعتقد أن هذا يحدث دائمًا، أم أنه قد يكون نتيجة لكيفية
تهيئة دالة القيمة التقريبية؟</span>

<span dir="rtl">**<u>تمرين 6.6</u>**:</span> <span dir="rtl">في **مثال
6.2** ذكرنا أن القيم الحقيقية لمثال السير العشوائي هي</span>
$`\frac{1}{6},\frac{2}{6},\frac{3}{6},\frac{4}{6},\frac{5}{6}`$
<span dir="rtl"></span>​, <span dir="rtl">للحالات</span> $`\mathbf{A}`$
<span dir="rtl">إلى</span> $`\mathbf{E}`$**<span dir="rtl">.</span>**
<span dir="rtl">صف طريقتين مختلفتين على الأقل يمكن من خلالهما حساب هذه
القيم. أي منهما تعتقد أننا استخدمنا فعليًا؟ ولماذا؟</span>

**<u>6.3 <span dir="rtl">المثالية في</span> (Optimality of TD (0))
TD</u>**

<span dir="rtl">افترض أن هناك كمية محدودة فقط من الخبرة المتاحة، لنقل 10
حلقات أو 100 خطوة زمنية. في هذه الحالة، يكون النهج الشائع مع طرق التعليم
المتزايد هو تقديم التجربة بشكل متكرر حتى تتقارب الطريقة نحو إجابة.
بالنظر إلى دالة القيمة التقريبية</span> $`V`$<span dir="rtl">، يتم حساب
الزيادات المحددة بواسطة المعادلتين (6.1) أو (6.2) لكل خطوة زمنية</span>
$`t`$ <span dir="rtl">يتم فيها زيارة حالة غير نهائية، ولكن يتم تغيير
دالة القيمة مرة واحدة فقط، بمجموع جميع الزيادات. ثم تتم معالجة كل الخبرة
المتاحة مرة أخرى باستخدام دالة القيمة الجديدة لإنتاج زيادة جديدة شاملة،
وهكذا، حتى تتقارب دالة القيمة. نسمي هذا **التحديث على دفعات**</span>
**<span dir="rtl">(</span>Batch
<span dir="rtl"></span>Updating<span dir="rtl">)
</span>**<span dir="rtl">لأن التحديثات تتم فقط بعد معالجة كل مجموعة
كاملة من بيانات التدريب</span>.

<span dir="rtl">تحت **التحديث على دفعات** </span>**(Batch
Updating)**<span dir="rtl">، تتقارب طريقة</span> **TD(0)
<span dir="rtl"></span>**<span dir="rtl">بشكل حتمي إلى إجابة واحدة
مستقلة عن معامل حجم الخطوة</span> $`\alpha`$<span dir="rtl">، طالما تم
اختيار</span> $`\alpha`$ <span dir="rtl">ليكون صغيرًا بما يكفي. تتقارب
طريقة **مونت كارلو ذات معامل الخطوة الثابت**</span> **(Constant-α MC)**
<span dir="rtl">أيضًا بشكل حتمي تحت نفس الظروف، ولكن إلى إجابة مختلفة.
فهم هاتين الإجابتين سيساعدنا على فهم الفرق بين الطريقتين. تحت التحديث
العادي، لا تتحرك الطرق بالكامل نحو إجاباتها الخاصة بالدفعات، ولكن بطريقة
ما تتخذ خطوات في هذه الاتجاهات. قبل محاولة فهم الإجابتين بشكل عام، لجميع
المهام الممكنة، سننظر أولاً في بعض الأمثلة</span>.

<img src="./media/image50.png"
style="width:3.04167in;height:2.21875in" />**<span dir="rtl">مثال 6.3:
السير العشوائي تحت التحديث على دفعات</span>
<span dir="rtl">(</span>Random walk under batch
<span dir="rtl"></span>updating<span dir="rtl">)</span>**  
<span dir="rtl">تم تطبيق نسخ التحديث على دفعات لكل من</span> **TD (0)
<span dir="rtl"></span>**<span dir="rtl">ومونت **كارلو ذات معامل الخطوة
الثابت** </span>**(Constant-α MC)
<span dir="rtl"></span>**<span dir="rtl">كما يلي على مثال التنبؤ بالسير
العشوائي (مثال 6.2). بعد كل حلقة جديدة، تم التعامل مع جميع الحلقات التي
شوهدت حتى الآن كدفعة واحدة. تم تقديمها بشكل متكرر إلى الخوارزمية، سواء
كانت</span> **TD (0) <span dir="rtl"></span>**<span dir="rtl">أو **مونت
كارلو ذات معامل الخطوة الثابت** </span>**(Constant-α
MC)**<span dir="rtl">، مع</span> $`\alpha`$ <span dir="rtl">صغير بما
يكفي لتتقارب دالة القيمة. ثم تم مقارنة دالة القيمة الناتجة مع</span>
$`v\pi`$<span dir="rtl">، وتم رسم متوسط جذر مربع الخطأ</span>
<span dir="rtl">(</span>Root Mean-Squared Error –
RMS<span dir="rtl">)</span> <span dir="rtl">عبر الحالات الخمس (وعبر 100
تكرار مستقل للتجربة بأكملها) للحصول على منحنيات التعليم الموضحة في الشكل
6.2. لاحظ أن طريقة</span> **Batch TD
<span dir="rtl"></span>**<span dir="rtl">كانت باستمرار أفضل من
طريقة</span> **Batch Monte Carlo**<span dir="rtl">.</span>

<span dir="rtl">الشكل 6.2: أداء الزمن الفعلي</span> (TD(0))
<span dir="rtl">وطرق مونت كارلو</span> (MC) <span dir="rtl">مع
ثابت</span> $`\alpha`$ <span dir="rtl"></span> <span dir="rtl">تحت
التدريب الدفعي على مهمة السير العشوائي</span> (**Random** **Walk**
**Task**)<span dir="rtl">.</span>

<span dir="rtl">تحت التدريب على دفعات، تتقارب طريقة **مونت كارلو ذات
معامل الخطوة الثابت**</span> **<span dir="rtl">(</span>Constant-α
MC<span dir="rtl">)</span>** <span dir="rtl">إلى القيم</span> $`V(s)`$
<span dir="rtl">التي هي متوسطات عينات العوائد الفعلية التي تمت تجربتها
بعد زيارة كل حالة</span> s<span dir="rtl">.</span> <span dir="rtl">هذه
هي التقديرات المثلى من حيث أنها تقلل من مربع الخطأ المتوسط من العوائد
الفعلية في مجموعة التدريب. من هذا المنطلق، من المدهش أن تكون
طريقة</span> **Batch TD <span dir="rtl"></span>**<span dir="rtl">قادرة
على الأداء بشكل أفضل وفقًا لمقياس جذر مربع الخطأ الموضح في الشكل إلى
اليمين. كيف تمكنت</span> **Batch TD
<span dir="rtl"></span>**<span dir="rtl">من الأداء بشكل أفضل من هذه
الطريقة المثلى؟ الجواب هو أن طريقة **مونت كارلو** </span>**(Monte Carlo)
<span dir="rtl"></span>**<span dir="rtl">مثلى فقط بطريقة محدودة،
وأن</span> **TD <span dir="rtl"></span>**<span dir="rtl">مثلى بطريقة
أكثر صلة بتنبؤ العوائد</span>.

**<span dir="rtl"><u>مثال 6.4</u>: أنت المتنبئ</span>
<span dir="rtl">(</span>You are the
Predictor<span dir="rtl">)</span>**  
<span dir="rtl">ضع نفسك الآن في دور المتنبئ بالعوائد لعملية مكافأة
ماركوف غير معروفة. افترض أنك تلاحظ الحلقات الثماني التالية</span>:

<span dir="rtl">  
</span><img src="./media/image51.png" style="width:6.19444in;height:1.425in" /><span dir="rtl">  
هذا يعني أن الحلقة الأولى بدأت في الحالة</span>
$`\mathbf{A}`$<span dir="rtl">، وانتقلت إلى</span> $`\mathbf{B}`$
<span dir="rtl">مع مكافأة قدرها 0، ثم انتهت في</span> $`\mathbf{B}`$
<span dir="rtl">مع مكافأة قدرها 0. أما الحلقات السبع الأخرى فكانت أقصر،
حيث بدأت من</span> $`\mathbf{B}`$ <span dir="rtl">وانتهت فورًا. بالنظر
إلى هذه الدفعة من البيانات، ما الذي ستقوله عن التنبؤات المثلى، وأفضل
القيم للتقديرات</span> $`V(A)`$
<span dir="rtl">و</span>$`V(B)`$<span dir="rtl">؟ ربما يتفق الجميع على
أن القيمة المثلى لـ</span> $`V(B)`$ <span dir="rtl">هي</span>
$`\frac{3}{4}`$​<span dir="rtl">، لأن ست مرات من أصل ثماني مرات في
الحالة</span> $`\mathbf{B}`$ <span dir="rtl">انتهت العملية فورًا بعائد
قدره 1، وفي المرتين الأخريين انتهت العملية فورًا بعائد قدره 0</span>.

<span dir="rtl">لكن ما هي القيمة المثلى للتقدير</span> $`V(A)`$
<span dir="rtl">بالنظر إلى هذه البيانات؟ هنا يوجد جوابان معقولان. أحدهما
هو ملاحظة أن 100% من المرات التي كانت العملية فيها في الحالة</span>
$`\mathbf{A}`$ <span dir="rtl">انتقلت فورًا إلى</span> $`\mathbf{B}`$
<span dir="rtl">(مع مكافأة قدرها 0)؛ وبما أننا قررنا بالفعل أن</span>
$`\mathbf{B}`$ <span dir="rtl">لها قيمة</span>
$`\frac{3}{4}`$​<span dir="rtl">، فإن</span> $`\mathbf{A}`$
<span dir="rtl">يجب أن تكون لها نفس القيمة</span> $`\frac{3}{4}`$
<span dir="rtl"></span>​ <span dir="rtl">أيضًا. إحدى طرق النظر إلى هذا
الجواب هي أنه يعتمد على نمذجة عملية ماركوف أولاً، كما هو موضح في اليمين،
ثم حساب التقديرات الصحيحة بناءً على النموذج، والذي في هذه الحالة يعطي
بالفعل</span> $`\ V(A)\  = \ \frac{3}{4}`$​<span dir="rtl">هذا هو أيضًا
الجواب الذي تعطيه طريقة</span> **Batch TD (0)<span dir="rtl">.</span>**

<img src="./media/image52.png"
style="width:2.49306in;height:1.48264in" /><span dir="rtl">  
  
الجواب المعقول الآخر هو ببساطة ملاحظة أننا رأينا</span> $`\mathbf{A}`$
<span dir="rtl">مرة واحدة والعائد الذي تلاها كان 0؛ لذلك نقدر</span>
$`V(A)`$ <span dir="rtl">على أنه 0. هذا هو الجواب الذي تعطيه طرق</span>
**Batch Monte Carlo<span dir="rtl">.</span>** <span dir="rtl">لاحظ أن
هذا هو أيضًا الجواب الذي يعطي أقل خطأ مربع على بيانات التدريب. في الواقع،
يعطي صفر خطأ على البيانات. ولكن لا يزال من المتوقع أن يكون الجواب الأول
أفضل. إذا كانت العملية **ماركوف
(**</span>**Markov<span dir="rtl">)</span>**<span dir="rtl">، فمن
المتوقع أن ينتج الجواب الأول خطأ أقل في البيانات المستقبلية، على الرغم
من أن جواب</span> **Monte Carlo
<span dir="rtl"></span>**<span dir="rtl">أفضل في البيانات
الحالية</span>.

<span dir="rtl">**<u>مثال 6.4:</u>** يوضح فرقًا عامًا بين التقديرات التي
تم العثور عليها باستخدام طرق</span> **Batch TD (0)
<span dir="rtl"></span>**<span dir="rtl">وطرق</span> **Batch Monte
Carlo<span dir="rtl">.</span>** <span dir="rtl">دائمًا ما تجد طرق</span>
**Batch Monte Carlo <span dir="rtl"></span>**<span dir="rtl">التقديرات
التي تقلل من  
**مربع الخطأ المتوسط**</span> **(Mean-Squared Error)**
<span dir="rtl">على مجموعة التدريب، في حين أن</span> **Batch TD (0)
<span dir="rtl"></span>**<span dir="rtl">دائمًا ما يجد التقديرات التي
ستكون صحيحة تمامًا بالنسبة للنموذج الأرجح لعملية ماركوف. بشكل عام، فإن
التقدير الأرجح لبارامتر هو القيمة التي تجعل احتمال توليد البيانات أكبر
ما يمكن. في هذه الحالة، يكون التقدير الأرجح هو نموذج عملية ماركوف الذي
يتم تشكيله بطريقة واضحة من الحلقات التي تم ملاحظتها: حيث تكون الاحتمالية
الانتقالية المقدرة من الحالة</span> $`\mathbf{i}`$ <span dir="rtl">إلى
الحالة</span> $`\mathbf{j}`$ <span dir="rtl">هي نسبة الانتقالات الملاحظة
من</span> $`\mathbf{I}`$ <span dir="rtl">التي انتقلت إلى</span>
$`\mathbf{j}`$<span dir="rtl">، والمكافأة المتوقعة المرتبطة هي متوسط
المكافآت الملاحظة في تلك الانتقالات. بالنظر إلى هذا النموذج، يمكننا حساب
تقدير دالة القيمة الذي سيكون صحيحًا تمامًا إذا كان النموذج صحيحًا تمامًا.
يُسمى هذا التقدير **تقدير التكافؤ اليقيني**</span>
**<span dir="rtl">(</span>Certainty-Equivalence
<span dir="rtl"></span>Estimate<span dir="rtl">)</span>**
<span dir="rtl">لأنه يعادل افتراض أن تقدير العملية الأساسية كان معروفًا
بيقين بدلاً من أن يكون مقاربًا. بشكل عام، تتقارب</span> **Batch TD (0)
<span dir="rtl"></span>**<span dir="rtl">مع تقدير التكافؤ
اليقيني</span>.

<span dir="rtl">هذا يساعد في تفسير سبب تقارب طرق</span> **TD
<span dir="rtl"></span>**<span dir="rtl">بسرعة أكبر من طرق **مونت
كارلو** </span>**(Monte Carlo)**<span dir="rtl">.</span>
<span dir="rtl">في شكل دفعات، تكون</span> **TD (0)
<span dir="rtl"></span>**<span dir="rtl">أسرع من طرق **مونت
كارلو**</span> **(Monte Carlo)** <span dir="rtl">لأنها تحسب  
**تقدير التكافؤ اليقيني**</span> **(Certainty-Equivalence Estimate)**
<span dir="rtl">الحقيقي.</span>

<span dir="rtl">هذا يفسر ميزة</span> **TD(0)
<span dir="rtl"></span>**<span dir="rtl">التي ظهرت في نتائج الدفعات في
مهمة السير العشوائي (الشكل 6.2). قد تفسر العلاقة مع **تقدير التكافؤ
اليقيني** أيضًا جزئيًا ميزة السرعة في</span> **TD (0) <span dir="rtl">غير
الدفعي</span> <span dir="rtl">(</span>Nonbatch <span dir="rtl"></span>TD
(0)<span dir="rtl">)</span>** <span dir="rtl">(على سبيل المثال، **مثال
6.2**، الصفحة 125، الرسم البياني الأيمن). على الرغم من أن الطرق غير
الدفعيّة لا تحقق إما **تقدير التكافؤ اليقيني** أو **تقدير الحد الأدنى
لمربع الخطأ**</span> **<span dir="rtl">(</span>Minimum
<span dir="rtl"></span>Squared-Error
Estimate<span dir="rtl">)</span>**<span dir="rtl">، يمكن فهمها على أنها
تتحرك تقريبًا في هذه الاتجاهات. قد يكون</span> **TD (0)
<span dir="rtl">غير الدفعي</span>** <span dir="rtl">أسرع من **مونت كارلو
ذات معامل الخطوة الثابت**</span> **(Constant-α MC)**
<span dir="rtl">لأنه يتحرك نحو تقدير أفضل، على الرغم من أنه لا يصل تمامًا
إلى هناك. في الوقت الحالي، لا يمكن قول شيء أكثر تحديدًا عن الكفاءة
النسبية لطرق</span> **TD <span dir="rtl">عبر الإنترنت</span> (Online
TD)** <span dir="rtl">وطرق **مونت كارلو** </span>**(Monte
Carlo)**<span dir="rtl">.</span>

<span dir="rtl">أخيرًا، من الجدير بالذكر أنه على الرغم من أن **تقدير
التكافؤ اليقيني** هو في بعض النواحي حل مثالي، إلا أنه يكاد لا يكون من
الممكن حسابه مباشرة. إذا كان</span> $`n = \mid S \mid`$
<span dir="rtl">هو عدد الحالات، فقد يتطلب تكوين تقدير الاحتمال الأقصى
للعملية فقط على ترتيب</span> $`n^{2}`$ <span dir="rtl">من الذاكرة، وحساب
دالة القيمة المقابلة يتطلب على ترتيب</span> $`n^{3}`$
<span dir="rtl"></span> <span dir="rtl">من الخطوات الحسابية إذا تم
إجراؤه بالطريقة التقليدية. من هذه الناحية، من اللافت حقًا أن طرق</span>
**TD <span dir="rtl"></span>**<span dir="rtl">يمكنها تقريب نفس الحل
باستخدام ذاكرة لا تزيد عن ترتيب</span> $`n`$ <span dir="rtl">وحسابات
متكررة على مجموعة التدريب. في المهام ذات المساحات الكبيرة للحالات، قد
تكون طرق</span> **TD <span dir="rtl"></span>**<span dir="rtl">هي الطريقة
الوحيدة القابلة للتنفيذ لتقريب حل **التكافؤ اليقيني**</span>
**<span dir="rtl">(</span>Certainty-Equivalence
Solution<span dir="rtl">)</span>**<span dir="rtl">.</span>

<span dir="rtl">**<u>تمرين 6.7</u>**:</span> <span dir="rtl">صمم نسخة
خارج السياسة</span> (Off-Policy) <span dir="rtl">من تحديث</span> **TD(0)
<span dir="rtl"></span>**<span dir="rtl">يمكن استخدامها مع أي سياسة
هدف</span> $`\pi`$ <span dir="rtl">وسياسة سلوك تغطية</span>
$`b`$<span dir="rtl">، وذلك باستخدام نسبة أخذ العينات بالأهمية</span>
$`\rho t:`$ <span dir="rtl">(المعادلة 5.3) في كل خطوة زمنية</span>
$`t`$<span dir="rtl">.</span>

**<u>6.4 <span dir="rtl">سارسا: التحكم بالتعليم بالتفرقة الزمنية داخل
السياسة</span> (Sarsa: On-policy TD Control)</u>**

<span dir="rtl">ننتقل الآن إلى استخدام طرق التنبؤ بالتعليم بالتفرقة
الزمنية</span> (TD) <span dir="rtl">لحل مشكلة التحكم. كما هو معتاد، نتبع
نمط **التكرار السياسي العام**</span>
**<span dir="rtl">(</span>Generalized Policy Iteration –
GPI<span dir="rtl">)</span>**<span dir="rtl">، لكن هذه المرة باستخدام
طرق</span> **TD <span dir="rtl"></span>**<span dir="rtl">للجزء المتعلق
بالتقييم أو التنبؤ. كما هو الحال مع طرق **مونت كارلو** </span>**(Monte
Carlo)**<span dir="rtl">، نواجه الحاجة إلى الموازنة بين الاستكشاف
والاستغلال، ومرة أخرى تنقسم الأساليب إلى فئتين رئيسيتين</span>:
**<span dir="rtl">داخل السياسة</span> (On-Policy)**
<span dir="rtl">وخارج **السياسة**
</span>**(Off-Policy)**<span dir="rtl">.</span> <span dir="rtl">في هذا
القسم، نقدم طريقة **التحكم بالتعليم بالتفرقة الزمنية داخل
السياسة**</span> **(On-Policy TD Control)**<span dir="rtl">.</span>

<span dir="rtl">الخطوة الأولى هي تعلم دالة قيمة الإجراء</span>
(Action-Value Function) <span dir="rtl">بدلاً من دالة قيمة الحالة</span>
(State-Value Function)<span dir="rtl">.</span> <span dir="rtl">على وجه
الخصوص، بالنسبة لطريقة **داخل السياسة**</span>
**<span dir="rtl">(</span>On-Policy<span dir="rtl">)</span>**<span dir="rtl">،
يجب علينا تقدير،</span> $`q_{\pi}`$ (s,a) <span dir="rtl">للسياسة
السلوكية الحالية</span> $`\pi`$ <span dir="rtl">ولكل الحالات</span>
$`s`$ <span dir="rtl">والإجراءات</span> $`a`$<span dir="rtl">.</span>
<span dir="rtl">يمكن القيام بذلك باستخدام طريقة</span> **TD
<span dir="rtl"></span>**<span dir="rtl">نفسها التي تم وصفها سابقًا
لتعلم</span> vπ<span dir="rtl">.</span> <span dir="rtl">تذكر أن الحلقة
تتكون من تسلسل متناوب من الحالات وأزواج الحالة–الإجراء</span>:

<img src="./media/image53.png"
style="width:6.26806in;height:0.66181in" />

<span dir="rtl">في القسم السابق، قمنا بدراسة الانتقالات من حالة إلى أخرى
وتعلمنا قيم الحالات. الآن ندرس الانتقالات من زوج الحالة–الإجراء إلى زوج
الحالة–الإجراء، ونتعلم قيم أزواج الحالة–الإجراء. من الناحية الرسمية، هذه
الحالات متطابقة: كلاهما سلاسل ماركوف مع عملية مكافأة. النظريات التي تضمن
تقارب قيم الحالات تحت</span> **TD (0)
<span dir="rtl"></span>**<span dir="rtl">تنطبق أيضًا على الخوارزمية
المقابلة لقيم الإجراءات</span>:

``` math
Q\left( S_{t},A_{t} \right) \leftarrow Q\left( S_{t},A_{t} \right) + \alpha\left\lbrack R_{t + 1} + \gamma Q\left( S_{t + 1},A_{t + 1} \right) - Q\left( S_{t},A_{t} \right) \right\rbrack
```

<img src="./media/image54.png"
style="width:0.67361in;height:1.06944in" /><span dir="rtl">يتم تنفيذ هذا
التحديث بعد كل انتقال من حالة غير نهائية</span> $`S_{t}`$
<span dir="rtl"></span>​<span dir="rtl">.</span> <span dir="rtl">إذا
كانت</span> $`S_{t + 1}`$ <span dir="rtl"></span>​ <span dir="rtl">حالة
نهائية، فإن</span> $`Q\left( S_{t + 1},A_{t + 1} \right)`$
<span dir="rtl">تُعرَّف بأنها صفر. تستخدم هذه القاعدة كل عنصر من عناصر
الخماسي</span> ​,$`At + 1`$​ <span dir="rtl">الذي يشكل انتقالًا من زوج
حالة–إجراء إلى الزوج التالي. هذه الخماسية هي التي أدت إلى تسمية
الخوارزمية **سارسا** </span>**(Sarsa)**<span dir="rtl">.</span>
<span dir="rtl">يظهر مخطط النسخ الاحتياطي لخوارزمية</span> **Sarsa
<span dir="rtl"></span>**<span dir="rtl">كما هو موضح في الجانب
الايسر</span>

<span dir="rtl">من السهل تصميم خوارزمية تحكم داخل السياسة تعتمد على
طريقة التنبؤ</span> **Sarsa<span dir="rtl">.</span>**
<span dir="rtl">كما هو الحال في جميع طرق **داخل السياسة**
</span>**(On-Policy)**<span dir="rtl">، نقوم باستمرار بتقدير</span>
$`q_{\pi}`$ <span dir="rtl">لسياسة السلوك</span>
$`\pi`$<span dir="rtl">، وفي نفس الوقت نغير</span> π <span dir="rtl">نحو
الجشع بالنسبة إلى</span> $`q_{\pi}`$<span dir="rtl">.</span>
<span dir="rtl">يتم إعطاء الشكل العام لخوارزمية التحكم</span> **Sarsa
<span dir="rtl"></span>**<span dir="rtl">في الصندوق بالصفحة التالية.
تعتمد خصائص التقارب لخوارزمية</span> **Sarsa
<span dir="rtl"></span>**<span dir="rtl">على طبيعة اعتماد السياسة
على</span> $`Q`$<span dir="rtl">.</span> <span dir="rtl">على سبيل
المثال، يمكن استخدام سياسات</span> **ε-greedy
<span dir="rtl"></span>**<span dir="rtl">أو</span>
**ε-soft<span dir="rtl">.</span>** <span dir="rtl">تتقارب</span> **Sarsa
<span dir="rtl"></span>**<span dir="rtl">باحتمالية 1 إلى سياسة مثلى
ودالة قيمة الإجراء المثلى طالما تم زيارة جميع أزواج الحالة–الإجراء عددًا
لا نهائيًا من المرات وتقترب السياسة في النهاية من السياسة الجشعة (وهو ما
يمكن ترتيبه، على سبيل المثال، باستخدام سياسات</span> **ε-greedy
<span dir="rtl"></span>**<span dir="rtl">من خلال تعيين</span> **ε =
1/t**<span dir="rtl">)</span>.

<span dir="rtl">**<u>تمرين 6.8</u>**:</span> <span dir="rtl">أظهر أن
نسخة دالة القيمة الإجرائية من المعادلة (6.6) تنطبق على الشكل الإجرائي
لخطأ</span> **TD**
<span dir="rtl"></span>$`\delta_{t} = R_{t + 1} + \gamma Q\left( S_{t + 1},A_{t + 1} \right) - Q\left( S_{t},A_{t} \right)`$
<span dir="rtl">، بافتراض مرة أخرى أن القيم لا تتغير من خطوة إلى
أخرى</span>.

<span dir="rtl">سارسا</span> <span dir="rtl">(التحكم بالتعليم بالتفرقة
الزمنية داخل السياسة</span> - On-policy TD
control<span dir="rtl">)</span> <span dir="rtl">لتقدير</span> Q≈q∗

<img src="./media/image55.png"
style="width:6.26806in;height:2.73681in" /> <span dir="rtl"><u>مثال
6.5</u>: شبكة الرياح</span> (Windy Gridworld)

<span dir="rtl">في هذا المثال، نستخدم شبكة قياسية مع حالة البداية وحالة
الهدف، لكن هناك اختلاف: هناك رياح عرضية تعمل على رفع الخلايا في وسط
الشبكة. الأفعال هي الأربعة القياسية: لأعلى، لأسفل، لليمين، ولليسار. ولكن
في المنطقة الوسطى، يتم تحويل الحالات التالية إلى الأعلى بسبب "الرياح"،
والتي تختلف قوتها من عمود إلى عمود. قوة الرياح معطاة أدناه لكل عمود،
بعدد الخلايا المرفوعة إلى الأعلى. على سبيل المثال، إذا كنت على بعد خلية
واحدة إلى يمين الهدف، فإن الفعل "يسار" يأخذك إلى الخلية مباشرة فوق
الهدف. هذه مهمة غير مخصومة ومحددة بحلقات، مع مكافآت ثابتة قدرها 1- حتى
يتم الوصول إلى حالة الهدف</span>.<img src="./media/image56.png"
style="width:2.93333in;height:2.02083in" /> <span dir="rtl">يوضح الرسم
البياني على اليسار نتائج تطبيق سياسة</span> "$`\varepsilon`$-greedy"
<span dir="rtl">مع طريقة سارس</span> (Sarsa) <span dir="rtl">في هذه
المهمة، حيث</span> $`\epsilon = 0.1`$
<span dir="rtl">و</span>$`\alpha = 0.5`$<span dir="rtl">، والقيم
الأولية</span> $`Q(s,a)\  = 0`$ <span dir="rtl">لجميع</span>
$`s`$<span dir="rtl">، و</span>$`a`$<span dir="rtl">.</span>
<span dir="rtl">يُظهر الانحدار المتزايد للرسم البياني أن الهدف تم الوصول
إليه بسرعة أكبر مع مرور الوقت. بحلول 8000 خطوة زمنية، كانت السياسة
الجشعة قد أصبحت مثالية منذ فترة طويلة (تُظهر المسار من هذه السياسة في
الرسم البياني المضمن). استمر الاستكشاف باستخدام</span> " $`\varepsilon`$
-greedy" <span dir="rtl">في الحفاظ على متوسط طول الحلقة عند حوالي 17
خطوة، وهو أكثر بخطوتين من الحد الأدنى البالغ 15 خطوة. لاحظ أن الطرق
المبنية على مونت كارلو لا يمكن استخدامها بسهولة هنا لأن إنهاء المهمة غير
مضمون لجميع السياسات. إذا تم العثور على سياسة تجعل العميل يبقى في نفس
الحالة، فإن الحلقة التالية لن تنتهي أبدًا. طرق التعليم عبر الإنترنت مثل
سارس لا تواجه هذه المشكلة لأنها تتعلم بسرعة خلال الحلقة أن مثل هذه
السياسات ضعيفة، وتنتقل إلى شيء آخر</span>.

<span dir="rtl"><u>تمرين 6.9</u>: شبكة الأفق العاصف مع حركات الملك
(برمجة)</span> <span dir="rtl">(</span>Windy Gridworld with King’s
<span dir="rtl"></span>Moves (programming)<span dir="rtl">)</span>

<span dir="rtl">أعد حل مهمة الشبكة العاصفة بافتراض ثمانية إجراءات ممكنة،
بما في ذلك الحركات القطرية، بدلاً من الأربعة المعتادة. ما مدى تحسين
الأداء الذي يمكنك تحقيقه مع الإجراءات الإضافية؟ هل يمكنك تحسين الأداء
بشكل أكبر بإضافة إجراء تاسع لا يسبب أي حركة على الإطلاق بخلاف الحركة
الناتجة عن الرياح؟</span>

<span dir="rtl">**<u>تمرين 6.10</u>**: الرياح العشوائية (برمجة)</span>
(Stochastic Wind (programming))  
<span dir="rtl">أعد حل مهمة الشبكة العاصفة باستخدام حركات الملك، مع
افتراض أن تأثير الرياح، إذا كان موجودًا، هو عشوائي، ويتفاوت أحيانًا بمقدار
1 عن القيم المتوسطة المعطاة لكل عمود. أي، ثلث الوقت تتحرك تمامًا وفقًا
لهذه القيم، كما في التمرين السابق، ولكن أيضًا ثلث الوقت تتحرك خلية واحدة
فوق ذلك، وثلث آخر من الوقت تتحرك خلية واحدة تحت ذلك. على سبيل المثال،
إذا كنت على بعد خلية واحدة إلى اليمين من الهدف وانتقلت إلى اليسار، فإن
ثلث الوقت تتحرك خلية واحدة فوق الهدف، وثلث الوقت تتحرك خليتين فوق الهدف،
وثلث الوقت تتحرك إلى الهدف</span>.

**<u>6.5 <span dir="rtl"></span>Q-learning<span dir="rtl">:</span>
<span dir="rtl">التحكم</span> TD <span dir="rtl">خارج السياسة</span>
(Off-policy TD Control)</u>**

<span dir="rtl">إحدى الاكتشافات المبكرة في مجال التعليم المعزز كانت
تطوير خوارزمية التحكم</span> TD <span dir="rtl">خارج السياسة</span>
(Off-policy TD Control) <span dir="rtl">المعروفة باسم</span> Q-learning
<span dir="rtl">(واتكنز، 1989)، والتي تُعرَّف كالتالي</span>:

``` math
Q\left( S_{t},A_{t} \right) \leftarrow Q\left( S_{t},A_{t} \right) + \alpha\left\lbrack R_{t + 1} + \gamma\max_{a}Q\left( S_{t + 1},a \right) - Q\left( S_{t},A_{t} \right) \right\rbrack
```

<span dir="rtl">في هذه الحالة، تقوم دالة قيمة الإجراء</span>
(Action-Value Function) <span dir="rtl">المتعلمة،</span>
$`Q`$<span dir="rtl">، بالتقريب المباشر  
لـ</span> $`q*`$ <span dir="rtl">دالة قيمة الإجراء المثلى</span>
(Optimal Action-Value Function)<span dir="rtl">، بشكل مستقل عن
السياسة</span> (Policy) <span dir="rtl">المتبعة. هذا يبسط بشكل كبير
تحليل الخوارزمية ومكّن من إثباتات التقاء مبكرة. لا تزال السياسة</span>
(Policy) <span dir="rtl">تؤثر لأنها تحدد أي أزواج من الحالات
والإجراءات</span> (State-Action Pairs) <span dir="rtl">يتم زيارتها
وتحديثها. ومع ذلك، كل ما هو مطلوب للتقارب الصحيح هو أن تستمر جميع
الأزواج في التحديث. كما لاحظنا في الفصل 5، هذا هو الحد الأدنى من
المتطلبات بمعنى أن أي طريقة مضمونة لاكتشاف السلوك الأمثل في الحالة
العامة يجب أن تتطلب ذلك. تحت هذا الافتراض ومع نوع مختلف من الشروط
المعتادة للتقريب العشوائي</span> (Stochastic Approximation)
<span dir="rtl">على تسلسل معلمات حجم الخطوة، تم إثبات أن</span> $`Q`$
<span dir="rtl">يتقارب مع الاحتمال 1 إلى</span>
$`q*`$<span dir="rtl">.</span> <span dir="rtl">تم عرض خوارزمية</span>
Q-learning <span dir="rtl">أدناه بشكل إجرائي</span>.

Q-learning <span dir="rtl">(التحكم</span> TD <span dir="rtl">خارج
السياسة)</span> <span dir="rtl">لتقدير</span> π≈π∗

<img src="./media/image57.png"
style="width:6.26806in;height:2.55208in" />

<span dir="rtl">ما هو مخطط النسخ الاحتياطي</span> (Backup Diagram)
<span dir="rtl">لـ</span> Q-learning<span dir="rtl">؟ تنص القاعدة (6.8)
على تحديث زوج الحالة–الإجراء</span> (State-Action Pair)<span dir="rtl">،
لذا يجب أن تكون العقدة العلوية، جذر التحديث، عقدة إجراء صغيرة ومملوءة.
يتم التحديث أيضًا من عقد الإجراءات، حيث يتم أخذ الحد الأقصى من بين جميع
تلك الإجراءات الممكنة في الحالة التالية. لذلك، يجب أن تكون العقد السفلية
في مخطط النسخ الاحتياطي جميعها عقد إجراءات. وأخيرًا، تذكر أننا نشير إلى
أخذ الحد الأقصى من هذه "العقدة التالية" بقوس يمتد عبرها (الشكل
3.4-اليمين). هل يمكنك الآن تخمين شكل المخطط؟ إذا كان الأمر كذلك، فيرجى
القيام بالتخمين قبل الرجوع إلى الإجابة في الشكل 6.4 على الصفحة
134</span>.

**<span dir="rtl"><u>المثال 6.6</u>: السير على الجرف</span> (Cliff
Walking)**

<span dir="rtl">يُقارن هذا المثال في العالم الشبكي</span> (Gridworld)
<span dir="rtl">بين طريقتي</span> Sarsa
<span dir="rtl">و</span>Q-learning<span dir="rtl">، مع تسليط الضوء على
الفرق بين الطرق داخل السياسة</span> (On-policy)
<span dir="rtl">مثل</span> Sarsa <span dir="rtl">وخارج السياسة</span>
<span dir="rtl">(</span>Off-policy<span dir="rtl">) مثل</span>
Q-learning<span dir="rtl">.</span> <span dir="rtl">لنفكر في العالم
الشبكي الموضح إلى اليمين. هذه مهمة معيارية غير مخفضة</span>
(Undiscounted) <span dir="rtl">وتحدث على مراحل، مع وجود حالات</span>
<img src="./media/image58.png"
style="width:3.61944in;height:3.55556in" /><span dir="rtl">بدء ونهاية،
والإجراءات المعتادة التي تسبب الحركة للأعلى، للأسفل، لليمين، ولليسار.
المكافأة هي 1- في جميع التحولات باستثناء تلك التي تدخل المنطقة
البارامترية بـ “الجرف". الدخول إلى هذه المنطقة يمنح مكافأة قدرها 100-
ويرسل الوكيل</span> (Agent) <span dir="rtl">مباشرة إلى البداية</span>.

<span dir="rtl">يظهر الرسم البياني إلى اليسار أداء طريقتي</span> Sarsa
<span dir="rtl">و</span>Q-learning <span dir="rtl">مع اختيار إجراء  
</span> " $`\varepsilon`$ -greedy"<span dir="rtl">، حيث</span>
$`\epsilon = 0.1`$<span dir="rtl">.</span> <span dir="rtl">بعد مرور فترة
انتقالية أولية، يتعلم</span> <span dir="rtl">  
</span>Q-learning <span dir="rtl">قيم السياسة المثلى، وهي التي تتحرك
مباشرة على طول حافة الجرف. لسوء الحظ، يؤدي هذا أحيانًا إلى سقوطه من الجرف
بسبب اختيار الإجراء</span> " $`\varepsilon`$
-greedy"<span dir="rtl">.</span> <span dir="rtl">من ناحية أخرى،
يأخذ</span> Sarsa <span dir="rtl">اختيار الإجراء في الاعتبار ويتعلم
الطريق الأطول ولكن الأكثر أمانًا عبر الجزء العلوي من الشبكة. رغم أن  
</span>Q-learning <span dir="rtl">يتعلم بالفعل قيم السياسة المثلى، إلا
أن أدائه الفوري أسوأ من أداء</span> Sarsa <span dir="rtl">الذي يتعلم
سياسة أكثر أمانًا. بالطبع، إذا تم تقليل</span> $`\varepsilon`$
<span dir="rtl">تدريجيًا، فسوف يتقارب كلا الطريقتين إلى السياسة المثلى
بشكل غير تناسبي</span>.

<span dir="rtl">**<u>التمرين 6.11:</u>** لماذا يُعتبر</span> Q-learning
<span dir="rtl">طريقة تحكم خارج السياسة</span>
<span dir="rtl">(</span>Off-policy Control
<span dir="rtl"></span>Method<span dir="rtl">)؟</span>

<span dir="rtl">**<u>التمرين 6.12:</u>** افترض أن اختيار الإجراء يعتمد
على الطمع</span> (Greedy)<span dir="rtl">.</span> <span dir="rtl">هل
سيكون</span> Q-learning <span dir="rtl">في هذه الحالة نفس
خوارزمية</span> Sarsa <span dir="rtl">تمامًا؟ هل سيقومان بنفس اختيار
الإجراءات وتحديث الأوزان؟</span>

**<u><span dir="rtl">6.6: سارسا المتوقع</span>
<span dir="rtl">(</span>Expected Sarsa<span dir="rtl">)</span></u>**

<span dir="rtl">اعتبر خوارزمية التعليم التي تشبه خوارزمية</span>
Q-learning <span dir="rtl">باستثناء أنه بدلاً من أخذ الحد الأقصى على
أزواج الحالة–الإجراء التالية، تستخدم القيمة المتوقعة، مع الأخذ في
الاعتبار مدى احتمالية كل إجراء تحت السياسة الحالية. بمعنى آخر، اعتبر
الخوارزمية ذات قاعدة التحديث التالية</span>:

``` math
Q\left( S_{t},A_{t} \right) \leftarrow Q\left( S_{t},A_{t} \right) + \alpha\left\lbrack R_{t + 1} + \gamma E_{\pi}\left\lbrack Q\left( S_{t + 1},A_{t + 1} \right) \middle| S_{t + 1} \right\rbrack - Q\left( S_{t},A_{t} \right) \right\rbrack
```

``` math
Q\left( S_{t},A_{t} \right) \leftarrow Q\left( S_{t},A_{t} \right) + \alpha\left\lbrack R_{t + 1} + \gamma\sum_{a}^{}{\pi\left( a \middle| S_{t + 1} \right)Q\left( S_{t + 1},a \right)} - Q\left( S_{t},A_{t} \right) \right\rbrack
```

<span dir="rtl">ولكن بخلاف ذلك، تتبع الخوارزمية مخطط</span>
Q-learning<span dir="rtl">.</span> <span dir="rtl">بالنظر إلى الحالة
التالية</span> $`S_{t + 1}`$ <span dir="rtl"></span>​<span dir="rtl">،
تتحرك هذه الخوارزمية بشكل حتمي في نفس الاتجاه الذي يتحرك فيه
سارسا</span> (Sarsa) <span dir="rtl">في التوقع، وبالتالي يطلق عليها اسم
سارسا المتوقع</span> (Expected Sarsa)<span dir="rtl">.</span>
<span dir="rtl">يظهر مخطط النسخ الاحتياطي لها في الجهة اليمنى في الشكل
6.4</span>.

<span dir="rtl">سارسا المتوقع</span> (Expected Sarsa)
<span dir="rtl">أكثر تعقيدًا من الناحية الحسابية مقارنةً بسارسا، ولكنه في
المقابل يلغي التباين الناتج عن الاختيار العشوائي لـ</span> $`At + 1`$
<span dir="rtl"></span>​. <span dir="rtl">بالنظر إلى نفس كمية التجربة،
يمكننا توقع أن يؤدي بشكل أفضل قليلاً من سارسا، وهذا بالفعل ما يحدث عادةً.
يوضح الشكل 6.3 ملخص النتائج في مهمة السير على الجرف باستخدام سارسا
المتوقع مقارنة بسارسا و</span>Q-learning<span dir="rtl">.</span>
<span dir="rtl">يحتفظ سارسا المتوقع بالميزة الكبيرة التي يتمتع بها سارسا
على</span> Q-learning <span dir="rtl">في هذه المشكلة. بالإضافة إلى ذلك،
يُظهر سارسا المتوقع تحسنًا ملحوظًا</span>.

<span dir="rtl">  
</span><img src="./media/image59.png"
style="width:6.04167in;height:3.19931in" /><span dir="rtl">**الشكل
6.3**: الأداء المرحلي والنهائي لطرق التحكم</span> TD <span dir="rtl">في
مهمة السير على الجرف كدالة للـ</span> $`\alpha`$<span dir="rtl">.
استخدمت جميع الخوارزميات سياسة</span> "$`\varepsilon`$-greedy"
<span dir="rtl">حيث</span> $`\epsilon = 0.1`$<span dir="rtl">.</span>
<span dir="rtl">الأداء النهائي هو متوسط على 100,000 حلقة، بينما الأداء
المرحلي هو متوسط على أول 100 حلقة. هذه البيانات هي متوسطات لأكثر من
50,000 و10 تجارب للحالات المرحلية والنهائية على التوالي. تشير الدوائر
الممتلئة إلى أفضل أداء مرحلي لكل طريقة. مقتبسة من</span> van Seijen
<span dir="rtl">وآخرون</span> (2009)<span dir="rtl">.</span>

<img src="./media/image60.png"
style="width:5.85469in;height:1.73349in" />

**<span dir="rtl">الشكل 6.4: مخططات النسخ الاحتياطي لـ</span> Q-learning
<span dir="rtl">وسارسا المتوقع</span> (Expected
Sarsa)<span dir="rtl">.</span>**

<span dir="rtl">على مدى نطاق واسع من قيم معامل حجم الخطوة</span>
$`\alpha`$<span dir="rtl">، يُظهر سارسا المتوقع</span> (Expected Sarsa)
<span dir="rtl">تحسنًا على سارسا</span> (Sarsa)<span dir="rtl">.</span>
<span dir="rtl">في مهمة السير على الجرف</span> (Cliff
Walking)<span dir="rtl">، تكون جميع انتقالات الحالة حتمية، وكل العشوائية
تأتي من السياسة. في مثل هذه الحالات، يمكن لسارسا المتوقع</span>
(Expected Sarsa) <span dir="rtl">تعيين</span> $`\alpha = 1`$
<span dir="rtl">بأمان دون التعرض لأي تدهور في الأداء النهائي، بينما لا
يستطيع سارسا</span> (Sarsa) <span dir="rtl">الأداء بشكل جيد على المدى
الطويل إلا عند قيمة صغيرة لـ</span> α<span dir="rtl">، حيث يكون الأداء
على المدى القصير ضعيفًا. في هذا المثال وأمثلة أخرى، هناك ميزة تجريبية
مستمرة لصالح سارسا المتوقع على سارسا</span>.

<span dir="rtl">في هذه النتائج لمهمة السير على الجرف، تم استخدام سارسا
المتوقع ضمن السياسة</span> (On-policy)<span dir="rtl">، ولكن بشكل عام قد
يستخدم سياسة مختلفة عن السياسة المستهدفة</span> $`\pi`$
<span dir="rtl">لتوليد السلوك، وفي هذه الحالة يصبح خوارزمية خارج
السياسة</span> (Off-policy)<span dir="rtl">.</span> <span dir="rtl">على
سبيل المثال، افترض أن</span> $`\pi`$ <span dir="rtl">هي السياسة
الجشعة</span> (Greedy Policy) <span dir="rtl">بينما السلوك أكثر
استكشافًا؛ حينها يصبح سارسا المتوقع تمامًا  
مثل</span> Q-learning<span dir="rtl">.</span> <span dir="rtl">بهذا
المعنى، سارسا المتوقع يعمم ويشمل</span> Q-learning <span dir="rtl">مع
تحسين موثوق على سارسا. باستثناء التكلفة الحسابية الإضافية الصغيرة، قد
يهيمن سارسا المتوقع بالكامل على كل من خوارزميتي التحكم</span> TD
<span dir="rtl">الأخريين الأكثر شهرة</span>.

**<u><span dir="rtl">6.7 انحياز التفضيل والتعليم المزدوج</span>
<span dir="rtl">(</span>Maximization Bias and Double
Learning<span dir="rtl">)</span></u>**

<span dir="rtl">جميع خوارزميات التحكم التي ناقشناها حتى الآن تتضمن عملية
التفضيل</span> (Maximization) <span dir="rtl">في بناء سياساتها
المستهدفة. على سبيل المثال، في</span> Q-learning <span dir="rtl">تكون
السياسة المستهدفة هي السياسة الجشعة</span> (Greedy Policy)
<span dir="rtl">بناءً على قيم الإجراءات الحالية، والتي تُعرّف باستخدام
عملية التفضيل القصوى</span> (Max)<span dir="rtl">.</span>
<span dir="rtl">في سارسا</span> (Sarsa) <span dir="rtl">تكون السياسة
غالبًا هي</span> "$`\varepsilon`$-greedy"<span dir="rtl">، والتي تتضمن
أيضًا عملية تفضيل. في هذه الخوارزميات، يتم استخدام الحد الأقصى للقيم
المقدرة كإسقاط للقيمة القصوى، مما يمكن أن يؤدي إلى انحياز إيجابي كبير.
لفهم السبب، لنفترض وجود حالة واحدة</span> $`s`$ <span dir="rtl">حيث هناك
العديد من الإجراءات</span> $`a`$ <span dir="rtl">التي تكون قيمها
الحقيقية</span> $`q(s,a)`$ <span dir="rtl">تساوي صفرًا، ولكن قيمها
المقدرة</span> $`Q(s,a)`$ <span dir="rtl">غير مؤكدة وبالتالي موزعة بين
فوق وتحت الصفر. الحد الأقصى للقيم الحقيقية هو صفر، لكن الحد الأقصى للقيم
المقدرة يكون موجبًا، مما يؤدي إلى انحياز إيجابي. نحن نسمي هذا "انحياز
التفضيل</span> (Maximization Bias)<span dir="rtl">.</span>

**<span dir="rtl"><u>المثال 6.7</u>: مثال على انحياز التفضيل</span>
(Maximization Bias Example)**

<span dir="rtl">يوفر نموذج</span> MDP <span dir="rtl">الصغير الموضح في
الشكل 6.5 مثالًا بسيطًا على كيفية تأثير انحياز التفضيل سلبًا على أداء
خوارزميات التحكم</span> **TD**<span dir="rtl">.</span>
<span dir="rtl">يحتوي</span> **MDP** <span dir="rtl">على حالتين غير
نهائيتين،</span> $`A`$
<span dir="rtl">و</span>$`B`$<span dir="rtl">.</span>
<span dir="rtl">تبدأ الحلقات دائمًا في الحالة</span> $`A`$
<span dir="rtl">مع وجود خيار بين إجراءين: اليسار واليمين. الإجراء
اليميني ينقل مباشرةً إلى الحالة النهائية مع مكافأة وعائد يساوي صفرًا.
الإجراء اليساري ينقل إلى الحالة</span> $`B`$<span dir="rtl">، أيضًا مع
مكافأة تساوي صفرًا، ومن هذه الحالة</span> ($`B`$) <span dir="rtl">يمكن
اتخاذ العديد من الإجراءات الأخرى التي تؤدي جميعها إلى إنهاء فوري مع
مكافأة تُسحب من توزيع طبيعي بمتوسط 0.1− وتباين 1.0. لذلك، فإن العائد
المتوقع لأي مسار يبدأ بالإجراء اليساري هو 0.1−، وبالتالي فإن اتخاذ
الإجراء اليساري في الحالة</span> $`A`$ <span dir="rtl">دائمًا ما يكون
خطأً</span>. <span dir="rtl"></span>

<span dir="rtl">ومع ذلك، قد تفضل طرق التحكم الخاصة بنا الإجراء اليساري
بسبب انحياز التفضيل، مما يجعل الحالة</span> $`B`$ <span dir="rtl">تبدو
وكأنها لها قيمة إيجابية. يظهر الشكل 6.5 أن</span> Q-learning
<span dir="rtl">مع اختيار  
الإجراء باستخدام</span> " $`\varepsilon`$ -greedy" <span dir="rtl">يتعلم
في البداية تفضيل الإجراء اليساري بشدة في هذا المثال. حتى عند الاستقرار،
يقوم</span> Q-learning <span dir="rtl">باتخاذ الإجراء اليساري بنسبة تزيد
عن النسبة المثلى بحوالي 5% عند إعدادات المعلمات الخاصة بنا</span>
$`\epsilon = 0.1`$<span dir="rtl">،</span>
$`\alpha = 0.1`$<span dir="rtl">،
و</span>$`\gamma = 1`$<span dir="rtl">.</span>

<img src="./media/image61.png"
style="width:6.26806in;height:3.13403in" /><span dir="rtl">**الشكل
6.5**: مقارنة بين</span> Q-learning <span dir="rtl">و</span>Double
Q-learning <span dir="rtl">على نموذج</span> MDP <span dir="rtl">بسيط في
حلقات (كما هو موضح في الشكل الصغير). يتعلم</span> Q-learning
<span dir="rtl">في البداية اتخاذ الإجراء اليساري أكثر بكثير من الإجراء
اليميني، ودائمًا ما يتخذه بشكل أكبر من الحد الأدنى بنسبة 5% الذي تفرضه
سياسة اختيار الإجراء</span> " $`\varepsilon`$ -greedy"
<span dir="rtl">مع</span> $`\epsilon = 0.1`$<span dir="rtl">.</span>
<span dir="rtl">على النقيض من ذلك، فإن</span> Double Q-learning
<span dir="rtl">غير متأثر تقريبًا بانحياز التفضيل. تمثل هذه البيانات
متوسطًا لأكثر من 10,000 تجربة. كانت تقديرات قيم الإجراءات الأولية تساوي
صفرًا. تم كسر أي تساوٍ في اختيار الإجراء</span> " $`\varepsilon`$ -greedy"
<span dir="rtl">بشكل عشوائي</span>.

<span dir="rtl">هل هناك خوارزميات تتجنب انحياز التفضيل</span>
(Maximization Bias)<span dir="rtl">؟ لنبدأ بالنظر في حالة الباندت</span>
(Bandit) <span dir="rtl">حيث لدينا تقديرات ضبابية لقيمة كل إجراء من بين
العديد من الإجراءات، والتي تم الحصول عليها كمتوسطات للعينات من المكافآت
المستلمة في جميع مرات اللعب لكل إجراء. كما ناقشنا سابقًا، سيكون هناك
انحياز إيجابي إذا استخدمنا الحد الأقصى من التقديرات كإسقاط للحد الأقصى
للقيم الحقيقية. إحدى الطرق لفهم المشكلة هي أن السبب يكمن في استخدام نفس
العينات (مرات اللعب) لتحديد الإجراء الأقصى وتقدير قيمته. لنفترض أننا
قسمنا مرات اللعب إلى مجموعتين واستخدمناهما لتعلم تقديرين مستقلين، دعنا
نسميهما</span> $`Q1(a)`$ <span dir="rtl">و</span>
$`Q2(a)`$<span dir="rtl">، كل منهما يمثل تقديرًا للقيمة الحقيقية</span>
$`q(a)`$ <span dir="rtl">لكل إجراء</span> a <span dir="rtl"></span>∈
<span dir="rtl"></span>A<span dir="rtl">.</span> <span dir="rtl">يمكننا
بعد ذلك استخدام أحد التقديرين، لنقل</span> $`Q1`$<span dir="rtl">،
لتحديد الإجراء الأقصى</span>
$`A^{*} = \arg{\max_{a}Q_{1}}(a)`$<span dir="rtl">، واستخدام التقدير
الآخر</span>$`Q2`$ <span dir="rtl">لتوفير تقدير لقيمته  
</span>​$`Q_{2}\left( A^{*} \right) = Q_{2}\left( \text{argmax}_{a}Q_{1}(a) \right)`$
<span dir="rtl"></span> <span dir="rtl">سيكون هذا التقدير عندئذ غير
متحيز بمعنى أن</span>
$`\mathbb{E}\left\lbrack Q_{2}\left( A^{*} \right) \right\rbrack = q\left( A^{*} \right)`$
<span dir="rtl">يمكننا أيضًا تكرار العملية مع عكس دور التقديرين للحصول
على تقدير غير متحيز آخر</span>
$`Q_{1}\left( \text{argmax}_{a}Q_{2}(a) \right)`$<span dir="rtl">.</span>
<span dir="rtl">هذه هي فكرة التعليم المزدوج</span> (Double
Learning)<span dir="rtl">.</span> <span dir="rtl">لاحظ أنه على الرغم من
أننا نتعلم تقديرين، إلا أن أحد التقديرين فقط يتم تحديثه في كل مرة لعب؛
التعليم المزدوج يضاعف متطلبات الذاكرة، ولكنه لا يزيد من كمية الحسابات
المطلوبة في كل خطوة</span>.

<span dir="rtl">تمتد فكرة التعليم المزدوج بشكل طبيعي إلى الخوارزميات
لمشاكل عملية اتخاذ القرارات متعددة المراحل</span>
(MDPs)<span dir="rtl">.</span> <span dir="rtl">على سبيل المثال، خوارزمية
التعليم المزدوج المشابهة لـ</span> Q-learning<span dir="rtl">، والتي
تُسمى</span> Double Q-learning<span dir="rtl">، تقسم الخطوات الزمنية إلى
قسمين، ربما عن طريق رمي قطعة نقدية في كل خطوة. إذا كانت نتيجة الرمية
"وجه"، يكون التحديث هو</span>:

``` math
Q1\left( S_{t},A_{t} \right) \leftarrow Q1\left( S_{t},A_{t} \right) + \alpha\left\lbrack R_{t + 1} + \gamma Q2\left( S_{t + 1},\text{argmax}_{a}Q1\left( S_{t + 1},a \right) \right) - Q1\left( S_{t},A_{t} \right) \right\rbrack
```

<span dir="rtl">إذا كانت نتيجة رمي القطعة النقدية "كتابة"، فيتم إجراء
نفس التحديث مع تبديل</span> $`Q1`$
<span dir="rtl">و</span>$`Q2`$<span dir="rtl">، بحيث يتم تحديث</span>
$`Q2`$<span dir="rtl">.</span> <span dir="rtl">يتم التعامل مع دالتي
القيمة التقريبية بشكل متماثل تمامًا. يمكن لسياسة السلوك استخدام كلا
التقديرين لقيمة الإجراء. على سبيل المثال، يمكن أن تعتمد سياسة</span>
"$`\varepsilon`$-greedy" <span dir="rtl">لـ</span> <span dir="rtl">  
</span>Double Q-learning <span dir="rtl">على المتوسط (أو المجموع) بين
تقديري قيمة الإجراء. يتم تقديم خوارزمية كاملة لـ</span> Double
Q-learning <span dir="rtl">في المربع أدناه. هذه هي الخوارزمية المستخدمة
لإنتاج النتائج في الشكل 6.5. في هذا المثال، يبدو أن التعليم المزدوج يقضي
على الضرر الناتج عن انحياز التفضيل (</span>Maximization
<span dir="rtl"></span>Bias<span dir="rtl">).</span>
<span dir="rtl">بالطبع هناك أيضًا نسخ مزدوجة من خوارزميات سارسا</span>
(Sarsa) <span dir="rtl">وسارسا المتوقع</span> (Expected
Sarsa)<span dir="rtl">.</span>

<span dir="rtl">التعليم المزدوج</span> Q-learning
<span dir="rtl">لتقدير</span> Q1≈Q2≈q∗

<img src="./media/image62.png"
style="width:6.31659in;height:2.9775in" />

**<u>6.8 <span dir="rtl">الألعاب، الحالات اللاحقة، وحالات خاصة
أخرى</span> <span dir="rtl">(</span>Games, Afterstates, and Other
Special Cases<span dir="rtl">)</span></u>**

<span dir="rtl">في هذا الكتاب نحاول تقديم منهج موحد لطبقة واسعة من
المهام، ولكن بالطبع هناك دائمًا مهام استثنائية تُعالج بطريقة متخصصة. على
سبيل المثال، نهجنا العام ينطوي على تعلم  
دالة قيمة الإجراء</span> <span dir="rtl">(</span>Action-value
function<span dir="rtl">)، ولكن في الفصل 1 قدمنا طريقة</span> TD
<span dir="rtl">لتعلم كيفية اللعب في لعبة إكس-أو التي تعلمت شيئًا أقرب
إلى دالة قيمة الحالة</span> (State-value
function)<span dir="rtl">.</span> <span dir="rtl">إذا نظرنا عن كثب إلى
هذا المثال، يصبح واضحًا أن الدالة التي تم تعلمها هناك ليست دالة قيمة
الإجراء</span> (Action-value function) <span dir="rtl">ولا دالة قيمة
الحالة</span> (State-value function) <span dir="rtl">بالمعنى المعتاد،
دالة قيمة الحالة</span> (State-value function) <span dir="rtl">التقليدية
تقيم الحالات التي يمكن للعميل</span> (Agent) <span dir="rtl">فيها اختيار
إجراء</span> (Action)<span dir="rtl">، ولكن دالة قيمة الحالة</span>
(State-value function) <span dir="rtl">المستخدمة في إكس-أو تقيم أوضاع
اللوحة بعد أن يقوم العميل</span> (Agent) <span dir="rtl">بتحركه. دعونا
نسمي هذه الحالات اللاحقة</span> (Afterstates)<span dir="rtl">، ودوال
القيمة المتعلقة بها دوال قيمة الحالات اللاحقة</span>
<span dir="rtl">(</span>Afterstate value
<span dir="rtl"></span>functions<span dir="rtl">)</span>.
<span dir="rtl">الحالات اللاحقة</span> (Afterstates)
<span dir="rtl">مفيدة عندما يكون لدينا معرفة بجزء أولي من ديناميات
البيئة</span> (Environment Dynamics) <span dir="rtl">ولكن ليس بالضرورة
الديناميات الكاملة. على سبيل المثال، في الألعاب نعرف عادة التأثيرات
الفورية لتحركاتنا. نعرف لكل حركة شطرنج ممكنة ما ستكون عليه الوضعية
الناتجة، ولكننا لا نعرف كيف سيرد خصمنا. دوال قيمة الحالات اللاحقة</span>
<span dir="rtl">(</span>Afterstate <span dir="rtl"></span>value
functions<span dir="rtl">) هي وسيلة طبيعية للاستفادة من هذا النوع من
المعرفة ومن ثم إنتاج طريقة تعلم أكثر كفاءة. السبب في كون من الأكثر كفاءة
تصميم الخوارزميات من حيث الحالات اللاحقة</span> (Afterstates)
<span dir="rtl">واضح من مثال إكس-أو. دالة قيمة الإجراء</span>
(Action-value function) <span dir="rtl">التقليدية ستربط من الأوضاع
والحركات إلى تقدير للقيمة. لكن العديد من أزواج الوضعية–الحركة ينتج عنها
نفس الوضعية الناتجة، كما في المثال أدناه</span>:

<img src="./media/image63.png"
style="width:6.26806in;height:3.46042in" /><span dir="rtl">في مثل هذه
الحالات، تكون أزواج الوضعية–الحركة مختلفة ولكنها تنتج نفس "الوضعية
اللاحقة</span>" <span dir="rtl"></span>(Afterposition)<span dir="rtl">،
وبالتالي يجب أن يكون لها نفس القيمة. دالة قيمة الإجراء</span>
<span dir="rtl">(</span>Action-value
<span dir="rtl"></span>function<span dir="rtl">) التقليدية ستكون مضطرة
لتقييم كل من الزوجين بشكل منفصل، بينما دالة قيمة الحالة اللاحقة</span>
(Afterstate value function) <span dir="rtl">ستقيم كلا الزوجين على الفور
وبالتساوي. أي تعلم حول زوج الوضعية–الحركة على اليسار سينتقل مباشرة إلى
الزوج على اليمين</span>.

<span dir="rtl">تظهر الحالات اللاحقة</span> (Afterstates)
<span dir="rtl">في العديد من المهام، وليس فقط في الألعاب. على سبيل
المثال، في مهام الانتظار هناك إجراءات مثل تخصيص العملاء إلى الخوادم، أو
رفض العملاء، أو التخلص من المعلومات. في مثل هذه الحالات، تُعرّف الإجراءات
فعليًا بناءً على تأثيراتها الفورية التي تكون معروفة تمامًا</span>.

<span dir="rtl">من المستحيل وصف جميع أنواع المشاكل المتخصصة الممكنة
والخوارزميات التعليمية المتخصصة المقابلة. ومع ذلك، المبادئ التي تم
تطويرها في هذا الكتاب يجب أن تنطبق على نطاق واسع. على سبيل المثال، تُوصف
طرق الحالة اللاحقة</span> (Afterstate methods) <span dir="rtl">بشكل
مناسب من حيث تكرار السياسة العامة</span> (Generalized Policy
Iteration)<span dir="rtl">، حيث تتفاعل السياسة ودالة القيمة (الحالة
اللاحقة) بنفس الطريقة الأساسية. في كثير من الحالات، ستظل تواجه الاختيار
بين الأساليب المتبعة</span> (On-policy) <span dir="rtl">والأساليب غير
المتبعة</span> (Off-policy) <span dir="rtl">لإدارة الحاجة إلى الاستكشاف
المستمر</span>.

<span dir="rtl">**التمرين 6.14** وصف كيف يمكن إعادة صياغة مهمة تأجير
سيارات جاك (مثال 4.2) من حيث الحالات اللاحقة</span>
<span dir="rtl">(</span>Afterstates<span dir="rtl">).</span>
<span dir="rtl">لماذا، من حيث هذه المهمة المحددة، من المحتمل أن تسهم مثل
هذه إعادة الصياغة في تسريع التقاء النتيجة؟</span>

**<u>6.9 <span dir="rtl">ملخص</span> (Summary)</u>**

<span dir="rtl">في هذا الفصل، قدمنا نوعًا جديدًا من طرق التعليم، وهو
التعليم باستخدام الفرق الزمني</span>
<span dir="rtl">(</span>Temporal-Difference (TD)
Learning<span dir="rtl">)، وشرحنا كيفية تطبيقه على مشكلة التعليم
المعزز</span> (Reinforcement Learning)<span dir="rtl">.</span>
<span dir="rtl">كما هو معتاد، قمنا بتقسيم المشكلة الكلية إلى مشكلة
التنبؤ ومشكلة التحكم. تعتبر طرق</span> TD <span dir="rtl">بدائل لطرق
مونت كارلو</span> (Monte Carlo Methods) <span dir="rtl">في حل مشكلة
التنبؤ. في كلا الحالتين، يتم التوسع إلى مشكلة التحكم عبر فكرة تكرار
السياسة العامة</span> <span dir="rtl">(</span>Generalized
<span dir="rtl"></span>Policy Iteration (GPI)<span dir="rtl">)</span>
<span dir="rtl">التي قمنا بتجريدها من البرمجة الديناميكية</span>
<span dir="rtl">(</span>Dynamic Programming<span dir="rtl">) وهذه الفكرة
هي أن السياسات التقريبية ودوال القيمة</span> (Value Functions)
<span dir="rtl">يجب أن تتفاعل بطريقة تجعلها تتحرك نحو قيمها
المثلى</span>.

<span dir="rtl">واحدة من العمليتين اللتين تشكلان</span> GPI
<span dir="rtl">تدفع دالة القيمة</span> (Value Function)
<span dir="rtl">للتنبؤ بدقة بالعائدات للسياسة الحالية؛ هذه هي مشكلة
التنبؤ. العملية الأخرى تدفع السياسة لتحسين محليًا (مثلاً،  
لتكون</span> "ε-greedy"<span dir="rtl">)</span> <span dir="rtl">بالنسبة
لدالة القيمة الحالية. عندما تستند العملية الأولى إلى الخبرة، تنشأ مشكلة
تتعلق بالحفاظ على الاستكشاف الكافي. يمكننا تصنيف طرق التحكم
باستخدام</span> TD <span dir="rtl">حسب ما إذا كانت تتعامل مع هذه المشكلة
باستخدام نهج يتبع السياسة</span> (On-policy) <span dir="rtl">أو نهج  
لا يتبع السياسة</span> (Off-policy)<span dir="rtl">.</span>
<span dir="rtl">سارسا</span> (Sarsa) <span dir="rtl">هو طريقة تتبع
السياسة</span> <span dir="rtl">(</span>On-policy<span dir="rtl">)  
و</span>Q-learning <span dir="rtl">هو طريقة لا تتبع السياسة</span>
(Off-policy)<span dir="rtl">.</span> Expected Sarsa <span dir="rtl">هو
أيضًا طريقة لا تتبع السياسة</span> (Off-policy) <span dir="rtl">كما
نقدمها هنا. هناك طريقة ثالثة يمكن توسيع طرق</span> TD
<span dir="rtl">بها للتحكم والتي لم نتناولها في هذا الفصل، تُسمى طرق
"الممثل-الناقد</span> (Actor–Critic Methods)<span dir="rtl">.</span>
<span dir="rtl">يتم تناول هذه الطرق بشكل كامل في الفصل 13</span>.

<span dir="rtl">الطرق المقدمة في هذا الفصل هي اليوم أكثر طرق التعليم
المعزز استخدامًا. يرجع ذلك على الأرجح إلى بساطتها الكبيرة: يمكن تطبيقها
على الإنترنت، مع الحد الأدنى من الحسابات، على الخبرة الناتجة من التفاعل
مع البيئة</span> (Environment)<span dir="rtl">؛ يمكن التعبير عنها تقريبًا
بالكامل بمعادلات فردية يمكن تنفيذها ببرامج حاسوبية صغيرة. في الفصول
القادمة، سنقوم بتوسيع هذه الخوارزميات، مما يجعلها أكثر تعقيدًا بقليل
وأكثر قوة بشكل كبير. جميع الخوارزميات الجديدة ستحتفظ بجوهر تلك المقدمة
هنا: ستكون قادرة على معالجة الخبرة على الإنترنت، مع حسابات نسبية قليلة،
وستدفعها أخطاء</span> TD<span dir="rtl">.</span> <span dir="rtl">الحالات
الخاصة من طرق</span> TD <span dir="rtl">التي تم تقديمها في هذا الفصل يجب
أن تُسمى بشكل صحيح طرق</span> TD <span dir="rtl">الجدولية ذات الخطوة
الواحدة</span> (One-step, Tabular, Model-free TD
Methods)<span dir="rtl">.</span> <span dir="rtl">في الفصلين التاليين،
سنقوم بتوسيعها إلى أشكال ذات</span> n <span dir="rtl">خطوات</span>
(<span dir="rtl">رابط إلى طرق مونت كارلو</span> (Monte Carlo Methods))
<span dir="rtl">وأشكال تشمل نموذجًا للبيئة</span> (<span dir="rtl">رابط
إلى التخطيط</span> (Planning) <span dir="rtl">والبرمجة
الديناميكية</span> <span dir="rtl">(</span>Dynamic
<span dir="rtl"></span>Programming<span dir="rtl">)</span>)
<span dir="rtl">ثم، في الجزء الثاني من الكتاب، سنقوم بتوسيعها إلى أشكال
مختلفة من تقريب الدوال بدلاً من الجداول</span> (<span dir="rtl">رابط إلى
التعليم العميق</span> (Deep Learning) <span dir="rtl">والشبكات العصبية
الاصطناعية</span> Artificial Neural Networks)<span dir="rtl">)</span>.

<span dir="rtl">أخيرًا، في هذا الفصل ناقشنا طرق</span> TD
<span dir="rtl">بالكامل في سياق مشكلات التعليم المعزز</span>
<span dir="rtl">(</span>Reinforcement <span dir="rtl"></span>Learning
Problems<span dir="rtl">)، ولكن طرق</span> TD <span dir="rtl">في الواقع
أكثر عمومية من ذلك. إنها طرق عامة لتعلم كيفية إجراء التنبؤات الطويلة
الأجل حول الأنظمة الديناميكية</span> (Dynamical
Systems)<span dir="rtl">.</span> <span dir="rtl">على سبيل المثال، قد
تكون طرق</span> TD <span dir="rtl">ذات صلة بتنبؤ البيانات المالية، أعمار
الأفراد، نتائج الانتخابات، أنماط الطقس، سلوك الحيوانات، الطلبات على
محطات الطاقة، أو مشتريات العملاء. كان فقط عندما تم تحليل طرق</span> TD
<span dir="rtl">كطرق تنبؤية بحتة، مستقلة عن استخدامها في التعليم المعزز،
أن خصائصها النظرية بدأت تُفهم بشكل جيد. ومع ذلك، لم يتم استكشاف هذه
التطبيقات المحتملة الأخرى لطرق تعلم</span> TD <span dir="rtl">بشكل مكثف
بعد</span>.

**<span dir="rtl">الفصل السابع:</span>**

**<span dir="rtl">التقدير باستخدام</span>** (n-step Bootstrapping)
**n-step**

<span dir="rtl">في هذا الفصل، نقوم بتوحيد طرق مونت كارلو</span> (MC)
<span dir="rtl">وطرق الفرق الزمني</span> (TD) <span dir="rtl">ذات الخطوة
الواحدة التي تم تقديمها في الفصلين السابقين. لا تكون طرق</span> MC
<span dir="rtl">ولا طرق</span> TD <span dir="rtl">ذات الخطوة الواحدة
دائمًا الأفضل. في هذا الفصل، نقدم طرق</span> TD
<span dir="rtl">ذات</span> n <span dir="rtl">خطوة</span> (n-step TD)
<span dir="rtl">التي تعمم كلا الطريقتين بحيث يمكن الانتقال بسلاسة من
واحدة إلى أخرى حسب الحاجة لتلبية متطلبات مهمة معينة. تمتد طرق</span> n
<span dir="rtl">خطوة عبر طيف مع طرق</span> MC <span dir="rtl">في أحد
الطرفين وطرق</span> TD <span dir="rtl">ذات الخطوة الواحدة في الطرف
الآخر. غالبًا ما تكون أفضل الطرق متوسطة بين النقيضين</span>.

<span dir="rtl">طريقة أخرى للنظر إلى فوائد طرق</span>
n<span dir="rtl">-خطوة هي أنها تحررك من قيد خطوة الزمن</span> (Time
Step)<span dir="rtl">.</span> <span dir="rtl">مع طرق</span> TD
<span dir="rtl">ذات الخطوة الواحدة، تحدد نفس خطوة الزمن مدى تكرار تغيير
الإجراء والفترة الزمنية التي يتم فيها التقدير الذاتي</span>
(Bootstrapping)<span dir="rtl">.</span> <span dir="rtl">في العديد من
التطبيقات، يرغب المرء في تحديث الإجراء بسرعة لتأخذ في الاعتبار أي
تغييرات حدثت، لكن التقدير الذاتي يعمل بشكل أفضل إذا كان على مدى فترة
زمنية حدث فيها تغيير ملحوظ وقابل للتعرف عليه في الحالة. مع طرق</span> TD
<span dir="rtl">ذات الخطوة الواحدة، تكون هذه الفترات الزمنية هي نفسها،
لذا يجب إجراء تسوية. تمكن طرق</span> n<span dir="rtl">-خطوة التقدير
الذاتي من الحدوث عبر خطوات متعددة، مما يحررنا من قيد خطوة الزمن
الواحدة</span>.

<span dir="rtl">فكرة طرق</span> n<span dir="rtl">-خطوة تُستخدم عادةً
كمقدمة لفكرة الخوارزمية لتتبع الأهلية  
</span>(Eligibility Traces) <span dir="rtl">(الفصل 12)، التي تمكن
التقدير الذاتي على مدى فترات زمنية متعددة في وقت واحد. هنا، نعتبر فكرة
التقدير باستخدام</span> n<span dir="rtl">-خطوة بمفردها، مؤجلين معالجة
آليات تتبع الأهلية إلى وقت لاحق. يسمح لنا هذا بفصل القضايا بشكل أفضل،
والتعامل مع أكبر عدد ممكن منها في إعداد  
</span>n<span dir="rtl">-خطوة الأبسط</span>.

<span dir="rtl">كما هو معتاد، نبدأ بمشكلة التنبؤ ثم بمشكلة التحكم. أي،
نبدأ بدراسة كيفية مساعدة طرق</span> n<span dir="rtl">-خطوة في التنبؤ
بالعائدات كدالة للحالة لسياسة ثابتة (أي، في تقدير</span>
$`v\pi`$<span dir="rtl">)</span> <span dir="rtl">ثم نمدد الأفكار إلى قيم
الإجراءات وطرق التحكم</span>.

<img src="./media/image64.png"
style="width:6.26806in;height:5.10625in" /><span dir="rtl">**الشكل
7.1**: مخططات التقدير لطرق</span> n_step <span dir="rtl">تشكل هذه الطرق
طيفًا يمتد من طرق</span> TD <span dir="rtl">ذات الخطوة الواحدة إلى طرق
مونت كارلو</span>.

<span dir="rtl">الطرق التي تستخدم تحديثات ذات</span>
n<span dir="rtl">-خطوة تظل طرق</span> TD <span dir="rtl">لأنها لا تزال
تغير تقديرًا سابقًا بناءً على كيفية اختلافه عن تقدير لاحق. الآن، التقدير
اللاحق ليس بعد خطوة واحدة، بل بعد</span> n<span dir="rtl">-خطوة. الطرق
التي يمتد فيها الفرق الزمني عبر</span> n<span dir="rtl">-خطوات تُسمى
طرق</span> TD <span dir="rtl">ذات</span> n<span dir="rtl">-خطوة.
الطرق</span> TD <span dir="rtl">التي تم تقديمها في الفصل السابق كانت
تستخدم تحديثات ذات خطوة واحدة، ولهذا السبب أسميناها طرق</span> TD
<span dir="rtl">ذات خطوة واحدة</span>.

<span dir="rtl">بشكل أكثر رسمية، اعتبر تحديث تقدير قيمة الحالة</span>
$`St`$ <span dir="rtl"></span>​ <span dir="rtl">كنتيجة لتسلسل
الحالة–المكافأة،</span>
$`S_{t},R_{t + 1},S_{t + 1},R_{t + 2},\ldots,R_{T},S_{T}`$
<span dir="rtl"></span>​, <span dir="rtl">(مع إغفال الأفعال). نعلم أن في
تحديثات مونت كارلو، يتم تحديث تقدير</span> $`v\pi(St)`$
<span dir="rtl">في اتجاه العائد الكامل</span>:

``` math
G_{t} = R_{t + 1} + \gamma R_{t + 2} + \gamma^{2}R_{t + 3} + \cdots + \gamma^{T - t - 1}R_{T}
```

<span dir="rtl">حيث</span> $`T`$ <span dir="rtl">هو آخر خطوة زمنية في
الحلقة. دعنا نسمي هذه الكمية هدف التحديث. بينما في تحديثات مونت كارلو
يكون الهدف هو العائد، في التحديثات ذات الخطوة الواحدة يكون الهدف هو
المكافأة الأولى زائد تقدير القيمة المخصوم للحالة التالية، والذي نسميه
العائد ذو الخطوة الواحدة</span>:<span dir="rtl">  
</span>
``` math
G_{t:t + 1} = R_{t + 1} + \gamma V_{t}\left( S_{t + 1} \right)
```

<span dir="rtl">حيث</span> $`V_{t}:S \rightarrow R`$ <span dir="rtl">هو
التقدير في الوقت</span> $`t`$ <span dir="rtl">لـ</span>
$`v\pi`$<span dir="rtl">.</span> <span dir="rtl">المؤشرات الفرعية
على</span> $`Gt:t + 1`$ <span dir="rtl">تشير إلى أنه عائد مقطوع
للوقت</span> $`t`$ <span dir="rtl">باستخدام المكافآت حتى الوقت</span>
$`t + 1`$<span dir="rtl">، مع تقدير مخصوم</span>
$`\gamma V_{t}\left( S_{t + 1} \right)`$

<span dir="rtl">الذي يحل محل المصطلحات الأخرى</span>
$`\gamma R_{t + 2} + \gamma^{2}R_{t + 3} + \cdots + \gamma^{T - t - 1}R_{T}`$
<span dir="rtl"></span>​<span dir="rtl">من العائد الكامل، كما نوقش في
الفصل السابق. نقطتنا الآن هي أن هذه الفكرة لها نفس المنطق بعد خطوتين كما
هي بعد خطوة واحدة. الهدف لتحديث ذي خطوتين هو العائد ذو الخطوتين</span>:

``` math
G_{t:t + 2} = R_{t + 1} + \gamma R_{t + 2} + \gamma^{2}V_{t + 1}\left( S_{t + 2} \right)
```

<span dir="rtl">حيث الآن</span>
$`\gamma^{2}V_{t + 1}\left( S_{t + 2} \right)`$ <span dir="rtl">يصحح
لعدم وجود المصطلحات</span>
$`\gamma^{2}R_{t + 3} + \gamma^{3}R_{t + 4} + \cdots + \gamma^{T - t - 1}R_{T}`$
<span dir="rtl">وبالمثل، الهدف من تحديث</span> n-step
<span dir="rtl">عشوائي هو العائد</span> n-step<span dir="rtl">:</span>

``` math
G_{t:t + n} = R_{t + 1} + \gamma R_{t + 2} + \cdots + \gamma^{n - 1}R_{t + n} + \gamma^{n}V_{t + n - 1}\left( S_{t + n} \right)
```

**<span dir="rtl">لكل من</span>** $`\mathbf{n}`$
**<span dir="rtl">و</span>**$`\mathbf{t}`$
**<span dir="rtl">بحيث</span>**
$`\mathbf{0n \geq 0\ }`$**<span dir="rtl">و</span>**$`\mathbf{0 \leq t < T - n0\ }`$**<span dir="rtl">:</span>**

<span dir="rtl">يمكن اعتبار جميع العوائد المكونة من</span>
n<span dir="rtl">-خطوات</span> (n-step returns) <span dir="rtl">كأنها
تقريبات للعائد الكامل</span> (full return)<span dir="rtl">، يتم اقتطاعها
بعد</span> n<span dir="rtl">-خطوة ثم تصحيحها للمصطلحات المفقودة المتبقية
بواسطة</span> $`Vt + n - 1(St + n)`$<span dir="rtl">.</span>

<span dir="rtl">  
</span> $`Tt + n \geq T`$<span dir="rtl">(إذا كانت العوائد</span> n-step
<span dir="rtl">تمتد إلى ما بعد أو تتجاوز الانتهاء)، فإن جميع المصطلحات
المفقودة تؤخذ كصفر، ويتم تعريف عائد</span> n-step <span dir="rtl">ليكون
مساوياً للعائد الكامل العادي  
(</span>$`Gt:t + n`$​ <span dir="rtl">إذا كان</span>
$`t + n \geq Tt\  + \ n`$<span dir="rtl">).</span>

<span dir="rtl">لاحظ أن العوائد</span> n-step <span dir="rtl">لـ</span>
$`n > 1n\  > \ 1n > 1`$ <span dir="rtl">تتضمن مكافآت</span> (rewards)
<span dir="rtl">وحالات</span> (states) <span dir="rtl">مستقبلية غير
متاحة عند الانتقال من</span> $`t`$ <span dir="rtl">إلى</span> $`t + 1`$
<span dir="rtl">لا يمكن لأي خوارزمية حقيقية استخدام عائد  
</span>n-step <span dir="rtl">حتى بعد أن ترى</span> $`Rt + n`$
<span dir="rtl">وتحسب</span> $`Vt + n - 1V`$​<span dir="rtl">.</span>
<span dir="rtl">أول مرة تتوفر فيها هذه القيم هي</span>
$`t + n`$<span dir="rtl">.</span> <span dir="rtl">وبالتالي، فإن خوارزمية
تعلم قيمة الحالة الطبيعية</span> <span dir="rtl">(</span>natural
state-value learning <span dir="rtl"></span>algorithm<span dir="rtl">)
لاستخدام عوائد</span> n-step <span dir="rtl">هي</span>:

``` math
V_{t + n}\left( S_{t} \right) = V_{t + n - 1}\left( S_{t} \right) + \alpha\left( G_{t:t + n} - V_{t + n - 1}\left( S_{t} \right) \right),\quad 0 \leq t < T
```

<span dir="rtl">بينما تظل قيم جميع الحالات الأخرى دون تغيير:</span>
$`Vt + n(s) = Vt + n - 1(s)`$<span dir="rtl">، لجميع</span>
$`s \neq St`$ <span dir="rtl"></span>​ <span dir="rtl">نطلق على هذه
الخوارزمية اسم **الـ** </span>**n-step TD**<span dir="rtl">.</span>
<span dir="rtl">لاحظ أنه لا يتم إجراء أي تغييرات على الإطلاق خلال
أول</span> $`n - 1n`$ <span dir="rtl">خطوات من كل حلقة. لتعويض ذلك، يتم
إجراء عدد متساوٍ من التحديثات الإضافية في نهاية الحلقة، بعد الانتهاء وقبل
بدء الحلقة التالية. يتم تقديم الكود الوهمي الكامل في الصندوق في الصفحة
التالية</span>.

<span dir="rtl">**<u>التمرين 7.1:</u>** في الفصل 6، لاحظنا أن خطأ مونت
كارلو يمكن كتابته كمجموع لأخطاء</span> TD (6.6) <span dir="rtl">إذا لم
تتغير تقديرات القيم من خطوة إلى خطوة. أظهر أن الخطأ المكون من</span>
n-step <span dir="rtl">المستخدم في (7.2) يمكن أيضًا كتابته كمجموع
لأخطاء</span> TD <span dir="rtl">(مرة أخرى إذا لم تتغير تقديرات القيم)
مما يعمم النتيجة السابقة</span>.

<span dir="rtl"><u>**التمرين 7.2** (برمجة)</u>: باستخدام طريقة</span>
n-step<span dir="rtl">، تتغير تقديرات القيم من خطوة إلى خطوة، لذا فإن
خوارزمية تستخدم مجموع أخطاء</span> TD <span dir="rtl">(انظر التمرين
السابق) في</span>...

**<span dir="rtl">الـ</span> n-step TD <span dir="rtl">لتقدير
</span>**$`\mathbf{V \approx}\mathbf{v}_{\mathbf{\pi}}`$

<img src="./media/image65.png"
style="width:6.35542in;height:4.1054in" />

<span dir="rtl">المكان الذي يحدث فيه الخطأ</span> (error)
<span dir="rtl">في (7.2) سيكون في الواقع خوارزمية</span> (algorithm)
<span dir="rtl">مختلفة قليلاً. هل ستكون خوارزمية</span> (algorithm)
<span dir="rtl">أفضل أم أسوأ؟ قم بتصميم وبرمجة تجربة صغيرة</span>
(experiment) <span dir="rtl">للإجابة على هذا السؤال تجريبيًا</span>.

<span dir="rtl">تستخدم عوائد الـ</span>-n <span dir="rtl">خطوة</span>
(n-step return) <span dir="rtl">دالة القيمة</span> $`Vt + n - 1`$
<span dir="rtl"></span>​ <span dir="rtl">لتصحيح المكافآت</span> (rewards)
<span dir="rtl">المفقودة بعد</span> $`Rt + n`$<span dir="rtl">.</span>
<span dir="rtl">خاصية هامة لعوائد ال</span>-n<span dir="rtl">خطوة هي أن
توقعها</span> (expectation) <span dir="rtl">مضمون أن يكون تقديرًا أفضل
لـ</span> $`v\pi`$ <span dir="rtl">من</span> $`Vt + n - 1`$
<span dir="rtl"></span>​ <span dir="rtl">من حيث أسوأ حالة
(</span>worst-state <span dir="rtl"></span>sense<span dir="rtl">). بمعنى
آخر، يُضمن أن أسوأ خطأ</span> (error) <span dir="rtl">في التوقع لعائد
الـ</span>-n<span dir="rtl">خطوة أقل من أو يساوي</span> $`\gamma n`$
<span dir="rtl">مرات أسوأ خطأ تحت</span> $`Vt + n - 1`$

``` math
max_{s}\left| E^{\pi}\left\lbrack G_{t:t + n} \middle| S_{t} = s \right\rbrack - v^{\pi}(s) \right| \leq \gamma^{n}max_{s}\left| V_{t + n - 1}(s) - v^{\pi}(s) \right|
```

<span dir="rtl">لجميع</span> n≥0n<span dir="rtl">.</span>
<span dir="rtl">تُسمى هذه الخاصية بخاصية تقليل الخطأ</span> (error
reduction property) <span dir="rtl">لعوائد الـ</span> <span dir="rtl">  
</span>(n-step returns)<span dir="rtl">.</span> <span dir="rtl">بسبب
خاصية تقليل الخطأ، يمكن إثبات رسميًا أن جميع طرق</span> TD
<span dir="rtl">ذات  
الـ</span> <span dir="rtl">خطوة</span>n-<span dir="rtl">.</span> (n-step
TD methods) <span dir="rtl">تتقارب إلى التنبؤات الصحيحة تحت ظروف تقنية
مناسبة. لذا، تشكل طرق</span> TD <span dir="rtl">ذات
الـ</span>-n<span dir="rtl">خطوة</span> (n-step TD methods)
<span dir="rtl">عائلة من الأساليب السليمة</span> (sound
methods)<span dir="rtl">، مع أساليب</span> TD <span dir="rtl">ذات الخطوة
الواحدة</span> (one-step TD methods) <span dir="rtl">**وطرق** **مونت**
**كارلو**</span> (**Monte** **Carlo** **methods**)
<span dir="rtl">كأعضاء متطرفين</span> (extreme
members)<span dir="rtl">.</span>

<span dir="rtl">فكر في استخدام طرق</span> TD <span dir="rtl">ذات</span>
n-step <span dir="rtl"></span>(n-step TD methods) <span dir="rtl">على
مهمة المشي العشوائي</span> (random walk) <span dir="rtl">ذات الخمسة
حالات</span> (5-state random walk task) <span dir="rtl">الموصوفة في
**مثال** **6.2** (الصفحة 125)</span>.

<span dir="rtl">افترض أن الحلقة الأولى تقدمت مباشرة من الحالة
الوسطى،</span> $`C`$<span dir="rtl">، إلى اليمين، مرورًا بـ</span> $`D`$
<span dir="rtl">و</span>$`E`$<span dir="rtl">، ثم انتهت على اليمين مع
عائد</span> (return) <span dir="rtl">قدره 1. تذكر أن تقديرات القيم لجميع
الحالات بدأت بقيمة متوسطة،</span> $`V(s) = 0.5`$<span dir="rtl">.</span>
<span dir="rtl">كنتيجة لهذه التجربة، ستقوم الطريقة ذات الخطوة
الواحدة</span> (one-step method) <span dir="rtl">بتغيير تقدير الحالة
الأخيرة فقط،</span> V(E)<span dir="rtl">، والذي سيتم زيادته نحو 1، وهو
العائد الملاحظ. من ناحية أخرى، ستقوم الطريقة ذات الخطوتين</span>
(two-step method) <span dir="rtl">بزيادة قيم الحالتين السابقتين
للإنهاء</span>: <span dir="rtl"></span>$`V(D)`$
<span dir="rtl">و</span>$`V(E)`$<span dir="rtl">، حيث سيتم زيادتهما
نحو 1. ستقوم الطريقة ذات الثلاث خطوات</span>
<span dir="rtl">(</span>three-step
<span dir="rtl"></span>method<span dir="rtl">)</span> <span dir="rtl">أو
أي طريقة</span> -n<span dir="rtl">خطوة أخرى لـ</span> $`n > 2`$
<span dir="rtl">بزيادة قيم جميع الحالات الثلاثة التي تمت زيارتها نحو 1،
جميعها بنفس المقدار</span>.

<span dir="rtl">أي قيمة لـ</span> $`n`$ <span dir="rtl">هي الأفضل؟ تعرض
**الشكل** 7.2 نتائج اختبار تجريبي بسيط لعملية المشي العشوائي الأكبر، مع
19 حالة بدلاً من 5 (ومع نتيجة -1 على اليسار، وجميع القيم مبدئيًا تساوي 0)،
والتي نستخدمها كمثال متواصل في هذا الفصل. يتم عرض النتائج لطرق</span> TD
<span dir="rtl">ذات الـ</span> -n<span dir="rtl">خطوة مع مجموعة من القيم
لـ</span> $`n`$ <span dir="rtl">و</span>
$`\alpha`$<span dir="rtl">.</span> <span dir="rtl">مقياس الأداء</span>
(performance measure) <span dir="rtl">لكل إعداد من الإعدادات، الموضح على
المحور العمودي، هو الجذر التربيعي</span> (square-root)
<span dir="rtl">للخطأ المربع المتوسط</span>
<span dir="rtl">(</span>average <span dir="rtl"></span>squared
error<span dir="rtl">) بين التنبؤات في نهاية الحلقة لـ 19 حالة وقيمها
الحقيقية، ثم يتم حسابه كمتوسط عبر أول 10 حلقات و100 تكرار للتجربة
الكاملة (تم استخدام نفس مجموعات المشي لجميع إعدادات المعلمات). لاحظ أن
الأساليب ذات القيمة المتوسطة لـ</span> $`n`$ <span dir="rtl">كانت
الأفضل. يوضح هذا كيف أن تعميم طرق</span> TD <span dir="rtl">وطرق مونت
كارلو</span> (Monte Carlo methods) <span dir="rtl">إلى طرق</span> n-step
<span dir="rtl">يمكن أن يؤدي إلى أداء أفضل من أي من الطريقتين
المتطرفتين</span>.

<img src="./media/image66.png"
style="width:6.26806in;height:3.65625in" />

<span dir="rtl">**الشكل 7.2:** أداء طرق</span> TD <span dir="rtl">ذات
الـ خطوة</span> n- <span dir="rtl">كدالة لـ</span> α<span dir="rtl">،
لقيم مختلفة من</span> n<span dir="rtl">، على مهمة المشي العشوائي ذات الـ
19 حالة (مثال 7.1)</span>.

**<span dir="rtl"><u>التمرين 7.3</u>:</span>** <span dir="rtl">لماذا
تعتقد أنه تم استخدام مهمة المشي العشوائي الأكبر (19 حالة بدلاً من 5) في
الأمثلة في هذا الفصل؟ هل كان من الممكن أن يؤدي استخدام مشي عشوائي أصغر
إلى تحويل الأفضلية إلى قيمة مختلفة لـ</span> $`n`$<span dir="rtl">؟ ماذا
عن تغيير النتيجة على الجانب الأيسر من 0 إلى 1- الذي تم إجراؤه في المشي
الأكبر؟ هل تعتقد أن هذا أحدث فرقًا في أفضل قيمة لـ</span>
$`n`$<span dir="rtl">؟</span>

**<u>7.2 <span dir="rtl">سارسا</span>
step<span dir="rtl">\_</span>n</u>**

**<span dir="rtl">كيف يمكن استخدام طرق</span> n-step <span dir="rtl">ليس
فقط للتنبؤ، ولكن للتحكم؟</span>**

<span dir="rtl">في هذا القسم، نعرض كيفية دمج طرق</span> n-step
<span dir="rtl">مع</span> Sarsa <span dir="rtl">بطريقة بسيطة لإنتاج
طريقة التحكم في</span> TD <span dir="rtl">وفقًا للسياسة</span> (on-policy
TD control)<span dir="rtl">.</span> <span dir="rtl">نطلق على النسخة ذات
الـ</span> n-step <span dir="rtl">من</span> Sarsa
<span dir="rtl">اسم</span> **n-step Sarsa**<span dir="rtl">، ونسمي
النسخة الأصلية التي قدمناها في الفصل السابق</span> **one-step Sarsa
<span dir="rtl"></span>**<span dir="rtl">أو</span>
**Sarsa(0)**<span dir="rtl">.</span>

<span dir="rtl">الفكرة الرئيسية هي ببساطة استبدال الحالات بالإجراءات
(حالة–إجراء) ثم استخدام سياسة</span> ε-greedy<span dir="rtl">. المخططات
الاحتياطية لـ</span> n-step Sarsa <span dir="rtl">(الموضحة في الشكل
7.3)، مثل تلك الخاصة بـ</span> n-step <span dir="rtl"></span>TD
<span dir="rtl">(الشكل 7.1)، هي سلاسل من الحالات والإجراءات المتناوبة،
باستثناء أن مخططات</span> Sarsa <span dir="rtl">تبدأ وتنتهي دائمًا بإجراء
وليس حالة. نعيد تعريف عوائد الـ</span> n-step (update targets)
<span dir="rtl">من حيث تقديرات قيم الإجراءات</span>.

``` math
G_{t:t + n} = R_{t + 1} + \gamma R_{t + 2} + \cdots + \gamma^{n - 1}R_{t + n} + \gamma^{n}Q_{t + n - 1}\left( S_{t + n},A_{t + n} \right)
```

<span dir="rtl">حيث</span> $`Gt:t + n`$ <span dir="rtl"></span>​
<span dir="rtl">إذا كان</span>
$`t + n = Tt\  + \ n`$<span dir="rtl">.</span>
<span dir="rtl">الخوارزمية الطبيعية بعد ذلك هي</span>

``` math
Q_{t + n}\left( S_{t},A_{t} \right) = Q_{t + n - 1}\left( S_{t},A_{t} \right) + \alpha\left\lbrack G_{t:t + n} - Q_{t + n - 1}\left( S_{t},A_{t} \right) \right\rbrack,\quad 0 \leq t < T,
```

<span dir="rtl">بينما تبقى قيم جميع الحالات الأخرى دون تغيير:</span>
$`Qt + n - 1(s,a)`$ <span dir="rtl">لجميع</span> $`s`$
<span dir="rtl">و</span>$`a`$ <span dir="rtl">بحيث</span>
$`s \neq St\ `$​ <span dir="rtl">أو</span> $`a \neq At`$
<span dir="rtl"></span>​ <span dir="rtl">هذه هي الخوارزمية التي نسميها
**سارسا** ذات الخطوات</span> n<span dir="rtl">.</span>
<span dir="rtl">يُظهر الشكل التالي (الشكل 7.4) مثالاً يوضح لماذا يمكن أن
تسرع هذه الطريقة في عملية التعليم مقارنة بالأساليب ذات الخطوة
الواحدة</span>.

<img src="./media/image67.png"
style="width:6.26806in;height:3.71597in" />

<span dir="rtl">الشكل 7.3: مخططات النسخ الاحتياطي لطيف الأساليب
ذات</span> **n-step <span dir="rtl"></span>**<span dir="rtl">لقيم
**الحالة–الإجراء  
(**</span>**State–Action
Values<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">تتراوح هذه الأساليب من التحديث ذو الخطوة الواحدة لـ
**سارسا** </span>**(0) <span dir="rtl"></span>(Sarsa(0))
<span dir="rtl"></span>** <span dir="rtl">إلى التحديث حتى النهاية لطريقة
**مونت كارلو** </span>**(Monte Carlo)**<span dir="rtl">.</span>
<span dir="rtl">بين هذين النهجين توجد التحديثات ذات</span>
**n-step**<span dir="rtl">، والتي تستند إلى</span> n
<span dir="rtl">خطوة من المكافآت الحقيقية وقيمة **الحالة–الإجراء**
</span>**(State–Action) <span dir="rtl"></span>**<span dir="rtl">التالية
المقدرة في الخطوة</span> n<span dir="rtl">، مع أخذ جميع هذه القيم بخصم
مناسب. في أقصى اليمين يوجد مخطط النسخ الاحتياطي لـ **سارسا المتوقع
ذو**</span> **n-step <span dir="rtl">(</span>n-step Expected
Sarsa<span dir="rtl">)</span>**.

**<span dir="rtl">سارسا ذو</span> n-step** <span dir="rtl">لتقدير</span>
$`Q \approx q*\ \ or\ \ q_{\pi}`$

<img src="./media/image68.png"
style="width:6.26806in;height:5.14444in" />

<img src="./media/image69.png"
style="width:6.26806in;height:1.7625in" />

<span dir="rtl">الشكل 7.4: مثال على العالم الشبكي</span> **(Gridworld)
<span dir="rtl"></span>**<span dir="rtl">يوضح تسريع تعلم
**السياسة**</span> **(Policy)** <span dir="rtl">بفضل استخدام
أساليب</span> **n-step**<span dir="rtl">.</span> <span dir="rtl">يظهر
اللوح الأول المسار الذي اتبعه **العميل**</span> **(Agent)**
<span dir="rtl">في حلقة واحدة، وينتهي عند موقع ذو مكافأة عالية، مُشار
إليه بالحرف</span> $`\mathbf{G}`$**<span dir="rtl">.</span>**
<span dir="rtl">في هذا المثال، كانت جميع القيم في البداية 0، وكانت جميع
المكافآت صفرية باستثناء مكافأة إيجابية عند</span>
$`\mathbf{G}`$**<span dir="rtl">.</span>** <span dir="rtl">تُظهر الأسهم
في اللوحين الآخرين القيم التي تم تعزيزها نتيجة لهذا المسار بواسطة أساليب
**سارسا**</span> **(Sarsa)** <span dir="rtl">ذات الخطوة الواحدة  
و</span>**n-step**<span dir="rtl">. أسلوب الخطوة الواحدة يعزز فقط
الإجراء الأخير من سلسلة الإجراءات التي أدت إلى المكافأة العالية، في حين
أن أسلوب</span> **n-step <span dir="rtl"></span>**<span dir="rtl">يعزز
آخر</span> $`\mathbf{n}`$ <span dir="rtl">إجراءات من السلسلة، مما يعني
أن هناك تعلمًا أكبر بكثير من الحلقة الواحدة</span>.

<span dir="rtl">**<u>التمرين 7.4</u>**:</span> <span dir="rtl">أثبت أن
العائد ذو</span> **n-step <span dir="rtl"></span>**<span dir="rtl">في
**سارسا**</span> **(Sarsa)** <span dir="rtl">يمكن كتابته بدقة من حيث
خطأ</span> **TD** <span dir="rtl">الجديد كما يلي</span>:

``` math
G_{t:t + n} = Q_{t - 1}\left( S_{t},A_{t} \right) + \sum_{k = t}^{\min(t + n,T) - 1}{\gamma^{k - t}\left\lbrack R_{k + 1} + \gamma Q_{k}\left( S_{k + 1},A_{k + 1} \right) - Q_{k - 1}\left( S_{k},A_{k} \right) \right\rbrack}
```

<span dir="rtl">ماذا عن **سارسا المتوقع** </span>**(Expected
Sarsa)**<span dir="rtl">؟ يُظهر الشكل 7.3 على اليمين مخطط النسخ الاحتياطي
لإصدار</span> **n-step <span dir="rtl"></span>**<span dir="rtl">من
**سارسا المتوقع** </span>**(Expected Sarsa)**<span dir="rtl">.</span>
<span dir="rtl">يتكون هذا المخطط من سلسلة خطية من الإجراءات</span>
**(Actions) <span dir="rtl"></span>**<span dir="rtl">والحالات</span>
**(States)<span dir="rtl">،</span>** <span dir="rtl">تمامًا كما هو الحال
في</span> **n-step <span dir="rtl">سارسا</span>
<span dir="rtl">(</span>n-step
<span dir="rtl"></span>Sarsa<span dir="rtl">)</span>**<span dir="rtl">،
باستثناء أن العنصر الأخير فيه هو تفرع يشمل جميع الاحتمالات للإجراءات،
مُرجحة دائمًا باحتمالاتها وفقًا لـ **السياسة**</span>
**<span dir="rtl">(</span>Policy<span dir="rtl">)
</span>**$`\mathbf{\pi}`$<span dir="rtl">.</span> <span dir="rtl">يمكن
وصف هذه الخوارزمية بنفس المعادلة المستخدمة في  
</span>**n-step <span dir="rtl">سارسا</span> (n-step Sarsa)**
<span dir="rtl">(أعلاه) باستثناء أن العائد ذو</span> **n-step
<span dir="rtl"></span>**<span dir="rtl">يُعاد تعريفه كما يلي</span>.

``` math
G_{t:t + n} = R_{t + 1} + \cdots + \gamma^{n - 1}R_{t + n} + \gamma^{n}{\overline{V}}_{t + n - 1}\left( S_{t + n} \right),\quad t + n < T
```

<span dir="rtl">(حيث</span> $`Gt:t + n`$ <span dir="rtl">إذا كان</span>
$`t + n`$<span dir="rtl">)</span> <span dir="rtl">حيث تمثل</span>
$`V‾t(s)`$ <span dir="rtl">القيمة التقريبية المتوقعة للحالة</span>
$`s`$<span dir="rtl">، باستخدام القيم التقديرية للإجراءات في
الزمن</span> $`t`$<span dir="rtl">، بموجب السياسة المستهدفة</span>.

``` math
{\overline{V}}_{t}(s) = \sum_{a}^{}{\pi\left( a \middle| s \right)Q_{t}(s,a)},\quad\text{for all }s \in S.
```

<span dir="rtl">تُستخدم القيم التقريبية المتوقعة في تطوير العديد من
أساليب **قيمة الإجراء**</span> **<span dir="rtl">(</span>Action-Value
<span dir="rtl"></span>Methods<span dir="rtl">)
</span>**<span dir="rtl">في بقية هذا الكتاب. إذا كانت الحالة</span>
$`s`$ <span dir="rtl">نهائية، فإن قيمتها التقريبية المتوقعة تُعرّف بأنها
0</span>.

**<u>7.3 <span dir="rtl">التعليم خارج السياسة</span> (Off-Policy)
<span dir="rtl">ذو</span> n-step</u>**

<span dir="rtl">تذكر أن التعليم خارج السياسة</span> **(Off-Policy
Learning) <span dir="rtl"></span>**<span dir="rtl">هو تعلم دالة القيمة  
**(**</span>**Value Function<span dir="rtl">)</span>**
<span dir="rtl">لسياسة معينة</span> $`\pi`$ <span dir="rtl">أثناء اتباع
سياسة أخرى</span> $`b`$<span dir="rtl">.</span> <span dir="rtl">غالبًا ما
تكون</span> $`\pi`$ <span dir="rtl">هي السياسة الجشعة</span> **(Greedy
Policy)** <span dir="rtl">بالنسبة لتقدير دالة قيمة الإجراء الحالية،
بينما تكون</span> $`b`$ <span dir="rtl">سياسة أكثر استكشافية، ربما
جشعة</span> **(ε-greedy)**<span dir="rtl">.</span> <span dir="rtl">من
أجل استخدام البيانات من</span> $`b`$<span dir="rtl">، يجب أن نأخذ في
الاعتبار الاختلاف بين السياسات، باستخدام الاحتمالية النسبية لاتخاذ
الإجراءات التي تم اتخاذها  
(انظر القسم 5.5). في أساليب</span> **n-step**<span dir="rtl">، يتم تكوين
العوائد على مدى</span> n<span dir="rtl">-خطوات، لذلك نحن مهتمون
بالاحتمالية النسبية لهذه الإجراءات</span> $`n`$ <span dir="rtl">فقط. على
سبيل المثال، لجعل نسخة بسيطة خارج السياسة من</span> **n-step
TD**<span dir="rtl">، يمكن ببساطة وزن التحديث للوقت</span> t
<span dir="rtl">(الذي يتم في الواقع عند الوقت</span>
$`t + n`$<span dir="rtl">)</span> <span dir="rtl">بواسطة</span>
$`\rho t:t + n - 1`$​.

``` math
V_{t + n}\left( S_{t} \right) = V_{t + n - 1}\left( S_{t} \right) + \alpha\rho_{t:t + n - 1}\left\lbrack G_{t:t + n} - V_{t + n - 1}\left( S_{t} \right) \right\rbrack,\quad 0 \leq t < T
```

<span dir="rtl">حيث</span> $`\rho t:t + n - 1`$​<span dir="rtl">، الذي
يُسمى نسبة التوزيع العيني</span> **(Importance Sampling
Ratio)**<span dir="rtl">، هو الاحتمال النسبي تحت السياسات المختلفة
لاتخاذ</span> $`n`$ <span dir="rtl">من الإجراءات من</span> $`At`$
<span dir="rtl"></span>​ <span dir="rtl">إلى</span> $`At + n - 1`$
<span dir="rtl">  
(راجع المعادلة 5.3)</span>:

``` math
\rho_{t:h} = \prod_{k = t}^{\min(h,T - 1)}\frac{\pi\left( A_{k} \middle| S_{k} \right)}{b\left( A_{k} \middle| S_{k} \right)}
```

<span dir="rtl">على سبيل المثال، إذا كان أي من الإجراءات لن يتم اتخاذه
أبدًا بواسطة **السياسة المستهدفة**</span>
**<span dir="rtl">(</span>Target <span dir="rtl"></span>Policy**
$`\mathbf{\pi}`$**<span dir="rtl">)</span>**) <span dir="rtl">أي إذا
كانت</span> $`\pi(Ak \mid Sk) = 0`$<span dir="rtl">)، فيجب إعطاء العائد
ذو</span> **n-step <span dir="rtl"></span>**<span dir="rtl">وزن</span>
**(Weight)** <span dir="rtl">صفري ويتم تجاهله تمامًا. من ناحية أخرى، إذا
تم اتخاذ إجراء عن طريق الصدفة وكان</span> $`\pi`$ <span dir="rtl">سيأخذ
هذا الإجراء باحتمالية أكبر بكثير مما يفعله **السياسة السلوكية**
</span>**(Behavior Policy)** $`\mathbf{b}`$<span dir="rtl">**،** فإن هذا
سيزيد من **الوزن**</span> **(Weight)** <span dir="rtl">الذي كان سيتم
إعطاؤه للعائد. هذا منطقي لأن هذا الإجراء يتميز بسياسة</span> $`\pi`$
<span dir="rtl"></span>)<span dir="rtl">وبالتالي نريد التعليم عنه)، لكنه
يُختار نادرًا بواسطة</span> $`b`$ <span dir="rtl">وبالتالي يظهر نادرًا في
البيانات. للتعويض عن ذلك، علينا إعطاؤه **وزنًا**</span> **(Weight)**
<span dir="rtl">زائدًا عندما يحدث. لاحظ أنه إذا كانت السياسات متطابقة
فعليًا (الحالة **داخل السياسة** </span>**(On-Policy)** (<span dir="rtl">،
فإن نسبة التوزيع العيني **(**</span>**Importance Sampling
<span dir="rtl"></span>Ratio<span dir="rtl">)</span>**
<span dir="rtl">تكون دائمًا 1. وبالتالي، يمكن لتحديثنا الجديد (7.9) أن
يعمم ويستبدل تمامًا تحديث</span> **TD
<span dir="rtl"></span>**<span dir="rtl">ذو</span> **n-step
<span dir="rtl"></span>**<span dir="rtl">السابق. وبالمثل، يمكن أن يتم
استبدال تحديث **سارسا**</span> **(Sarsa)** <span dir="rtl">ذو</span>
**n-step <span dir="rtl"></span>**<span dir="rtl">السابق بالكامل بصيغة
بسيطة **خارج السياسة** </span>**(Off-Policy)**<span dir="rtl">.</span>

``` math
Q_{t + n}\left( S_{t},A_{t} \right) = Q_{t + n - 1}\left( S_{t},A_{t} \right) + \alpha\rho_{t + 1:t + n}\left\lbrack G_{t:t + n} - Q_{t + n - 1}\left( S_{t},A_{t} \right) \right\rbrack,
```

<span dir="rtl">بالنسبة لـ</span>
$`0 \leq t < T0`$<span dir="rtl">.</span> <span dir="rtl">لاحظ أن نسبة
التوزيع العيني</span> **(Importance Sampling Ratio)**
<span dir="rtl">هنا تبدأ وتنتهي خطوة واحدة بعد</span> **n-step TD
<span dir="rtl"></span>**<span dir="rtl">(المعادلة 7.9). ذلك لأننا هنا
نقوم بتحديث زوج  
**الحالة–الإجراء** </span>**(State–Action
Pair)**<span dir="rtl">.</span> <span dir="rtl">لا يهمنا مدى احتمال
اختيارنا للإجراء؛ الآن بعد أن اخترناه، نريد أن نتعلم بالكامل مما يحدث،
مع استخدام التوزيع العيني فقط للإجراءات اللاحقة. يُظهر الصندوق أدناه
الكود الزائف</span> **(Pseudocode)
<span dir="rtl"></span>**<span dir="rtl">للخوارزمية الكاملة</span>.

**<span dir="rtl">سارسا</span> (Sarsa)** <span dir="rtl">ذو</span>
**n-step <span dir="rtl"></span>**<span dir="rtl">خارج
**السياسة**</span> **(Off-Policy)** <span dir="rtl">لتقدير</span>
$`Q \approx q*{\ o}_{r}{\ \ q}_{\pi}`$

<img src="./media/image70.png" style="width:6.2919in;height:4.9534in" />

<span dir="rtl">نسخة **خارج السياسة**</span> **(Off-Policy)**
<span dir="rtl">من **سارسا المتوقع**</span> **(Expected Sarsa)**
<span dir="rtl">ذات</span> **n-step** <span dir="rtl">ستستخدم نفس
التحديث المذكور أعلاه لـ **سارسا**</span> **(Sarsa)**
<span dir="rtl">ذات</span> **n-step** <span dir="rtl">باستثناء أن نسبة
التوزيع العيني</span> **(Importance Sampling Ratio)
<span dir="rtl"></span>**<span dir="rtl">ستحتوي على عامل واحد أقل. بمعنى
أن المعادلة السابقة ستستخدم</span> $`\rho t + 1:t + n - 1`$
<span dir="rtl"></span>​ <span dir="rtl">بدلاً من</span>
$`\rho t + 1:t + n`$ <span dir="rtl"></span>​<span dir="rtl">، وبالطبع
ستستخدم نسخة **سارسا المتوقع** </span>**(Expected Sarsa)
<span dir="rtl"></span>**<span dir="rtl">من العائد ذو</span> **n-step
<span dir="rtl"></span>**<span dir="rtl">(المعادلة 7.7). وذلك لأن في
**سارسا المتوقع** </span>**(Expected Sarsa)
<span dir="rtl"></span>**<span dir="rtl">يتم أخذ جميع الإجراءات الممكنة
في الاعتبار في الحالة الأخيرة؛ الإجراء الذي تم اتخاذه بالفعل ليس له
تأثير ولا يحتاج إلى تصحيح</span>.

<u>**7.4 <span dir="rtl">طرق</span>** <span dir="rtl">**اتخاذ القرار مع
متغيرات التحكم**</span> (Per-decision Methods with Control
Variates**)**</u>

<span dir="rtl">تعتبر طرق **خارج السياسة**</span> **(Off-Policy)**
<span dir="rtl">ذات **الخطوات المتعددة**</span> **(Multi-step)**
<span dir="rtl">التي تم عرضها في القسم السابق بسيطة ومفهومة من الناحية
المفاهيمية، ولكنها ربما ليست الأكثر كفاءة. نهج أكثر تطورًا قد يستخدم
أفكار **التوزيع العيني**</span> **(Importance Sampling)**
<span dir="rtl">لكل قرار كما تم تقديمه في القسم 5.9. لفهم هذا النهج، يجب
أولاً ملاحظة أن العائد العادي ذو</span> **n-step** (7.1)<span dir="rtl">،
مثل جميع العوائد، يمكن كتابته بشكل تكراري. بالنسبة للخطوات</span>
$`\mathbf{n}`$ <span dir="rtl">التي تنتهي عند الأفق</span>
$`\mathbf{h}`$<span dir="rtl">، يمكن كتابة العائد ذو  
</span>**n-step <span dir="rtl"></span>**<span dir="rtl">كما يلي</span>:

``` math
G_{t:h} = R_{t + 1} + \gamma G_{t + 1:h},\quad t < h < T
```

<span dir="rtl">حيث</span> $`Gh:h = Vh - 1`$ <span dir="rtl">(تذكر أن
هذا العائد يُستخدم في الوقت</span> $`h`$<span dir="rtl">، والذي كان يُشار
إليه سابقًا بـ</span> $`t + n`$<span dir="rtl">) الآن، اعتبر تأثير اتباع
سياسة سلوك</span> $`b`$ <span dir="rtl">التي لا تتطابق مع السياسة
المستهدفة</span> $`\pi`$<span dir="rtl">.</span> <span dir="rtl">يجب أن
تُوزن كل الخبرة الناتجة، بما في ذلك المكافأة الأولى</span> $`R_{t + 1}`$
<span dir="rtl">والحالة التالية</span> $`S_{t + 1}`$​<span dir="rtl">،
بنسبة التوزيع العيني للوقت</span> $`t`$<span dir="rtl">، أي</span>
$`\rho t = \pi(At \mid St)`$<span dir="rtl">.</span> <span dir="rtl">قد
يُغري البعض ببساطة وزن الجانب الأيمن من المعادلة السابقة، لكن يمكن القيام
بعمل أفضل. افترض أن الإجراء في الوقت</span> $`t`$ <span dir="rtl">لن يتم
اختياره أبدًا بواسطة</span> $`\pi`$<span dir="rtl">، بحيث تكون</span>
$`\rho t\ `$​ <span dir="rtl">صفرًا. في هذه الحالة، سيؤدي الوزن البسيط إلى
أن يكون العائد</span> n<span dir="rtl">-ذو الخطوات صفرًا، مما قد يتسبب في
تباين كبير عند استخدامه كهدف. بدلاً من ذلك، في هذا النهج الأكثر تطورًا،
يتم استخدام تعريف بديل خارج السياسة للعائد</span> n<span dir="rtl">-ذو
الخطوات الذي ينتهي عند الأفق</span> $`h`$<span dir="rtl">، كما
يلي</span>:

``` math
G_{t:h} = \rho_{t}\left( R_{t + 1} + \gamma G_{t + 1:h} \right) + \left( 1 - \rho_{t} \right)V_{h - 1}\left( S_{t} \right),\quad t < h < T
```

<span dir="rtl">حيث</span> $`Gh:h = Vh - 1`$<span dir="rtl">.</span>
<span dir="rtl">في هذا النهج، إذا كانت</span> $`\rho t`$
<span dir="rtl">صفرًا، فإن الهدف بدلاً من أن يكون صفرًا ويسبب تقلص التقدير،
يكون الهدف هو نفسه التقدير ويؤدي إلى عدم حدوث أي تغيير. نسبة التوزيع
العيني التي تكون صفرًا تعني أننا يجب أن نتجاهل العينة، لذا يبدو من
المناسب ترك التقدير دون تغيير. المصطلح الثاني، الإضافي في (7.13)، يُسمى
**متغير تحكم**</span> **(Control Variate)** <span dir="rtl">(لأسباب غير
واضحة). لاحظ أن متغير التحكم لا يغير التحديث المتوقع؛ حيث أن نسبة
التوزيع العيني لها قيمة متوقعة تساوي واحد (انظر القسم 5.9) وهي غير
مرتبطة بالتقدير، لذا فإن القيمة المتوقعة لمتغير التحكم هي صفر. أيضًا،
لاحظ أن تعريف **خارج السياسة**</span> **(Off-Policy)**
<span dir="rtl">في (7.13) هو تعميم صارم لتعريف **داخل السياسة**</span>
**(On-Policy)** <span dir="rtl">السابق للعائد</span>
n<span dir="rtl">-ذو الخطوات</span> (7.1)<span dir="rtl">، حيث أن
الاثنين متطابقان في حالة **داخل السياسة**، والتي تكون فيها</span>
$`\rho t`$ <span dir="rtl"></span>​ <span dir="rtl">دائمًا 1</span>.

<span dir="rtl">لطريقة</span> **n-step** <span dir="rtl">التقليدية،
قاعدة التعليم التي تُستخدم مع (7.13) هي تحديث</span> **TD
<span dir="rtl">ذو الخطوات المتعددة</span> (n-step TD Update)
<span dir="rtl">(</span>**7.2<span dir="rtl">)، والتي ليس بها نسب توزيع
عيني صريحة سوى تلك المدمجة في العائد</span>.

**<span dir="rtl">تمرين 7.5</span>**: <span dir="rtl">اكتب **الكود**
**الزائف**</span> (**Pseudocode**) <span dir="rtl">لخوارزمية تقدير قيمة
الحالة **خارج السياسة**</span>
**<span dir="rtl">(</span>Off-Policy<span dir="rtl">)
</span>**<span dir="rtl">الموضحة أعلاه</span>.

<span dir="rtl">بالنسبة لقيم الأفعال، فإن تعريف **خارج السياسة**</span>
**(Off-Policy)** <span dir="rtl">للعائد ذو الخطوات المتعددة يختلف قليلاً
لأن الفعل الأول لا يلعب دوراً في التوزيع العيني. هذا الفعل الأول هو ما
يتم تعلمه؛ فلا يهم إذا كان غير محتمل أو حتى مستحيل بموجب السياسة
المستهدفة—لقد تم اتخاذه والآن يجب منح وزن كامل للوصول والمكافأة التي
تليه. ستطبق تقنية التوزيع العيني فقط على الأفعال التي تليه</span>.

<span dir="rtl">أولاً، لاحظ أنه بالنسبة لقيم الأفعال، يمكن كتابة عائد
**داخل السياسة**</span> **(On-Policy)** <span dir="rtl">ذو الخطوات
المتعددة المنتهي عند الأفق</span> $`h`$<span dir="rtl">، بشكل متوقع كما
في (7.7)، بشكل تكراري تماماً كما في (7.12)، باستثناء أنه لقيم الأفعال
ينتهي التكرار بـ</span> $`Gh:h = Vˉh - 1`$ <span dir="rtl">كما في (7.8).
الشكل **خارج السياسة**</span> **(Off-Policy)
<span dir="rtl"></span>**<span dir="rtl">مع **متغيرات التحكم**</span>
**(Control Variates)** <span dir="rtl">هو</span>:

``` math
G_{t:h} = R_{t + 1} + \gamma\left\lbrack \rho_{t + 1}G_{t + 1:h} + \overline{V_{h - 1}}\left( S_{t + 1} \right) - \rho_{t + 1}Q_{h - 1}\left( S_{t + 1},A_{t + 1} \right) \right\rbrack,
```

``` math
G_{t:h} = R_{t + 1} + \gamma\rho_{t + 1}\left( G_{t + 1:h} - Q_{h - 1}\left( S_{t + 1},A_{t + 1} \right) \right) + \gamma\overline{V_{h - 1}}\left( S_{t + 1} \right),\quad t < h \leq T.
```

<span dir="rtl">إذا كان</span> $`h < T`$<span dir="rtl">، فإن الاستدعاء
ينتهي بـ</span> $`Gh:h = Qh - 1(Sh,Ah)`$<span dir="rtl">، بينما إذا
كان</span> $`h \geq Th\ `$<span dir="rtl">، فإن الاستدعاء ينتهي
بـ</span> $`GT - 1:h = RT`$​<span dir="rtl">.</span>
<span dir="rtl">خوارزمية التنبؤ الناتجة (بعد دمجها مع المعادلة</span>
(7.5)<span dir="rtl">) مشابهة لـ</span> Expected
Sarsa<span dir="rtl">.</span>

<u>**<span dir="rtl">التمرين 7.6</span>:**</u> <span dir="rtl">أثبت أن
متغير التحكم في المعادلات أعلاه لا يغير القيمة المتوقعة للعائد</span>.

**<span dir="rtl"><u>التمرين 7.7</u></span>:** <span dir="rtl">اكتب
الكود الزائف</span> (Pseudocode) <span dir="rtl">لخوارزمية التنبؤ بقيم
الأفعال خارج السياسة الموصوفة أعلاه. انتبه بشكل خاص لشروط إنهاء التكرار
عند الوصول إلى الأفق أو نهاية الحلقة</span>.

**<span dir="rtl"><u>التمرين 7.8</u></span>:** <span dir="rtl">أظهر أن
النسخة العامة (خارج السياسة) من العائد ذو</span> n
<span dir="rtl"></span>-<span dir="rtl">خطوة (7.13) يمكن كتابتها بدقة
وبتنسيق مضغوط كجمع لأخطاء</span> TD <span dir="rtl">المستندة إلى الحالة
(6.5) إذا لم يتغير دالة القيمة التقريبية للحالة</span>.

**<span dir="rtl"><u>التمرين 7.9</u></span>: <span dir="rtl"></span>**
<span dir="rtl">كرر التمرين السابق للنسخة الخاصة بقيم الأفعال من العائد
ذو</span> n <span dir="rtl"></span>-<span dir="rtl">خطوة خارج السياسة
(7.14) وخطأ</span> TD <span dir="rtl">في</span> Expected Sarsa
<span dir="rtl">(الكمية بين الأقواس في المعادلة 6.9)</span>.

**<span dir="rtl"><u>التمرين 7.10 (برمجة)</u>:</span>**
<span dir="rtl">صمم مشكلة صغيرة للتنبؤ خارج السياسة واستخدمها لإظهار أن
خوارزمية التعليم خارج السياسة باستخدام (7.13) و(7.2) أكثر كفاءة في
استخدام البيانات مقارنة بالخوارزمية البسيطة التي تستخدم (7.1)
و</span>(7.9).

<span dir="rtl">تتيح طريقة أخذ العينات وفقًا للأهمية التي استخدمناها في
هذا القسم، والقسم السابق، وفي الفصل 5، تعلم السياسة خارج السياسة بشكل
سليم، ولكنها تؤدي أيضًا إلى تحديثات ذات تباين عالٍ، مما يجبر على استخدام
بارامتر حجم خطوة صغيرة، مما يتسبب في بطء التعليم. من المحتمل أن التدريب
خارج السياسة أبطأ من التدريب داخل السياسة—بعد كل شيء، البيانات أقل صلة
بما يتم تعلمه. ومع ذلك، من المحتمل أيضًا أن هذه الطرق يمكن تحسينها.
متغيرات التحكم هي إحدى طرق تقليل التباين. طريقة أخرى هي التكيف السريع
لحجوم الخطوات مع التباين الملاحظ، كما في طريقة</span> Autostep
<span dir="rtl">(</span>Mahmood <span dir="rtl">و</span>Sutton
<span dir="rtl">و</span>Degris
<span dir="rtl">و</span>Pilarski<span dir="rtl">، 2012).</span>
<span dir="rtl">نهج واعد آخر هو التحديثات الثابتة التي اقترحها</span>
Karampatziakis <span dir="rtl">و</span>Langford (2010)
<span dir="rtl">وتم توسيعها إلى</span> TD <span dir="rtl">بواسطة</span>
Tian <span dir="rtl">(تحت الإعداد). قد تكون تقنية الاستخدام لـ</span>
Mahmood <span dir="rtl">(</span>2017<span dir="rtl">؛</span> Mahmood
<span dir="rtl">و</span>Sutton<span dir="rtl">، 2015)</span>
<span dir="rtl">أيضًا جزءًا من الحل. في القسم التالي، سننظر في طريقة تعلم
خارج السياسة التي لا تستخدم أخذ العينات وفقًا للأهمية</span>.

**<u>7.5 <span dir="rtl">التعليم خارج السياسة بدون أخذ عينات ذات
أهمية:  
خوارزمية النسخ الاحتياطي لشجرة</span> n-step<span dir="rtl">  
</span>(Off-policy Learning Without Importance Sampling): (The n-step
Tree Backup Algorithm)</u>**

**<span dir="rtl">هل يمكن التعليم خارج السياسة بدون أخذ عينات ذات
أهمية؟</span>**  
**Q-learning <span dir="rtl"></span>**<span dir="rtl">و</span>**Expected
Sarsa <span dir="rtl"></span>**<span dir="rtl">من الفصل 6 يقومان بذلك
لحالة الخطوة الواحدة، ولكن هل يوجد خوارزمية متعددة الخطوات مقابلة؟ في
هذه الفقرة، نقدم طريقة</span> n-step <span dir="rtl">التي تُدعى خوارزمية
النسخ الاحتياطي لشجرة</span>.

<span dir="rtl">فكرة الخوارزمية مستوحاة من مخطط النسخ الاحتياطي لشجرة ذو
3 خطوات الموضح إلى اليمين. على العمود المركزي والمُسمى في المخطط، توجد
ثلاث حالات مكافآت وعملين عشوائيين. هذه هي المتغيرات العشوائية التي تمثل
الأحداث التي تحدث بعد زوج الحالة–الإجراء الأولي</span>
$`St,At`$<span dir="rtl">.</span> <span dir="rtl">على جوانب كل حالة،
توجد الإجراءات التي لم تُختَر. (بالنسبة للحالة الأخيرة، يُعتبر أن جميع
الإجراءات لم تُختَر (بعد)). لأن لدينا بيانات عشوائية قليلة للإجراءات غير
المُختارة، فإننا نقوم بعملية الاستدلال ونستخدم تقديرات قيمها لتشكيل الهدف
للتحديث. هذا يمتد قليلاً عن فكرة مخطط النسخ الاحتياطي. حتى الآن، كنا
دائماً نُحدِّث القيمة المقدَّرة للعقدة في قمة المخطط نحو هدف يجمع المكافآت على
طول الطريق (المخفَّضة بشكل مناسب) وتقديرات قيم العقد في الأسفل. في تحديث
النسخ الاحتياطي لشجرة، يشمل الهدف جميع هذه الأمور بالإضافة إلى</span>
<img src="./media/image71.png"
style="width:1.22292in;height:3.40972in" /><span dir="rtl">تقديرات قيم
العقدة المعلقة على الجوانب، على جميع المستويات. لهذا السبب يُطلق عليه
تحديث النسخ الاحتياطي لشجرة؛ إنه تحديث من شجرة كاملة من تقديرات قيم
الإجراءات</span>.

<span dir="rtl">بشكل أكثر دقة، التحديث يأتي من تقديرات قيم الإجراءات
لعقد الأوراق في الشجرة. العقد الداخلية، التي تتوافق مع الإجراءات الفعلية
المتخذة، لا تشارك. تسهم كل عقدة ورقة في الهدف بوزن يتناسب مع احتمال
حدوثها تحت السياسة المستهدفة</span> $`\pi`$<span dir="rtl">.</span>
<span dir="rtl">لذلك، كل إجراء من المستوى الأول</span> $`a`$
<span dir="rtl">يساهم بوزن</span>
$`\pi(a \mid S_{t + 1})`$<span dir="rtl">، باستثناء الإجراء الذي تم
اتخاذه فعلياً،</span> $`At + 1`$​<span dir="rtl">، الذي لا يساهم على
الإطلاق. يتم استخدام احتماليته</span> $`\pi(At + 1 \mid S_{t + 1})`$
<span dir="rtl"></span> <span dir="rtl">لوزن جميع قيم الإجراءات من
المستوى الثاني. وهكذا، يساهم كل إجراء غير مختار من المستوى الثاني</span>
$`a'`$ <span dir="rtl">بوزن</span>
$`\pi(At + 1 \mid S_{t + 1})\pi(a' \mid St + 2)`$<span dir="rtl">.</span>
<span dir="rtl">كل إجراء من المستوى الثالث يساهم بوزن</span>
$`\pi(At + 1 \mid S_{t + 1})\pi(At + 2 \mid St + 2)`$<span dir="rtl">،
وهكذا. كأن كل سهم إلى عقدة إجراء في المخطط يُوزَّن باحتمال اختيار الإجراء
تحت السياسة المستهدفة وإذا كان هناك شجرة تحت الإجراء، فإن ذلك الوزن
ينطبق على جميع عقد الأوراق في الشجرة</span>.

<span dir="rtl">يمكننا أن نعتبر تحديث النسخ الاحتياطي لشجرة ذو 3 خطوات
كأنه يتكون من 6 نصف خطوات، تتناوب بين نصف خطوات عشوائية من إجراء إلى
حالة لاحقة، ونصف خطوات متوقعة تأخذ في الاعتبار جميع الإجراءات الممكنة من
تلك الحالة مع احتمالات حدوثها تحت السياسة</span>.

<span dir="rtl">الآن دعنا نطور المعادلات التفصيلية لخوارزمية النسخ
الاحتياطي لشجرة ذات</span> n<span dir="rtl">-خطوة. العائد (الهدف) في
خطوة واحدة هو نفس عائد خوارزمية</span> **Expected Sarsa**

``` math
G_{t:t + 1} = R_{t + 1} + \gamma\sum_{a}^{}{\pi\left( a\mid S_{t + 1} \right)Q\left( S_{t + 1},a \right)}
```

``` math
t < T - 1
```

``` math
G_{t:t + 2} = R_{t + 1} + \gamma\sum_{a \neq A_{t + 1}}^{}{\pi\left( a\mid S_{t + 1} \right)Q_{t + 1}\left( S_{t + 1},a \right)} + \gamma\pi\left( A_{t + 1}\mid S_{t + 1} \right)\left( R_{t + 2} + \gamma\sum_{a}^{}{\pi\left( a\mid S_{t + 2} \right)Q_{t + 1}\left( S_{t + 2},a \right)} \right)
```

``` math
G_{t:t + 2} = R_{t + 1} + \gamma\sum_{a \neq A_{t + 1}}^{}{\pi\left( a\mid S_{t + 1} \right)Q_{t + 1}\left( S_{t + 1},a \right)} + \gamma\pi\left( A_{t + 1}\mid S_{t + 1} \right)G_{t + 1:t + 2}
```

``` math
t\  < \ T\  - 2
```

``` math
G_{t:t + n} = R_{t + 1} + \gamma\sum_{a \neq A_{t + 1}}^{}{\pi\left( a\mid S_{t + 1} \right)Q_{t + n - 1}\left( S_{t + 1},a \right)} + \gamma\pi\left( A_{t + 1}\mid S_{t + 1} \right)G_{t + 1:t + n}
```

``` math
t\  < \ T\  - \ 1,\ n\  > = \ 2
```

``` math
Q_{t + n}\left( S_{t},A_{t} \right) = Q_{t + n - 1}\left( S_{t},A_{t} \right) + \alpha\left\lbrack G_{t:t + n} - Q_{t + n - 1}\left( S_{t},A_{t} \right) \right\rbrack
```

**<span dir="rtl"><u>التمرين 7.11</u></span>**: <span dir="rtl"></span>
<span dir="rtl">أظهر أنه إذا كانت **القيم التقديرية للإجراءات**</span>
**(Action Values)** <span dir="rtl">ثابتة، فيمكن كتابة **العائد**</span>
**(Return)** <span dir="rtl">في **النسخ الاحتياطي لشجرة**</span>
**(Tree-Backup)** (7.16) <span dir="rtl">كجمع لأخطاء</span> **TD
(Temporal Difference)** <span dir="rtl">القائمة على التوقعات</span>.

``` math
G_{t:t + n} = Q\left( S_{t},A_{t} \right) + \sum_{k = t}^{\min(t + n - 1,T - 1)}{\gamma^{k - t}\prod_{i = t + 1}^{k}{\pi\left( A_{i}\mid S_{i} \right)}}
```

<span dir="rtl">عندما</span>

``` math
\delta_{t} = R_{t + 1} + \gamma\overline{V_{t}}\left( S_{t + 1} \right) - Q\left( S_{t},A_{t} \right)\text{ and }\overline{V_{t}\text{ is given by}\text{ (7.8).}}
```

n_step <span dir="rtl">ذات شجرة النسخ الاحتياطي لتقدير</span>
$`Q \approx q*or\ \ \ q_{\pi}`$

<img src="./media/image72.png"
style="width:6.26806in;height:5.72153in" />

**<u>7.6 <span dir="rtl">خوارزمية موحدة</span>: (A Unifying Algorithm:
n-step Q (</u>**$`\mathbf{\sigma}`$**<u>))</u>**

<span dir="rtl">في هذا الفصل، نظرنا حتى الآن إلى ثلاثة أنواع مختلفة من
**خوارزميات قيمة الإجراء  
(**</span>**action-value
algorithms<span dir="rtl">)</span>**<span dir="rtl">، والتي تقابل
**المخططات الثلاثة الأولى  
(**</span>**first three backup diagrams<span dir="rtl">)</span>**
<span dir="rtl">في **الشكل 7.5.**</span> <span dir="rtl">تحتوي
**خوارزمية  **
</span>**n-step Sarsa <span dir="rtl"></span>
<span dir="rtl">(</span>n-step Sarsa<span dir="rtl">)</span>**
<span dir="rtl">على جميع **التحولات العشوائية**</span>
<span dir="rtl"></span>**(sample transitions)**<span dir="rtl">، في حين
أن **خوارزمية النسخ الاحتياطي لشجرة**</span> **(tree-backup algorithm)**
<span dir="rtl">تحتوي على جميع **التحولات من الحالة إلى الإجراء**
</span>**(state-to-action transitions) <span dir="rtl"></span>**
<span dir="rtl">متفرعة بالكامل دون **أخذ عينات**
</span>**(sampling)**<span dir="rtl">، و**خوارزمية**</span> **n-step
Expected Sarsa <span dir="rtl">(</span>n-step Expected
<span dir="rtl"></span>Sarsa<span dir="rtl">) </span>**
<span dir="rtl">تحتوي على جميع **التحولات العشوائية**</span> **(sample
transitions)** <span dir="rtl">باستثناء **التحول الأخير من الحالة إلى
الإجراء** </span>**(last state-to-action transition)**<span dir="rtl">،
الذي يكون متفرعًا بالكامل بقيمة **متوقعة** </span>**(expected
value)**<span dir="rtl">.</span> <span dir="rtl">إلى أي مدى يمكن توحيد
هذه الخوارزميات؟</span>

<span dir="rtl">تتمثل إحدى أفكار **التوحيد**</span> **(unification)**
<span dir="rtl">في **المخطط الرابع**</span> **(fourth backup diagram)**
<span dir="rtl">في **الشكل 7.5.**</span> <span dir="rtl">وهذه الفكرة هي
أنه يمكن اتخاذ قرار **خطوة بخطوة**</span>
**<span dir="rtl">(</span>step-by-step basis<span dir="rtl">)</span>**
<span dir="rtl">بشأن ما إذا كنت ترغب في **أخذ الإجراء كعينة**
</span>**(take the action as a sample)**<span dir="rtl">، كما في</span>
**Sarsa (Sarsa)**<span dir="rtl">، أو النظر في **التوقع على جميع
الإجراءات**</span> **<span dir="rtl">(</span>expectation over all
<span dir="rtl"></span>actions<span dir="rtl">)</span>**
<span dir="rtl">بدلاً من ذلك، كما في **تحديث النسخ الاحتياطي لشجرة**
</span>**(tree-backup update)**<span dir="rtl">.</span>
<span dir="rtl">ثم، إذا قررت دائمًا **أخذ عينات**
</span>**(sample)**<span dir="rtl">، فستحصل على</span>
**(Sarsa)**<span dir="rtl">، بينما إذا قررت عدم **أخذ عينات**
</span>**(sampling)** <span dir="rtl">على الإطلاق، فستحصل على **خوارزمية
النسخ الاحتياطي لشجرة  
(**</span>**tree-backup
algorithm<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">أما</span> **Expected Sarsa
<span dir="rtl"></span>(Expected Sarsa)
<span dir="rtl"></span>**<span dir="rtl">فستكون الحالة التي تقرر فيها
**أخذ عينات**</span> **(sample)** <span dir="rtl">لجميع الخطوات باستثناء
**الأخيرة**</span> **(last one)** <span dir="rtl">منها</span>.

<img src="./media/image73.png"
style="width:6.26806in;height:4.68264in" />

<span dir="rtl">**الشكل 7.5**: مخططات النسخ الاحتياطي لأنواع تحديثات
قيمة الإجراء</span> (action-value updates) <span dir="rtl"></span>n-step
<span dir="rtl">الثلاثة التي تم النظر فيها حتى الآن في هذا الفصل</span>
(<span dir="rtl">حالة 4</span>-step) <span dir="rtl">بالإضافة إلى مخطط
النسخ الاحتياطي لنوع رابع من التحديثات الذي يوحدها جميعًا. تُشير الرموز
‘⇢’ إلى التحولات النصفية</span> (half transitions) <span dir="rtl">التي
يتطلب فيها **أخذ عينات ذات أهمية** </span>**(importance sampling)**
<span dir="rtl">في حالة **خارج السياسة**</span> **(off-policy)**.
<span dir="rtl">يوحد النوع الرابع من التحديث جميع الأنواع الأخرى عن طريق
الاختيار على أساس حالة بحالة</span> (state-by-state basis)
<span dir="rtl">ما إذا كان يجب **أخذ عينات** </span>**(sample)**
<span dir="rtl"></span>($`t\  = \ 1`$) <span dir="rtl">أو لا</span>
($`t\  = \ 0`$)<span dir="rtl">.</span>

<span dir="rtl">وبالطبع، ستكون هناك العديد من الإمكانيات الأخرى، كما هو
مقترح في الرسم البياني الأخير في الشكل. لزيادة الإمكانيات بشكل أكبر،
يمكننا النظر في تباين مستمر بين **أخذ العينات** </span>**(sampling)**
<span dir="rtl">و**التوقع**
</span>**(expectation)**<span dir="rtl">.</span>
<span dir="rtl">لندع</span> $`\gamma t \in \ \lbrack 0,1\rbrack`$
<span dir="rtl">تمثل درجة **أخذ العينات**</span> **(sampling)**
<span dir="rtl">في الخطوة</span> $`t`$<span dir="rtl">، حيث</span>
$`\gamma = 1`$ <span dir="rtl">تعني **أخذ عينات كاملة**</span> **(full
sampling)** <span dir="rtl">و</span>$`\ \gamma = 0`$<span dir="rtl">تعني
**توقعًا نقيًا**</span> **<span dir="rtl">(</span>pure
<span dir="rtl"></span>expectation<span dir="rtl">)</span>**
<span dir="rtl">دون أخذ عينات. قد يتم تعيين المتغير العشوائي</span>
$`\gamma t`$ <span dir="rtl"></span>​ <span dir="rtl">كدالة للحالة، أو
الإجراء، أو زوج الحالة–الإجراء في الزمن</span>
$`t`$<span dir="rtl">.</span> <span dir="rtl">نطلق على هذا **الخوارزمية
الجديدة المقترحة**</span> **<span dir="rtl">(</span>proposed new
<span dir="rtl"></span>algorithm<span dir="rtl">)</span>**
<span dir="rtl">اسم</span> **n-step**
$`\mathbf{Q(\gamma)}`$<span dir="rtl">.</span>

<span dir="rtl">الآن، دعونا نطور المعادلات لـ</span> **n-step**
$`\mathbf{Q(\gamma)}`$<span dir="rtl">.</span> <span dir="rtl">أولاً،
نكتب **العائد**</span> **(return)** <span dir="rtl">لخوارزمية النسخ
الاحتياطي لشجرة</span> n-step (7.16) <span dir="rtl">من حيث الأفق</span>
$`h = t + n`$ <span dir="rtl">ثم من حيث القيمة التقريبية المتوقعة</span>
$`Vˉ`$ <span dir="rtl"></span>(7.8)<span dir="rtl">.</span>

``` math
G_{t:h} = R_{t + 1} + \gamma\sum_{a \neq A_{t + 1}}^{}{\pi\left( a\mid S_{t + 1} \right)Q_{h - 1}\left( S_{t + 1},a \right)} + \gamma\pi\left( A_{t + 1}\mid S_{t + 1} \right)G_{t + 1:h}
```

``` math
G_{t:h} = R_{t + 1} + \gamma\overline{V_{h - 1}}\left( S_{t + 1} \right) - \gamma\pi\left( A_{t + 1}\mid S_{t + 1} \right)Q_{h - 1}\left( S_{t + 1},A_{t + 1} \right) + \gamma\pi\left( A_{t + 1}\mid S_{t + 1} \right)G_{t + 1:h}
```

``` math
G_{t:h} = R_{t + 1} + \gamma\overline{V_{h - 1}}\left( S_{t + 1} \right) - \gamma\pi\left( A_{t + 1}\mid S_{t + 1} \right)Q_{h - 1}\left( S_{t + 1},A_{t + 1} \right) + \gamma\pi\left( A_{t + 1}\mid S_{t + 1} \right)G_{t + 1:h}
```

<span dir="rtl">بعد ذلك، يصبح تمامًا مثل عائد</span> n-step
<span dir="rtl">لخوارزمية</span> Sarsa <span dir="rtl">مع متغيرات التحكم
(7.14)، باستثناء استبدال احتمال الإجراء</span>
$`\pi(At + 1 \mid S_{t + 1})`$ <span dir="rtl"></span>
<span dir="rtl">بنسبة أخذ العينات ذات الأهمية</span>
$`\rho t + 1`$<span dir="rtl">.</span> <span dir="rtl">بالنسبة لـ</span>
$`Q(\lambda)`$<span dir="rtl">، ننتقل تدريجياً بين هاتين الحالتين</span>:

``` math
G_{t:h} = R_{t + 1} + \gamma\left( \lambda_{t + 1}\rho_{t + 1} + \left( 1 - \lambda_{t + 1} \right)\pi\left( A_{t + 1}\mid S_{t + 1} \right) \right)\left( G_{t + 1:h} - Q_{h - 1}\left( S_{t + 1},A_{t + 1} \right) \right) + \gamma\overline{V_{h - 1}}\left( S_{t + 1} \right),
```

<span dir="rtl">لـ</span> $`t < h \leq Tt`$ <span dir="rtl">.</span>
<span dir="rtl">تنتهي التكرار بـ</span> $`Gh:h = Qh - 1(Sh,Ah)`$
<span dir="rtl">إذا كان</span> $`h < T`$<span dir="rtl">، أو بـ</span>
$`GT - 1:T = R`$ <span dir="rtl"></span>​​ <span dir="rtl">إذا كان</span>
$`h = T`$ <span dir="rtl"></span> <span dir="rtl">ثم نستخدم التحديث
العام (خارج السياسة) لخوارزمية</span> n-step Sarsa
(7.11)<span dir="rtl">.</span> <span dir="rtl">يتم تقديم خوارزمية كاملة
في الصندوق</span>.

<span dir="rtl">خوارزمية</span> n-step Q($`\sigma`$)
<span dir="rtl">خارج السياسة لتقدير</span>
$`Q \approx q*or{\ \ \ q}_{\pi}`$

<img src="./media/image74.png"
style="width:6.08386in;height:6.35055in" />

**<u>7.7 <span dir="rtl">الملخص</span>
(Summary)</u>**<img src="./media/image75.png"
style="width:1.89861in;height:3.48889in" />

<span dir="rtl">في هذا الفصل، قمنا بتطوير مجموعة من طرق **التعليم
بالتفاضل الزمني**</span> **(Temporal-Difference Learning)**
<span dir="rtl">التي تقع بين طرق **التفاضل الزمني ذات الخطوة
الواحدة**</span> **<span dir="rtl">(</span>One-Step TD
<span dir="rtl"></span>Methods<span dir="rtl">)</span>**
<span dir="rtl">التي تمت مناقشتها في الفصل السابق وطرق **مونت
كارلو**</span> **(Monte Carlo Methods)** <span dir="rtl">التي تمت
مناقشتها في الفصل الذي قبله. الطرق التي تتضمن مقداراً متوسطاً من **التقدير
الذاتي** </span>**(Bootstrapping)
<span dir="rtl"></span>**<span dir="rtl">مهمة لأنها عادة ما تؤدي بشكل
أفضل من أي من الحالتين</span> **extremes**<span dir="rtl">.</span>

<span dir="rtl">ركزنا في هذا الفصل على طرق</span>
**n-step**<span dir="rtl">، التي تتطلع إلى **المكافآت**
</span>**(Rewards)**<span dir="rtl">، **الحالات**
</span>**(States)**<span dir="rtl">، و**الإجراءات**</span> **(Actions)**
<span dir="rtl">التالية لـ</span>
**n<span dir="rtl">-</span>**<span dir="rtl">خطوة. تلخص المخططات المرفقة
مع</span> **4-step <span dir="rtl"></span>**<span dir="rtl">معظم الطرق
التي تم تقديمها. التحديث لقيمة **الحالة**</span> **(State-Value)**
<span dir="rtl">المبين هو لـ</span> **n-step TD
<span dir="rtl"></span>**<span dir="rtl">مع أخذ **عينات هامة**</span>
**<span dir="rtl">(</span>Importance
<span dir="rtl"></span>Sampling<span dir="rtl">)</span>**<span dir="rtl">،
والتحديث لقيمة **الإجراء**</span> **(Action-Value)** <span dir="rtl">هو
لـ</span> **n-step** $`\mathbf{Q(\lambda)}`$<span dir="rtl">، والذي
يعمم</span> **Expected Sarsa
<span dir="rtl"></span>**<span dir="rtl">و</span>Q**-learning<span dir="rtl">.</span>**
<span dir="rtl">جميع طرق</span> **n-step
<span dir="rtl"></span>**<span dir="rtl">تتطلب تأخيرًا قدره</span> **n**
<span dir="rtl">خطوة زمنية قبل التحديث، حيث فقط بعد ذلك تكون جميع
الأحداث المستقبلية المطلوبة معروفة</span>.

<span dir="rtl">من العيوب الأخرى أنها تتطلب حسابات أكثر في كل خطوة زمنية
مقارنة بالطرق السابقة. مقارنة بالطرق ذات الخطوة الواحدة، تتطلب
طرق</span> **n-step <span dir="rtl"></span>**<span dir="rtl">أيضًا المزيد
من الذاكرة لتسجيل **الحالات** </span>**(States)**<span dir="rtl">،
**الإجراءات** </span>**(Actions)**<span dir="rtl">، **المكافآت**
</span>**(Rewards)**<span dir="rtl">، وأحيانًا متغيرات أخرى على مدى
آخر</span> **n<span dir="rtl">-</span>**<span dir="rtl">خطوة زمنية. في
النهاية، في الفصل</span> **12**<span dir="rtl">، سنرى كيف يمكن تنفيذ
طرق</span> **TD <span dir="rtl"></span>**<span dir="rtl">متعددة الخطوات
بذاكرة مع الحد الأدنى من التعقيد الحسابي باستخدام **تتبع الأهلية**
</span>**(Eligibility Traces)**<span dir="rtl">، ولكن سيكون هناك دائمًا
بعض الحسابات الإضافية مقارنة بالطرق ذات الخطوة الواحدة. يمكن أن تكون هذه
التكاليف مجدية تمامًا للهروب من قيود خطوة الزمن الواحدة</span>.

<span dir="rtl">على الرغم من أن طرق</span> **n-step
<span dir="rtl"></span>**<span dir="rtl">أكثر تعقيدًا من تلك التي تستخدم
**تتبع الأهلية**</span> **<span dir="rtl">(</span>Eligibility
<span dir="rtl"></span>Traces<span dir="rtl">)</span>**<span dir="rtl">،
إلا أن لديها فائدة كبيرة تتمثل في كونها واضحة من الناحية المفاهيمية.
حاولنا الاستفادة من ذلك من خلال تطوير نهجين للتعليم **خارج
السياسة**</span> **(Off-Policy Learning)** <span dir="rtl">في
حالة</span> **n-step<span dir="rtl">.</span>** <span dir="rtl">الأول،
الذي يعتمد على أخذ **العينات الهامة** </span>**(Importance
Sampling)**<span dir="rtl">، بسيط من الناحية المفاهيمية ولكن قد يكون له
تباين عالٍ. إذا كانت **السياسات المستهدفة** </span>**(Target Policies)**
<span dir="rtl">و**سلوكيات الأفعال**</span> **(Behavior Policies)**
<span dir="rtl">مختلفة جداً، فقد يحتاج إلى بعض الأفكار الخوارزمية الجديدة
قبل أن يصبح فعالاً وقابلاً للتطبيق. الآخر، الذي يعتمد على **تحديثات
الشجرة  
(**</span>**Tree-Backup
Updates<span dir="rtl">)</span>**<span dir="rtl">، هو التمديد الطبيعي
لـ</span> **Q-learning <span dir="rtl"></span>**<span dir="rtl">إلى
الحالة متعددة الخطوات مع **السياسات المستهدفة العشوائية**
</span>**(Stochastic Target Policies)**<span dir="rtl">.</span>
<span dir="rtl">لا يتضمن أخذ **العينات الهامة**</span> **(Importance
Sampling)** <span dir="rtl">ولكنه، مرة أخرى، إذا كانت **السياسات
المستهدفة**</span> **<span dir="rtl">(</span>Target
<span dir="rtl"></span>Policies<span dir="rtl">)</span>**
<span dir="rtl">و**سلوكيات الأفعال**</span> **(Behavior Policies)**
<span dir="rtl">مختلفة بشكل كبير، فإن **التقدير الذاتي**</span>
**(Bootstrapping) <span dir="rtl"></span>**<span dir="rtl">قد يمتد فقط
عبر بضع خطوات حتى لو كان</span> $`\mathbf{n}`$
<span dir="rtl">كبيرًا</span>.

<span dir="rtl">الفصل الثامن:</span>

<span dir="rtl">التخطيط والتعليم باستخدام الطرق الجدولية</span>
<span dir="rtl">  
</span>(Planning and Learning with Tabular Methods)

<span dir="rtl">في هذا الفصل، نقوم بتطوير رؤية موحدة لطرق التعليم
المعزز</span> (Reinforcement Learning) <span dir="rtl">التي تتطلب وجود
نموذج للبيئة</span>
<span dir="rtl">(</span>Environment<span dir="rtl">)، مثل البرمجة
الديناميكية</span> (Dynamic Programming) <span dir="rtl">والبحث
الاستدلالي</span> (Heuristic Search)<span dir="rtl">، وطرق أخرى يمكن
استخدامها دون الحاجة إلى نموذج، مثل طرق مونت كارلو</span> (Monte Carlo)
<span dir="rtl">وطرق الفارق الزمني</span>
(Temporal-Difference)<span dir="rtl">.</span> <span dir="rtl">هذه الطرق
تُعرف على التوالي بطرق التعليم المعزز المستندة إلى نموذج</span>
<span dir="rtl">(</span>Model-Based Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>
<span dir="rtl">وطرق التعليم المعزز غير المستندة إلى نموذج</span>
<span dir="rtl">(</span>Model-Free Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">).</span>
<span dir="rtl">تعتمد الطرق المستندة إلى نموذج بشكل رئيسي على
التخطيط</span> (Planning) <span dir="rtl">كعنصر أساسي، بينما تعتمد الطرق
غير المستندة إلى نموذج بشكل رئيسي على التعليم</span>
(Learning)<span dir="rtl">.</span> <span dir="rtl">وعلى الرغم من وجود
اختلافات حقيقية بين هذين النوعين من الطرق، إلا أن هناك أيضًا تشابهات
كبيرة بينهما. على وجه الخصوص، يتمثل قلب كلا النوعين من الطرق في حساب
دوال القيمة</span> <span dir="rtl">(</span>Value
<span dir="rtl"></span>Functions<span dir="rtl">).</span>
<span dir="rtl">بالإضافة إلى ذلك، تعتمد جميع الطرق على التطلع إلى
الأحداث المستقبلية، وحساب قيمة مرتجعة</span> (Backed-up
Value)<span dir="rtl">، ثم استخدامها كهدف لتحديث دالة القيمة
التقريبية</span> (Approximate Value Function)<span dir="rtl">.</span>
<span dir="rtl">في وقت سابق من هذا الكتاب، قدمنا طرق مونت كارلو وطرق
الفارق الزمني كخيارات متميزة، ثم أظهرنا كيف يمكن توحيدها باستخدام طرق
الخطوات  
المتعددة</span> (n-Step Methods)<span dir="rtl">.</span>
<span dir="rtl">هدفنا في هذا الفصل هو تحقيق دمج مشابه بين الطرق المستندة
إلى نموذج والطرق غير المستندة إلى نموذج. بعد أن قمنا بتحديد هذه الطرق
كمتمايزة في الفصول السابقة، سنقوم الآن باستكشاف مدى إمكانية
مزجها</span>.

**<u>8.1 <span dir="rtl">النماذج والتخطيط</span>
<span dir="rtl">(</span>Models and
Planning<span dir="rtl">)</span></u>**

<span dir="rtl">بمفهوم النموذج</span> (Model)
<span dir="rtl">للبيئة</span> (Environment)<span dir="rtl">، نعني به أي
شيء يمكن للعميل</span> (Agent) <span dir="rtl">استخدامه للتنبؤ بكيفية
استجابة البيئة لإجراءاته</span> (Actions)<span dir="rtl">.</span>
<span dir="rtl">عند إعطاء الحالة</span> (State)
<span dir="rtl">والإجراء</span> (Action)<span dir="rtl">، يقوم النموذج
بإنتاج توقع للحالة التالية والمكافأة التالية. إذا كان النموذج
عشوائيًا</span> (Stochastic)<span dir="rtl">، فهناك عدة حالات ومكافآت
محتملة تالية، كل منها يحدث بفرصة معينة. بعض النماذج تنتج وصفًا لجميع
الاحتمالات واحتمالات حدوثها؛ هذه النماذج نطلق عليها نماذج التوزيع</span>
(Distribution Models)<span dir="rtl">.</span> <span dir="rtl">نماذج أخرى
تنتج فقط إحدى الاحتمالات، والتي تم اختيارها بناءً على الاحتمالات؛ هذه
النماذج نطلق عليها نماذج العينة</span> (Sample
Models)<span dir="rtl">.</span> <span dir="rtl">على سبيل المثال، عند
التفكير في نمذجة مجموع دزينة من النرد، فإن نموذج التوزيع سيقدم جميع
المجاميع المحتملة واحتمالات حدوثها، بينما نموذج العينة سيقدم مجموعًا
فرديًا تم اختياره بناءً على توزيع الاحتمالات هذا</span>.

<span dir="rtl">النموذج المفترض في البرمجة الديناميكية</span> (Dynamic
Programming) - <span dir="rtl">وهو تقدير ديناميكيات عملية اتخاذ القرار
ماركوف</span> (MDP)<span dir="rtl">،</span> $`p(s',r \mid s,a)`$
<span dir="rtl">هو نموذج توزيع. أما النوع من النموذج المستخدم في مثال
البلاك جاك في الفصل الخامس، فهو نموذج عينة. نماذج التوزيع أقوى من نماذج
العينة من حيث أنها يمكن أن تُستخدم دائمًا لإنتاج عينات. ومع ذلك، في العديد
من التطبيقات، يكون من الأسهل بكثير الحصول على نماذج العينة مقارنة بنماذج
التوزيع. مثال النرد هو مثال بسيط على هذا. سيكون من السهل كتابة برنامج
كمبيوتر لمحاكاة رمي النرد وإعادة المجموع، ولكنه سيكون أصعب وأكثر عرضة
للخطأ عند محاولة حساب جميع المجاميع المحتملة واحتمالاتها</span>.

<span dir="rtl">يمكن استخدام النماذج لتقليد أو محاكاة التجربة</span>
(**Simulated Experience**)<span dir="rtl">.</span> <span dir="rtl">عند
إعطاء حالة بداية وإجراء، ينتج نموذج العينة انتقالًا ممكنًا، وينتج نموذج
التوزيع جميع الانتقالات الممكنة موزونة باحتمالات حدوثها. عند إعطاء حالة
بداية وسياسة</span> (Policy)<span dir="rtl">، يمكن لنموذج العينة إنتاج
حلقة كاملة، ويمكن لنموذج التوزيع أن يولد جميع الحلقات الممكنة
واحتمالاتها. في كلتا الحالتين، نقول إن النموذج يستخدم لمحاكاة البيئة
وإنتاج تجربة محاكاة</span>.

<span dir="rtl">يُستخدم مصطلح "التخطيط</span>" (Planning)
<span dir="rtl">بطرق مختلفة في مجالات مختلفة. نحن نستخدم هذا المصطلح
للإشارة إلى أي عملية حسابية تأخذ نموذجًا كمدخل وتنتج أو تحسن سياسة
للتفاعل مع البيئة النموذجية</span>.

<img src="./media/image76.png"
style="width:6.04646in;height:0.76892in" />

<span dir="rtl">في الذكاء الاصطناعي، هناك نهجان متميزان للتخطيط وفقًا
لتعريفنا</span> **<span dir="rtl">التخطيط في فضاء الحالة</span>
(State-Space Planning)**<span dir="rtl">، والذي يتضمن النهج الذي نتبعه
في هذا الكتاب، يُنظر إليه بشكل رئيسي على أنه بحث عبر **فضاء
الحالة**</span> **(State Space)** <span dir="rtl">للوصول إلى
**سياسة**</span> **(Policy)** <span dir="rtl">مثلى أو مسار أمثل نحو
الهدف. تقوم **الإجراءات**</span> **(Actions)** <span dir="rtl">بإحداث
انتقالات من **حالة**</span> **(State)** <span dir="rtl">إلى أخرى، وتُحسب
**دوال القيمة**</span> **(Value Functions)** <span dir="rtl">على
الحالات. فيما نسميه **التخطيط في فضاء الخطة** </span>**(Plan-Space
Planning)**<span dir="rtl">، يكون التخطيط بدلاً من ذلك بحثًا عبر **فضاء
الخطط  
(**</span>**Plan Space<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">تحوّل **العمليات**</span> **(Operators)**
<span dir="rtl">خطة إلى أخرى، وتُعرّف **دوال القيمة  
(**</span>**Value Functions<span dir="rtl">)</span>**<span dir="rtl">،
إن وجدت، على فضاء الخطط</span>.

<span dir="rtl">يتضمن **التخطيط في فضاء الخطة**</span> **(Plan-Space
Planning)** <span dir="rtl">طرقًا تطورية **(**</span>**Evolutionary
<span dir="rtl"></span>Methods<span dir="rtl">)
</span>**<span dir="rtl">و</span>"**<span dir="rtl">التخطيط بالترتيب
الجزئي</span> (Partial-Order Planning)**"<span dir="rtl">، وهو نوع شائع
من التخطيط في **الذكاء الاصطناعي**</span> **(Artificial Intelligence)**
<span dir="rtl">حيث لا يتم تحديد ترتيب الخطوات بالكامل في جميع مراحل
التخطيط. تطبيق طرق **فضاء الخطة**</span> **(Plan-Space)**
<span dir="rtl">بكفاءة على مشاكل القرارات المتسلسلة العشوائية</span>
**(Stochastic Sequential Decision Problems)** <span dir="rtl">التي تركز
عليها **التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">يعد أمرًا صعبًا، ولذلك لن نناقشها أكثر</span>
<span dir="rtl">(ولكن يمكن الرجوع إلى، مثلاً،</span> Russell and Norvig,
2010<span dir="rtl">)</span>.

<span dir="rtl">الرؤية الموحدة التي نقدمها في هذا الفصل هي أن جميع طرق
**التخطيط في فضاء الحالة**</span> **<span dir="rtl">(</span>State-Space
Planning**<span dir="rtl">) تشترك في هيكلية مشتركة، وهيكلية توجد أيضًا في
طرق **التعليم** </span>**(Learning)** <span dir="rtl">التي قدمناها في
هذا الكتاب. يتطلب الأمر بقية الفصل لتطوير هذه الرؤية، ولكن هناك فكرتان
أساسيتان: (1) جميع طرق **التخطيط في فضاء الحالة**</span> **(State-Space
Planning)** <span dir="rtl">تتضمن حساب **دوال القيمة**</span> **(Value
Functions)** <span dir="rtl">كخطوة وسيطة رئيسية نحو تحسين **السياسة**
</span>**(Policy)**<span dir="rtl">، و(2) تقوم بحساب **دوال
القيمة**</span> **(Value Functions)** <span dir="rtl">من خلال تحديثات أو
عمليات ارتجاع</span> <span dir="rtl">(</span>Backup
Operations<span dir="rtl">)</span> <span dir="rtl">تطبق على **تجربة
محاكاة** </span>**(Simulated Experience)**<span dir="rtl">.</span>
<span dir="rtl">يمكن تخطيط هذه الهيكلية المشتركة كما يلي</span>:

<img src="./media/image77.png"
style="width:6.26806in;height:0.67361in" />

<span dir="rtl">طرق البرمجة الديناميكية</span> (Dynamic Programming)
<span dir="rtl">تتوافق بوضوح مع هذه الهيكلية: فهي تقوم بتمشيط عبر فضاء
الحالات</span> (State Space)<span dir="rtl">، حيث تولد لكل حالة
توزيعًا</span> (Distribution) <span dir="rtl">للانتقالات الممكنة. ثم يتم
استخدام كل توزيع لحساب قيمة مرتجعة</span> (Backed-up Value)
<span dir="rtl">كهدف تحديث</span> (Update Target) <span dir="rtl">وتحديث
القيمة المقدرة للحالة. في هذا الفصل، نناقش أن العديد من طرق التخطيط في
فضاء الحالة</span> (State-Space Planning) <span dir="rtl">الأخرى تتوافق
أيضًا مع هذه الهيكلية، مع اختلاف الطرق الفردية فقط في نوعية التحديثات
التي تقوم بها، وترتيب تنفيذها، وكيفية الاحتفاظ بالمعلومات
المرتجعة</span>.

<span dir="rtl">النظر إلى طرق التخطيط</span> (Planning Methods)
<span dir="rtl">بهذه الطريقة يبرز علاقتها بطرق التعليم</span> (Learning
Methods) <span dir="rtl">التي وصفناها في هذا الكتاب. يكمن جوهر كل من طرق
التعليم والتخطيط في تقدير دوال القيمة</span> (Value Functions)
<span dir="rtl">من خلال عمليات التحديث</span> (Update Operations)
<span dir="rtl">المستندة إلى الارتجاع</span>
(Backing-up)<span dir="rtl">.</span> <span dir="rtl">الفرق هو أن التخطيط
يستخدم تجربة محاكاة</span> <span dir="rtl">(</span>Simulated
<span dir="rtl"></span>Experience<span dir="rtl">)</span>
<span dir="rtl">يتم توليدها بواسطة نموذج</span> (Model)<span dir="rtl">،
بينما تستخدم طرق التعليم تجربة حقيقية</span>
<span dir="rtl">(</span>Real
<span dir="rtl"></span>Experience<span dir="rtl">)</span>
<span dir="rtl">يتم توليدها بواسطة البيئة</span>
(Environment)<span dir="rtl">.</span> <span dir="rtl">بالطبع، يؤدي هذا
الفرق إلى العديد من الفروقات الأخرى، مثل كيفية تقييم الأداء ومدى مرونة
توليد التجربة. ولكن الهيكلية المشتركة تعني أنه يمكن نقل العديد من
الأفكار والخوارزميات بين التخطيط والتعليم. على وجه الخصوص، في العديد من
الحالات يمكن استبدال خوارزمية التعليم بالخطوة الأساسية لتحديث طريقة
التخطيط. تتطلب طرق التعليم فقط التجربة كمدخل، وفي العديد من الحالات يمكن
تطبيقها على تجربة محاكاة</span> <span dir="rtl">(</span>Simulated
<span dir="rtl"></span>Experience<span dir="rtl">) بنفس الكفاءة كما في
التجربة الحقيقية</span>.

<span dir="rtl">الصندوق أدناه يظهر مثالًا بسيطًا على طريقة تخطيط تعتمد على
التعليم الجدولي بخطوة واحدة  
(</span>One-Step Tabular Q-Learning<span dir="rtl">)</span>
<span dir="rtl">وعلى عينات عشوائية من نموذج عينة</span>
<span dir="rtl">(</span>Sample
<span dir="rtl"></span>Model<span dir="rtl">).</span>
<span dir="rtl">هذه الطريقة، التي نسميها التخطيط الجدولي بخطوة واحدة مع
عينات عشوائية  
(</span>Random-Sample One-Step Tabular Q-Planning<span dir="rtl">)،
تتقارب إلى السياسة المثلى</span> (Optimal Policy)
<span dir="rtl">للنموذج تحت نفس الشروط التي تتقارب فيها التعليم الجدولي
بخطوة واحدة</span> (One-Step Tabular Q-Learning) <span dir="rtl">إلى
السياسة المثلى للبيئة الحقيقية</span> <span dir="rtl">(</span>Real
Environment<span dir="rtl">) (يجب أن يتم اختيار كل زوج من الحالة
والإجراء عددًا لا نهائيًا من المرات في الخطوة 1، ويجب أن يتناقص</span>
$`\mathbf{\alpha}`$ <span dir="rtl">بشكل مناسب مع مرور الوقت)</span>.

<span dir="rtl">التخطيط الجدولي بخطوة واحدة مع عينات عشوائية</span>
<span dir="rtl">(</span>Random-Sample One-Step Tabular
<span dir="rtl"></span>Q-Planning<span dir="rtl">)</span>

<img src="./media/image78.png"
style="width:6.11533in;height:1.6828in" />

<span dir="rtl">بالإضافة إلى الرؤية الموحدة لطرق التخطيط</span>
(Planning) <span dir="rtl">وطرق التعليم</span>
(Learning)<span dir="rtl">، هناك موضوع ثانٍ في هذا الفصل يتمثل في فوائد
التخطيط بخطوات صغيرة وتدريجية. هذا النهج يمكن من مقاطعة أو إعادة توجيه
التخطيط في أي وقت مع القليل من الحسابات المهدورة، وهو ما يبدو أنه متطلب
رئيسي لتحقيق التمازج الفعال بين التخطيط والتفاعل مع البيئة وتعلم
النموذج</span> (Model Learning)<span dir="rtl">.</span>
<span dir="rtl">قد يكون التخطيط بخطوات صغيرة جدًا هو النهج الأكثر كفاءة
حتى في مشاكل التخطيط البحتة، إذا كانت المشكلة كبيرة جدًا بحيث لا يمكن
حلها بدقة</span>.

**<u>8.2 <span dir="rtl">داينا: التخطيط والتفاعل والتعليم
المتكامل</span> <span dir="rtl">(</span>Dyna: Integrated Planning,
Acting, and
<span dir="rtl"></span>Learning<span dir="rtl">)</span></u>**

<span dir="rtl">عند إجراء التخطيط عبر الإنترنت أثناء التفاعل مع البيئة،
تنشأ مجموعة من القضايا المثيرة للاهتمام. قد تؤدي المعلومات الجديدة
المكتسبة من التفاعل إلى تغيير النموذج</span> (Model)
<span dir="rtl">وبالتالي التفاعل مع عملية التخطيط</span>
(Planning)<span dir="rtl">.</span> <span dir="rtl">قد يكون من المرغوب
تخصيص عملية التخطيط بطريقة ما لتتناسب مع الحالات</span> (States)
<span dir="rtl">أو القرارات الحالية أو المتوقعة في المستقبل القريب. إذا
كانت عملية اتخاذ القرارات وتعلم النموذج تتطلبان الكثير من الحسابات، فقد
يكون من الضروري تقسيم الموارد الحسابية المتاحة بينهما</span>.

<span dir="rtl">لبدء استكشاف هذه القضايا، نقدم في هذا
القسم</span>**-Q<span dir="rtl">داينا</span> (Dyna-Q)**<span dir="rtl">،
وهي بنية بسيطة تدمج الوظائف الرئيسية المطلوبة في عميل تخطيط عبر
الإنترنت. كل وظيفة تظهر في</span> **Q<span dir="rtl">-داينا</span>**
<span dir="rtl">**ا**في شكل بسيط للغاية. في الأقسام اللاحقة، سنقوم
بتفصيل بعض الطرق البديلة لتحقيق كل وظيفة والمقايضات بينها. في الوقت
الحالي، نسعى فقط لتوضيح الأفكار وتحفيز الحدس لديك</span>.

<span dir="rtl">داخل عميل التخطيط</span>
<span dir="rtl">(</span>Planning Agent<span dir="rtl">)، هناك على الأقل
دورين للتجربة الحقيقية: يمكن استخدامها لتحسين النموذج</span> (Model)
<span dir="rtl">لجعله أكثر دقة في محاكاة البيئة الحقيقية، ويمكن
استخدامها لتحسين دالة القيمة</span> (Value Function)
<span dir="rtl">والسياسة</span> (Policy) <span dir="rtl">مباشرة باستخدام
أنواع من طرق التعليم المعزز</span> (Reinforcement Learning)
<span dir="rtl">التي ناقشناها في الفصول السابقة. الدور الأول نسميه
**تعلم النموذج** </span>**(Model-Learning)**<span dir="rtl">، والدور
الثاني نسميه **التعليم المعزز المباشر**</span>
**<span dir="rtl">(</span>Direct Reinforcement
<span dir="rtl"></span>Learning <span dir="rtl">أو</span> Direct
RL<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">العلاقات المحتملة بين التجربة والنموذج والقيم والسياسة
ملخصة في الرسم البياني على اليمين. يظهر كل سهم علاقة تأثير وتحسين مفترض.
لاحظ كيف يمكن للتجربة أن تحسن دوال القيمة والسياسات إما بشكل مباشر أو
غير مباشر عبر النموذج. يتمثل الأخير، الذي يسمى أحيانًا **التعليم المعزز
غير المباشر** </span>**(Indirect Reinforcement
Learning)**<span dir="rtl">، في عملية التخطيط</span>.

<img src="./media/image79.png"
style="width:4.48372in;height:3.4253in" />

<span dir="rtl">لكل من الطرق المباشرة وغير المباشرة مزايا وعيوب. غالبًا
ما تجعل الطرق غير المباشرة الاستفادة الكاملة من كمية محدودة من التجربة
وتحقق سياسة أفضل مع تفاعلات أقل مع البيئة. من ناحية أخرى، تكون الطرق
المباشرة أبسط بكثير ولا تتأثر بالانحيازات في تصميم النموذج. جادل البعض
بأن الطرق غير المباشرة دائمًا متفوقة على المباشرة، في حين جادل آخرون بأن
الطرق المباشرة هي المسؤولة عن معظم التعليم البشري والحيواني. النقاشات
ذات الصلة في علم النفس والذكاء الاصطناعي تتعلق بالأهمية النسبية للإدراك
مقابل التعليم من خلال المحاولة والخطأ، والتخطيط المدروس مقابل اتخاذ
القرارات التفاعلي (سيتم مناقشة بعض هذه القضايا من منظور علم النفس في
الفصل 14)</span>.

<span dir="rtl">رؤيتنا هي أن التباين بين البدائل في كل هذه النقاشات قد
تم تضخيمه، وأنه يمكن اكتساب المزيد من الفهم من خلال التعرف على أوجه
التشابه بين الجانبين بدلاً من معارضتهما. على سبيل المثال، في هذا الكتاب
قمنا بتسليط الضوء على أوجه التشابه العميقة بين البرمجة
الديناميكية</span> <span dir="rtl">(</span>Dynamic
<span dir="rtl"></span>Programming<span dir="rtl">) وطرق الفارق
الزمني</span> (Temporal-Difference Methods)<span dir="rtl">، على الرغم
من أن الأولى تم تصميمها للتخطيط والأخرى للتعليم غير المستند إلى
نموذج</span> (Model-Free Learning)<span dir="rtl">.</span>

<span dir="rtl">تتضمن</span> **-Q<span dir="rtl">داينا</span> (Dyna-Q)**
<span dir="rtl">جميع العمليات الموضحة في الرسم البياني
أعلاه—التخطيط</span> (Planning)<span dir="rtl">، التفاعل</span>
(Acting)<span dir="rtl">، تعلم النموذج</span>
<span dir="rtl">(</span>Model-Learning<span dir="rtl">)، والتعليم المعزز
المباشر</span> (Direct RL) <span dir="rtl">وكلها تحدث باستمرار. طريقة
التخطيط المستخدمة هي طريقة **التخطيط الجدولي بخطوة واحدة مع عينات
عشوائية**</span> **(Random-Sample One-Step Tabular Q-Planning)**
<span dir="rtl">التي تمت مناقشتها في الصفحة 161. طريقة التعليم المعزز
المباشر هي **التعليم الجدولي بخطوة واحدة** </span>**(One-Step Tabular
Q-Learning)**<span dir="rtl">. طريقة تعلم النموذج</span>
(Model-Learning) <span dir="rtl">تعتمد أيضًا على الجداول وتفترض أن البيئة
حتمية</span> (Deterministic)<span dir="rtl">.</span> <span dir="rtl">بعد
كل انتقال</span> $`St,At \rightarrow R_{t + 1}`$ ​<span dir="rtl">، يسجل
النموذج في مدخل جدوله الخاص بـ</span> $`St,At`$ <span dir="rtl"></span>​
<span dir="rtl">التوقع بأن</span> $`R_{t + 1}`$, $`S_{t + 1}`$
<span dir="rtl">سيتبعان بشكل حتمي. وبالتالي، إذا تم استعلام النموذج بزوج
من الحالة والإجراء سبق تجربته، فإنه ببساطة يعود إلى الحالة التالية
والمكافأة التالية التي لوحظت مؤخرًا كتوقع له. أثناء التخطيط، تقوم
خوارزمية **التخطيط الجدولي  
(**</span>**Q-Planning<span dir="rtl">)</span>** <span dir="rtl">بأخذ
عينات عشوائية فقط من أزواج الحالة-الإجراء التي تمت تجربتها سابقًا  
(في الخطوة 1)، لذلك لا يتم استعلام النموذج أبدًا بزوج ليس لديه معلومات
عنه</span>.

<span dir="rtl">الهيكلية العامة لوكلاء **داينا** </span>**(Dyna
Agents)**<span dir="rtl">، والتي تعد خوارزمية</span> **-Q
<span dir="rtl">داينا</span> (Dyna-Q)** <span dir="rtl">مثالًا عليها،
موضحة في الشكل 8.1. العمود المركزي يمثل التفاعل الأساسي بين
**العميل**</span> **(Agent)** <span dir="rtl">و**البيئة**
</span>**(Environment)**<span dir="rtl">، مما يؤدي إلى مسار من **التجربة
الحقيقية** </span>**(Real Experience)**<span dir="rtl">.</span>
<span dir="rtl">السهم الموجود على يسار الشكل يمثل **التعليم المعزز
المباشر** </span>**(Direct Reinforcement Learning)**
<span dir="rtl">الذي يعمل على التجربة الحقيقية لتحسين **دالة
القيمة**</span> **(Value Function)** <span dir="rtl">و**السياسة**
</span>**(Policy)**<span dir="rtl">. على اليمين توجد العمليات المستندة
إلى **النموذج** </span>**(Model-Based
Processes)**<span dir="rtl">.</span> <span dir="rtl">يتم تعلم
**النموذج**</span> **(Model)** <span dir="rtl">من التجربة الحقيقية وينتج
عن ذلك **تجربة محاكاة**</span> **<span dir="rtl">(</span>Simulated
<span dir="rtl"></span>Experience<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">نستخدم مصطلح **التحكم في البحث**</span> **(Search
Control)** <span dir="rtl">للإشارة إلى العملية التي تختار
**الحالات**</span> **(States)** <span dir="rtl">و**الإجراءات**</span>
**(Actions)** <span dir="rtl">المبدئية للتجارب المحاكاة التي يتم توليدها
بواسطة النموذج</span>.

<span dir="rtl">أخيرًا، يتم تحقيق **التخطيط**</span> **(Planning)**
<span dir="rtl">من خلال تطبيق **طرق التعليم المعزز  
(**</span>**Reinforcement Learning Methods<span dir="rtl">)
</span>**<span dir="rtl">على التجارب المحاكاة كما لو أنها حدثت فعليًا.
عادةً، كما هو الحال في</span> **Q<span dir="rtl">-داينا</span>
(Dyna-Q)**<span dir="rtl">، يتم استخدام نفس طريقة التعليم المعزز سواءً
للتعليم من التجربة الحقيقية أو للتخطيط من التجربة المحاكاة. وبالتالي،
فإن طريقة التعليم المعزز تعتبر "المسار المشترك النهائي" لكل من
**التعليم**</span> **(Learning)** <span dir="rtl">و**التخطيط**</span>
**<span dir="rtl">(</span>Planning<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">يتم دمج التعليم والتخطيط بعمق بمعنى أنهما يشتركان تقريبًا
في جميع الآليات نفسها، ويختلفان فقط في مصدر التجربة الخاصة بكل
منهما</span>.

<img src="./media/image80.png"
style="width:6.26806in;height:4.45694in" />**<span dir="rtl">الشكل 8.1:
هيكلية داينا العامة</span> (The General Dyna
Architecture)<span dir="rtl">.</span>** <span dir="rtl">تؤثر **التجربة
الحقيقية** </span>**(Real Experience)**<span dir="rtl">، التي تتبادل
ذهابًا وإيابًا بين **البيئة**</span> **(Environment)**
<span dir="rtl">و**السياسة** </span>**(Policy)**<span dir="rtl">، على
**السياسة**</span> **(Policy)** <span dir="rtl">و**دوال القيمة**</span>
**(Value Functions)** <span dir="rtl">بنفس الطريقة التي تؤثر بها **تجربة
المحاكاة**</span> **(Simulated Experience)** <span dir="rtl">التي يتم
توليدها بواسطة **نموذج البيئة**</span> **<span dir="rtl">(</span>Model
of the <span dir="rtl"></span>Environment<span dir="rtl">)</span>**.

<span dir="rtl">من الناحية المفاهيمية، فإن عمليات **التخطيط**
</span>**(Planning)**<span dir="rtl">، **التفاعل**
</span>**(Acting)**<span dir="rtl">، **تعلم النموذج**
</span>**(Model-Learning)**<span dir="rtl">، و**التعليم المعزز
المباشر**</span> **<span dir="rtl">(</span>Direct Reinforcement
Learning - <span dir="rtl"></span>Direct RL<span dir="rtl">)
</span>**<span dir="rtl">تحدث بشكل متزامن ومتوازي في وكلاء **داينا**
</span>**(Dyna Agents)**<span dir="rtl">.</span> <span dir="rtl">ولكن من
أجل التحديد الدقيق والتنفيذ على حاسوب تسلسلي، نقوم بتحديد الترتيب الذي
تحدث فيه هذه العمليات ضمن خطوة زمنية واحدة. في</span>**-Q
<span dir="rtl">داينا</span> (Dyna-Q)**<span dir="rtl">، تتطلب عمليات
**التفاعل** </span>**(Acting)**<span dir="rtl">، **تعلم النموذج  
(**</span>**Model-Learning<span dir="rtl">)</span>**<span dir="rtl">،
و**التعليم المعزز المباشر**</span> **(Direct RL)**
<span dir="rtl">القليل من الحسابات، ونفترض أنها تستهلك جزءًا صغيرًا فقط من
الوقت. يمكن تخصيص الوقت المتبقي في كل خطوة لعملية **التخطيط**
</span>**(Planning)**<span dir="rtl">، التي تعتبر ذات طبيعة حسابية
مكثفة</span>.

<span dir="rtl">لنفرض أن هناك وقتًا كافيًا في كل خطوة، بعد
**التفاعل**</span> **(Acting)**<span dir="rtl">، **تعلم النموذج**</span>
**<span dir="rtl">(</span>Model-Learning<span dir="rtl">)</span>**<span dir="rtl">،
و**التعليم المعزز المباشر** </span>**(Direct RL)**<span dir="rtl">،
لإكمال</span> $`n`$ <span dir="rtl">من التكرارات (الخطوات 1–3) لخوارزمية
**التخطيط الجدولي** </span>**(Q-Planning)**<span dir="rtl">.</span>
<span dir="rtl">في **الكود الزائف**</span> **(Pseudocode)**
<span dir="rtl">لخوارزمية **  **
</span>**Q<span dir="rtl">-</span>** **<span dir="rtl">داينا</span>
(Dyna-Q) <span dir="rtl"></span>**<span dir="rtl">في الصندوق
أدناه،</span> **Model** $`\mathbf{(s,\ a)}`$ <span dir="rtl">تشير إلى
محتويات (الحالة التالية المتوقعة والمكافأة) لزوج الحالة–الإجراء</span>
$`(s,\ a)`$<span dir="rtl">.</span> <span dir="rtl">يتم تنفيذ **التعليم
المعزز المباشر**</span> **<span dir="rtl">(</span>Direct
<span dir="rtl"></span>Reinforcement
Learning<span dir="rtl">)</span>**<span dir="rtl">، **تعلم النموذج**
</span>**(Model-Learning)**<span dir="rtl">، و**التخطيط**
</span>**(Planning)** <span dir="rtl">في الخطوات</span>
($`d`$)<span dir="rtl">،</span> ($`e`$)<span dir="rtl">، و</span>($`f`$)
<span dir="rtl">على التوالي. إذا تم إهمال الخطوتين</span> ($`e`$)
<span dir="rtl">و</span>($`f`$)<span dir="rtl">، فإن الخوارزمية المتبقية
ستكون **التعليم الجدولي بخطوة واحدة** </span>**(One-Step Tabular
Q-Learning)**<span dir="rtl">.</span>

<span dir="rtl">داينا</span>-Q <span dir="rtl">الجدولي</span> (Tabular
Dyna-Q)

<img src="./media/image81.png"
style="width:6.04219in;height:2.76691in" />

### <span dir="rtl">**<u>المثال 8.1</u>**: متاهة داينا</span> (Dyna Maze)

<span dir="rtl">لننظر في المتاهة البسيطة الموضحة في الشكل الصغير ضمن
الشكل 8.2. في كل واحدة من الحالات الـ 47، هناك أربعة إجراءات: أعلى،
أسفل، يمين، ويسار، والتي تأخذ **العميل**</span> **(Agent)**
<span dir="rtl">بشكل حتمي إلى الحالات المجاورة المقابلة، باستثناء عندما
يتم حجب الحركة بواسطة عائق أو حافة المتاهة، حيث يبقى العميل في مكانه.
تكون المكافأة صفرًا في جميع الانتقالات، باستثناء تلك التي تؤدي إلى **حالة
الهدف** </span>**(Goal State)**<span dir="rtl">، حيث تكون المكافأة 1+.
بعد الوصول إلى **حالة الهدف**
</span>**(**$`\mathbf{G}`$**)**<span dir="rtl">، يعود العميل إلى **حالة
البدء**</span> **(Start State - S)** <span dir="rtl">لبدء حلقة جديدة.
هذه مهمة متسلسلة بخصم، حيث</span>
$`\gamma = 0.95`$<span dir="rtl">.</span>

<span dir="rtl">الجزء الرئيسي من الشكل 8.2 يظهر منحنيات التعليم المتوسطة
من تجربة تم فيها تطبيق **وكلاء**  
</span>**-Q<span dir="rtl">داينا</span> (Dyna-Q Agents)
<span dir="rtl"></span>**<span dir="rtl">على مهمة المتاهة. كانت قيم
الإجراءات الأولية صفرًا، وكانت **بارامتر حجم الخطوة** </span>**(Step-Size
Parameter)** $`\alpha = 0.1`$<span dir="rtl">، وبارامتر الاستكشاف</span>
$`\epsilon = 0.1`$<span dir="rtl">.</span> <span dir="rtl">عند اختيار
الإجراءات الجشعة، تم كسر التعادلات بشكل عشوائي. ّ  
اختلف الوكلاء في عدد خطوات التخطيط</span> $`n`$ <span dir="rtl">التي
قاموا بها لكل خطوة حقيقية. لكل قيمة من</span> $`n`$<span dir="rtl">،
تظهر المنحنيات عدد الخطوات التي استغرقها العميل للوصول إلى الهدف في كل
حلقة، وكان المتوسط مأخوذًا على 30 تكرارًا للتجربة. في كل تكرار، تم
الاحتفاظ بالبذرة الأولية لتوليد الأرقام العشوائية ثابتة عبر الخوارزميات.
بسبب هذا، كانت الحلقة الأولى متطابقة تمامًا (حوالي 1700 خطوة) لجميع
قيم</span> $`n`$<span dir="rtl">، ولم يتم عرض بياناتها في الشكل. بعد
الحلقة الأولى، تحسن الأداء لجميع قيم</span> $`n`$<span dir="rtl">، ولكن
بشكل أسرع بكثير للقيم الأكبر. تذكر أن العميل الذي يستخدم</span>
$`n = 0`$ <span dir="rtl">هو عميل غير مخطط</span>
<span dir="rtl">(</span>Nonplanning Agent<span dir="rtl">)، يستخدم فقط
**التعليم المعزز المباشر**</span> **(Direct Reinforcement Learning)**
<span dir="rtl">باستخدام **التعليم الجدولي بخطوة واحدة**
</span>**(One-Step Tabular Q-Learning)**<span dir="rtl">.</span>
<span dir="rtl">كان هذا العميل هو الأبطأ بفارق كبير في حل هذه المشكلة،
على الرغم من أن قيم المعلمات</span> $`\alpha`$
<span dir="rtl">و</span>$`\epsilon`$ <span dir="rtl">تم تحسينها له.
استغرق العميل غير المخطط حوالي 25 حلقة للوصول إلى الأداء المثالي
تقريبيًا، في حين استغرق العميل</span> $`n = 5`$ <span dir="rtl">حوالي خمس
حلقات، واستغرق العميل</span> $`n = 50`$ <span dir="rtl">ثلاث حلقات
فقط</span>.

<img src="./media/image82.png"
style="width:6.26806in;height:4.75625in" /><span dir="rtl">الشكل 8.2:
متاهة بسيطة (موضحة في الجزء الصغير) ومنحنيات التعليم المتوسطة  
**لوكلاء**</span>**-Q <span dir="rtl">داينا</span> (Dyna-Q Agents)**
<span dir="rtl">الذين يختلفون في عدد خطوات التخطيط</span> ($`n`$)
<span dir="rtl">لكل خطوة حقيقية. المهمة هي الانتقال من</span> $`S`$
<span dir="rtl">إلى</span> $`G`$ <span dir="rtl">بأسرع ما يمكن</span>.

<span dir="rtl">**<u>الشكل 8.3</u>:** يوضح لماذا وجدت الوكلاء
المخططة</span> (Planning Agents) <span dir="rtl">الحل بشكل أسرع بكثير من
الوكيل غير المخطط</span> (Nonplanning Agent)<span dir="rtl">.</span>
<span dir="rtl">تظهر السياسات التي وجدها العميل</span> $`n = 0`$
<span dir="rtl">والعميل</span> $`n = 50`$ <span dir="rtl">في منتصف
الطريق خلال الحلقة الثانية. بدون تخطيط</span>
($`n = 0`$)<span dir="rtl">، كل حلقة تضيف خطوة إضافية واحدة فقط إلى
السياسة، وبالتالي تم تعلم خطوة واحدة فقط (الأخيرة) حتى الآن. مع التخطيط،
يتم تعلم خطوة واحدة فقط خلال الحلقة الأولى، ولكن هنا خلال الحلقة الثانية
تم تطوير سياسة واسعة النطاق بحيث تصل في نهاية الحلقة تقريبًا إلى حالة
البدء. تم بناء هذه السياسة بواسطة عملية التخطيط بينما لا يزال العميل
يتجول بالقرب من حالة البدء. بنهاية الحلقة الثالثة، سيتم العثور على سياسة
مثلى كاملة وتحقيق أداء مثالي</span>.

<img src="./media/image83.png"
style="width:6.26806in;height:2.02778in" /><span dir="rtl">**<u>الشكل
8.3</u>**: السياسات التي وجدها وكلاء داينا</span>-Q
<span dir="rtl">المخططة</span> (Planning) <span dir="rtl">وغير
المخططة</span> (Nonplanning) <span dir="rtl">في منتصف الحلقة الثانية.
تشير الأسهم إلى الإجراء الجشع</span> (Greedy Action) <span dir="rtl">في
كل حالة؛ إذا لم تظهر أي سهم في حالة ما، فهذا يعني أن جميع قيم الإجراءات
فيها كانت متساوية. يشير المربع الأسود إلى موقع العميل</span>
(Agent)<span dir="rtl">.</span>

<span dir="rtl">في</span>**-Q <span dir="rtl">داينا</span>
(Dyna-Q)**<span dir="rtl">، يتم إنجاز التعليم والتخطيط بواسطة نفس
الخوارزمية تمامًا، حيث تعمل على **التجربة الحقيقية**</span> **(Real
Experience)** <span dir="rtl">لأغراض التعليم، وعلى **تجربة
المحاكاة**</span> **<span dir="rtl">(</span>Simulated
<span dir="rtl"></span>Experience<span dir="rtl">)
</span>**<span dir="rtl">لأغراض التخطيط. وبما أن التخطيط يتم بشكل
تدريجي، فمن السهل جدًا دمج التخطيط مع التفاعل</span>
(**Acting**)<span dir="rtl">.</span> <span dir="rtl">كلاهما يتقدمان
بأسرع ما يمكن. يظل **العميل**</span> **(Agent)** <span dir="rtl">دائمًا
تفاعليًا ودائمًا مدروسًا، حيث يستجيب فورًا لأحدث المعلومات الحسية، ومع ذلك
يستمر في التخطيط في الخلفية. كما يستمر في الخلفية أيضًا عملية **تعلم
النموذج**</span>
**<span dir="rtl">(</span>Model-Learning<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">مع اكتساب معلومات جديدة، يتم تحديث **النموذج**</span>
**(Model)** <span dir="rtl">ليكون أكثر تطابقًا مع الواقع. وعندما يتغير
النموذج، ستقوم عملية التخطيط المستمرة تدريجيًا بحساب طريقة مختلفة للسلوك
لتتوافق مع النموذج الجديد</span>.

<span dir="rtl">**<u>تمرين 8.1</u>**:</span> <span dir="rtl">يبدو أن
الطريقة غير المخططة</span> (Nonplanning Method) <span dir="rtl">تظهر
بشكل سيئ في الشكل 8.3 لأنها تعتمد على طريقة خطوة واحدة</span> (One-Step
Method)<span dir="rtl">؛ يمكن أن تؤدي طريقة تستخدم **التقوية المتعددة
الخطوات**</span> **(Multi-Step Bootstrapping)** <span dir="rtl">بشكل
أفضل. هل تعتقد أن إحدى طرق التقوية المتعددة الخطوات من الفصل 7 يمكن أن
تؤدي كما تفعل طريقة **داينا**</span>
**<span dir="rtl">(</span>Dyna<span dir="rtl">)</span>**<span dir="rtl">؟
اشرح لماذا أو لماذا لا</span>.

**<u>8.3 <span dir="rtl">عندما يكون النموذج خاطئًا</span> (When the Model
Is Wrong)</u>**

<span dir="rtl">في مثال المتاهة</span> (**Maze**) <span dir="rtl">الذي
قُدم في القسم السابق، كانت التغييرات في **النموذج**</span> **(Model)**
<span dir="rtl">متواضعة نسبيًا. بدأ النموذج فارغًا، ثم تم ملؤه فقط
بالمعلومات الصحيحة تمامًا. بشكل عام، لا يمكننا أن نتوقع دائمًا أن نكون
محظوظين بهذا الشكل. قد تكون النماذج غير صحيحة لأن **البيئة**
</span>**(Environment)** <span dir="rtl">عشوائية وتمت ملاحظة عدد محدود
فقط من العينات، أو لأن النموذج تم تعلمه باستخدام **تقريب الدوال**
</span>**(Function Approximation)** <span dir="rtl">الذي تم تعميمه بشكل
غير مثالي، أو ببساطة لأن البيئة قد تغيرت ولم يتم ملاحظة سلوكها الجديد
بعد. عندما يكون النموذج غير صحيح، فمن المحتمل أن تقوم عملية
**التخطيط**</span> **(Planning)** <span dir="rtl">بحساب **سياسة**</span>
**(Policy)** <span dir="rtl">دون المستوى الأمثل</span>.

<span dir="rtl">في بعض الحالات، تؤدي السياسة دون المستوى الأمثل التي تم
حسابها بواسطة التخطيط بسرعة إلى اكتشاف وتصحيح الخطأ في النموذج. يميل هذا
إلى الحدوث عندما يكون النموذج متفائلًا بمعنى أنه يتوقع مكافآت أكبر أو
انتقالات حالة أفضل مما هو ممكن فعليًا. تحاول السياسة المخططة استغلال هذه
الفرص، وفي القيام بذلك تكتشف أنها غير موجودة</span>.

### <span dir="rtl"><u>مثال 8.2</u>:</span> <span dir="rtl">متاهة الحَجب</span> (Blocking Maze)

<span dir="rtl">مثال على متاهة يوضح هذا النوع البسيط نسبيًا من الأخطاء في
النمذجة والتعافي منه موضح في **الشكل** **8.4**. في البداية، يوجد مسار
قصير من نقطة البداية إلى الهدف، على يمين الحاجز، كما هو موضح في الجزء
العلوي الأيسر من الشكل. بعد 1000 خطوة زمنية، يتم "حجب" المسار القصير،
ويتم فتح مسار أطول على الجانب الأيسر من الحاجز، كما هو موضح في الجزء
العلوي الأيمن من الشكل. يظهر الرسم البياني المكافأة التراكمية المتوسطة
لوكيل</span> **-Q<span dir="rtl">داينا  
</span>(Dyna-Q Agent)** <span dir="rtl">ووكيل</span>
**-Q+<span dir="rtl">داينا المحسن</span> (Enhanced Dyna-Q+ Agent)**
<span dir="rtl">الذي سيتم وصفه قريبًا. الجزء الأول من الرسم البياني يظهر
أن كلا الوكيلين من وكلاء داينا قد وجدوا المسار القصير خلال 1000 خطوة.
عندما تغيرت البيئة، أصبحت الرسوم البيانية مسطحة، مما يشير إلى فترة لم
يحصل فيها الوكلاء على أي مكافأة لأنهم كانوا يتجولون خلف الحاجز. بعد فترة
من الوقت، تمكنوا من العثور على الفتحة الجديدة والسلوك الأمثل
الجديد</span>.

<span dir="rtl">تظهر صعوبات أكبر عندما تتغير البيئة لتصبح أفضل مما كانت
عليه من قبل، ومع ذلك لا تكشف السياسة الصحيحة السابقة عن هذا التحسن. في
هذه الحالات، قد لا يتم اكتشاف خطأ النمذجة لفترة طويلة، إذا تم اكتشافه
على الإطلاق</span>.

<img src="./media/image84.png"
style="width:6.26806in;height:4.21806in" />

**<span dir="rtl">الشكل 8.4: الأداء المتوسط لوكلاء داينا</span> (Dyna
Agents) <span dir="rtl">في مهمة الحجب</span>
<span dir="rtl">(</span>Blocking Task<span dir="rtl">).</span>**
<span dir="rtl">تم استخدام البيئة اليسرى لأول 1000 خطوة، والبيئة اليمنى
لبقية الخطوات</span>**-Q+** **<span dir="rtl">داينا</span> (Dyna-Q+)
<span dir="rtl"></span>**<span dir="rtl">هو نسخة محسنة من</span>
**(Dyna-Q) <span dir="rtl"></span>**<span dir="rtl">مع مكافأة
**الاستكشاف**</span> **(Exploration)** <span dir="rtl">إضافية تشجع على
**الاستكشاف** </span>**(Exploration)**<span dir="rtl">.</span>

### <span dir="rtl">**<u>مثال 8.3</u>**:</span> <span dir="rtl">متاهة الاختصار</span> (Shortcut Maze)

<span dir="rtl">المشكلة الناتجة عن هذا النوع من التغير البيئي موضحة في
مثال **المتاهة**</span> **(Maze)** <span dir="rtl">في **الشكل** 8.5. في
البداية، كان **المسار الأمثل**</span> **(Optimal Path)**
<span dir="rtl">هو الانتقال حول الجانب الأيسر من الحاجز  
(الجزء العلوي الأيسر). ولكن بعد 3000 خطوة، تم فتح مسار أقصر على الجانب
الأيمن، دون تغيير **المسار الأطول**</span> **(Longer Path)**
<span dir="rtl">(الجزء العلوي الأيمن). يُظهر الرسم البياني أن
وكيل</span>**-Q <span dir="rtl">داينا</span> (Dyna-Q Agent)
<span dir="rtl"></span>**<span dir="rtl">العادي لم يتحول أبدًا إلى
**الاختصار** </span>**(Shortcut)**<span dir="rtl">.</span>
<span dir="rtl">في الواقع، لم يدرك أبدًا وجوده. كان **النموذج**</span>
**(Model)** <span dir="rtl">الخاص به يشير إلى أنه لا يوجد اختصار، لذا
كلما زاد</span>  
**<span dir="rtl">التخطيط</span> (Planning)**<span dir="rtl">، قلت
احتمالية أن يتحرك نحو اليمين ويكتشفه. حتى مع **سياسة** </span>**ϵ
-greedy**<span dir="rtl">، من غير المحتمل جدًا أن يقوم **العميل**</span>
**(Agent)** <span dir="rtl">بعدد كافٍ من إجراءات</span>  
**<span dir="rtl">الاستكشاف</span>
(Exploration)**<span dir="rtl">لاكتشاف الاختصار</span>.

<img src="./media/image85.png"
style="width:5.89218in;height:5.06711in" />

**<span dir="rtl">الشكل 8.5: الأداء المتوسط لوكلاء داينا</span> (Dyna
Agents) <span dir="rtl">في مهمة الاختصار</span>
<span dir="rtl">(</span>Shortcut
<span dir="rtl"></span>Task<span dir="rtl">).</span>**

<span dir="rtl">تم استخدام البيئة اليسرى لأول 3000 خطوة، والبيئة اليمنى
لبقية الخطوات</span>. <span dir="rtl">المشكلة العامة هنا هي نسخة أخرى من
الصراع بين الاستكشاف</span> (**Exploration**)
<span dir="rtl">**و**الاستغلال</span>
(**Exploitation**)**<span dir="rtl">.</span>** <span dir="rtl">في سياق
التخطيط</span> (**Planning**)<span dir="rtl">**،** يعني الاستكشاف</span>
(**Exploration**) <span dir="rtl">تجربة الإجراءات التي تحسن
النموذج</span> (**Model**)<span dir="rtl">**،** بينما يعني
الاستغلال</span> (**Exploitation**) <span dir="rtl">التصرف بالطريقة
المثلى بالنظر إلى النموذج الحالي. نريد أن يقوم العميل</span> (**Agent**)
<span dir="rtl">بالاستكشاف لاكتشاف التغييرات في البيئة</span>
<span dir="rtl">(</span>**Environment**<span dir="rtl">)**،** ولكن ليس
لدرجة أن يتدهور الأداء بشكل كبير. كما هو الحال في الصراع السابق بين
الاستكشاف</span> (**Exploration**) <span dir="rtl">والاستغلال</span>
(**Exploitation**)<span dir="rtl">**،** ربما لا توجد حل مثالي وعملي في
آن واحد، ولكن الاستراتيجيات البسيطة</span> (**Heuristics**)
<span dir="rtl">تكون غالبًا فعالة</span>.

**<span dir="rtl">وكيل</span>** -Q+ <span dir="rtl">داينا</span>
(**Dyna**-Q+ **Agent**) <span dir="rtl">الذي تمكن من حل متاهة
الاختصار</span> (**Shortcut** **Maze**) <span dir="rtl">يستخدم إحدى هذه
الاستراتيجيات</span> (**Heuristics**)**<span dir="rtl">.</span>**
<span dir="rtl">هذا الوكيل يحتفظ بتتبع للمدة الزمنية التي مرت منذ **آخر
مرة تم فيها تجربة زوج** الحالة</span>
(**State**)**–**<span dir="rtl">الإجراء</span> (**Action**)
<span dir="rtl">في تفاعل حقيقي مع البيئة</span>
(**Environment**)**<span dir="rtl">.</span>** <span dir="rtl">كلما زادت
المدة الزمنية، زادت (من المفترض) احتمالية أن ديناميكية هذا الزوج قد
تغيرت وأن النموذج</span> (**Model**) <span dir="rtl">الخاص به غير صحيح.
لتشجيع السلوك الذي يختبر الإجراءات التي لم تُجرَّب منذ فترة طويلة، يتم منح
"مكافأة إضافية</span> **(Bonus Reward)** <span dir="rtl">على التجارب
المحاكية</span> (**Simulated** **Experiences**) <span dir="rtl">التي
تتضمن هذه الإجراءات. بشكل خاص، إذا كانت المكافأة النموذج للانتقال
هي</span> $`\mathbf{r}`$<span dir="rtl">**،** ولم يتم تجربة الانتقال
في</span> $`\mathbf{\tau}`$ <span dir="rtl">خطوة زمنية، فإن تحديثات
التخطيط</span> (**Planning**) <span dir="rtl">تتم كما لو أن هذا الانتقال
أنتج مكافأة قدرها</span> $`\mathbf{r + \kappa\tau}`$
<span dir="rtl">لقيمة صغيرة من</span>
$`\mathbf{\kappa}`$**<span dir="rtl">.</span>** <span dir="rtl">هذا يشجع
العميل</span> (**Agent**) <span dir="rtl">على الاستمرار في اختبار جميع
انتقالات الحالات</span> (**State** **Transitions**)
<span dir="rtl">**الممكنة** وحتى العثور على تسلسلات طويلة من
الإجراءات</span> (**Actions**) <span dir="rtl">لإجراء هذه الاختبارات.
بالطبع، كل هذا الاختبار له تكلفته، ولكن في العديد من الحالات، كما هو
الحال في متاهة الاختصار (</span>**Shortcut**
<span dir="rtl"></span>**Maze**<span dir="rtl">)**،** يكون هذا النوع من
الفضول الحسابي جديرًا بالاستكشاف الإضافي</span>**.**

### **<span dir="rtl"><u>تمرين 8.2</u>: لماذا أدى وكيل</span>** -Q+<span dir="rtl">داينا</span> (Dyna-Q+ Agent) <span dir="rtl">**مع مكافأة** الاستكشاف</span> (Exploration) <span dir="rtl">أداءً **أفضل في المرحلة الأولى وكذلك في المرحلة الثانية من تجارب** الحجب</span> (Blocking) <span dir="rtl">**و**الاختصار</span> (Shortcut)**<span dir="rtl">؟</span>**

### <span dir="rtl">**<u>تمرين 8.3</u>:** الفحص الدقيق للشكل 8.5 يكشف أن الفرق بين</span> **-Q+<span dir="rtl">داينا </span>**<span dir="rtl"> </span>(Dyna-Q+ Agent) <span dir="rtl">**و**</span>-Q <span dir="rtl">داينا</span> (Dyna-Q) <span dir="rtl">قد تقلص قليلاً خلال الجزء الأول من التجربة. ما السبب في ذلك؟</span>

### <span dir="rtl">**<u>تمرين 8.4 (برمجة)</u>:** مكافأة الاستكشاف</span> (Exploration Bonus) <span dir="rtl">الموضحة أعلاه تغير بالفعل القيم المقدرة **لـ**لحالات</span> (States) <span dir="rtl">**و**الإجراءات</span> (Actions)**<span dir="rtl">.</span>** <span dir="rtl">هل هذا ضروري؟ افترض أنه تم استخدام المكافأة</span> $`\mathbf{\kappa\tau}`$ <span dir="rtl"></span>**​** <span dir="rtl">ليس في التحديثات، ولكن فقط في اختيار الإجراءات</span> (Actions)**<span dir="rtl">.</span>** <span dir="rtl">أي، افترض أن الإجراء</span> (Action) <span dir="rtl">المختار كان دائمًا هو الذي تكون فيه</span> $`\mathbf{Q(St,a) + \kappa\tau}`$ <span dir="rtl">هي القصوى. قم بإجراء تجربة شبكة العالم</span> (Gridworld) <span dir="rtl">التي تختبر وتوضح نقاط القوة والضعف في هذا النهج البديل</span>**.**

### <span dir="rtl">**<u>تمرين 8.5</u>:** كيف يمكن تعديل خوارزمية داينا</span>-Q <span dir="rtl">الجدولية</span> (Tabular Dyna-Q) <span dir="rtl">الموضحة في الصفحة 164 للتعامل مع البيئات العشوائية</span> (Stochastic Environments)<span dir="rtl">**؟** كيف يمكن أن يؤدي هذا التعديل بشكل سيئ في البيئات المتغيرة</span> (Changing Environments) <span dir="rtl">مثل تلك التي نوقشت في هذا القسم؟ كيف يمكن تعديل الخوارزمية للتعامل مع كل من البيئات العشوائية</span> <span dir="rtl">(</span>Stochastic <span dir="rtl"></span>Environments<span dir="rtl">)</span> <span dir="rtl">والبيئات المتغيرة</span> (Changing Environments)**<span dir="rtl">؟</span>**

**<u>8.4 <span dir="rtl">المسح بالأولوية</span>
<span dir="rtl">(</span>Prioritized
Sweeping<span dir="rtl">)</span></u>**

<span dir="rtl">ما يحدث خلال الحلقة الثانية من مهمة المتاهة الأولى
(الشكل 8.3). في بداية الحلقة الثانية، فقط زوج **الحالة–الإجراء**
</span>**(State–Action Pair)
<span dir="rtl"></span>**<span dir="rtl">الذي يؤدي مباشرة إلى الهدف لديه
قيمة إيجابية؛ أما قيم جميع الأزواج الأخرى فلا تزال صفرًا. هذا يعني أنه لا
فائدة من إجراء تحديثات على معظم الانتقالات، لأنها تأخذ **العميل**</span>
**(Agent)** <span dir="rtl">من حالة ذات قيمة صفرية إلى أخرى ذات قيمة
صفرية أيضًا، وبالتالي لن يكون لهذه التحديثات أي تأثير. فقط التحديث على
انتقال يؤدي إلى الحالة التي تسبق الهدف مباشرة، أو منها، سيغير أي قيم.
إذا تم توليد الانتقالات المحاكية بشكل موحد، فسيتم إجراء العديد من
التحديثات غير المجدية قبل الوصول إلى واحدة من تلك المفيدة. مع تقدم
التخطيط، تنمو منطقة التحديثات المفيدة، لكن التخطيط يظل أقل كفاءة بكثير
مما سيكون عليه إذا كان موجهًا حيث يمكن أن يكون له الأثر الأكبر. في
المشاكل الأكبر بكثير التي تشكل هدفنا الحقيقي، يكون عدد الحالات كبيرًا جدًا
بحيث يكون البحث غير الموجه غير فعال للغاية</span>.

<span dir="rtl">يشير هذا المثال إلى أن البحث قد يكون أكثر فعالية إذا تم
التركيز على العمل من الخلف بدءًا من  
**حالات الهدف**</span> **<span dir="rtl">(</span>Goal
States<span dir="rtl">).</span>** <span dir="rtl">بالطبع، لا نريد حقًا
استخدام أي طرق خاصة بفكرة "حالة الهدف". نريد طرقًا تعمل مع **دوال
المكافأة العامة**</span> **<span dir="rtl">(</span>General Reward
Functions<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">حالات الهدف هي مجرد حالة خاصة، مفيدة لتحفيز الفهم. بشكل
عام، نريد العمل من الخلف ليس فقط من حالات الهدف ولكن من أي حالة تغيرت
قيمتها. افترض أن القيم كانت صحيحة في البداية بناءً على **النموذج
(**</span>**Model<span dir="rtl">)</span>**<span dir="rtl">، كما كانت في
مثال المتاهة قبل اكتشاف الهدف. افترض الآن أن **العميل**</span>
**(Agent)** <span dir="rtl">اكتشف تغييرًا في **البيئة**</span>
**(Environment)** <span dir="rtl">وقام بتغيير القيمة المقدرة لحالة
واحدة، سواء بزيادة أو نقصان. عادةً، هذا يعني أن قيم العديد من الحالات
الأخرى يجب أن تتغير أيضًا، ولكن التحديثات المفيدة الوحيدة بخطوة واحدة هي
تلك الخاصة بالإجراءات التي تؤدي مباشرة إلى الحالة التي تغيرت قيمتها. إذا
تم تحديث قيم هذه الإجراءات، فقد تتغير قيم **الحالات السابقة**</span>
**(Predecessor States)** <span dir="rtl">بدورها. إذا حدث ذلك، فإن
الإجراءات التي تؤدي إليها تحتاج إلى تحديث، ثم قد تكون حالات الأسلاف
الخاصة بها قد تغيرت. بهذه الطريقة يمكن العمل من الخلف من حالات عشوائية
تغيرت قيمتها، إما بإجراء تحديثات مفيدة أو إنهاء الانتشار. قد يتم تسمية
هذه الفكرة العامة بـ “التركيز الخلفي لحسابات التخطيط</span> (Backward
Focusing of Planning Computations)<span dir="rtl">.</span>

<span dir="rtl">مع تقدم حدود التحديثات المفيدة للخلف، فإنها غالبًا ما
تنمو بسرعة، مما ينتج العديد من أزواج الحالة–الإجراء التي يمكن تحديثها
بشكل مفيد. ولكن ليس جميعها ستكون مفيدة بنفس القدر. قد تكون قيم بعض
الحالات قد تغيرت بشكل كبير، في حين أن أخرى قد تغيرت قليلاً. أزواج الأسلاف
لتلك التي تغيرت كثيرًا من المرجح أن تتغير أيضًا بشكل كبير. في بيئة
عشوائية</span> (Stochastic Environment)<span dir="rtl">، تساهم
الاختلافات في **الاحتمالات الانتقالية المقدرة**</span> **(Estimated
Transition Probabilities)** <span dir="rtl">أيضًا في الاختلافات في حجم
التغييرات وفي مدى إلحاح الحاجة إلى تحديث الأزواج. من الطبيعي إعطاء
الأولوية للتحديثات وفقًا لمقياس إلحاحها، وإجرائها بترتيب الأولوية. هذه هي
الفكرة وراء **التمشيط بالأولوية**</span> **(Prioritized
Sweeping)**<span dir="rtl">. يتم الحفاظ على قائمة انتظار</span>
(**Queue**) <span dir="rtl">تحتوي على كل زوج حالة–إجراء الذي من المحتمل
أن تتغير قيمته المقدرة بشكل كبير إذا تم تحديثه، مع إعطاء الأولوية حسب
حجم التغيير. عندما يتم تحديث الزوج الأعلى في قائمة الانتظار، يتم حساب
التأثير على كل من أزواج أسلافه. إذا كان التأثير أكبر من عتبة صغيرة
معينة، يتم إدخال الزوج في قائمة الانتظار مع الأولوية الجديدة (إذا كان
هناك إدخال سابق للزوج في قائمة الانتظار، فإن الإدخال الأعلى أولوية فقط
هو الذي يبقى في قائمة الانتظار). بهذه الطريقة، يتم نشر تأثيرات التغييرات
بكفاءة إلى الخلف حتى يحدث الهدوء</span>.

<span dir="rtl">الخوارزمية الكاملة لحالة البيئات الحتمية</span>
(Deterministic Environments) <span dir="rtl">مذكورة في الصندوق في الصفحة
التالية</span>.

<span dir="rtl">المسح بالأولوية</span> (Prioritized Sweeping)
<span dir="rtl">في بيئة حتمية</span> (Deterministic Environment)

<img src="./media/image86.png"
style="width:6.08386in;height:3.54197in" />

### <span dir="rtl"><u>مثال 8.4</u>:</span> <span dir="rtl">المسح بالأولوية</span> (Prioritized Sweeping) <span dir="rtl">في المتاهات</span> (Mazes)

<span dir="rtl">لقد وجد أن **المسح بالأولوية**</span> **(Prioritized
Sweeping)** <span dir="rtl">يزيد بشكل كبير من سرعة العثور على الحلول
المثلى في مهام المتاهة</span> (Maze Tasks)<span dir="rtl">، غالبًا بمقدار
5 إلى 10 مرات. يظهر مثال نموذجي على اليمين. هذه البيانات تتعلق بسلسلة من
مهام المتاهة التي لها نفس البنية بالضبط كما هو موضح في **الشكل**
**8.2**، باستثناء أنها تختلف في دقة الشبكة. حافظ **المسح
بالأولوية**</span> **<span dir="rtl">(</span>Prioritized
<span dir="rtl"></span>Sweeping<span dir="rtl">)
</span>**<span dir="rtl">على ميزة حاسمة على</span> **-Q
<span dir="rtl">داينا</span> (Dyna-Q)** <span dir="rtl">غير الموجه
بالأولوية. كلا النظامين قاما بأقصى حد من التحديثات</span> $`n = 5`$
<span dir="rtl">لكل تفاعل بيئي. مقتبس من</span> Peng and Williams
(1993)<span dir="rtl">.</span>

<img src="./media/image87.png"
style="width:5.1511in;height:3.50282in" />

<span dir="rtl">تمديدات **المسح بالأولوية**</span> **(Prioritized
Sweeping)** <span dir="rtl">إلى البيئات العشوائية</span>
<span dir="rtl">(</span>Stochastic
<span dir="rtl"></span>Environments<span dir="rtl">) تعتبر بسيطة نسبيًا.
يتم الحفاظ على **النموذج**</span> **(Model)** <span dir="rtl">عن طريق
الاحتفاظ بعدد مرات تجربة كل زوج من **الحالة–الإجراء**</span>
**(State–Action Pair)** <span dir="rtl">وتسجيل الحالات التالية التي
حدثت. من الطبيعي في هذه الحالة أن يتم تحديث كل زوج ليس باستخدام تحديث
العينة</span> (Sample Update) <span dir="rtl">كما كنا نفعل حتى الآن، بل
باستخدام **تحديث التوقع** </span>**(Expected Update)**<span dir="rtl">،
مع أخذ جميع الحالات التالية الممكنة واحتمالات حدوثها في الاعتبار</span>.

**<span dir="rtl">المسح بالأولوية</span> (Prioritized Sweeping)**
<span dir="rtl">هو مجرد طريقة واحدة لتوزيع الحسابات من أجل تحسين كفاءة
**التخطيط** </span>**(Planning)**<span dir="rtl">، وربما ليست الطريقة
الأفضل. إحدى قيود **المسح بالأولوية** </span>**(Prioritized Sweeping)**
<span dir="rtl">هي أنه يستخدم تحديثات التوقع</span> (Expected
Updates)<span dir="rtl">، والتي قد تهدر الكثير من الحسابات على
الانتقالات ذات الاحتمالية المنخفضة في البيئات العشوائية</span>
(Stochastic Environments)<span dir="rtl">.</span> <span dir="rtl">كما
سنوضح في القسم التالي، يمكن أن تكون **التحديثات العينية**</span>
**(Sample Updates)** <span dir="rtl">...</span>

**<span dir="rtl"><u>مثال 8.5</u></span>**: **<span dir="rtl">المسح
بالأولوية</span> (Prioritized Sweeping) <span dir="rtl">لتوجيه
العمود</span> (Rod Maneuvering)**

<span dir="rtl">الهدف في هذه المهمة هو **توجيه العمود**</span> **(Rod
Maneuvering)** <span dir="rtl">حول بعض **العوائق** </span>**(Obstacles)
<span dir="rtl"></span>**<span dir="rtl">الموضوعة بشكل غير مريح داخل
**مساحة عمل مستطيلة محدودة**</span> **<span dir="rtl">(</span>Limited
Rectangular <span dir="rtl"></span>Work Space<span dir="rtl">)</span>**
<span dir="rtl">للوصول إلى **موضع الهدف**</span> **(Goal Position)**
<span dir="rtl">بأقل عدد ممكن من الخطوات. يمكن تحريك **العمود**</span>
**(Rod)** <span dir="rtl">على طوله أو بشكل عمودي على هذا المحور، أو يمكن
تدويره في أي من الاتجاهين حول مركزه. تبلغ مسافة كل حركة حوالي 1/20 من
**مساحة العمل** </span>**(Work Space)**<span dir="rtl">، وزاوية الدوران
هي 10 درجات. تكون التحركات **حتمية**</span> **(Deterministic)**
<span dir="rtl">ومكممه إلى واحدة من  
20 × 20 **موضع** </span>**(Positions)**<span dir="rtl">.</span>
<span dir="rtl">على اليمين تظهر **العوائق**</span> **(Obstacles)**
<span dir="rtl">وأقصر حل من البداية إلى **الهدف**
</span>**(Goal)**<span dir="rtl">، الذي تم العثور عليه بواسطة **المسح
بالأولوية** </span>**(Prioritized Sweeping)**<span dir="rtl">. هذه
المشكلة **حتمية** </span>**(Deterministic)**<span dir="rtl">، ولكنها
تحتوي على أربع **إجراءات**</span> **(Actions)** <span dir="rtl">و14,400
**حالة** </span>**(States)** <span dir="rtl">محتملة (بعضها غير قابل
للوصول بسبب **العوائق** </span>**(Obstacles)**<span dir="rtl">)</span>.
<span dir="rtl">هذه المشكلة ربما تكون كبيرة جدًا بحيث لا يمكن حلها
باستخدام الطرق غير الموجهة بالأولوية. الشكل مأخوذ من</span> Moore and
Atkeson (1993)<span dir="rtl">.</span>

<img src="./media/image88.png"
style="width:4.14203in;height:4.2087in" />

<span dir="rtl">يمكن في العديد من الحالات الاقتراب من **دالة القيمة
الحقيقية**</span> **(True Value Function)** <span dir="rtl">باستخدام
حسابات أقل، على الرغم من التباين الذي يتم إدخاله بواسطة **التحديثات
العينية**</span> **<span dir="rtl">(</span>Sample
<span dir="rtl"></span>Updates<span dir="rtl">)</span>**
<span dir="rtl">يمكن أن تتفوق **التحديثات العينية**</span> **(Sample
Updates)** <span dir="rtl">لأنها تقسم عملية الارتجاع الكلية إلى قطع أصغر
— تلك المرتبطة بالانتقالات الفردية — مما يمكن من التركيز بشكل أضيق على
الأجزاء التي سيكون لها أكبر تأثير. تم أخذ هذه الفكرة إلى ما قد يكون حدها
المنطقي في النسخ الاحتياطية الصغيرة</span> (Small Backups)
<span dir="rtl">التي قدمها</span> van Seijen
<span dir="rtl">و</span>Sutton <span dir="rtl">في عام 2013. هذه
التحديثات تتم على طول انتقال واحد، مثل **التحديث العيني**
</span>**(Sample Update)**<span dir="rtl">، ولكنها تعتمد على احتمال
الانتقال دون أخذ عينات، كما في **التحديث المتوقع**</span>
**<span dir="rtl">(</span>Expected
Update<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">من خلال اختيار ترتيب القيام بالتحديثات الصغيرة، يمكن
تحسين كفاءة **التخطيط**</span> **(Planning)** <span dir="rtl">بشكل كبير
بما يتجاوز ما هو ممكن باستخدام **المسح بالأولوية**</span>
**<span dir="rtl">(</span>Prioritized
Sweeping<span dir="rtl">)</span>**<span dir="rtl">.</span>

<span dir="rtl">لقد اقترحنا في هذا الفصل أن جميع أنواع **التخطيط في فضاء
الحالة** </span>**(State-Space Planning)** <span dir="rtl">يمكن اعتبارها
تسلسلات من **تحديثات القيمة** </span>**(Value
Updates)**<span dir="rtl">، تختلف فقط في نوع  
التحديث — سواء كان متوقعًا أو عينيًا، كبيرًا أو صغيرًا — وفي ترتيب تنفيذ
التحديثات. في هذا القسم، ركزنا على **التركيز الخلفي** </span>**(Backward
Focusing)**<span dir="rtl">، لكن هذه مجرد استراتيجية واحدة. على سبيل
المثال، سيكون هناك استراتيجية أخرى وهي التركيز على **الحالات**</span>
**(States)** <span dir="rtl">وفقًا لمدى سهولة الوصول إليها من الحالات
التي يتم زيارتها بشكل متكرر بموجب **السياسة الحالية** </span>**(Current
Policy)**<span dir="rtl">، والتي قد يطلق عليها **التركيز
الأمامي**</span> **<span dir="rtl">(</span>Forward
Focusing<span dir="rtl">)</span>**<span dir="rtl">.</span> Peng
<span dir="rtl">و</span>Williams <span dir="rtl"></span>(1993)
<span dir="rtl">و</span>Barto<span dir="rtl">،</span> Bradtke
<span dir="rtl">و</span>Singh <span dir="rtl"></span>(1995)
<span dir="rtl">قد استكشفوا نسخًا من **التركيز الأمامي**</span>
**<span dir="rtl">(</span>Forward Focusing<span dir="rtl">)</span>**
<span dir="rtl">والطرق التي سيتم تقديمها في الأقسام القليلة القادمة
تأخذها إلى شكل متطرف</span>.

**<u>8.5 <span dir="rtl">التحديثات التوقعية مقابل التحديثات
بالعينات</span> <span dir="rtl">(</span>Expected vs. Sample
Updates<span dir="rtl">)</span></u>**

<span dir="rtl">**<u>  
</u>**الأمثلة في الأقسام السابقة تقدم فكرة عن نطاق الإمكانيات المتاحة
لدمج أساليب **التعليم** </span>**(Learning)**
<span dir="rtl">و**التخطيط**</span>
**<span dir="rtl">(</span>Planning<span dir="rtl">).</span>**
<span dir="rtl">في بقية هذا الفصل، نقوم بتحليل بعض الأفكار المكونة، بدءًا
من المزايا النسبية للتحديثات التوقعية</span> (Expected Updates)
<span dir="rtl">والتحديثات بالعينات</span>
<span dir="rtl">(</span>Sample Updates<span dir="rtl">).</span>

<img src="./media/image89.png"
style="width:2.79097in;height:4.91389in" /><span dir="rtl">لقد تناول هذا
الكتاب أنواعًا مختلفة من **تحديثات دالة القيمة** </span>**(Value-Function
Updates)**<span dir="rtl">، وقد نظرنا في العديد من الأنواع المتنوعة
منها. عند التركيز على **التحديثات ذات الخطوة الواحدة**
</span>**(One-Step Updates)**<span dir="rtl">، فإنها تختلف بشكل أساسي
على طول ثلاثة أبعاد ثنائية. البعدان الأولان هما: ما إذا كانت التحديثات
تخص **قيم الحالات  
(**</span>**State Values<span dir="rtl">) </span>**<span dir="rtl">أو
**قيم الإجراءات** </span>**(Action Values)**<span dir="rtl">، وما إذا
كانت التحديثات تقدّر القيمة لـ **السياسة المثلى  
(**</span>**Optimal Policy<span dir="rtl">)</span>** <span dir="rtl">أو
**لأي سياسة أخرى معطاة** </span>**(Arbitrary Given
Policy)**<span dir="rtl">.</span> <span dir="rtl">هذان البعدان يؤديان
إلى أربعة أنواع من التحديثات لتقدير **دوال القيمة الأربعة**
</span>**(Four Value
Functions)**<span dir="rtl">:</span>$`q*`$<span dir="rtl">،</span>
$`v*`$<span dir="rtl">،</span> $`q`$<span dir="rtl">،
و</span>$`v\pi`$<span dir="rtl">.</span> <span dir="rtl">البعد الثنائي
الآخر هو ما إذا كانت التحديثات توقعية</span> (Expected
Updates)<span dir="rtl">، تأخذ في الاعتبار جميع الأحداث المحتملة، أو
تعتمد على العينات</span> (Sample Updates)<span dir="rtl">، التي تأخذ في
الاعتبار عينة واحدة مما قد يحدث</span>.

<span dir="rtl">هذه الأبعاد الثلاثة الثنائية تنتج ثماني حالات، سبع منها
تتوافق مع خوارزميات محددة، كما هو موضح في الشكل على اليمين. (الحالة
الثامنة لا يبدو أنها تتوافق مع أي تحديث مفيد). يمكن استخدام أي من هذه
**التحديثات ذات الخطوة الواحدة**</span> **(One-Step Updates)**
<span dir="rtl">في أساليب التخطيط. وكلاء  
</span>**-Q<span dir="rtl">داينا</span> (Dyna-Q Agents)
<span dir="rtl"></span>**<span dir="rtl">الذين تم مناقشتهم سابقًا
يستخدمون تحديثات عينية</span> $`q*`$<span dir="rtl">، لكنهم يمكنهم كذلك
استخدام تحديثات توقعية</span> $`q*`$<span dir="rtl">، أو أي من التحديثات
التوقعية أو العينية</span> $`q_{\pi}`$ <span dir="rtl"></span>.
<span dir="rtl">نظام  
</span>**-AC<span dir="rtl">داينا</span> (Dyna-AC System)**
<span dir="rtl">يستخدم تحديثات عينية</span> $`v\pi`$ <span dir="rtl">مع
هيكل سياسة التعليم  
(كما في الفصل 13). بالنسبة للمشاكل العشوائية</span> (Stochastic
Problems)<span dir="rtl">، يتم دائمًا تنفيذ **المسح بالأولوية**</span>
**(Prioritized Sweeping)** <span dir="rtl">باستخدام أحد التحديثات
التوقعية</span>.

<span dir="rtl">عندما قدمنا **التحديثات العينية ذات الخطوة الواحدة**
</span>**(One-Step Sample Updates)<span dir="rtl">  
</span>**<span dir="rtl">في الفصل 6، قدمناها كبدائل للتحديثات التوقعية.
في غياب **نموذج التوزيع**</span> **<span dir="rtl">(</span>Distribution
<span dir="rtl"></span>Model<span dir="rtl">)</span>**<span dir="rtl">،
لا يمكن إجراء التحديثات التوقعية، لكن يمكن تنفيذ التحديثات العينية
باستخدام **الانتقالات العينية**</span> **(Sample Transitions)**
<span dir="rtl">من البيئة أو نموذج عينة. يتضمن هذا المنظور الضمني أن
التحديثات التوقعية، إذا كانت ممكنة، فهي مفضلة على التحديثات العينية. لكن
هل هي كذلك؟ التحديثات التوقعية بالتأكيد تقدم تقديرًا أفضل لأنها غير
متأثرة بأخطاء العينة، ولكنها تتطلب أيضًا حسابات أكثر، وغالبًا ما تكون
الحسابات هي المورد المحدود في التخطيط. لتقييم المزايا النسبية للتحديثات
التوقعية والعينية في التخطيط بشكل صحيح، يجب أن نأخذ في الاعتبار متطلبات
الحساب المختلفة لكل منهما</span>.

<span dir="rtl">للتوضيح، دعنا نعتبر **التحديثات التوقعية**</span>
**(Expected Updates)** <span dir="rtl">و**التحديثات العينية**</span>
**<span dir="rtl">(</span>Sample
<span dir="rtl"></span>Updates<span dir="rtl">)</span>**
<span dir="rtl">لتقريب</span> $`q*`$<span dir="rtl">، والحالة الخاصة
بالحالات والإجراءات المتقطعة</span> <span dir="rtl">(</span>Discrete
States and <span dir="rtl"></span>Actions<span dir="rtl">)، مع تمثيل
**دالة القيمة التقريبية**</span> **(Approximate Value Function)**
<span dir="rtl">في صورة جدول بحث</span>
<span dir="rtl">(</span>Table-Lookup Representation<span dir="rtl">)،
ونموذج في صورة ديناميكيات مقدرة</span> <span dir="rtl">(</span>Estimated
<span dir="rtl"></span>Dynamics<span dir="rtl">)،</span>
$`p\hat{}(s',r \mid s,a)`$<span dir="rtl">.</span>
**<span dir="rtl">التحديث التوقعي</span> (Expected Update)**
<span dir="rtl">لزوج  
**الحالة–الإجراء**</span> **(State–Action Pair)** $`s,a`$
<span dir="rtl">هو</span>:

``` math
Q(s,a) \leftarrow \sum_{s',r}^{}{\widehat{p}\left( s',r\mid s,a \right)\left\lbrack r + \gamma\max_{a'}Q\left( s',a' \right) \right\rbrack}
```

<span dir="rtl">التحديث العيني</span> (Sample Update)
<span dir="rtl">المقابل لـ</span> $`s,a`$<span dir="rtl">، بالنظر إلى
عينة الحالة التالية والمكافأة</span> $`S'`$
<span dir="rtl">و</span>$`R`$ <span dir="rtl">(من النموذج)، هو التحديث
الشبيه بـ</span> **Q-learning**<span dir="rtl">:</span>

``` math
Q(s,a) \leftarrow Q(s,a) + \alpha\left\lbrack R + \gamma\max_{a'}Q\left( S',a' \right) - Q(s,a) \right\rbrack
```

<span dir="rtl">حيث</span> α <span dir="rtl">هو **معامل حجم الخطوة
الإيجابي المعتاد** </span>**(Positive Step-Size
Parameter)**<span dir="rtl">.</span> <span dir="rtl">الفرق بين هذه
**التحديثات التوقعية**</span> **(Expected Updates)**
<span dir="rtl">و**التحديثات العينية** </span>**(Sample Updates)**
<span dir="rtl">يكون كبيرًا إلى الحد الذي تكون فيه **البيئة**</span>
**(Environment)** <span dir="rtl">عشوائية، وبشكل محدد، إلى الحد الذي قد
تحدث فيه العديد من **الحالات التالية**</span> **(Next States)**
<span dir="rtl">الممكنة مع احتمالات مختلفة بعد إجراء معين في حالة معينة.
إذا كانت هناك حالة واحدة فقط ممكنة كالتالي، فإن التحديثات التوقعية
والعينية المقدمة أعلاه ستكون متطابقة (مع أخذ</span>
$`\alpha = 1`$<span dir="rtl">).</span> <span dir="rtl">ولكن إذا كانت
هناك العديد من الحالات التالية الممكنة، فقد يكون هناك اختلافات
كبيرة</span>.

<span dir="rtl">في صالح **التحديث التوقعي** </span>**(Expected
Update)**<span dir="rtl">، فهو عبارة عن **عملية حسابية دقيقة**</span>
**<span dir="rtl">(</span>Exact
<span dir="rtl"></span>Computation<span dir="rtl">)</span>**<span dir="rtl">،
ينتج عنها قيمة جديدة لـ</span> $`Q(s,a)`$ <span dir="rtl">تعتمد فقط على
دقة</span> $`Q(s',a')`$ <span dir="rtl">في الحالات التالية. من ناحية
أخرى، **التحديث العيني**</span> **(Sample Update)**
<span dir="rtl">يتأثر أيضًا بخطأ العينة  
(</span>Sampling Error<span dir="rtl">).</span> <span dir="rtl">ولكن
التحديث العيني يكون أرخص من الناحية الحسابية لأنه يأخذ في الاعتبار حالة
تالية واحدة فقط، وليس جميع الحالات التالية الممكنة. في الممارسة العملية،
غالبًا ما تهيمن الحسابات المطلوبة لعمليات التحديث على عدد أزواج
الحالة–الإجراء التي يتم تقييم</span> $`Q`$ <span dir="rtl">عندها. لنفترض
أن هناك زوجًا معينًا من **الحالة والإجراء** </span>**(State-Action
Pair)**<span dir="rtl">، ولنفترض أن العامل التفرعي</span> $`b`$
<span dir="rtl">هو عدد **الحالات التالية**</span> **(Next States)**
<span dir="rtl">الممكنة التي يكون فيها</span>
$`p\hat{}(s' \mid s,a) > 0`$<span dir="rtl">.</span> <span dir="rtl">ثم
يتطلب **التحديث التوقعي**</span> **(Expected Update)**
<span dir="rtl">لهذا الزوج حسابات أكثر بحوالي</span> $`b`$
<span dir="rtl">مرة مقارنة بالتحديث العيني</span>.

<span dir="rtl">إذا كان هناك وقت كافٍ لإكمال **التحديث التوقعي**
</span>**(Expected Update)**<span dir="rtl">، فإن التقدير الناتج عادةً ما
يكون أفضل من</span> $`b`$ <span dir="rtl">تحديثات عينية بسبب غياب خطأ
العينة. ولكن إذا لم يكن هناك وقت كافٍ لإكمال التحديث التوقعي، فإن
التحديثات العينية دائمًا ما تكون مفضلة لأنها تحقق بعض التحسينات في تقدير
القيمة مع عدد أقل من التحديثات. في مشكلة كبيرة تحتوي على العديد من أزواج
الحالة–الإجراء، غالبًا ما نكون في الحالة الأخيرة. مع وجود العديد من أزواج
الحالة–الإجراء، فإن التحديثات التوقعية لجميعها ستستغرق وقتًا طويلاً جدًا.
قبل ذلك، قد يكون من الأفضل بكثير إجراء بعض التحديثات العينية عند العديد
من أزواج الحالة–الإجراء بدلاً من التحديثات التوقعية عند عدد قليل من
الأزواج. بالنظر إلى وحدة من الجهد الحسابي، هل من الأفضل تخصيصها لبعض
التحديثات التوقعية أو لإجراء عدد من التحديثات العينية بمقدار</span>
$`b`$<span dir="rtl">؟</span>

<img src="./media/image90.png"
style="width:6.26806in;height:2.82847in" />

<span dir="rtl">**الشكل 8.7** يظهر نتائج تحليل يقترح إجابة على هذا
السؤال. يظهر **خطأ التقدير  
(**</span>**Estimation Error<span dir="rtl">)
</span>**<span dir="rtl">كدالة للوقت الحسابي للتحديثات التوقعية والعينية
لمجموعة متنوعة من عوامل التفرع</span> $`b`$<span dir="rtl">.</span>
<span dir="rtl">الحالة المدروسة هي تلك التي تكون فيها جميع الحالات
التالية</span> $`b`$ <span dir="rtl">متساوية الاحتمال، وحيث يكون الخطأ
في التقدير الأولي 1. يتم افتراض أن قيم الحالات التالية صحيحة، وبالتالي
يقوم التحديث التوقعي بتقليل الخطأ إلى الصفر عند اكتماله. في هذه الحالة،
تقلل التحديثات العينية الخطأ وفقًا للمعادلة</span>:

``` math
b - 1bt\ 
```

<span dir="rtl">حيث</span> t <span dir="rtl">هو عدد التحديثات العينية
التي تم تنفيذها على افتراض متوسط العينات، أي</span> $`\alpha = 1t`$
<span dir="rtl">الملاحظة الرئيسية هي أنه بالنسبة لقيم</span> $`b`$
<span dir="rtl">الكبيرة نسبيًا، ينخفض الخطأ بشكل كبير مع جزء صغير من  
التحديثات</span> $`b`$<span dir="rtl">.</span> <span dir="rtl">في هذه
الحالات، يمكن للعديد من أزواج الحالة–الإجراء أن تتحسن قيمتها بشكل كبير،
لتصل إلى أقل من بضع في المئة من تأثير التحديث التوقعي، في نفس الوقت الذي
يمكن فيه أن يخضع زوج حالة–إجراء واحد لتحديث توقعي</span>.

<span dir="rtl">ميزة التحديثات العينية الموضحة في الشكل 8.7 ربما تكون
أقل من التأثير الحقيقي. في مشكلة حقيقية، ستكون قيم الحالات التالية
تقديرات يتم تحديثها بحد ذاتها. عن طريق جعل التقديرات أكثر دقة في وقت
أقرب، سيكون للتحديثات العينية ميزة ثانية تتمثل في أن القيم التي يتم
نسخها احتياطيًا من الحالات التالية ستكون أكثر دقة. تقترح هذه النتائج أن
التحديثات العينية من المحتمل أن تكون متفوقة على التحديثات التوقعية في
المشاكل التي تحتوي على عوامل تفرع عشوائية كبيرة وعدد كبير من الحالات
بحيث لا يمكن حلها بدقة</span>.

### <span dir="rtl">**<u>تمرين 8.6</u>**: التحليل أعلاه افترض أن جميع الحالات التالية</span> $`b`$ <span dir="rtl">كانت متساوية الاحتمال. افترض بدلًا من ذلك أن التوزيع كان منحرفًا بشكل كبير، وأن بعض الحالات</span> $`b`$ <span dir="rtl">كانت أكثر احتمالًا بكثير من غيرها. هل سيقوي هذا الفرضية لصالح التحديثات العينية على التحديثات التوقعية أم يضعفها؟ دعم إجابتك</span>.

**<u>8.6 <span dir="rtl">أخذ العينات من المسار</span> (Trajectory
Sampling)</u>**

<span dir="rtl">في هذا القسم، نقارن بين طريقتين لـ **توزيع التحديثات**
</span>**(Updates Distribution)**<span dir="rtl">.</span>
<span dir="rtl">النهج الكلاسيكي، المأخوذ من **البرمجة
الديناميكية**</span> **<span dir="rtl">(</span>Dynamic
Programming<span dir="rtl">)</span>**<span dir="rtl">، هو إجراء
**مسحات** </span>**(Sweeps)** <span dir="rtl">  
  
عبر **فضاء الحالة**</span> **(State Space)** <span dir="rtl">(أو **فضاء
الحالة–الإجراء** </span>**(State–Action Space)**<span dir="rtl">)، حيث
يتم تحديث كل حالة (أو زوج حالة–إجراء) مرة واحدة في كل مسح. هذه الطريقة
تصبح إشكالية في المهام الكبيرة لأن الوقت قد لا يكون كافيًا لإكمال حتى مسح
واحد. في العديد من المهام، تكون الغالبية العظمى من الحالات غير ذات صلة
لأنها تُزار فقط في ظل **سياسات**</span> **(Policies)**
<span dir="rtl">سيئة جدًا أو باحتمالية منخفضة جدًا</span>.
**<span dir="rtl">المسحات الشاملة</span> (Exhaustive Sweeps)**
<span dir="rtl">تُخصص وقتًا متساويًا لجميع أجزاء **فضاء الحالة**</span>
**(State Space)** <span dir="rtl">بدلاً من التركيز على الأجزاء التي تكون
بحاجة ماسة للتحديثات. كما ناقشنا في الفصل 4، المسحات الشاملة والمعاملة
المتساوية لجميع الحالات ليست بالضرورة من خصائص **البرمجة الديناميكية**
</span>**(Dynamic Programming)**<span dir="rtl">.</span>
<span dir="rtl">من حيث المبدأ، يمكن توزيع التحديثات بأي طريقة يفضلها
المرء (لضمان التقارب، يجب زيارة جميع الحالات أو أزواج الحالة–الإجراء
عددًا لا نهائيًا من المرات؛ على الرغم من أن استثناءً لهذه القاعدة سيُناقش في
القسم 8.7 أدناه)، ولكن في الممارسة العملية، تُستخدم المسحات الشاملة
غالبًا</span>.

<span dir="rtl">النهج الثاني هو أخذ عينات من **فضاء الحالة أو
الحالة–الإجراء**</span> **<span dir="rtl">(</span>State or State–Action
<span dir="rtl"></span>Space<span dir="rtl">)</span>**
<span dir="rtl">وفقًا لتوزيع معين. يمكن أخذ العينات بشكل موحد، كما هو
الحال في وكيل</span> **-Q <span dir="rtl">داينا</span> (Dyna-Q
Agent)**<span dir="rtl">، ولكن هذا سيعاني من بعض نفس المشكلات الموجودة
في المسحات الشاملة. الأكثر جاذبية هو **توزيع التحديثات**</span>
**(Updates Distribution)** <span dir="rtl">وفقًا لتوزيع السياسات الحالي  
(</span>**On-Policy Distribution**<span dir="rtl">)، أي وفقًا للتوزيع
الذي يُلاحظ عند اتباع السياسة الحالية. إحدى مزايا هذا التوزيع هي أنه يتم
توليده بسهولة؛ يتفاعل المرء ببساطة مع **النموذج**
</span>**(Model)**<span dir="rtl">، باتباع السياسة الحالية. في مهمة
حلقية، يبدأ المرء في حالة البداية (أو وفقًا لتوزيع حالة البداية) ويستمر
في المحاكاة حتى يصل إلى **الحالة النهائية** </span>**(Terminal
State)**<span dir="rtl">.</span> <span dir="rtl">في مهمة مستمرة، يبدأ
المرء في أي مكان ويستمر في المحاكاة. في كلتا الحالتين، يتم إعطاء
**الانتقالات الحالة** </span>**(State Transitions)**
<span dir="rtl">و**المكافآت**</span> **(Rewards)**
<span dir="rtl">بواسطة **النموذج** </span>**(Model)**<span dir="rtl">،
ويتم إعطاء الإجراءات بواسطة السياسة الحالية. بعبارة أخرى، يتم محاكاة
**مسارات فردية صريحة**</span> **(Explicit Individual Trajectories)**
<span dir="rtl">ويتم تنفيذ التحديثات عند أزواج الحالة أو الحالة–الإجراء
التي يتم مواجهتها على طول الطريق. نسمي هذه الطريقة في توليد التجربة
والتحديثات بـ **أخذ العينات من المسار** </span>**(Trajectory
Sampling)**<span dir="rtl">.</span>

<span dir="rtl">من الصعب تخيل أي طريقة فعالة لتوزيع التحديثات وفقًا لـ
**توزيع السياسات الحالي**</span> **<span dir="rtl">(</span>On-Policy
<span dir="rtl"></span>Distribution<span dir="rtl">)</span>**
<span dir="rtl">بخلاف **أخذ العينات من المسار** </span>**(Trajectory
Sampling)**<span dir="rtl">.</span> <span dir="rtl">إذا كان لدى المرء
تمثيل صريح لتوزيع السياسات الحالي، فيمكن إجراء **مسحات**</span>
**(Sweeps)** <span dir="rtl">عبر جميع الحالات، مع ترجيح التحديث لكل حالة
وفقًا لتوزيع السياسات الحالي، ولكن هذا يعيدنا مرة أخرى إلى جميع التكاليف
الحسابية للمسحات الشاملة. من الممكن أن يقوم المرء بأخذ العينات وتحديث
أزواج الحالة–الإجراء الفردية من التوزيع، ولكن حتى لو كان ذلك ممكنًا
بكفاءة، ما الفائدة التي قد يوفرها هذا مقارنة بمحاكاة المسارات؟ حتى معرفة
توزيع السياسات الحالي في شكل صريح أمر غير مرجح. التوزيع يتغير كلما تغيرت
السياسة، وحساب التوزيع يتطلب حسابات مشابهة لتقييم السياسة بالكامل. النظر
في مثل هذه الاحتمالات الأخرى يجعل **أخذ العينات من المسار**</span>
**(Trajectory Sampling)** <span dir="rtl">يبدو فعالًا وأنيقًا في نفس
الوقت</span>.

<span dir="rtl">هل **توزيع السياسات الحالي**</span> **(On-Policy
Distribution)** <span dir="rtl">للتحديثات خيار جيد؟ يبدو بشكل بديهي أنه
خيار جيد، على الأقل أفضل من **التوزيع الموحد** </span>**(Uniform
Distribution)**<span dir="rtl">.</span> <span dir="rtl">على سبيل المثال،
إذا كنت تتعلم لعب الشطرنج، ستدرس المواقع التي قد تنشأ في المباريات
الحقيقية، وليس المواقع العشوائية لقطع الشطرنج. الأخيرة قد تكون حالات
صالحة، ولكن القدرة على تقييمها بدقة هي مهارة مختلفة عن تقييم المواقع في
الألعاب الحقيقية. سنرى أيضًا في الجزء الثاني من الكتاب أن **توزيع
السياسات الحالي**</span> **(On-Policy Distribution)** <span dir="rtl">له
مزايا كبيرة عند استخدام **تقريب الدالة** </span>**(Function
Approximation)**<span dir="rtl">.</span> <span dir="rtl">سواء تم استخدام
تقريب الدالة أم لا، قد تتوقع أن يساهم التركيز على السياسات الحالية بشكل
كبير في تحسين سرعة التخطيط</span>.

<span dir="rtl">التركيز على **توزيع السياسات الحالي**</span>
**(On-Policy Distribution)** <span dir="rtl">قد يكون مفيدًا لأنه يتسبب في
تجاهل أجزاء واسعة وغير مثيرة للاهتمام من **فضاء الحالة** </span>**(State
Space)**<span dir="rtl">، أو قد يكون ضارًا لأنه يتسبب في تحديث نفس
الأجزاء القديمة من الفضاء مرارًا وتكرارًا. قمنا بإجراء تجربة صغيرة لتقييم
التأثير بشكل تجريبي. لعزل تأثير **توزيع التحديثات** </span>**(Updates
Distribution)**<span dir="rtl">، استخدمنا بالكامل **التحديثات التوقعية
الجدولية ذات الخطوة الواحدة**</span> **<span dir="rtl">(</span>One-Step
Expected Tabular
<span dir="rtl"></span>Updates<span dir="rtl">)</span>**<span dir="rtl">،
كما هو محدد في المعادلة (8.1). في حالة التوزيع الموحد، قمنا بالتنقل عبر
جميع أزواج الحالة–الإجراء، وتحديث كل زوج في مكانه، وفي حالة **السياسة
الحالية**</span> **(On-Policy)** <span dir="rtl">قمنا بمحاكاة الحلقات،
بدءًا من نفس الحالة، وتحديث كل زوج حالة–إجراء ظهر تحت **السياسة
الجشعة**</span> **<span dir="rtl">(</span>ε-greedy
policy<span dir="rtl">) </span>**<span dir="rtl">الحالية</span>
($`\epsilon = 0.1`$)<span dir="rtl">.</span> <span dir="rtl">كانت المهام
**حلقية غير مخفضة**</span> **<span dir="rtl">(</span>Undiscounted
<span dir="rtl"></span>Episodic
Tasks<span dir="rtl">)</span>**<span dir="rtl">، وتم توليدها بشكل عشوائي
كما يلي: من كل **حالة**</span> **(State)** <span dir="rtl">من
حالات</span> $`S`$ <span dir="rtl">المحتملة، كان هناك إجراءان ممكنان، كل
منهما يؤدي إلى واحدة من</span> $`b`$ <span dir="rtl">حالات تالية، وكلها
متساوية الاحتمال، مع اختيار عشوائي مختلف لمجموعة</span> $`b`$
<span dir="rtl">من الحالات لكل زوج حالة–إجراء. كان  
**عامل التفرع**</span> **(Branching Factor)** $`b`$ <span dir="rtl">هو
نفسه لجميع أزواج الحالة–الإجراء. بالإضافة إلى ذلك، في جميع الانتقالات
كان هناك احتمال 0.1 للانتقال إلى **الحالة النهائية**</span>
**<span dir="rtl">(</span>Terminal
State<span dir="rtl">)</span>**<span dir="rtl">، مما ينهي الحلقة. تم
اختيار **المكافأة التوقعية**</span> **(Expected Reward)**
<span dir="rtl">على كل انتقال من **توزيع غاوسي** </span>**(Gaussian
Distribution)** <span dir="rtl">بمتوسط 0 وتباين 1</span>.

<span dir="rtl">في أي نقطة في عملية التخطيط يمكن التوقف وحساب القيمة
الحقيقية لحالة البداية</span> $`v\pi(s0)`$ <span dir="rtl">تحت السياسة
الجشعة</span> $`\pi\sim`$ <span dir="rtl">باستخدام **دالة القيمة
الإجرائية الحالية**</span> **<span dir="rtl">(</span>Current
Action-Value <span dir="rtl"></span>Function<span dir="rtl">)</span>**
<span dir="rtl"></span>$`Q`$<span dir="rtl">، كمؤشر على مدى جودة أداء
العميل في حلقة جديدة يتصرف فيها جشعًا (مع افتراض أن النموذج صحيح طوال
الوقت)</span>.

<img src="./media/image91.png"
style="width:2.86597in;height:4.07153in" /><span dir="rtl">الجزء العلوي
من الشكل على اليسار يظهر النتائج المتوسطة على 200 مهمة تجريبية تحتوي على
1000 حالة وعوامل تفرع</span> b <span dir="rtl">من 1، 3، و10. تم رسم جودة
السياسات التي تم العثور عليها كدالة لعدد التحديثات التوقعية المكتملة. في
جميع الحالات، أدى **أخذ العينات وفقًا لتوزيع السياسات الحالي**
</span>**(On-Policy Distribution)** <span dir="rtl">إلى تخطيط أسرع في
البداية وتأخير التخطيط على المدى الطويل. كان التأثير أقوى، وكانت فترة
التخطيط الأسرع الأولية أطول، عند عوامل التفرع الأصغر. في تجارب أخرى،
وجدنا أن هذه التأثيرات تصبح أقوى أيضًا مع زيادة عدد الحالات. على سبيل
المثال، يظهر الجزء السفلي من الشكل النتائج لعامل تفرع 1 للمهام التي
تحتوي على 10,000 حالة. في هذه الحالة، تكون ميزة التركيز على السياسات
الحالية كبيرة وطويلة الأمد</span>.

<span dir="rtl">كل هذه النتائج منطقية. على المدى القصير، يساعد **أخذ
العينات وفقًا لتوزيع السياسات الحالي  **
</span>**(On-Policy Distribution)
<span dir="rtl"></span>**<span dir="rtl">من خلال التركيز على الحالات
التي تكون خلفاء قريبة من حالة البداية. إذا كان هناك العديد من الحالات
وعامل تفرع صغير، سيكون هذا التأثير كبيرًا وطويل الأمد. على المدى الطويل،
قد يضر التركيز على **توزيع السياسات الحالي** </span>**(On-Policy
Distribution)** <span dir="rtl">لأن الحالات التي تحدث بشكل شائع قد حصلت
جميعها على قيمها الصحيحة. أخذ العينات منها يكون عديم الفائدة، بينما قد
يكون أخذ العينات من حالات أخرى يؤدي إلى بعض العمل المفيد بالفعل. من
المحتمل أن يكون هذا هو السبب في أن النهج الشامل وغير الموجه يقوم بعمل
أفضل على المدى الطويل، على الأقل في المشاكل الصغيرة. هذه النتائج ليست
حاسمة لأنها تنطبق فقط على مشاكل تم توليدها بطريقة عشوائية معينة، لكنها
تشير إلى أن أخذ العينات وفقًا لتوزيع السياسات الحالي يمكن أن يكون ميزة
كبيرة في المشاكل الكبيرة، خصوصًا في المشاكل التي يتم فيها زيارة مجموعة
فرعية صغيرة من **فضاء الحالة–الإجراء**</span> **(State-Action Space)**
<span dir="rtl">تحت **توزيع السياسات الحالي  
(**</span>**On-Policy Distribution<span dir="rtl">)</span>**.

### <span dir="rtl">**<u>تمرين 8.7</u>**: يبدو أن بعض الرسوم البيانية في الشكل 8.8 تحتوي على تدرجات في الأجزاء المبكرة منها، خاصة في الرسم البياني العلوي لعامل التفرع</span> $`b = 1`$ <span dir="rtl">والتوزيع الموحد. لماذا تعتقد أن هذا هو السبب؟ ما الجوانب من البيانات الموضحة تدعم فرضيتك؟</span>

### <span dir="rtl">**<u>تمرين 8.8 (برمجة)</u>**: قم بتكرار التجربة التي تم عرض نتائجها في الجزء السفلي من الشكل 8.8، ثم جرب نفس التجربة ولكن مع عامل التفرع</span> $`b = 3`$<span dir="rtl">.</span> <span dir="rtl">ناقش معنى النتائج التي حصلت عليها</span>.

**<u>8.7 <span dir="rtl">البرمجة الديناميكية في الوقت الحقيقي</span>
(Real-time Dynamic Programming)</u>**

**<span dir="rtl">البرمجة الديناميكية في الوقت الحقيقي</span> (Real-time
Dynamic Programming)**<span dir="rtl">، أو</span>
**RTDP<span dir="rtl">،</span>** <span dir="rtl">هي نسخة تعتمد على **أخذ
العينات من المسار وفقًا للسياسة الحالية**</span>
**<span dir="rtl">(</span>On-Policy
Trajectory-Sampling<span dir="rtl">)</span>** <span dir="rtl">لخوارزمية
**تكرار القيمة**</span> **(Value Iteration)** <span dir="rtl">في
**البرمجة الديناميكية**</span> **<span dir="rtl">(</span>Dynamic
<span dir="rtl"></span>Programming, DP<span dir="rtl">)</span>**.
<span dir="rtl">نظرًا لأنها ترتبط ارتباطًا وثيقًا بتكرار السياسة التقليدي
المستند إلى المسحات</span> (sweeps)<span dir="rtl">، فإن</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">توضح بطريقة واضحة بشكل خاص بعض
المزايا التي يمكن أن يوفرها **أخذ العينات من المسار وفقًا للسياسة
الحالية**</span> **<span dir="rtl">(</span>On-Policy Trajectory
Sampling<span dir="rtl">).</span>** <span dir="rtl">تقوم</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">بتحديث قيم **الحالات**</span>
**(States)** <span dir="rtl">التي تمت زيارتها في المسارات الفعلية أو
المحاكية باستخدام تحديثات تكرار القيمة التوقعية الجدولية كما هو محدد في
المعادلة (4.10). في الأساس، هي الخوارزمية التي أنتجت النتائج المستندة
إلى السياسة الحالية الموضحة في **الشكل** 8.8</span>.

<span dir="rtl">الارتباط الوثيق بين</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">و</span>DP
**<span dir="rtl">التقليدي</span> (Conventional DP)**
<span dir="rtl">يجعل من الممكن اشتقاق بعض النتائج النظرية من خلال تعديل
النظرية الموجودة.</span> **RTDP** <span dir="rtl">هي مثال على **خوارزمية
البرمجة الديناميكية غير المتزامنة**</span> **(Asynchronous DP
Algorithm)** <span dir="rtl">كما هو موضح في **القسم** **4.5**. خوارزميات
البرمجة الديناميكية غير المتزامنة لا تُنظم من حيث المسحات المنهجية
لمجموعة الحالات؛ بل تقوم بتحديث قيم الحالات بأي ترتيب كان، باستخدام أي
قيم للحالات الأخرى التي تكون متاحة. في</span> **RTDP**<span dir="rtl">،
يُملى ترتيب التحديث من خلال ترتيب زيارة الحالات في المسارات الفعلية أو
المحاكية</span>.

<img src="./media/image92.png" style="width:2.20208in;height:1.525in" /><span dir="rtl">إذا
كانت المسارات يمكن أن تبدأ فقط من مجموعة معينة من حالات البداية، وإذا
كنت مهتمًا بمشكلة **التنبؤ** </span>**(Prediction Problem)**
<span dir="rtl">لسياسة معينة، فإن **أخذ العينات من المسار وفقًا للسياسة
الحالية  **
</span> **(On-Policy Trajectory Sampling)
<span dir="rtl"></span>**<span dir="rtl">يسمح للخوارزمية بتجاوز الحالات
التي لا يمكن الوصول إليها بواسطة السياسة المعطاة من أي من حالات البداية:
هذه الحالات غير ذات صلة بمشكلة التنبؤ. بالنسبة لمشكلة التحكم، حيث الهدف
هو العثور على سياسة مثلى بدلاً من تقييم سياسة معطاة، قد تكون هناك حالات
لا يمكن الوصول إليها بواسطة أي سياسة مثلى من أي من حالات البداية، ولا
توجد حاجة لتحديد إجراءات مثلى لهذه الحالات غير ذات الصلة. ما هو مطلوب هو
**سياسة جزئية مثلى**</span> **<span dir="rtl">(</span>Optimal
<span dir="rtl"></span>Partial
Policy<span dir="rtl">)</span>**<span dir="rtl">، أي سياسة تكون مثلى
للحالات ذات الصلة ولكن يمكن أن تحدد إجراءات عشوائية، أو حتى تكون غير
معرفة، للحالات غير ذات الصلة</span>.

<span dir="rtl">لكن العثور على مثل هذه السياسة الجزئية المثلى باستخدام
**طريقة تحكم تعتمد على أخذ العينات من المسار وفقًا للسياسة
الحالية**</span> **<span dir="rtl">(</span>On-Policy Trajectory-Sampling
Control Method<span dir="rtl">)</span>**<span dir="rtl">، مثل
**سارسا**</span> **(Sarsa)** <span dir="rtl">(**القسم** **6**.**4**)،
يتطلب بشكل عام زيارة جميع أزواج الحالة–الإجراء، حتى تلك التي ستتضح أنها
غير ذات صلة، عددًا لا نهائيًا من المرات. يمكن القيام بذلك، على سبيل
المثال، باستخدام **البدايات الاستكشافية**</span> **(Exploring Starts)**
<span dir="rtl">(**القسم** 5.3). هذا صحيح أيضًا بالنسبة لـ</span>
**RTDP<span dir="rtl">:</span>** <span dir="rtl">بالنسبة للمهام الحلقية
مع **البدايات الاستكشافية** </span>**(Exploring
Starts)**<span dir="rtl">،</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">هي خوارزمية تكرار قيمة غير
متزامنة تتقارب إلى سياسات مثلى لمشاكل عملية **ماركوف القرارية**
</span>**(MDPs)** <span dir="rtl">المخفضة (وأيضًا في الحالة غير المخفضة
تحت شروط معينة). على عكس الوضع في مشكلة التنبؤ، لا يمكن بشكل عام التوقف
عن تحديث أي حالة أو زوج حالة–إجراء إذا كان التقارب إلى سياسة مثلى
مهمًا</span>.

<span dir="rtl">النتيجة الأكثر إثارة للاهتمام لـ</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">هي أنه بالنسبة لأنواع معينة من
المشاكل التي تلبي شروطًا معقولة، فإن</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">تضمن العثور على سياسة مثلى
للحالات ذات الصلة دون زيارة كل حالة عددًا لا نهائيًا من المرات، أو حتى دون
زيارة بعض الحالات على الإطلاق. في الواقع، في بعض المشاكل، يلزم زيارة جزء
صغير فقط من الحالات. يمكن أن تكون هذه ميزة كبيرة في المشاكل التي تحتوي
على مجموعات حالات كبيرة جدًا، حيث قد لا يكون إجراء مسح واحد ممكنًا</span>.

<span dir="rtl">المهام التي تنطبق عليها هذه النتيجة هي المهام الحلقية
غير المخفضة لمشاكل عملية **ماركوف القرارية** </span>**(MDPs)
<span dir="rtl"></span>**<span dir="rtl">مع حالات هدف **ماصة**</span>
**(Absorbing Goal States)** <span dir="rtl">التي تولد مكافآت صفرية، كما
هو موضح في القسم 3.4. في كل خطوة من مسار فعلي أو محاكي، تختار</span>
**RTDP <span dir="rtl"></span>**<span dir="rtl">إجراءً جشعًا (مع كسر
التعادلات بشكل عشوائي) وتطبق عملية تحديث تكرار القيمة التوقعية على
الحالة الحالية. يمكنها أيضًا تحديث قيم مجموعة عشوائية من الحالات الأخرى
في كل خطوة؛ على سبيل المثال، يمكنها تحديث قيم الحالات التي تمت زيارتها
في بحث نظر إلى الأمام محدود الأفق من الحالة الحالية</span>.

<span dir="rtl">بالنسبة لهذه المشاكل، مع كل حلقة تبدأ في حالة يتم
اختيارها عشوائيًا من مجموعة حالات البداية وتنتهي في حالة هدف،
تتقارب</span> **RTDP <span dir="rtl"></span>** <span dir="rtl">باحتمالية
واحدة إلى سياسة مثلى لجميع الحالات ذات الصلة شريطة: **1)** أن تكون
القيمة الأولية لكل حالة هدف صفرية، **2)** أن تكون هناك سياسة واحدة على
الأقل تضمن الوصول إلى حالة هدف باحتمالية واحدة من أي حالة بداية، **3)**
أن تكون جميع المكافآت على الانتقالات من الحالات غير الهدفية سلبية تمامًا،
و **4)** أن تكون القيم الأولية لجميع الحالات مساوية لقيمها المثلى أو
أكبر منها (يمكن تحقيق ذلك ببساطة عن طريق تعيين القيم الأولية لجميع
الحالات إلى صفر). أثبت</span> Barto<span dir="rtl">،</span> Bradtke,
<span dir="rtl">و</span>Singh <span dir="rtl"></span>(1995)
<span dir="rtl">هذه النتيجة عن طريق الجمع بين نتائج البرمجة الديناميكية
غير المتزامنة مع نتائج خوارزمية بحث استرشادي تُعرف باسم تعلم الوقت
الحقيقي</span> $`A*`$<span dir="rtl">، التي قدمها</span> Korf
(1990)<span dir="rtl">.</span>

<span dir="rtl">المهام التي تمتلك هذه الخصائص هي أمثلة على **مشاكل
المسار المثلى العشوائي  **
</span>**(Stochastic Optimal Path Problems)**<span dir="rtl">، والتي
عادةً ما يتم التعبير عنها من حيث تقليل التكلفة بدلاً من تعظيم المكافأة كما
نفعل هنا. تعظيم العوائد السلبية في نسختنا يكافئ تقليل تكاليف المسارات من
حالة البداية إلى حالة الهدف. من أمثلة هذه المهام مهام التحكم في الحد
الأدنى للوقت، حيث ينتج عن كل خطوة زمنية مطلوبة للوصول إلى الهدف مكافأة
قدرها 1-، أو مشاكل مثل مثال الغولف في **القسم** **3.5**، حيث يكون الهدف
هو الوصول إلى الحفرة بأقل عدد من الضربات</span>.

### <span dir="rtl">**<u>مثال 8.6</u>**: البرمجة الديناميكية في الوقت الحقيقي</span> (RTDP) <span dir="rtl">على مسار السباق</span>

<span dir="rtl">مشكلة **مسار السباق**</span> **(Racetrack Problem)**
<span dir="rtl">في تمرين 5.12 (صفحة 111) هي مشكلة **مسار مثالي
عشوائي**</span> **<span dir="rtl">(</span>Stochastic Optimal Path
Problem<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">مقارنة</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">بخوارزمية تكرار القيمة في
البرمجة الديناميكية التقليدية</span> (DP) <span dir="rtl">على مثال مشكلة
مسار السباق توضح بعض المزايا التي يقدمها **أخذ العينات من المسار وفقًا
للسياسة الحالية** </span>**(On-Policy Trajectory
Sampling)**<span dir="rtl">.</span>

<span dir="rtl">تذكر من التمرين أن العميل يجب أن يتعلم كيفية قيادة
السيارة حول منعطفات مثل تلك الموضحة في **الشكل** **5.5** وعبور خط
النهاية بأسرع ما يمكن مع البقاء على المسار. حالات البداية هي جميع
الحالات ذات السرعة الصفرية على خط البداية؛ وحالات الهدف هي جميع الحالات
التي يمكن الوصول إليها في خطوة زمنية واحدة بعبور خط النهاية من داخل
المسار. على **عكس** **تمرين** 5.12، لا يوجد هنا حد أقصى لسرعة السيارة،
لذلك فإن مجموعة الحالات المحتملة تكون لانهائية. ومع ذلك، فإن مجموعة
الحالات التي يمكن الوصول إليها من حالات البداية عبر أي سياسة تكون محدودة
ويمكن اعتبارها مجموعة حالات المشكلة. تبدأ كل حلقة في حالة بداية مختارة
عشوائيًا وتنتهي عندما تعبر السيارة خط النهاية. المكافآت هي 1- لكل خطوة
حتى تعبر السيارة خط النهاية. إذا اصطدمت السيارة بحافة المسار، يتم
إرجاعها إلى حالة بداية عشوائية، وتستمر الحلقة</span>.

<span dir="rtl">مسار سباق مشابه للمسار الصغير على يسار **الشكل** **5.5**
يحتوي على 9,115 حالة يمكن الوصول إليها من حالات البداية عبر أي سياسة،
منها 599 فقط ذات صلة، مما يعني أنه يمكن الوصول إليها من بعض حالات
البداية عبر سياسة مثلى ما. (تم تقدير عدد الحالات ذات الصلة عن طريق عد
الحالات التي تمت زيارتها أثناء تنفيذ الإجراءات المثلى في 10^7
حلقات)</span>.

<span dir="rtl">الجدول أدناه يقارن حل هذه المهمة باستخدام **البرمجة
الديناميكية التقليدية**</span> **(DP)** <span dir="rtl">وباستخدام</span>
**RTDP<span dir="rtl">.</span>** <span dir="rtl">هذه النتائج هي متوسطات
على 25 تشغيل، كل منها بدأ ببذرة رقم عشوائي مختلفة</span>.
**<span dir="rtl">البرمجة الديناميكية التقليدية</span> (DP)**
<span dir="rtl">في هذه الحالة هي تكرار القيمة باستخدام **المسحات الشاملة
لمجموعة الحالات** </span>**(Exhaustive Sweeps of the State
Set)**<span dir="rtl">، مع تحديث القيم حالة بحالة، بمعنى أن التحديث لكل
حالة يستخدم القيم الأحدث للحالات الأخرى</span> <span dir="rtl">(هذه هي
نسخة</span> Gauss-Seidel <span dir="rtl">من تكرار القيمة، والتي وُجدت
أنها أسرع بحوالي مرتين من نسخة</span> Jacobi <span dir="rtl">في هذه
المشكلة. انظر القسم 4.8).</span> <span dir="rtl">لم يتم إيلاء اهتمام خاص
لترتيب التحديثات؛ قد تكون الترتيبات الأخرى قد أنتجت تقاربًا أسرع. كانت
القيم الأولية صفرًا لجميع الحالات في كل تشغيل لكلتا الطريقتين. تم الحكم
على</span> **DP <span dir="rtl"></span>**<span dir="rtl">بأنه قد تقارب
عندما كان التغيير الأقصى في قيمة حالة ما خلال مسح أقل من</span>
10−410^<span dir="rtl">، وتم الحكم على</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">بأنه قد تقارب عندما استقر متوسط
الوقت لعبور خط النهاية على مدى 20 حلقة عند عدد ثابت من الخطوات. هذه
النسخة من</span> **RTDP <span dir="rtl"></span>**<span dir="rtl">قامت
بتحديث قيمة الحالة الحالية فقط في كل خطوة</span>.

<img src="./media/image93.png"
style="width:6.26806in;height:1.70556in" />

<span dir="rtl">في كل مسح من مسحات **البرمجة الديناميكية**
</span>**(DP)**<span dir="rtl">، ركزت</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">التحديثات على عدد أقل من
الحالات. في المتوسط، قامت</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">بتحديث قيم 98.45% من الحالات
أقل من 100 مرة و80.51% من الحالات أقل من 10 مرات؛ لم يتم تحديث قيم حوالي
290 حالة على الإطلاق في المتوسط</span>.

<span dir="rtl">ميزة أخرى لـ</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">هي أنه مع اقتراب **دالة
القيمة**</span> **(Value Function)** <span dir="rtl">من **الدالة**
</span>$`\mathbf{\ v*}`$**<span dir="rtl">المثلى  
</span>(Optimal Value Function)**<span dir="rtl">، فإن
**السياسة**</span> **(Policy)** <span dir="rtl">التي يستخدمها العميل
لتوليد المسارات تقترب من **السياسة المثلى**</span> **(Optimal Policy)**
<span dir="rtl">لأنها دائمًا جشعة بالنسبة لدالة القيمة الحالية. هذا على
عكس الوضع في **تكرار القيمة التقليدي**</span>
**<span dir="rtl">(</span>Conventional Value
Iteration<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">في الممارسة العملية، ينتهي **تكرار القيمة**</span>
**(Value Iteration)** <span dir="rtl">عندما تتغير **دالة القيمة  
(**</span>**Value Function<span dir="rtl">)</span>**
<span dir="rtl">بمقدار ضئيل فقط في مسح واحد، وهو ما استخدمناه لإنهاء
التكرار للحصول على النتائج في الجدول أعلاه. في هذه المرحلة، تكون **دالة
القيمة**</span> **(Value Function)** <span dir="rtl">قريبة من</span>
v∗<span dir="rtl">، وتكون **السياسة الجشعة**</span> **(Greedy Policy)**
<span dir="rtl">قريبة من **السياسة المثلى** </span>**(Optimal
Policy)**<span dir="rtl">. ومع ذلك، من الممكن أن تكون السياسات التي تكون
جشعة بالنسبة لدالة القيمة الأحدث قد أصبحت مثلى، أو شبه مثلى، بوقت طويل
قبل انتهاء **تكرار القيمة** </span>**(Value
Iteration)**<span dir="rtl">.</span> <span dir="rtl">(تذكر من الفصل 4 أن
**السياسات المثلى**</span> **(Optimal Policies)** <span dir="rtl">يمكن
أن تكون جشعة بالنسبة للعديد من **دوال القيمة**</span> **(Value
Functions) <span dir="rtl"></span>**<span dir="rtl">المختلفة، وليس
فقط</span> $`v*`$<span dir="rtl">).</span> <span dir="rtl">التحقق من
ظهور **سياسة مثلى**</span> **<span dir="rtl">(</span>Optimal
<span dir="rtl"></span>Policy<span dir="rtl">)
</span>**<span dir="rtl">قبل أن يتقارب **تكرار القيمة**</span> **(Value
Iteration)** <span dir="rtl">ليس جزءًا من خوارزمية **البرمجة الديناميكية
التقليدية**</span> **(DP)** <span dir="rtl">ويتطلب حسابات إضافية
كبيرة</span>.

<span dir="rtl">في مثال **مسار السباق**
</span>**(Racetrack)**<span dir="rtl">، من خلال تشغيل العديد من حلقات
الاختبار بعد كل مسح من مسحات</span> **DP<span dir="rtl">،</span>**
<span dir="rtl">مع اختيار الإجراءات بشكل جشع وفقًا لنتائج ذلك المسح، كان
من الممكن تقدير النقطة الزمنية المبكرة في حسابات</span> **DP
<span dir="rtl"></span>**<span dir="rtl">التي عندها كانت **دالة التقييم
المثلى التقريبية  
(**</span>**Approximated Optimal Evaluation
Function<span dir="rtl">)</span>** <span dir="rtl">جيدة بما يكفي لجعل
**السياسة الجشعة** </span>**(Greedy Policy)** <span dir="rtl">المقابلة
قريبة من المثلى. بالنسبة لهذا **مسار السباق**
</span>**(Racetrack)**<span dir="rtl">، ظهرت **السياسة القريبة من
المثلى**</span> **(Close-to-Optimal Policy)** <span dir="rtl">بعد 15
مسحًا من **تكرار القيمة** </span>**(Value Iteration)**<span dir="rtl">،
أو بعد 136,725 تحديثًا لـ **تكرار القيمة** </span>**(Value-Iteration
Updates)**<span dir="rtl">. هذا أقل بكثير من 252,784 تحديثًا التي
تطلبها</span> **DP <span dir="rtl"></span>**<span dir="rtl">للتقارب
مع</span> $`v*`$<span dir="rtl">، ولكنه لا يزال أكثر من 127,600 تحديثًا
التي تطلبها</span> **RTDP**<span dir="rtl">.</span>

<span dir="rtl">على الرغم من أن هذه المحاكاة ليست بالتأكيد مقارنات حاسمة
بين</span> **RTDP <span dir="rtl"></span>**<span dir="rtl">وتكرار
**القيمة التقليدي المستند إلى المسحات** </span>**(Sweep-Based Value
Iteration)**<span dir="rtl">، فإنها توضح بعض المزايا لـ **أخذ العينات من
المسار وفقًا للسياسة الحالية** </span>**(On-Policy Trajectory
Sampling)**<span dir="rtl">.</span> <span dir="rtl">بينما استمر  
**تكرار القيمة التقليدي**</span> **(Conventional Value Iteration)**
<span dir="rtl">في تحديث قيم جميع الحالات، ركزت</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">بشدة على مجموعات فرعية من
الحالات ذات الصلة بهدف المشكلة. أصبح هذا التركيز أكثر ضيقًا مع استمرار
التعليم. نظرًا لأن مبرهنة التقارب لـ</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">تنطبق على هذه المحاكاة، فإننا
نعلم أن</span> **RTDP <span dir="rtl"></span>**<span dir="rtl">في
النهاية كانت ستركز فقط على الحالات ذات الصلة، أي على الحالات التي تشكل
**المسارات المثلى** </span>**(Optimal Paths)**<span dir="rtl">.</span>
<span dir="rtl">حققت</span> **RTDP
<span dir="rtl"></span>**<span dir="rtl">تقريبًا تحكمًا مثاليًا باستخدام
حوالي 50% من الحسابات المطلوبة بواسطة **تكرار القيمة المستند إلى
المسحات**</span> **<span dir="rtl">(</span>Sweep-Based Value
Iteration<span dir="rtl">)</span>**<span dir="rtl">.</span>

**<u>8.8 <span dir="rtl">التخطيط في وقت اتخاذ القرار</span> (Planning at
Decision Time)</u>**

<span dir="rtl">يمكن استخدام التخطيط بطريقتين على الأقل. الطريقة التي
تناولناها حتى الآن في هذا الفصل، والمتمثلة في البرمجة الديناميكية
و</span>**Dyna**<span dir="rtl">، هي استخدام التخطيط لتحسين
**السياسة**</span> **(Policy)** <span dir="rtl">أو **دالة القيمة**
</span>**(Value Function)** <span dir="rtl">تدريجيًا بناءً على التجربة
المحاكية التي تم الحصول عليها من **النموذج** </span>**(Model)**
<span dir="rtl">(سواء كان نموذجًا عيناتيًا أو توزيعيًا). يتم بعد ذلك اختيار
**الإجراءات**</span> **(Actions)** <span dir="rtl">بمقارنة قيم الإجراءات
الحالية للحالة</span> $`\mathbf{St}`$ <span dir="rtl">التي تم الحصول
عليها من جدول (في حالة الجداول التي نظرنا فيها حتى الآن)، أو عن طريق
تقييم تعبير رياضي في الأساليب التقريبية التي سنناقشها في الجزء الثاني
أدناه. قبل اختيار إجراء لأي حالة حالية</span>
$`\mathbf{St}`$<span dir="rtl">، يكون التخطيط قد لعب دورًا في تحسين
إدخالات الجدول، أو التعبير الرياضي، اللازمة لاختيار الإجراء للعديد من
الحالات، بما في ذلك</span> $`\mathbf{St}`$**<span dir="rtl">.</span>**
<span dir="rtl">عند استخدام التخطيط بهذه الطريقة، لا يكون التخطيط مركزًا
على الحالة الحالية. نسمي التخطيط المستخدم بهذه الطريقة **التخطيط
الخلفي**</span> **<span dir="rtl">(</span>Background
Planning<span dir="rtl">)</span>**<span dir="rtl">.</span>

<span dir="rtl">الطريقة الأخرى لاستخدام التخطيط هي البدء وإكماله بعد
مواجهة كل حالة جديدة</span> $`\mathbf{St}`$<span dir="rtl">، كعملية
حسابية ناتجها هو اختيار إجراء واحد</span>
$`\mathbf{At}`$<span dir="rtl">؛ في الخطوة التالية، يبدأ التخطيط من جديد
مع</span> $`S_{t + 1}`$ <span dir="rtl">لإنتاج</span>
$`\mathbf{At + 1}`$<span dir="rtl">، وهكذا. أبسط مثال، وربما الأكثر
بديهية، لاستخدام التخطيط بهذه الطريقة هو عندما تكون قيم الحالات فقط
متاحة، ويتم اختيار إجراء بمقارنة قيم الحالات التالية التي يتنبأ بها
النموذج لكل إجراء  
(أو بمقارنة قيم **الحالات بعد الإجراءات**</span> **(After states)**
<span dir="rtl">كما في مثال لعبة **تيك تاك تو  
(**</span>**Tic-Tac-Toe<span dir="rtl">) </span>**<span dir="rtl">في
الفصل الأول). بشكل عام، يمكن للتخطيط المستخدم بهذه الطريقة أن ينظر أعمق
من مجرد خطوة واحدة إلى الأمام ويقيم خيارات الإجراءات التي تؤدي إلى
العديد من مسارات الحالة والمكافآت المتوقعة. على عكس الاستخدام الأول
للتخطيط، هنا يركز التخطيط على حالة معينة. نسمي هذا **التخطيط في وقت
اتخاذ القرار**</span> **<span dir="rtl">(</span>Decision-Time
Planning<span dir="rtl">)</span>**<span dir="rtl">.</span>

<span dir="rtl">هاتان الطريقتان للتفكير في التخطيط—استخدام التجربة
المحاكية لتحسين السياسة أو دالة القيمة تدريجيًا، أو استخدام التجربة
المحاكية لاختيار إجراء للحالة الحالية—يمكن دمجهما بطرق طبيعية ومثيرة
للاهتمام، ولكن عادةً ما يتم دراستهما بشكل منفصل، وهذا هو الأسلوب الأفضل
لفهمهما أولاً. دعونا نلقي نظرة أقرب على **التخطيط في وقت اتخاذ
القرار**</span> **<span dir="rtl">(</span>Decision-Time
Planning<span dir="rtl">)</span>**<span dir="rtl">.</span>

<span dir="rtl">حتى عندما يتم التخطيط فقط في وقت اتخاذ القرار، لا يزال
بإمكاننا النظر إليه، كما فعلنا في **القسم** **8.1**، على أنه يبدأ من
التجربة المحاكية ثم ينتقل إلى التحديثات والقيم، وأخيرًا إلى السياسة. ولكن
الآن القيم والسياسة تكون محددة للحالة الحالية وخيارات الإجراءات المتاحة
فيها، لدرجة أن القيم والسياسة التي يتم إنشاؤها بواسطة عملية التخطيط عادة
ما يتم تجاهلها بعد استخدامها لاختيار الإجراء الحالي. في العديد من
التطبيقات، لا يمثل هذا خسارة كبيرة لأن هناك العديد من الحالات ومن غير
المحتمل العودة إلى نفس الحالة لفترة طويلة. بشكل عام، قد يرغب المرء في
القيام بمزيج من الاثنين: التركيز على التخطيط في الحالة الحالية وتخزين
نتائج التخطيط بحيث يتم توفير الجهد إذا تم العودة إلى نفس الحالة
لاحقًا.</span>

**<span dir="rtl">التخطيط في وقت اتخاذ القرار</span> (Decision-Time
Planning)** <span dir="rtl">يكون أكثر فائدة في التطبيقات التي لا تتطلب
استجابات سريعة. في برامج لعب الشطرنج، على سبيل المثال، قد يتم منحك ثوانٍ
أو دقائق من الحساب لكل حركة، وقد تخطط البرامج القوية لعشرات الحركات
مسبقًا في هذا الوقت. من ناحية أخرى، إذا كانت الأولوية هي اختيار الإجراءات
بسرعة منخفضة التأخير، فمن الأفضل عمومًا القيام بالتخطيط في الخلفية لحساب
سياسة يمكن تطبيقها بسرعة على كل حالة جديدة يتم مواجهتها</span>.

**<u>8.9 <span dir="rtl">البحث الاسترشادي</span> (Heuristic
Search)</u>**

<span dir="rtl">الطرق التقليدية للتخطيط في **فضاء الحالة**</span>
**(State-Space Planning)** <span dir="rtl">في الذكاء الاصطناعي تُعرف بشكل
جماعي باسم **البحث الاسترشادي** </span>**(Heuristic
Search)**<span dir="rtl">.</span> <span dir="rtl">في البحث الاسترشادي،
عند مواجهة كل حالة، يتم النظر في شجرة كبيرة من الاحتمالات المستقبلية.
يتم تطبيق **دالة القيمة التقريبية** </span>**(Approximate Value
Function)** <span dir="rtl">على العقد الطرفية في الشجرة، ثم يتم تحديث
القيم بالرجوع نحو الحالة الحالية عند الجذر. هذا التحديث داخل شجرة البحث
يشبه تمامًا التحديثات التوقعية مع الحد الأقصى (لـ</span> $`و\ v*\ `$
<span dir="rtl"></span>$`q*`$<span dir="rtl">)</span>
<span dir="rtl">التي نوقشت في جميع أنحاء هذا الكتاب. يتوقف التحديث عند
عقد الحالة-الإجراء الخاصة بالحالة الحالية. بمجرد حساب القيم المحدثة لهذه
العقد، يتم اختيار أفضلها كالإجراء الحالي، ثم يتم تجاهل جميع القيم
المحدثة</span>.

<span dir="rtl">في **البحث الاسترشادي التقليدي**</span>
**<span dir="rtl">(</span>Conventional Heuristic
Search<span dir="rtl">)</span>**<span dir="rtl">، لا يتم بذل جهد لحفظ
القيم المحدثة عن طريق تغيير **دالة القيمة التقريبية**</span>
**<span dir="rtl">(</span>Approximate Value
Function<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">في الواقع، عادة ما يتم تصميم **دالة القيمة**</span>
**(Value Function)** <span dir="rtl">من قبل البشر ولا تتغير نتيجة البحث.
ومع ذلك، من الطبيعي أن نفكر في السماح بتحسين **دالة القيمة**</span>
**(Value Function)** <span dir="rtl">بمرور الوقت، باستخدام إما القيم
المحدثة التي تم حسابها أثناء **البحث الاسترشادي**</span> **(Heuristic
Search)** <span dir="rtl">أو أي من الطرق الأخرى المقدمة في هذا الكتاب.
بمعنى ما، لقد اتبعنا هذا النهج طوال الوقت</span>. **<span dir="rtl">طرق
اختيار الإجراءات الجشعة</span>
<span dir="rtl">(</span>Greedy<span dir="rtl">)</span>**<span dir="rtl">،
و**الجشعة بحدود** </span>**(ε-greedy)**<span dir="rtl">، و</span>**UCB
<span dir="rtl"></span>(Section 2.7)
<span dir="rtl"></span>**<span dir="rtl">ليست مختلفة عن **البحث
الاسترشادي** </span>**(Heuristic Search)**<span dir="rtl">، ولكن على
نطاق أصغر. على سبيل المثال، لحساب الإجراء الجشع بالنظر إلى
**نموذج**</span> **(Model)** <span dir="rtl">و**دالة قيمة
الحالة**</span> **<span dir="rtl">(</span>State-Value
<span dir="rtl"></span>Function<span dir="rtl">)</span>**<span dir="rtl">،
يجب النظر إلى الأمام من كل إجراء ممكن إلى كل حالة تالية ممكنة، مع أخذ
المكافآت والقيم المقدرة في الاعتبار، ثم اختيار أفضل إجراء. تمامًا كما في
**البحث الاسترشادي التقليدي** </span>**(Conventional Heuristic
Search)**<span dir="rtl">، تحسب هذه العملية القيم المحدثة للإجراءات
الممكنة، ولكنها لا تحاول حفظها. لذلك، يمكن اعتبار **البحث
الاسترشادي**</span> **(Heuristic Search)** <span dir="rtl">امتدادًا لفكرة
**السياسة الجشعة**</span> **(Greedy Policy)** <span dir="rtl">إلى ما بعد
خطوة واحدة</span>.

<span dir="rtl">الهدف من البحث إلى ما هو أبعد من خطوة واحدة هو الحصول
على اختيارات إجراءات أفضل. إذا كان لديك نموذج مثالي و**دالة قيمة
للإجراءات**</span> **(Action-Value Function)** <span dir="rtl">غير
مثالية، فإن البحث الأعمق سيوفر عادةً **سياسات**</span> **(Policies)**
<span dir="rtl">أفضل. بالطبع، إذا كان البحث حتى نهاية الحلقة، فإن تأثير
**دالة القيمة غير المثالية**</span> **(Imperfect Value Function)**
<span dir="rtl">يتم إزالته، ويجب أن يكون الإجراء المحدد بهذه الطريقة
مثاليًا. إذا كان عمق البحث كافيًا بحيث يكون</span> $`\gamma k`$
<span dir="rtl">صغيرًا جدًا، فإن الإجراءات ستكون قريبة من المثلى تبعًا
لذلك. من ناحية أخرى، كلما كان البحث أعمق، زاد الحساب المطلوب، مما يؤدي
عادةً إلى وقت استجابة أبطأ. مثال جيد على ذلك هو **نظام**</span>
**TD-Gammon** <span dir="rtl">بمستوى الأستاذ الكبير في لعبة
الطاولة</span> (Section 16.1)<span dir="rtl">.</span>
<span dir="rtl">استخدم هذا النظام التعليم</span> TD
<span dir="rtl">لتعلم **دالة القيمة للحالات بعد الإجراءات**</span>
**(Afterstate Value Function)** <span dir="rtl">من خلال العديد من
الألعاب الذاتية، مستخدمًا شكلًا من أشكال **البحث الاسترشادي**</span>
**(Heuristic Search)** <span dir="rtl">لاتخاذ حركاته. استخدم  
</span>**TD-Gammon <span dir="rtl"></span>**<span dir="rtl">كنموذج
المعرفة المسبقة لاحتمالات رميات النرد وفرضية أن الخصم دائمًا يختار
الإجراءات التي يعتبرها</span> **TD-Gammon
<span dir="rtl"></span>**<span dir="rtl">الأفضل له. وجد</span> **Tesauro
<span dir="rtl"></span>**<span dir="rtl">أنه كلما كان البحث الاسترشادي
أعمق، كانت الحركات التي يقوم بها</span> **TD-Gammon
<span dir="rtl"></span>**<span dir="rtl">أفضل، ولكن كان يستغرق وقتًا أطول
لاتخاذ كل حركة. لعبة الطاولة لديها عامل تفرع كبير، ومع ذلك يجب اتخاذ
الحركات في غضون ثوانٍ قليلة. كان من الممكن البحث إلى الأمام بشكل انتقائي
لبضع خطوات فقط، ولكن حتى مع ذلك، أدى البحث إلى اختيارات إجراءات أفضل
بشكل ملحوظ</span>.

<span dir="rtl">يجب ألا نتجاهل الطريقة الأكثر وضوحًا التي يركز بها
**البحث الاسترشادي** </span>**(Heuristic Search)** <span dir="rtl">على
التحديثات: وهي التركيز على الحالة الحالية. الكثير من فعالية **البحث
الاسترشادي**</span> **<span dir="rtl">(</span>Heuristic
<span dir="rtl"></span>Search<span dir="rtl">)</span>**
<span dir="rtl">يعود إلى تركيز شجرة البحث على الحالات والإجراءات التي قد
تتبع الحالة الحالية مباشرة. قد تقضي جزءًا كبيرًا من حياتك في لعب الشطرنج
أكثر من لعبة الداما، ولكن عندما تلعب الداما، من المفيد التفكير في وضعية
الداما الخاصة بك، وحركاتك التالية المحتملة، والحالات التالية. بغض النظر
عن كيفية اختيارك للإجراءات، فإن هذه الحالات والإجراءات هي ذات الأولوية
القصوى للتحديثات، وهي الأماكن التي تريد أن تكون فيها **دالة القيمة
التقريبية**</span> **(Approximate Value Function)**
<span dir="rtl">دقيقة. يجب أن تكرس حساباتك بشكل تفضيلي للأحداث القريبة
القادمة، وكذلك مواردك المحدودة من الذاكرة. في الشطرنج، على سبيل المثال،
هناك عدد كبير جدًا من الوضعيات الممكنة لتخزين تقديرات مميزة لكل منها،
ولكن برامج الشطرنج التي تعتمد على **البحث الاسترشادي**</span>
**(Heuristic Search)** <span dir="rtl">يمكنها بسهولة تخزين تقديرات مميزة
لملايين الوضعيات التي تواجهها عند النظر إلى الأمام من وضعية واحدة. هذا
التركيز الكبير للذاكرة والموارد الحسابية على القرار الحالي هو على الأرجح
السبب في أن **البحث الاسترشادي**</span> **(Heuristic Search)**
<span dir="rtl">يمكن أن يكون فعالًا للغاية</span>.

<span dir="rtl">يمكن تغيير **توزيع التحديثات**</span> **(Updates
Distribution)** <span dir="rtl">بطرق مماثلة للتركيز على الحالة الحالية
وخلفائها المحتملين. كحالة حدية، قد نستخدم تمامًا طرق **البحث الاسترشادي**
</span>**(Heuristic Search)** <span dir="rtl">لبناء شجرة بحث، ثم تنفيذ
التحديثات الفردية ذات الخطوة الواحدة من الأسفل إلى الأعلى، كما هو مقترح
في **الشكل** **8.9**. إذا تم ترتيب التحديثات بهذه الطريقة وتم استخدام
تمثيل جدولي، فإن التحديث الكلي سيكون تمامًا مثل البحث الاسترشادي الأول في
العمق. يمكن النظر إلى أي بحث في **فضاء الحالة** </span>**(State-Space
Search)** <span dir="rtl">بهذه الطريقة على أنه تجميع لعدد كبير من
التحديثات الفردية ذات الخطوة الواحدة. لذلك، فإن تحسين الأداء الملحوظ مع
البحث الأعمق لا يرجع إلى استخدام التحديثات متعددة الخطوات بحد ذاتها.
بدلاً من ذلك، يرجع إلى تركيز التحديثات على الحالات والإجراءات التي تقع
مباشرة بعد الحالة الحالية. من خلال تخصيص كمية كبيرة من الحسابات المتعلقة
بالإجراءات المرشحة، يمكن للتخطيط في وقت اتخاذ القرار أن ينتج قرارات أفضل
مما يمكن أن يتم إنتاجه بالاعتماد على التحديثات غير الموجهة</span>.

<img src="./media/image94.png"
style="width:6.26806in;height:3.00069in" />

<span dir="rtl">**الشكل** **8.9**: يمكن تنفيذ **البحث
الاسترشادي**</span> **(Heuristic Search)** <span dir="rtl">كسلسلة من
التحديثات ذات الخطوة الواحدة (الموضحة هنا بإطار أزرق) التي تقوم بتحديث
القيم من العقد الطرفية نحو الجذر. الترتيب الموضح هنا هو لبحث استرشادي
أولي في العمق الانتقائي</span>.

**<u>8.10 <span dir="rtl">خوارزميات "الـرول آوت</span>"
<span dir="rtl">(</span>Rollout Algorithms<span dir="rtl">)</span></u>**

<span dir="rtl">خوارزميات **الرول آوت**</span> **(Rollout Algorithms)**
<span dir="rtl">هي خوارزميات للتخطيط في وقت اتخاذ القرار تعتمد على
**التحكم مونت كارلو**</span> **(Monte Carlo Control)**
<span dir="rtl">المطبق على مسارات محاكية تبدأ جميعها من الحالة الحالية
للبيئة. تقوم هذه الخوارزميات بتقدير **قيم الإجراءات**</span> **(Action
Values)** <span dir="rtl">لسياسة معينة من خلال حساب متوسط العوائد للعديد
من المسارات المحاكية التي تبدأ بكل إجراء ممكن ثم تتبع السياسة المعطاة.
عندما تعتبر تقديرات **قيم الإجراءات**</span> **(Action-Value
Estimates)** <span dir="rtl">دقيقة بما يكفي، يتم تنفيذ الإجراء (أو أحد
الإجراءات) الذي لديه أعلى قيمة مقدرة، ثم يتم تكرار العملية مرة أخرى من
الحالة التالية الناتجة</span>.

<span dir="rtl">كما أوضح</span> **Tesauro**
<span dir="rtl">و</span>**Galperin** (1997) <span dir="rtl">الذين قاموا
بتجربة خوارزميات الرول آوت في لعبة الطاولة، فإن مصطلح "الرول آوت" يأتي
من تقدير قيمة وضعية في لعبة الطاولة عن طريق "لعبها"، أي تنفيذها حتى
النهاية باستخدام تسلسلات عشوائية من رميات النرد، حيث يتم تنفيذ حركات كلا
اللاعبين بواسطة سياسة ثابتة</span>.

<span dir="rtl">على عكس خوارزميات التحكم مونت كارلو التي تم وصفها في
الفصل 5، فإن هدف خوارزمية الرول آوت ليس تقدير **دالة**
</span>$`\ \mathbf{q*}`$**<span dir="rtl">قيمة الإجراءات المثلى</span>
(Optimal Action-Value Function)
<span dir="rtl"></span>**<span dir="rtl">بالكامل، أو تقدير **دالة قيمة
الإجراءات** </span>$`q_{\pi}`$ <span dir="rtl"></span>
**<span dir="rtl">لسياسة معينة</span>**
$`\mathbf{\pi}`$**<span dir="rtl">.</span>** <span dir="rtl">بدلاً من
ذلك، فإنها تنتج تقديرات مونت كارلو لقيم الإجراءات فقط لكل حالة حالية
وللسياسة المعطاة، والتي عادة ما تسمى **سياسة الرول آوت**
</span>**(Rollout Policy)**<span dir="rtl">.</span>
<span dir="rtl">كخوارزميات للتخطيط في وقت اتخاذ القرار، تقوم خوارزميات
الرول آوت باستخدام فوري لتقديرات قيم الإجراءات هذه، ثم تتجاهلها. هذا
يجعل خوارزميات الرول آوت بسيطة نسبيًا للتنفيذ لأنه لا توجد حاجة لأخذ
عينات نتائج لكل زوج حالة-إجراء، ولا توجد حاجة لتقريب دالة على **فضاء
الحالة**</span> **(State Space)** <span dir="rtl">أو **فضاء
الحالة-الإجراء** </span>**(State-Action Space)**<span dir="rtl">.</span>

<span dir="rtl">ما الذي تحققه خوارزميات الرول آوت إذًا؟ يخبرنا مبرهن
تحسين السياسة</span> <span dir="rtl">(</span>Policy Improvement
Theorem<span dir="rtl">)</span> <span dir="rtl">الموضح في القسم 4.2 أنه
بالنظر إلى أي سياستين</span> $`\pi`$ <span dir="rtl">و</span>$`\pi'`$
<span dir="rtl">متطابقتين باستثناء أن  
</span>$`\ \ `$<span dir="rtl">لحالة معينة</span> $`s`$<span dir="rtl">،
إذا كانت</span> $`q_{\pi}(s,a)\  \geq v\pi(s)q`$<span dir="rtl">، فإن
السياسة</span> $`\pi'`$ <span dir="rtl">تكون جيدة مثل</span> $`\pi`$
<span dir="rtl">أو أفضل منها. علاوة على ذلك، إذا كانت المتباينة صارمة،
فإن</span> $`\pi'`$ <span dir="rtl">في الواقع أفضل  
من</span> π<span dir="rtl">.</span> <span dir="rtl">ينطبق هذا على
خوارزميات الرول آوت حيث</span> $`s`$ <span dir="rtl">هي الحالة الحالية
و</span>$`\pi`$ <span dir="rtl">هي  
**سياسة الرول آوت** </span>**(Rollout Policy)**<span dir="rtl">.</span>
<span dir="rtl">حساب متوسط العوائد  
للمسارات المحاكية ينتج تقديرات لـ</span> $`q_{\pi}(s,a')`$
<span dir="rtl">لكل  
إجراء</span> $`a'`$ <span dir="rtl">في</span>
$`A(s)`$<span dir="rtl">.</span> <span dir="rtl">بعد ذلك، تكون السياسة
التي تختار إجراء في</span> s <span dir="rtl">والذي يعظم هذه التقديرات،
ثم تتبع</span> $`\pi`$ <span dir="rtl">بعد ذلك، مرشحًا جيدًا لسياسة تحسن
على</span> $`\pi`$<span dir="rtl">.</span> <span dir="rtl">النتيجة تشبه
خطوة واحدة من خوارزمية تكرار السياسة في البرمجة الديناميكية التي نوقشت
في **القسم** **4.3** (على الرغم من أنها تشبه أكثر خطوة واحدة من تكرار
القيمة غير المتزامن الموصوف في **القسم** **4.5** لأنها تغير الإجراء فقط
للحالة الحالية)</span>.

<span dir="rtl">بعبارة أخرى، فإن هدف خوارزمية الرول آوت هو تحسين **سياسة
الرول آوت** </span>**(Rollout Policy)**<span dir="rtl">؛ وليس العثور على
سياسة مثلى. لقد أظهرت التجربة أن خوارزميات الرول آوت يمكن أن تكون فعالة
بشكل مفاجئ. على سبيل المثال، تفاجأ</span> **Tesauro**
<span dir="rtl">و</span>**Galperin <span dir="rtl"></span>**(1997)
<span dir="rtl">بالتحسينات الدراماتيكية في القدرة على لعب لعبة الطاولة
التي تم إنتاجها بواسطة طريقة الرول آوت. في بعض التطبيقات، يمكن أن تنتج
خوارزمية الرول آوت أداءً جيدًا حتى لو كانت سياسة الرول آوت عشوائية تمامًا.
ولكن يعتمد أداء السياسة المحسنة على خصائص سياسة الرول آوت وعلى ترتيب
الإجراءات الناتج عن تقديرات قيمة مونت كارلو. الفطرة السليمة تشير إلى أنه
كلما كانت **سياسة الرول آوت**</span> **(Rollout Policy)**
<span dir="rtl">أفضل وكانت تقديرات القيمة أكثر دقة، كلما كانت السياسة
التي تنتجها خوارزمية الرول آوت أفضل (لكن انظر</span> **Gelly
<span dir="rtl"></span>**<span dir="rtl">و</span>**Silver**<span dir="rtl">،
2007)</span>.

<span dir="rtl">يتضمن هذا مقايضات مهمة لأن **سياسات الرول آوت**</span>
**(Rollout Policies)** <span dir="rtl">الأفضل عادة ما تعني أن هناك حاجة
إلى المزيد من الوقت لمحاكاة عدد كافٍ من المسارات للحصول على تقديرات جيدة
للقيمة. كطرق للتخطيط في وقت اتخاذ القرار، يجب على خوارزميات الرول آوت
عادةً أن تلتزم بقيود زمنية صارمة. يعتمد وقت الحساب المطلوب من خوارزمية
الرول آوت على عدد الإجراءات التي يجب تقييمها لكل قرار، وعدد خطوات الوقت
في المسارات المحاكية اللازمة للحصول على عوائد عينية مفيدة، والوقت الذي
تستغرقه **سياسة الرول آوت**</span> **(Rollout Policy)**
<span dir="rtl">لاتخاذ القرارات، وعدد المسارات المحاكية المطلوبة للحصول
على تقديرات جيدة لقيمة إجراءات مونت كارلو</span>.

<span dir="rtl">موازنة هذه العوامل أمر مهم في أي تطبيق لأساليب الرول
آوت، على الرغم من أن هناك عدة طرق لتسهيل التحدي. نظرًا لأن تجارب مونت
كارلو مستقلة عن بعضها البعض، فمن الممكن تشغيل العديد من التجارب بالتوازي
على معالجات منفصلة. نهج آخر هو قطع المسارات المحاكية قبل اكتمال الحلقات،
وتصحيح العوائد المقطوعة باستخدام دالة تقييم مخزنة (مما يجلب إلى الاعتبار
كل ما قلناه عن العوائد المقطوعة والتحديثات في الفصول السابقة). من الممكن
أيضًا، كما يقترح</span> **Tesauro** <span dir="rtl">و</span>**Galperin**
(1997)<span dir="rtl">، مراقبة محاكاة مونت كارلو واستبعاد الإجراءات
المرشحة التي من غير المحتمل أن تكون الأفضل، أو تلك التي تكون قيمها قريبة
بما يكفي من الأفضل الحالي بحيث لا يشكل اختيارها بدلاً من غيرها فرقًا
حقيقيًا (على الرغم من أن</span> **Tesauro
<span dir="rtl"></span>**<span dir="rtl">و</span>**Galperin
<span dir="rtl"></span>**<span dir="rtl">يشيران إلى أن هذا سيعقد التنفيذ
المتوازي)</span>.

<span dir="rtl">عادةً لا نعتبر خوارزميات الرول آوت كخوارزميات تعلم لأنها
لا تحتفظ بذكريات طويلة الأجل للقيم أو السياسات. ومع ذلك، تستفيد هذه
الخوارزميات من بعض ميزات التعليم المعزز التي أكدنا عليها في هذا الكتاب.
كحالات من التحكم مونت كارلو، فإنها تقدر قيم الإجراءات من خلال حساب متوسط
العوائد لمجموعة من المسارات العينية، في هذه الحالة مسارات التفاعلات
المحاكية مع نموذج عيني للبيئة. بهذه الطريقة، تشبه خوارزميات الرول آوت
خوارزميات التعليم المعزز في تجنب المسحات الشاملة للبرمجة الديناميكية عن
طريق أخذ عينات المسارات، وفي تجنب الحاجة إلى نماذج التوزيع عن طريق
الاعتماد على التحديثات العينية بدلاً من التحديثات المتوقعة. أخيرًا، تستفيد
خوارزميات الرول آوت من خاصية تحسين السياسة من خلال التصرف بشكل جشع
بالنسبة لتقديرات قيم الإجراءات</span>.

**<u>8.11 <span dir="rtl">مونت كارلو شجرة البحث</span> (Monte Carlo Tree
Search)</u>**

**<span dir="rtl">مونت كارلو شجرة البحث</span> (MCTS)**
<span dir="rtl">هي مثال حديث وناجح بشكل لافت على **التخطيط في وقت اتخاذ
القرار** </span>**(Decision-Time Planning)**<span dir="rtl">.</span>
<span dir="rtl">في جوهرها،</span> **MCTS
<span dir="rtl"></span>**<span dir="rtl">هي **خوارزمية رول آوت**</span>
**<span dir="rtl">(</span>Rollout
<span dir="rtl"></span>Algorithm<span dir="rtl">)
</span>**<span dir="rtl">كما تم وصفها سابقًا، ولكنها مُحسَّنة بإضافة وسيلة
لتجميع **تقديرات القيم**</span> **<span dir="rtl">(</span>Value
<span dir="rtl"></span>Estimates<span dir="rtl">)
</span>**<span dir="rtl">التي تم الحصول عليها من **محاكاة مونت
كارلو**</span> **<span dir="rtl">(</span>Monte Carlo
Simulations<span dir="rtl">)</span>** <span dir="rtl">من أجل توجيه
المحاكاة بشكل متتالي نحو مسارات أكثر مكافأة</span>. **MCTS**
<span dir="rtl">مسؤولة بشكل كبير عن تحسين مستوى اللعب في لعبة
**جو**</span> **(Go)** <span dir="rtl">على الكمبيوتر من مستوى الهواة
الضعيف في عام 2005 إلى مستوى الأستاذ الكبير (6 دان أو أكثر) في عام 2015.
تم تطوير العديد من التنويعات للخوارزمية الأساسية، بما في ذلك نوع نناقشه
في القسم 16.6، الذي كان حاسمًا في الانتصارات المذهلة لبرنامج</span>
**AlphaGo <span dir="rtl"></span>**<span dir="rtl">في عام 2016 على بطل
العالم في **جو**</span> **(Go)** <span dir="rtl">الذي فاز بـ 18 مرة.
أثبتت  
</span>**MCTS <span dir="rtl"></span>**<span dir="rtl">فعاليتها في
مجموعة متنوعة من البيئات التنافسية، بما في ذلك **اللعب العام للألعاب**
</span>**(General Game Playing)** <span dir="rtl">(على سبيل المثال،
انظر</span> **Finnsson
<span dir="rtl"></span>**<span dir="rtl">و</span>Björnsson<span dir="rtl">،
2008؛</span> **Genesereth
<span dir="rtl">و</span>Thielscher**<span dir="rtl">، 2014)، ولكنها ليست
مقتصرة على الألعاب؛ يمكن أن تكون فعالة في **مشاكل اتخاذ القرار المتسلسلة
لوكيل واحد**</span> **<span dir="rtl">(</span>Single-Agent Sequential
Decision <span dir="rtl"></span>Problems<span dir="rtl">)
</span>**<span dir="rtl">إذا كان هناك **نموذج بيئي**</span>
**(Environment Model)** <span dir="rtl">بسيط بما يكفي لمحاكاة متعددة
الخطوات بسرعة</span>.

**MCTS <span dir="rtl"></span>**<span dir="rtl">تُنفذ بعد مواجهة كل حالة
جديدة لاختيار **إجراء الوكيل**</span> **(Agent's Action)**
<span dir="rtl">لتلك الحالة؛ ويتم تنفيذها مرة أخرى لاختيار الإجراء
للحالة التالية، وهكذا. كما هو الحال في **خوارزمية رول آوت**
</span>**(Rollout Algorithm)**<span dir="rtl">، فإن كل تنفيذ هو عملية
تكرارية تحاكي العديد من المسارات التي تبدأ من **الحالة الحالية**</span>
**(Current State)** <span dir="rtl">وتستمر إلى **حالة نهائية**</span>
**(Terminal State)** (<span dir="rtl">أو حتى يصبح **الخصم**</span>
**(Discounting)** <span dir="rtl">عديم التأثير على المكافأة بحيث لا يمثل
أي مكافأة إضافية مساهمة في العائد). الفكرة الأساسية في</span> **MCTS
<span dir="rtl"></span>**<span dir="rtl">هي التركيز بشكل متتالي على
**محاكاة متعددة  
(**</span>**Multiple Simulations<span dir="rtl">)</span>**
<span dir="rtl">تبدأ من **الحالة الحالية**</span> **(Current State)**
<span dir="rtl">عن طريق تمديد الأجزاء الأولية من **المسارات**</span>
**(Trajectories)** <span dir="rtl">التي حصلت على تقييمات عالية من
المحاكاة السابقة</span>.

<img src="./media/image95.png"
style="width:6.26806in;height:3.56944in" />

<span dir="rtl">**الشكل 8.10**: مونت كارلو شجرة البحث</span> (Monte
Carlo Tree Search)<span dir="rtl">.</span> <span dir="rtl">عندما تنتقل
البيئة إلى حالة جديدة، تقوم</span> **MCTS
<span dir="rtl"></span>**<span dir="rtl">بتنفيذ أكبر عدد ممكن من
التكرارات قبل الحاجة إلى اختيار إجراء، حيث تقوم ببناء شجرة تدريجيًا تمثل
عقدتها الجذرية **الحالة الحالية** </span>**(Current
State)**<span dir="rtl">.</span> <span dir="rtl">تتكون كل تكرار من أربع
عمليات:</span> **<span dir="rtl">الاختيار</span>
(Selection)**<span dir="rtl">، **التوسيع**</span> **(Expansion)**
<span dir="rtl">(على الرغم من أنه يمكن تخطيها في بعض التكرارات)،
**المحاكاة** </span>**(Simulation)**<span dir="rtl">، والتحديث</span>
**(Backup)**<span dir="rtl">، كما هو موضح في النص ومبين بالأسهم الغامقة
في الأشجار. مقتبسة من</span> **Chaslot, Bakkes, Szita, and Spronck
<span dir="rtl"></span>(2008)**<span dir="rtl">.</span>

<span dir="rtl">تعتبر **مونت كارلو شجرة البحث**</span> **(MCTS)**
<span dir="rtl">خوارزمية فعالة للتخطيط في وقت اتخاذ القرار تعتمد على
التحكم بطريقة **مونت كارلو**</span> **(Monte Carlo Control)**
<span dir="rtl">المطبق على المحاكاة التي تبدأ من **الحالة الجذرية**
</span>**(Root State)**<span dir="rtl">.</span> <span dir="rtl">في
الأساس،</span> **MCTS <span dir="rtl"></span>**<span dir="rtl">هي نوع من
**خوارزمية الرول آوت**</span> **<span dir="rtl">(</span>Rollout
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**
<span dir="rtl">كما تم وصفها سابقًا، لكنها محسّنة بفضل إضافة وسيلة لتجميع
**تقديرات القيم**</span> **<span dir="rtl">(</span>Value
<span dir="rtl"></span>Estimates<span dir="rtl">)
</span>**<span dir="rtl">التي تم الحصول عليها من محاكاة **مونت كارلو**
</span>**(Monte Carlo Simulations)** <span dir="rtl">بهدف توجيه المحاكاة
نحو المسارات الأكثر مكافأة</span>.

### <span dir="rtl">خطوات خوارزمية</span> MCTS 

<span dir="rtl">كل تكرار من النسخة الأساسية لـ</span> **MCTS
<span dir="rtl"></span>**<span dir="rtl">يتكون من الخطوات الأربع التالية
كما هو موضح في **الشكل** **8.10**</span>:

1.  **<span dir="rtl">الاختيار</span>
    (Selection)**<span dir="rtl">:</span> <span dir="rtl">بدءًا من العقدة
    الجذرية، يتم استخدام **سياسة الشجرة**</span> **<span dir="rtl">  
    (</span>Tree Policy<span dir="rtl">)</span>**
    <span dir="rtl">المعتمدة على قيم الإجراءات المرتبطة بحواف الشجرة
    لاختيار عقدة طرفية</span>.

2.  **<span dir="rtl">التوسيع</span>
    (Expansion)**<span dir="rtl">:</span> <span dir="rtl">في بعض
    التكرارات (اعتمادًا على تفاصيل التطبيق)، يتم توسيع الشجرة من العقدة
    المختارة عن طريق إضافة عقد فرعية واحدة أو أكثر تم الوصول إليها من
    العقدة المختارة عبر إجراءات غير مستكشفة</span>.

3.  **<span dir="rtl">المحاكاة</span>
    (Simulation)**<span dir="rtl">:</span> <span dir="rtl">من العقدة
    المختارة، أو من واحدة من العقد الفرعية المضافة حديثًا (إن وجدت)، يتم
    تشغيل محاكاة لحلقة كاملة باستخدام الإجراءات المختارة بواسطة **سياسة
    الرول آوت** </span>**(Rollout Policy)**<span dir="rtl">.</span>
    <span dir="rtl">النتيجة هي تجربة **مونت كارلو**</span> **(Monte
    Carlo Trial)** <span dir="rtl">حيث يتم اختيار الإجراءات أولاً بواسطة
    **سياسة الشجرة**</span> **(Tree Policy)** <span dir="rtl">وخارج
    الشجرة بواسطة **سياسة الرول آوت**</span> **(Rollout
    Policy)**<span dir="rtl">.</span>

4.  **<span dir="rtl">التحديث</span> (Backup)**<span dir="rtl">:</span>
    <span dir="rtl">يتم تحديث أو تهيئة قيم الإجراءات المرتبطة بحواف
    الشجرة التي تم المرور بها بواسطة **سياسة الشجرة**</span> **(Tree
    Policy)** <span dir="rtl">في هذا التكرار من</span> **MCTS
    <span dir="rtl"></span>**<span dir="rtl">باستخدام العائد الناتج عن
    المحاكاة. لا يتم حفظ أي قيم للحالات والإجراءات التي تمت زيارتها
    بواسطة **سياسة الرول آوت**</span> **(Rollout Policy)**
    <span dir="rtl">خارج الشجرة</span>.

<span dir="rtl">تستمر</span> **MCTS
<span dir="rtl"></span>**<span dir="rtl">في تنفيذ هذه الخطوات الأربع،
بدءًا من كل مرة عند العقدة الجذرية للشجرة، حتى لا يتبقى وقت أو يتم
استنفاد بعض الموارد الحسابية الأخرى. ثم، يتم أخيرًا اختيار إجراء من
العقدة الجذرية (التي لا تزال تمثل **الحالة الحالية**</span> **(Current
State)** <span dir="rtl">للبيئة) بناءً على آلية تعتمد على الإحصائيات
المتراكمة في الشجرة؛ على سبيل المثال، قد يكون الإجراء الذي يحتوي على
أعلى **قيمة للإجراء** </span>**(Action Value)
<span dir="rtl"></span>**<span dir="rtl">من بين جميع الإجراءات المتاحة
من **الحالة الجذرية** </span>**(Root State)**<span dir="rtl">، أو ربما
الإجراء الذي يحتوي على أكبر عدد من الزيارات لتجنب اختيار الشواذ</span>.

### <span dir="rtl">العلاقة مع التعليم المعزز</span> <span dir="rtl">(</span>Reinforcement Learning<span dir="rtl">)</span>

<span dir="rtl">من خلال ربط</span> **MCTS
<span dir="rtl"></span>**<span dir="rtl">بمبادئ **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">التي تم
شرحها في هذا الكتاب، يمكننا الحصول على بعض الأفكار حول كيفية تحقيقها
لمثل هذه النتائج المثيرة للإعجاب. في جوهرها،</span> **MCTS
<span dir="rtl"></span>**<span dir="rtl">هي خوارزمية للتخطيط في وقت
اتخاذ القرار تعتمد على **التحكم بطريقة مونت كارلو**</span> **(Monte
Carlo Control)** <span dir="rtl">المطبق على المحاكاة التي تبدأ من
**الحالة الجذرية  
(**</span>**Root State<span dir="rtl">)</span>**<span dir="rtl">؛ بمعنى
آخر، هي نوع من **خوارزمية الرول آوت**</span> **(Rollout Algorithm)**
<span dir="rtl">كما تم وصفها في القسم السابق. وبالتالي، تستفيد من
التقدير التدريجي للقيم عبر الإنترنت وتحسين السياسات بناءً على العينات.
علاوة على ذلك، تحتفظ</span> **MCTS
<span dir="rtl"></span>**<span dir="rtl">بتقديرات **قيم الإجراءات**
</span>**(Action Values)** <span dir="rtl">المرتبطة بحواف الشجرة وتقوم
بتحديثها باستخدام **تحديثات العينات**</span> **(Sample Updates)**
<span dir="rtl">الخاصة بالتعليم المعزز. هذا يؤدي إلى تركيز تجارب **مونت
كارلو**</span> **(Monte Carlo Trials)** <span dir="rtl">على المسارات
التي تتضمن مقاطع أولية شائعة في المسارات عالية العائد التي تم محاكاتها
سابقًا</span>.

### <span dir="rtl">تأثير</span> MCTS <span dir="rtl">على الذكاء الاصطناعي</span>

<span dir="rtl">كان للنجاح اللافت لخوارزمية</span> **MCTS
<span dir="rtl"></span>**<span dir="rtl">في التخطيط في وقت اتخاذ القرار
تأثير عميق على الذكاء الاصطناعي، ويقوم العديد من الباحثين بدراسة
التعديلات والإضافات على الإجراء الأساسي لاستخدامه في كل من الألعاب
وتطبيقات الوكيل الواحد</span>.

**<u>8.12 <span dir="rtl">ملخص الفصل</span> (Summary of the
Chapter)<span dir="rtl">  
</span></u>**

<span dir="rtl">يتطلب **التخطيط**</span> **(Planning)**
<span dir="rtl">وجود **نموذج للبيئة**</span>
**<span dir="rtl">(</span>Model of the
Environment<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">يتكون **نموذج التوزيع**</span> **(Distribution Model)
<span dir="rtl"></span>**<span dir="rtl">من احتمالات الحالات التالية
والمكافآت للإجراءات الممكنة؛ بينما يُنتج **النموذج العيني**</span>
**<span dir="rtl">(</span>Sample Model<span dir="rtl">)
</span>**<span dir="rtl">انتقالات فردية ومكافآت تم توليدها وفقًا لهذه
الاحتمالات. تتطلب **البرمجة الديناميكية**</span>
**<span dir="rtl">(</span>Dynamic Programming<span dir="rtl">)
</span>**<span dir="rtl">نموذج **توزيع** </span>**(Distribution Model)
<span dir="rtl"></span>**<span dir="rtl">لأنها تستخدم **تحديثات
متوقعة**</span> **<span dir="rtl">(</span>Expected
Updates<span dir="rtl">)</span>**<span dir="rtl">، التي تتضمن حساب
التوقعات عبر جميع الحالات التالية والمكافآت الممكنة. من ناحية أخرى،
يحتاج **النموذج العيني** </span>**(Sample Model)**
<span dir="rtl">لمحاكاة التفاعل مع البيئة حيث يمكن استخدام **التحديثات
العينية**</span> **<span dir="rtl">(</span>Sample
<span dir="rtl"></span>Updates<span dir="rtl">)</span>**<span dir="rtl">،
مثل تلك المستخدمة من قبل العديد من خوارزميات **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">تعتبر النماذج العينية عمومًا أسهل في الحصول عليها من
نماذج التوزيع</span>.

<span dir="rtl">لقد قدمنا ​​وجهة نظر تؤكد على العلاقات الوثيقة بشكل مدهش
بين التخطيط لسلوك مثالي وتعلم سلوك مثالي. كلاهما يتضمن تقدير نفس **دوال
القيمة**</span> **<span dir="rtl">(</span>Value
Functions<span dir="rtl">)</span>**<span dir="rtl">، وفي كلا الحالتين
يكون من الطبيعي تحديث التقديرات تدريجيًا في سلسلة طويلة من عمليات النسخ
الاحتياطية الصغيرة. هذا يجعل من السهل دمج عمليات التعليم والتخطيط ببساطة
عن طريق السماح لكليهما بتحديث نفس **دالة القيمة المقدرة**
</span>**(Estimated Value Function)**<span dir="rtl">.</span>
<span dir="rtl">بالإضافة إلى ذلك، يمكن تحويل أي من أساليب التعليم إلى
أساليب التخطيط ببساطة عن طريق تطبيقها على **التجربة المحاكاة
(**</span>**Simulated <span dir="rtl">  
</span>Experience<span dir="rtl">)</span>** <span dir="rtl">التي يولدها
النموذج بدلاً من التجربة الحقيقية. في هذه الحالة، يصبح التعليم والتخطيط
أكثر تشابهًا؛ بل قد يكونان خوارزميات متطابقة تعمل على مصدرين مختلفين من  
**التجربة**</span>
**<span dir="rtl">(</span>Experience<span dir="rtl">).</span>**

<span dir="rtl">إن دمج أساليب التخطيط التدريجي مع **التصرف**</span>
**(Acting)** <span dir="rtl">والتعليم **من النموذج  
(**</span>**Model-Learning<span dir="rtl">)</span>** <span dir="rtl">أمر
بسيط. تتفاعل **عمليات التخطيط والتصرف وتعلم النموذج  
(**</span>**Planning, Acting, and
Model-Learning<span dir="rtl">)</span>** <span dir="rtl">بطريقة دائرية،
حيث يُنتج كل منها ما يحتاجه الآخر للتحسن؛ لا تتطلب هذه العمليات أي تفاعل
آخر ولا تحظر عليه. النهج الأكثر طبيعية هو أن تتم جميع العمليات بشكل غير
متزامن ومتوازي. إذا كان يجب على العمليات مشاركة الموارد الحاسوبية، فيمكن
التعامل مع هذا التقسيم بشكل تعسفي تقريبًا—وفقًا لأي تنظيم يكون أكثر ملاءمة
وكفاءة للمهمة المطلوبة</span>.

<span dir="rtl">في هذا الفصل، تناولنا عددًا من أبعاد التباين بين أساليب
**التخطيط في فضاء الحالة**</span> **<span dir="rtl">(</span>State-Space
<span dir="rtl"></span>Planning
Methods<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">أحد الأبعاد هو التباين في حجم **التحديثات**</span>
**<span dir="rtl">(</span>Updates<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">كلما كانت التحديثات أصغر، زادت قدرة أساليب التخطيط على
أن تكون تدريجية. من بين أصغر التحديثات هي **التحديثات العينية ذات الخطوة
الواحدة**</span> **<span dir="rtl">(</span>One-Step Sample
Updates<span dir="rtl">)</span>**<span dir="rtl">، كما هو الحال
في</span> **Dyna<span dir="rtl">.</span>** <span dir="rtl">بُعد آخر مهم
هو **توزيع التحديثات** </span>**(Distribution of
Updates)**<span dir="rtl">، أي **تركيز البحث**</span> **(Focus of
Search)**<span dir="rtl">.</span> <span dir="rtl">يركز **التمشيط
بالأولوية**</span> **(Prioritized Sweeping)** <span dir="rtl">على
التراجعات إلى **الحالات السابقة**</span> **(Predecessor States)**
<span dir="rtl">التي تغيرت قيمها مؤخرًا. يركز **أخذ عينات المسارات وفقًا
للسياسة الحالية**</span> **(On-Policy Trajectory Sampling)**
<span dir="rtl">على الحالات أو أزواج  
الحالة-الإجراء التي من المحتمل أن يواجهها الوكيل عند التحكم في بيئته.
يمكن أن يسمح هذا الحساب بتجاوز أجزاء من فضاء الحالة التي لا علاقة لها
بمشكلة التنبؤ أو التحكم</span>. **<span dir="rtl">البرمجة الديناميكية في
الوقت الحقيقي</span> <span dir="rtl">(</span>Real-Time Dynamic
Programming<span dir="rtl">)</span>**<span dir="rtl">، وهي نسخة من
**تكرار القيمة  
(**</span>**Value Iteration<span dir="rtl">)</span>**
<span dir="rtl">تعتمد على أخذ عينات المسار وفقًا للسياسة الحالية، توضح
بعض المزايا التي تمتلكها هذه الاستراتيجية مقارنة بـ **تكرار السياسة
التقليدي القائم على المسح**</span>
**<span dir="rtl">(</span>Conventional
<span dir="rtl"></span>Sweep-Based Policy
Iteration<span dir="rtl">)</span>**.

<span dir="rtl">يمكن أيضًا أن يركز **التخطيط**</span> **(Planning)**
<span dir="rtl">على الأمام بدءًا من الحالات ذات الصلة، مثل الحالات التي
تم مواجهتها بالفعل أثناء تفاعل الوكيل مع البيئة. الشكل الأكثر أهمية من
هذا هو عندما يتم تنفيذ التخطيط في وقت اتخاذ القرار، أي كجزء من عملية
اختيار الإجراء.</span> **<span dir="rtl">البحث الاسترشادي
الكلاسيكي</span> (Classical Heuristic Search)
<span dir="rtl"></span>**<span dir="rtl">كما درس في الذكاء الاصطناعي هو
مثال على ذلك. أمثلة أخرى تشمل **خوارزميات الرول آوت**</span> **(Rollout
Algorithms)** <span dir="rtl">ومونت **كارلو شجرة البحث  
(**</span>**Monte Carlo Tree Search<span dir="rtl">)
</span>**<span dir="rtl">التي تستفيد من تقدير القيم عبر الإنترنت تدريجيًا
بناءً على العينات وتحسين السياسات</span>.

**<u>8.13 <span dir="rtl">ملخص الجزء الأول: الأبعاد</span> (Summary of
Part I: Dimensions)</u>**

<span dir="rtl">يختتم هذا الفصل الجزء الأول من هذا الكتاب. حاولنا في هذا
الجزء تقديم **التعليم المعزز** </span>**(Reinforcement Learning)
<span dir="rtl"></span>**<span dir="rtl">ليس كمجموعة من الأساليب
الفردية، ولكن كمجموعة مترابطة من الأفكار التي تتقاطع عبر الأساليب
المختلفة. يمكن النظر إلى كل فكرة كـ **بُعد**</span> **(Dimension)**
<span dir="rtl">تتنوع الأساليب وفقًا له. مجموعة هذه الأبعاد تمتد عبر
مساحة كبيرة من الأساليب المحتملة. من خلال استكشاف هذه المساحة على مستوى
الأبعاد، نأمل في الحصول على فهم أوسع وأكثر ديمومة</span>.

<span dir="rtl">في هذا القسم، نستخدم مفهوم الأبعاد في **مساحة
الأساليب**</span> **(Method Space)** <span dir="rtl">لإعادة تلخيص وجهة
النظر التي تم تطويرها حتى الآن في هذا الكتاب حول **التعليم
المعزز**</span> **<span dir="rtl">(</span>Reinforcement
Learning<span dir="rtl">)</span>**<span dir="rtl">.</span>

### <span dir="rtl">الأفكار الأساسية المشتركة بين جميع الأساليب، جميع الأساليب التي استكشفناها حتى الآن في هذا الكتاب تشترك في ثلاث أفكار رئيسية</span>:

1.  **<span dir="rtl">تقدير دوال القيمة</span> (Estimating Value
    Functions)<span dir="rtl">:</span>** <span dir="rtl">تسعى جميع
    الأساليب إلى تقدير دوال القيمة</span>.

2.  **<span dir="rtl">النسخ الاحتياطي لقيم التحديثات</span> (Backing Up
    Value Updates)<span dir="rtl">:</span>** <span dir="rtl">تعمل جميع
    الأساليب على نسخ القيم على طول المسارات الفعلية أو المحتملة
    للحالة</span>.

3.  **<span dir="rtl">استراتيجية التكرار العام للسياسة</span>
    (Generalized Policy Iteration - GPI)<span dir="rtl">:</span>**
    <span dir="rtl">تتبع جميع الأساليب استراتيجية التكرار العام للسياسة،
    مما يعني أنها تحتفظ بدالة قيمة تقريبية وسياسة تقريبية، وتحاول
    باستمرار تحسين كل منهما بناءً على الأخرى</span>.

### <span dir="rtl">الأبعاد الهامة في الأساليب</span>

<span dir="rtl">تعتبر **دوال القيمة** </span>**(Value
Functions)**<span dir="rtl">، و**النسخ الاحتياطي لقيم التحديثات**</span>
**<span dir="rtl">(</span>Backing Up <span dir="rtl"></span>Value
Updates<span dir="rtl">)</span>**<span dir="rtl">، و**التكرار العام
للسياسة**</span> **(GPI)** <span dir="rtl">مبادئ تنظيمية قوية قد تكون
ذات صلة بأي نموذج للذكاء، سواء كان صناعيًا أو طبيعيًا</span>.

<span dir="rtl">من بين الأبعاد الأكثر أهمية التي تتنوع وفقًا لها
الأساليب، يُظهر الشكل 8.11 اثنين من هذه الأبعاد</span>:

1.  **<span dir="rtl">نوع التحديث المستخدم لتحسين دالة القيمة</span>
    <span dir="rtl">(</span>Type of Update Used to Improve the Value
    Function<span dir="rtl">)</span>:**

    - **<span dir="rtl">البعد الأفقي</span> (Horizontal
      Dimension)<span dir="rtl">:</span>** <span dir="rtl">يشير إلى ما
      إذا كانت التحديثات تعتمد على العينات</span> (Sample Updates)
      <span dir="rtl">(بناءً على مسار عيني) أو على التوقعات</span>
      (Expected Updates) <span dir="rtl">(بناءً على توزيع المسارات
      المحتملة). تتطلب التحديثات المتوقعة وجود **نموذج توزيع**</span>
      **<span dir="rtl">(</span>Distribution
      Model<span dir="rtl">)</span>**<span dir="rtl">، بينما تحتاج
      التحديثات العينية فقط إلى **نموذج عيني**</span>
      **<span dir="rtl">(</span>Sample
      Model<span dir="rtl">)</span>**<span dir="rtl">، أو يمكن إجراؤها
      من تجربة فعلية بدون نموذج على الإطلاق (بعد آخر من التنوع)</span>.

2.  **<span dir="rtl">عمق التحديثات</span> (Depth of
    Updates)<span dir="rtl">:</span>**

    - **<span dir="rtl">البعد العمودي</span> (Vertical
      Dimension)<span dir="rtl">:</span>** <span dir="rtl">يعكس درجة
      التمهيد</span> (Bootstrapping)<span dir="rtl">. في ثلاث من الزوايا
      الأربع للمساحة توجد الطرق الرئيسية لتقدير القيم:</span>
      **<span dir="rtl">البرمجة الديناميكية</span> (Dynamic
      Programming)**<span dir="rtl">، **الفرق الزمني**
      </span>**(TD)**<span dir="rtl">، و**مونت كارلو** </span>**(Monte
      Carlo)**<span dir="rtl">.</span> <span dir="rtl">على طول الحافة
      اليسرى من المساحة توجد **أساليب التحديث العيني**</span>
      **<span dir="rtl">(</span>Sample-Update
      Methods<span dir="rtl">)</span>**<span dir="rtl">.</span>

<img src="./media/image96.png"
style="width:6.00885in;height:5.51714in" />

**<span dir="rtl">الشكل 8.11</span>**: <span dir="rtl">مقطع خلال مساحة
أساليب التعلم المعزز</span> (Reinforcement Learning
Methods)<span dir="rtl">، يسلّط الضوء على اثنين من أهم الأبعاد التي تم
استكشافها في الجزء الأول من هذا الكتاب:</span> <span dir="rtl">**عمق
التحديثات**</span> (Depth of Updates) <span dir="rtl">و**عرض
التحديثات**</span> (Width of Updates)<span dir="rtl">.</span>

<span dir="rtl">يمتد هذا البُعد من **تحديثات الفرق الزمني ذات خطوة
واحدة**</span> (one-step TD updates) <span dir="rtl">إلى **تحديثات مونت
كارلو ذات العائد الكامل**</span> (full-return Monte Carlo
updates)<span dir="rtl">.</span> <span dir="rtl">وبين هذين الطرفين يوجد
طيف من الأساليب التي تعتمد على **تحديثات متعددة الخطوات**</span> (n-step
updates)<span dir="rtl">، والتي سيتم توسيعها لاحقًا في الفصل 12 لتشمل
**مزيجًا من تحديثات**</span> **n-step** <span dir="rtl">مثل</span>
$`\lambda`$ <span dir="rtl">تحديثات</span> (λ-updates)
<span dir="rtl">التي تُنفّذ باستخدام **آثار الأهلية**</span> (Eligibility
Traces)<span dir="rtl">.</span>

<span dir="rtl">تُعرض أساليب **البرمجة الديناميكية**</span> (Dynamic
Programming) <span dir="rtl">في الزاوية العليا اليمنى من الشكل لأنها
تعتمد على **تحديثات متوقعة ذات خطوة واحدة**</span> (one-step expected
updates)<span dir="rtl">.</span> <span dir="rtl">أما الزاوية السفلى
اليمنى فتمثّل الحالة القصوى لتحديثات متوقعة **عميقة للغاية** تصل حتى
الحالات النهائية</span> (terminal states)<span dir="rtl">، أو، في المهام
المستمرة</span> (continuing tasks)<span dir="rtl">، إلى النقطة التي تجعل
فيها **عملية الخصم**</span> (Discounting) <span dir="rtl">مساهمة أي
مكافآت مستقبلية ضئيلة جدًا. وتُعرف هذه الحالة باسم **البحث الشامل**</span>
(Exhaustive Search)<span dir="rtl">.</span>

<span dir="rtl">تشمل الأساليب المتوسطة على هذا البُعد أساليب مثل **البحث
الإرشادي**</span> (Heuristic Search) <span dir="rtl">والأساليب ذات الصلة
التي تقوم بالبحث والتحديث حتى عمق محدود، وربما بشكل انتقائي</span>.

<span dir="rtl">هناك أيضًا أساليب تقع في وسط البعد الأفقي. وتشمل</span>:

- <span dir="rtl">الأساليب التي تدمج بين التحديثات المتوقعة
  والعينية</span> <span dir="rtl">(</span>Mix of Expected and Sample
  <span dir="rtl"></span>Updates<span dir="rtl">)</span>

- <span dir="rtl">وإمكانية وجود أساليب تدمج **العينات**</span> (Samples)
  <span dir="rtl">و**التوزيعات**</span> (Distributions)
  <span dir="rtl">داخل نفس التحديث</span>.

<span dir="rtl">وتمتلئ **داخل المربع**</span> (Interior of the Square)
<span dir="rtl">لتمثيل مساحة هذه الأساليب المتوسطة</span>.

### **<u><span dir="rtl">البعد الثالث المهم</span>:</u>**

<span dir="rtl">بجانب هذين البعدين، هناك بُعد ثالث تم التأكيد عليه في هذا
الكتاب، وهو **التمييز الثنائي بين الأساليب المعتمدة على السياسة**</span>
**(On-policy) <span dir="rtl">والأساليب غير المعتمدة على السياسة</span>
(Off-policy)**<span dir="rtl">:</span>

- <span dir="rtl">في الأساليب **المعتمدة على السياسة**
  </span>**(On-policy)**<span dir="rtl">:</span> <span dir="rtl">يتعلم
  الوكيل</span> (Agent) <span dir="rtl">دالة القيمة الخاصة بالسياسة التي
  يتبعها حاليًا</span>.

- <span dir="rtl">أما في الأساليب **غير المعتمدة على السياسة**
  </span>**(Off-policy)**<span dir="rtl">:</span> <span dir="rtl">يتعلم
  دالة القيمة لسياسة مختلفة، غالبًا السياسة التي يعتقد الوكيل أنها الأفضل
  حاليًا</span>.

<span dir="rtl">عادةً ما تكون السياسة التي تولّد السلوك مختلفة عن تلك التي
يُعتقد أنها الأفضل، وذلك بسبب الحاجة إلى **الاستكشاف**</span>
(**Exploration**)<span dir="rtl">. يمكن تخيل هذا البُعد الثالث على أنه
**عمودي على مستوى الصفحة** (</span>Perpendicular to the
<span dir="rtl"></span>Plane of the Page<span dir="rtl">) في الشكل
8.11</span>.

### <span dir="rtl">أبعاد أخرى تم ذكرها في الكتاب</span>:

1.  **<span dir="rtl">تعريف العائد</span>
    (Return)**<span dir="rtl">:</span> <span dir="rtl">هل المهمة
    **حلقية**</span> (Episodic) <span dir="rtl">أم **مستمرة**</span>
    (Continuing)<span dir="rtl">، وهل هي **مخصومة**</span> (Discounted)
    <span dir="rtl">أم غير مخصومة</span>
    (Undiscounted)<span dir="rtl">؟</span>

2.  **<span dir="rtl">قِيَم الأفعال مقابل قِيَم الحالات مقابل القيم بعد
    الحالة</span>**:

    - <span dir="rtl">هل يجب تقدير **قِيَم الأفعال**</span> (Action
      Values)<span dir="rtl">، أو **قِيَم الحالات**</span> (State
      Values)<span dir="rtl">، أو **قِيَم ما بعد الحالة**</span>
      (Afterstate Values)<span dir="rtl">؟</span>

    - <span dir="rtl">إذا تم تقدير قِيَم الحالات فقط، فستحتاج إلى نموذج أو
      سياسة منفصلة لاختيار الأفعال</span> <span dir="rtl">(كما في أساليب
      الممثل–النّاقد</span> Actor–Critic<span dir="rtl">)</span>.

3.  **<span dir="rtl">اختيار الأفعال / الاستكشاف</span> (Action
    Selection / Exploration)**:

    - <span dir="rtl">كيف يتم اختيار الأفعال لتحقيق توازن مناسب بين
      **الاستكشاف والاستغلال**</span> (Exploration vs.
      Exploitation)<span dir="rtl">؟</span>

    - <span dir="rtl">ناقشنا أبسط الطرق مثل</span>:

      - ε-greedy

      - <span dir="rtl">التهيئة المتفائلة للقيم</span> (Optimistic
        Initialization)

      - soft-max

      - <span dir="rtl">حدود الثقة العليا</span> (Upper Confidence
        Bound - UCB)

4.  **<span dir="rtl">تزامني مقابل غير تزامني</span> (Synchronous vs.
    Asynchronous)**<span dir="rtl">:</span>

    - <span dir="rtl">هل يتم تحديث جميع الحالات في وقت واحد؟ أم واحدة
      تلو الأخرى وفق ترتيب ما؟</span>

5.  **<span dir="rtl">تجربة حقيقية مقابل تجربة مُحاكاة</span> (Real vs.
    Simulated Experience)**<span dir="rtl">:</span>

    - <span dir="rtl">هل يجب أن يتم التحديث اعتمادًا على تجربة حقيقية، أم
      محاكاة؟ وإذا استخدم الاثنان، ما هي النسبة بينهما؟</span>

6.  **<span dir="rtl">مكان التحديثات</span> (Location of
    Updates)**<span dir="rtl">:</span>

    - <span dir="rtl">ما هي الحالات أو أزواج الحالة–الفعل التي يجب
      تحديثها؟</span>

    - <span dir="rtl">الأساليب **الخالية من النماذج**</span>
      **(Model-Free)** <span dir="rtl">يمكنها التحديث فقط لما تم المرور
      به فعليًا، بينما الأساليب **المعتمدة على النموذج**</span>
      **(Model-Based)** <span dir="rtl">يمكنها التحديث بشكل حر</span>.

7.  **<span dir="rtl">توقيت التحديثات</span> (Timing of
    Updates)**<span dir="rtl">:</span>

    - <span dir="rtl">هل يتم تنفيذ التحديثات أثناء اختيار الأفعال، أم
      بعد ذلك فقط؟</span>

8.  **<span dir="rtl">الذاكرة للتحديثات</span> (Memory for
    Updates)**<span dir="rtl">:</span>

    - <span dir="rtl">كم من الوقت يجب الاحتفاظ بالقيم المُحدثة؟</span>

    - <span dir="rtl">هل تُخزّن بشكل دائم، أم فقط أثناء اختيار الفعل (كما
      في البحث الإرشادي)؟</span>

<u><span dir="rtl">ملاحظات ختامية</span>:</u>

- <span dir="rtl">هذه الأبعاد ليست شاملة ولا متعارضة فيما بينها</span>.

- <span dir="rtl">تختلف الخوارزميات في جوانب متعددة، وبعضها يقع في مواضع
  مختلفة على عدة أبعاد</span>.

- <span dir="rtl">مثال: خوارزميات</span> **Dyna** <span dir="rtl">تستخدم
  التجربة الحقيقية والمحاكاة معًا لتحديث نفس دالة القيمة</span>.

- <span dir="rtl">من المنطقي أيضًا الاحتفاظ بعدة دوال قيمة</span> (Value
  Functions) <span dir="rtl">تم حسابها بطرق مختلفة أو لتمثيلات مختلفة
  للحالات والأفعال</span>.

<span dir="rtl">هذه الأبعاد تشكل **مجموعة مترابطة من الأفكار** تساعد في
وصف واستكشاف مجموعة واسعة من الأساليب الممكنة</span>.

<u><span dir="rtl">ملاحظة أخيرة مهمة</span>:</u>

<span dir="rtl">أهم بُعد لم يتم التطرق له هنا، ولا في الجزء الأول من هذا
الكتاب، هو **تقريب الدالة**</span> **<span dir="rtl">(</span>Function
<span dir="rtl"></span>Approximation<span dir="rtl">)</span>**.  
<span dir="rtl">ويمكن النظر إليه كبعد مستقل يتراوح بين</span>:

- **<span dir="rtl">الطرق الجدولية</span> (Tabular Methods)**

- **<span dir="rtl">تجميع الحالات</span> (State Aggregation)**

- **<span dir="rtl">طرق خطية متنوعة</span> (Linear Methods)**

- <span dir="rtl">وأخيرًا **مجموعة من الطرق غير الخطية**
  </span>**(Nonlinear Methods)**<span dir="rtl">، مثل الشبكات
  العصبية</span>.

<span dir="rtl">وسيتم استكشاف هذا البُعد في الجزء الثاني من
الكتاب</span>.

<span dir="rtl">الجزء الثاني: طرق الحل التقريبية</span>
<span dir="rtl">(</span>Approximate <span dir="rtl"></span>Solution
Methods<span dir="rtl">)</span>

<span dir="rtl">في الجزء الثاني من الكتاب، نقوم بتوسيع **الطرق
الجدولية**</span> **(Tabular Methods)** <span dir="rtl">التي تم تقديمها
في الجزء الأول **لتطبيقها** على **مشكلات ذات فضاءات حالات كبيرة بشكل
عشوائي**. في العديد من المهام التي نود تطبيق **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">عليها، يكون **فضاء
الحالة**</span> **<span dir="rtl">(</span>State
<span dir="rtl"></span>Space<span dir="rtl">)</span>**
<span dir="rtl">مركبًا وضخمًا للغاية؛ عدد الصور المحتملة للكاميرا، على
سبيل المثال، أكبر بكثير من عدد الذرات في الكون. في مثل هذه الحالات، لا
يمكننا أن نتوقع إيجاد **سياسة**</span> **(Policy)** <span dir="rtl">مثلى
أو  
**دالة قيمة** </span>**(Value Function)
<span dir="rtl"></span>**<span dir="rtl">مثلى حتى مع توفر وقت وبيانات
غير محدودين؛ هدفنا بدلاً من ذلك هو إيجاد حل تقريبي جيد باستخدام موارد
حسابية محدودة. في هذا الجزء من الكتاب نستكشف مثل هذه الطرق التقريبية
للحلول</span>.

<span dir="rtl">**المشكلة في فضاءات الحالات الكبيرة** ليست فقط **الذاكرة
المطلوبة للجداول الكبيرة**، ولكن أيضًا **الوقت والبيانات اللازمة لملئها
بدقة**. في العديد من المهام المستهدفة، تقريبًا كل **حالة**</span>
**(State)** <span dir="rtl">يتم مواجهتها ستكون غير مألوفة ولم يسبق
رؤيتها من قبل. لاتخاذ قرارات منطقية في مثل هذه الحالات، من الضروري
التعميم من التجارب السابقة مع حالات مختلفة تشبه إلى حد ما الحالة
الحالية. بمعنى آخر، القضية الرئيسية هنا هي **التعميم**
</span>**(Generalization)**<span dir="rtl">.</span> <span dir="rtl">كيف
يمكننا تعميم التجارب مع مجموعة محدودة من **فضاء الحالة**</span> **(State
Space)** <span dir="rtl">بشكل يفيد في إنشاء تقريب جيد على مجموعة أكبر
بكثير؟</span>

<span dir="rtl">لحسن الحظ، تم دراسة **التعميم**</span>
**(Generalization)** <span dir="rtl">من الأمثلة بشكل مكثف مسبقًا، ولا
نحتاج إلى اختراع طرق جديدة تمامًا لاستخدامها في **التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">.</span>
<span dir="rtl">إلى حد ما، كل ما نحتاج إليه هو دمج طرق **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">مع طرق
التعميم القائمة. النوع من التعميم الذي نحتاجه يسمى غالبًا **تقريب
الدالة**</span> **<span dir="rtl">(</span>Function
<span dir="rtl"></span>Approximation<span dir="rtl">)</span>**
<span dir="rtl">لأنه يأخذ أمثلة من دالة مطلوبة مثل **دالة
القيمة**</span> **(Value Function)** <span dir="rtl">ويحاول التعميم منها
لإنشاء تقريب لكامل الدالة</span>. **<span dir="rtl">تقريب الدالة</span>
(Function Approximation)** <span dir="rtl">هو مثال على **التعليم
الموجه** </span>**(Supervised Learning)**<span dir="rtl">، الموضوع
الأساسي الذي يُدرس في **تعلم الآلة**</span> **(Machine
Learning)**<span dir="rtl">، **الشبكات العصبية الاصطناعية**
</span>**(Artificial Neural Networks)**<span dir="rtl">، **التعرف على
الأنماط** </span>**(Pattern Recognition)**<span dir="rtl">، و**التوفيق
الإحصائي للمنحنيات**</span> **<span dir="rtl">(</span>Statistical
<span dir="rtl"></span>Curve Fitting<span dir="rtl">).</span>**
<span dir="rtl">من الناحية النظرية، يمكن استخدام أي من الطرق المدروسة في
هذه المجالات في دور **مقرب الدالة**</span> **(Function Approximator)**
<span dir="rtl">ضمن خوارزميات **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">، على الرغم من أن بعضها في الممارسة يتناسب مع
هذا الدور أكثر من غيره</span>.

<span dir="rtl">**التعليم** **المعزز**</span> (**Reinforcement**
**Learning**) <span dir="rtl">مع **تقريب الدالة**</span> **(Function
Approximation)** <span dir="rtl">ينطوي على عدد من القضايا الجديدة التي
لا تنشأ عادةً في **التعليم الخاضع للإشراف**</span>
**<span dir="rtl">(</span>Supervised
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">التقليدي، مثل عدم الاستقرار، **التمهيد**
</span>**(Bootstrapping)**<span dir="rtl">، والأهداف المؤجلة. نقدم هذه
القضايا وغيرها تدريجيًا على مدار الفصول الخمسة من هذا الجزء. في البداية
نقتصر على التدريب **داخل السياسة** </span>**(On-policy
Training)**<span dir="rtl">، حيث نتناول في الفصل التاسع حالة التنبؤ، حيث
تكون السياسة معطاة ويتم تقريب **دالة القيمة**</span> **(Value
Function)** <span dir="rtl">فقط، ثم في الفصل العاشر حالة التحكم، حيث يتم
إيجاد تقريب للسياسة المثلى. المشكلة الصعبة للتعليم **خارج
السياسة**</span> **<span dir="rtl">(</span>Off-policy
Learning**<span dir="rtl">) مع **تقريب الدالة**</span> **(Function
Approximation)** <span dir="rtl">تُناقش في الفصل الحادي عشر. في كل من هذه
الفصول الثلاثة، سنحتاج إلى العودة إلى المبادئ الأساسية وإعادة فحص أهداف
التعليم لأخذ **تقريب الدالة**</span> **(Function Approximation)**
<span dir="rtl">في الاعتبار. يقدم الفصل الثاني عشر ويحلل الآلية
الخوارزمية **لآثار الاستحقاق** </span>**(Eligibility
Traces)**<span dir="rtl">، التي تحسن بشكل كبير الخصائص الحسابية لطرق
التعليم المعزز متعددة الخطوات في العديد من الحالات. الفصل الأخير من هذا
الجزء يستكشف نهجًا مختلفًا للتحكم، وهو **طرق تدرج السياسة**
</span>**(Policy-gradient Methods)**<span dir="rtl">، التي تقرب السياسة
المثلى مباشرةً ولا تحتاج أبدًا إلى تشكيل تقريب **لدالة القيمة**</span>
**(Value Function)** (<span dir="rtl">على الرغم من أنها قد تكون أكثر
فعالية بكثير إذا قامت بتقريب **لدالة القيمة**</span> **(Value
Function)** <span dir="rtl">أيضًا بجانب السياسة.</span>

<span dir="rtl">الفصل التاسع:</span>

<span dir="rtl">التنبؤ داخل السياسة</span> (On-policy Prediction)
<span dir="rtl">مع التقريب</span> (Approximation)

<span dir="rtl">في هذا الفصل، نبدأ دراستنا لتقريب الدوال</span>
(**Function** **Approximation**) <span dir="rtl">في **التعليم المعزز**
</span>**(Reinforcement Learning)** <span dir="rtl">من خلال النظر في
استخدامه لتقدير **دالة قيمة الحالة  
(**</span>**State-Value Function<span dir="rtl">)</span>**
<span dir="rtl">باستخدام بيانات **داخل السياسة** </span>**(On-policy
Data)**<span dir="rtl">، أي في تقريب</span> vπ <span dir="rtl">من
التجارب التي تم توليدها باستخدام **سياسة**</span> **(Policy)**
<span dir="rtl">معروفة</span> $`\pi`$<span dir="rtl">.</span>
<span dir="rtl">الجديد في هذا الفصل هو أن **دالة القيمة
التقريبية**</span> **(Approximate Value Function)** <span dir="rtl">لا
تُمثل كجدول، بل كصيغة دالية مع **متجه أوزان** </span>**(Weight Vector)**
$`w \in Rd`$<span dir="rtl">.</span> <span dir="rtl">سنكتب</span>
$`v\hat{}(s,w) \approx v\pi(s)`$
<span dir="rtl"></span>​<span dir="rtl">للإشارة إلى **القيمة التقريبية**
</span>**(Approximate Value)** <span dir="rtl">للحالة</span> $`s`$
<span dir="rtl">معطاة بمتجه الأوزان</span> $`w`$<span dir="rtl">.</span>
<span dir="rtl">على سبيل المثال، قد تكون</span> $`v\hat{}`$
<span dir="rtl">دالة خطية في خصائص الحالة، مع كون</span> $`w`$
<span dir="rtl">هو **متجه أوزان الخصائص  **
</span>**(Feature Weights)**<span dir="rtl">.</span>
<span dir="rtl">بشكل أكثر عمومية، قد تكون</span> $`v\hat{}`$
<span dir="rtl">الدالة المحسوبة بواسطة **شبكة عصبية اصطناعية متعددة
الطبقات** </span>**(Multi-Layer Artificial Neural
Network)**<span dir="rtl">، مع كون</span> $`w`$ <span dir="rtl">هو متجه
أوزان الروابط في جميع الطبقات. من خلال تعديل الأوزان، يمكن تنفيذ مجموعة
واسعة من الدوال المختلفة بواسطة الشبكة. أو قد تكون</span> $`v\hat{}`$
<span dir="rtl">الدالة المحسوبة بواسطة **شجرة قرار** </span>**(Decision
Tree)**<span dir="rtl">، حيث يمثل</span> $`w`$ <span dir="rtl">جميع
الأرقام التي تحدد نقاط الانقسام وقيم الأوراق في الشجرة. عادةً، يكون عدد
الأوزان (البُعدية</span> d<span dir="rtl">) أقل بكثير من عدد
الحالات</span> ∣S∣<span dir="rtl">، وتغيير وزن واحد يؤدي إلى تغيير
القيمة المقدرة للعديد من الحالات. وبالتالي، عندما يتم تحديث حالة واحدة،
فإن التغيير يعمم من تلك الحالة ليؤثر على قيم العديد من الحالات الأخرى.
هذا النوع من التعميم يجعل التعليم أكثر قوة بشكل محتمل ولكنه أيضًا أكثر
صعوبة في الإدارة والفهم</span>.

<span dir="rtl">ربما من المدهش أن توسيع التعليم المعزز ليشمل **تقريب
الدالة** </span>**(Function Approximation)** <span dir="rtl">يجعله قابلًا
للتطبيق أيضًا على المشكلات التي تتم ملاحظتها جزئيًا، حيث لا تكون الحالة
الكاملة متاحة للعميل. إذا كانت الصيغة الدالية البارامترية</span>
$`v\hat{}`$ <span dir="rtl">لا تسمح للقيمة المقدرة بالاعتماد على جوانب
معينة من الحالة، فإنه من الممكن اعتبار هذه الجوانب غير قابلة للملاحظة.
في الواقع، جميع النتائج النظرية للطرق التي تستخدم **تقريب
الدالة**</span> **(Function Approximation)** <span dir="rtl">المقدمة في
هذا الجزء من الكتاب تنطبق بشكل جيد على حالات الملاحظة الجزئية</span>.

<span dir="rtl">ما لا يمكن لـ **تقريب الدالة**</span> **(Function
Approximation)** <span dir="rtl">فعله، مع ذلك، هو تعزيز تمثيل الحالة
بذكريات من الملاحظات السابقة. بعض هذه الامتدادات الممكنة نناقشها بإيجاز
في **القسم** 17.3</span>.

**<u>9.1 <span dir="rtl">تقريب دالة القيمة</span> (Value-function
Approximation)</u>**

<span dir="rtl">جميع طرق التنبؤ التي تناولها هذا الكتاب وُصفت بأنها
تحديثات لدالة قيمة مقدرة، تقوم بتعديل قيمتها في حالات معينة نحو "قيمة
مدعومة"، أو هدف التحديث لتلك الحالة. دعونا نشير إلى تحديث معين
بالرمز</span> s→u<span dir="rtl">، حيث</span> s <span dir="rtl">هي
الحالة التي يتم تحديثها و</span>u <span dir="rtl">هو هدف التحديث الذي
تُعدَّل القيمة المقدرة لـ</span> s <span dir="rtl">نحوه. على سبيل المثال،
تحديث مونت كارلو للتنبؤ بالقيمة هو</span> $`St \rightarrow Gt`$
<span dir="rtl"></span>​<span dir="rtl">، وتحديث</span> TD(0)
<span dir="rtl">هو</span>
$`St \rightarrow R_{t + 1} + \gamma v\hat{}(S_{t + 1},wt)`$<span dir="rtl">،
وتحديث</span> TD <span dir="rtl">ذو الخطوات المتعددة</span> $`n`$
<span dir="rtl"></span> <span dir="rtl">هو  
</span>$`\ St \rightarrow Gt:t + nS\_ t`$​<span dir="rtl">في تحديث تقييم
السياسة باستخدام البرمجة الديناميكية (</span>Dynamic
<span dir="rtl"></span>Programming<span dir="rtl">)،</span>
s→Eπ\[Rt+1+γv^($`S_{t + 1}`$,$`wt) \mid St = s`$<span dir="rtl">، يتم
تحديث حالة عشوائية</span> s<span dir="rtl">، في حين أنه في الحالات
الأخرى يتم تحديث الحالة التي واجهتها في التجربة الفعلية</span>
$`St`$<span dir="rtl">.</span>

<span dir="rtl">من الطبيعي أن يتم تفسير كل تحديث على أنه يحدد مثالاً
للسلوك المطلوب لدالة القيمة. بمعنى ما، التحديث</span>
$`s \rightarrow u`$ <span dir="rtl">يعني أن القيمة المقدرة للحالة</span>
$`s`$ <span dir="rtl">يجب أن تكون أكثر شبهًا بهدف التحديث</span>
$`u`$<span dir="rtl">.</span> <span dir="rtl">حتى الآن، كان التحديث
الفعلي بسيطًا: يتم فقط تعديل قيمة الجدول المقدرة للحالة</span> $`s`$
<span dir="rtl">بجزء من الطريق نحو</span> u<span dir="rtl">، وتُترك القيم
المقدرة لجميع الحالات الأخرى دون تغيير. الآن نسمح بطرق معقدة ومتطورة
لتنفيذ التحديث، وتعميم التحديث عند</span> $`s`$ <span dir="rtl">بحيث
تتغير القيم المقدرة للعديد من الحالات الأخرى أيضًا. تُسمى طرق تعلم الآلة
التي تتعلم محاكاة أمثلة المدخلات والمخرجات بهذه الطريقة بـ "طرق التعليم
الموجه</span> (Supervised Learning Methods)"<span dir="rtl">، وعندما
تكون المخرجات أرقامًا مثل</span> $`u`$<span dir="rtl">، يُطلق على هذه
العملية غالبًا "تقريب الدالة</span> (Function
Approximation)"<span dir="rtl">.</span> <span dir="rtl">تتوقع طرق تقريب
الدالة تلقي أمثلة للسلوك المطلوب للمدخلات والمخرجات للدالة التي تحاول
تقريبها. نستخدم هذه الطرق للتنبؤ بالقيمة ببساطة عن طريق تمرير</span>
$`s \rightarrow gs`$ <span dir="rtl">لكل تحديث كمثال تدريبي. ثم نقوم
بتفسير الدالة التقريبية التي تنتجها هذه الطرق على أنها دالة قيمة
مقدرة</span>.

<span dir="rtl">إن اعتبار كل تحديث كمثال تدريبي تقليدي بهذه الطريقة يتيح
لنا استخدام أي من مجموعة واسعة من طرق تقريب الدوال الحالية للتنبؤ
بالقيمة. من حيث المبدأ، يمكننا استخدام أي طريقة للتعليم الموجه من
الأمثلة، بما في ذلك الشبكات العصبية الاصطناعية</span> (Artificial Neural
Networks)<span dir="rtl">، الأشجار القرارية</span> (Decision
Trees)<span dir="rtl">، وأنواع مختلفة من الانحدار المتعدد
المتغيرات</span> <span dir="rtl">(</span>Multivariate
<span dir="rtl"></span>Regression<span dir="rtl">)</span>.
<span dir="rtl">ومع ذلك، ليست جميع طرق تقريب الدوال مناسبة بنفس القدر
للاستخدام في التعليم المعزز</span> (Reinforcement
Learning)<span dir="rtl">.</span> <span dir="rtl">تفترض معظم الطرق
المتطورة مثل الشبكات العصبية الاصطناعية والطرق الإحصائية أن مجموعة
التدريب ثابتة ويتم المرور عليها عدة مرات. في التعليم المعزز، من المهم أن
يتم التعليم عبر الإنترنت أثناء تفاعل العميل مع بيئته أو مع نموذج من
بيئته. لتحقيق ذلك، يتطلب الأمر طرقًا قادرة على التعليم بكفاءة من البيانات
المكتسبة بشكل متزايد. بالإضافة إلى ذلك، يتطلب التعليم المعزز بشكل عام
طرقًا لتقريب الدوال قادرة على التعامل مع دوال الهدف غير الثابتة</span>
(Nonstationary Target Functions) <span dir="rtl">التي تتغير بمرور الوقت.
على سبيل المثال، في طرق التحكم المستندة إلى التكرار السياسي
المعمم</span> (Generalized Policy Iteration - GPI)<span dir="rtl">، نسعى
غالبًا لتعلم</span> $`q_{\pi}`$ <span dir="rtl"></span>​
<span dir="rtl">بينما تتغير السياسة</span>
$`\pi`$<span dir="rtl">.</span> <span dir="rtl">حتى إذا ظلت السياسة
ثابتة، فإن قيم الهدف لأمثلة التدريب تكون غير ثابتة إذا تم توليدها بواسطة
طرق التمهيد</span> (Bootstrapping Methods) <span dir="rtl">مثل البرمجة
الديناميكية</span> (DP) <span dir="rtl">و</span>TD
<span dir="rtl">التعليم. الطرق التي لا يمكنها التعامل بسهولة مع مثل هذا
عدم الثبات تكون أقل ملاءمة للتعليم المعزز</span>.

**<u>9.2 <span dir="rtl">هدف التنبؤ</span> (Prediction Objective) -
<span dir="rtl">خطأ القيمة</span> (VE: Value Error)</u>**

<span dir="rtl">حتى الآن، لم نحدد هدفًا صريحًا للتنبؤ. في الحالة الجدولية،
لم يكن من الضروري استخدام مقياس مستمر لجودة التنبؤ لأن دالة القيمة
المتعلمة يمكن أن تتساوى مع دالة القيمة الحقيقية بالضبط. علاوة على ذلك،
كانت القيم المتعلمة في كل حالة مستقلة - أي أن التحديث في حالة معينة لا
يؤثر على الحالات الأخرى. ولكن مع التقريب الحقيقي، يؤثر التحديث في حالة
واحدة على العديد من الحالات الأخرى، وليس من الممكن تحقيق القيم الصحيحة
لجميع الحالات بدقة</span>.

<span dir="rtl">وفقًا للفرضية، لدينا عدد كبير جدًا من الحالات مقارنة بعدد
الأوزان، لذا فإن جعل تقدير حالة واحدة أكثر دقة يعني بشكل لا مفر منه جعل
تقديرات الحالات الأخرى أقل دقة. لذلك نحن ملزمون بتحديد أي الحالات نهتم
بها أكثر. يجب علينا تحديد توزيع للحالات</span>
$`\mu(s) \geq 0`$<span dir="rtl">، بحيث يكون مجموع التوزيعات مساويًا
للواحد</span> $`\sum s\mu(s) = 1`$<span dir="rtl">، لتمثيل مدى اهتمامنا
بالخطأ في كل حالة</span> $`s`$<span dir="rtl">.</span>

<span dir="rtl">نعني بالخطأ في حالة معينة</span> $`s`$
<span dir="rtl">مربع الفرق بين القيمة التقريبية</span> $`v\hat{}(s,w)`$
<span dir="rtl">والقيمة الحقيقية</span>
$`v\pi(s)`$<span dir="rtl">.</span> <span dir="rtl">من خلال وزن هذا
الخطأ عبر فضاء الحالة باستخدام</span> $`\mu`$<span dir="rtl">، نحصل على
دالة هدف طبيعية، وهي **متوسط خطأ القيمة المربعة** </span>**(Mean Squared
Value Error)**<span dir="rtl">، والمشار إليها بالرمز</span>
**VE**<span dir="rtl">:</span>

``` math
\text{VE}(w) = \sum_{s \in S}^{}{\mu(s)\left\lbrack v_{\pi}(s) - \widehat{v}(s,w) \right\rbrack^{2}}
```

<span dir="rtl">الجذر التربيعي لهذا المقياس، **الجذر** </span>**VE (Root
VE)**<span dir="rtl">، يوفر مقياسًا تقريبيًا لمدى اختلاف القيم التقريبية
عن القيم الحقيقية، وغالبًا ما يُستخدم في الرسوم البيانية. غالبًا ما يتم
اختيار</span> $`\mu(s)`$ <span dir="rtl">ليكون النسبة المئوية للوقت الذي
يقضيه النظام في الحالة</span> $`s`$<span dir="rtl">.</span>
<span dir="rtl">في التدريب **داخل السياسة**</span>
**<span dir="rtl">(</span>On-policy
<span dir="rtl"></span>Training<span dir="rtl">)</span>**<span dir="rtl">،
يُسمى هذا التوزيع بـ "توزيع داخل السياسة</span> (On-policy
Distribution)"<span dir="rtl">، ونركز بالكامل على هذه الحالة في هذا
الفصل. في المهام المستمرة، يكون توزيع **داخل السياسة**</span>
**<span dir="rtl">(</span>On-policy
<span dir="rtl"></span>Distribution<span dir="rtl">)
</span>**<span dir="rtl">هو التوزيع المستقر وفقًا للسياسة</span>
$`\pi`$<span dir="rtl">.</span>

<span dir="rtl">توزيع داخل السياسة</span> (On-policy Distribution)
<span dir="rtl">في المهام الحلقية</span> (Episodic Tasks)

<span dir="rtl">في المهام الحلقية</span> (Episodic
Tasks)<span dir="rtl">، يكون **توزيع داخل السياسة** </span>**(On-policy
Distribution)** <span dir="rtl">مختلفًا بعض الشيء، حيث يعتمد على كيفية
اختيار الحالات الابتدائية للحلقات. دعنا نرمز إلى احتمال أن تبدأ الحلقة
في حالة معينة</span> $`s`$ <span dir="rtl">بالرمز</span>
$`h(s)`$<span dir="rtl">، ولنرمز إلى عدد خطوات الوقت التي تُقضى في
المتوسط في الحالة</span> $`s`$ <span dir="rtl">خلال حلقة واحدة
بالرمز</span> η(s)<span dir="rtl">.</span> <span dir="rtl">يتم قضاء
الوقت في حالة</span> $`s`$ <span dir="rtl">إذا بدأت الحلقات في</span>
$`s`$<span dir="rtl">، أو إذا تم الانتقال إلى</span> $`s`$
<span dir="rtl">من حالة سابقة</span> $`sˉ`$ <span dir="rtl"></span>ˉ
<span dir="rtl">تم قضاء الوقت فيها</span>:

``` math
\eta(s) = h(s) + \sum_{\overline{s}}^{}{\eta\left( \overline{s} \right)\sum_{a}^{}{\pi\left( a \middle| \overline{s} \right)p\left( s \middle| \overline{s},a \right)}},\text{ for all }s \in S.
```

<span dir="rtl">يمكن حل نظام المعادلات هذا لإيجاد عدد الزيارات
المتوقع</span> $`\eta(s)`$<span dir="rtl">.</span> <span dir="rtl">يكون
توزيع  
**داخل السياسة**</span> **(On-policy Distribution)** <span dir="rtl">بعد
ذلك هو النسبة المئوية للوقت الذي يُقضى في كل حالة، مُعدَّلة ليصبح مجموعها
واحدًا</span>.

``` math
\mu(s) = \frac{\eta(s)}{\sum_{s' \in S}^{}{\eta\left( s' \right)}},\text{ for all }s \in S.
```

<span dir="rtl">هذا هو الخيار الطبيعي بدون خصم. إذا كان هناك خصم</span>
$`(\gamma < 1)`$<span dir="rtl">، فيجب معاملته كشكل من أشكال الإنهاء،
ويمكن القيام بذلك ببساطة عن طريق تضمين عامل</span> $`\gamma`$
<span dir="rtl">في المصطلح الثاني من المعادلة</span>
(9.2)<span dir="rtl">.</span>

<span dir="rtl">الحالتان، المستمرة والحلقية، تتصرفان بشكل مشابه، لكن مع
وجود التقريب يجب التعامل معهما بشكل منفصل في التحليلات الرسمية، كما سنرى
بشكل متكرر في هذا الجزء من الكتاب. هذا يكمل تحديد الهدف التعليمي</span>.

<span dir="rtl">ولكن لا يزال من غير الواضح تمامًا أن **خطأ
القيمة**</span> **(VE: Value Error)** <span dir="rtl">هو الهدف الصحيح
للأداء في التعليم المعزز</span> (Reinforcement
Learning)<span dir="rtl">.</span> <span dir="rtl">تذكر أن هدفنا
النهائي - السبب الذي نتعلم من أجله دالة القيمة - هو العثور على سياسة
أفضل. أفضل دالة قيمة لهذا الغرض ليست بالضرورة الأفضل في تقليل **خطأ
القيمة** </span>**(VE)**<span dir="rtl">.</span> <span dir="rtl">ومع
ذلك، ليس من الواضح بعد ما قد يكون هدفًا بديلاً أكثر فائدة لتنبؤ القيمة. في
الوقت الحالي، سنركز على **خطأ القيمة**
</span>**(VE)**<span dir="rtl">.</span>

<span dir="rtl">الهدف المثالي من حيث **خطأ القيمة**</span> **(VE)**
<span dir="rtl">هو العثور على الحد الأمثل العالمي، وهو متجه
الأوزان</span> $`w*`$ <span dir="rtl">الذي يحقق</span>
$`VE(w*)\  \leq VE(w)`$ <span dir="rtl">لجميع الأوزان الممكنة</span>
$`w`$<span dir="rtl">.</span> <span dir="rtl">الوصول إلى هذا الهدف قد
يكون ممكنًا في بعض الأحيان مع **مقربات الدوال الخطية** </span>**(Linear
Function Approximators)** <span dir="rtl">البسيطة، لكنه نادرًا ما يكون
ممكنًا مع **مقربات الدوال المعقدة**</span>
**<span dir="rtl">(</span>Complex Function
<span dir="rtl"></span>Approximators<span dir="rtl">)
</span>**<span dir="rtl">مثل الشبكات العصبية الاصطناعية والأشجار
القرارية. إذا لم يكن هذا ممكنًا، فقد تسعى **مقربات الدوال
المعقدة**</span> **(Complex Function Approximators)**
<span dir="rtl">بدلاً من ذلك إلى التقارب إلى الحد الأمثل المحلي، وهو متجه
الأوزان</span> $`w*`$ <span dir="rtl">الذي يحقق</span>
$`VE(w*)\  \leq V`$ <span dir="rtl">لجميع الأوزان</span> $`w`$
<span dir="rtl">في جوار معين من</span> $`w*`$<span dir="rtl">.</span>

<span dir="rtl">على الرغم من أن هذا الضمان مطمئن بشكل طفيف فقط، إلا أنه
عادة ما يكون الأفضل الذي يمكن قوله لمقربات الدوال غير الخطية، وغالبًا ما
يكون كافيًا. ومع ذلك، في العديد من الحالات ذات الأهمية في التعليم المعزز،
لا يوجد ضمان على التقارب إلى الحد الأمثل، أو حتى إلى مسافة  
محدودة من الحد الأمثل. قد تتباعد بعض الطرق في الواقع، مما يجعل **خطأ
القيمة**</span> **(VE)** <span dir="rtl">يقترب من اللانهاية في
الحد</span>.

<span dir="rtl">في القسمين الأخيرين، قدمنا إطار عمل يجمع بين مجموعة
واسعة من طرق التعليم المعزز لتنبؤ القيمة مع مجموعة واسعة من طرق تقريب
الدوال، باستخدام التحديثات من الأولى لتوليد أمثلة تدريبية للأخيرة. كما
وصفنا مقياس الأداء **خطأ القيمة**</span> **(VE)** <span dir="rtl">الذي
قد تسعى هذه الطرق لتقليله. نطاق طرق تقريب الدوال المحتملة واسع للغاية
بحيث لا يمكن تغطيته بالكامل، وأيضًا القليل معروف عن معظمها بحيث يمكن
إجراء تقييم موثوق أو توصية. من الضروري أن نعتبر فقط بعض الاحتمالات. في
بقية هذا الفصل، نركز على طرق تقريب الدوال المستندة إلى مبادئ التدرج،
وعلى طرق الانحدار التدرجي الخطي بشكل خاص. نركز على هذه الطرق جزئيًا لأننا
نعتبرها واعدة بشكل خاص ولأنها تكشف عن قضايا نظرية رئيسية، ولكن أيضًا
لأنها بسيطة ومساحتنا محدودة</span>.

**<u>9.3 <span dir="rtl">طرق التدرج العشوائي</span>
(Stochastic-gradient) <span dir="rtl">ونصف التدرج</span>
(Semi-gradient)</u>**

<span dir="rtl">نقوم الآن بتطوير فئة من طرق التعليم لتقريب الدوال في
تنبؤ القيمة، وهي الطرق القائمة على **الانحدار التدرجي العشوائي**
</span>**(Stochastic Gradient Descent - SGD)**<span dir="rtl">.</span>
<span dir="rtl">تُعد طرق الانحدار التدرجي العشوائي من بين أكثر طرق تقريب
الدوال استخدامًا على نطاق واسع، وهي مناسبة بشكل خاص للتعليم المعزز عبر
الإنترنت</span>.

<span dir="rtl">في طرق الانحدار التدرجي، يكون متجه الأوزان عمودًا يحتوي
على عدد ثابت من المكونات ذات القيم الحقيقية، ويُرمز إليه بـ</span>
$`w = (w1,w2,\ldots,wd)\top`$ <span dir="rtl">، حيث تكون **دالة القيمة
التقريبية** </span>**(Approximate Value Function)**
<span dir="rtl"></span>$`v\hat{}(s,w)`$ <span dir="rtl"></span>
<span dir="rtl">دالة قابلة للتفاضل بالنسبة إلى</span> $`w`$
<span dir="rtl">لجميع الحالات</span> $`s \in S`$
<span dir="rtl"></span>. <span dir="rtl">سنقوم بتحديث</span> w
<span dir="rtl">في كل خطوة زمنية منفصلة،</span>
$`t = 0,1,2,3,\ldots`$<span dir="rtl">، لذا سنحتاج إلى تدوين</span>
$`wt`$ <span dir="rtl">للإشارة إلى متجه الأوزان في كل خطوة</span>.

<span dir="rtl">في الوقت الحالي، لنفترض أنه في كل خطوة نلاحظ مثالًا
جديدًا</span> $`St \rightarrow v\pi(St)`$ <span dir="rtl">يتكون من حالة
معينة</span> $`St`$ <span dir="rtl">وقيمتها الحقيقية تحت السياسة. قد
تكون هذه الحالات هي الحالات المتتالية الناتجة عن التفاعل مع البيئة،
ولكننا لا نفترض ذلك الآن. حتى مع إعطائنا القيم الدقيقة والصحيحة</span>
$`\ v\pi(St)`$<span dir="rtl">لكل</span> $`St`$ ​<span dir="rtl">، لا
يزال هناك تحدٍ صعب لأن مقرب الدوال لدينا يمتلك موارد محدودة وبالتالي دقة
محدودة. على وجه الخصوص، لا يوجد عادةً متجه أوزان</span> $`w`$
<span dir="rtl">يمكنه تحقيق الدقة التامة لجميع الحالات أو حتى لجميع
الأمثلة</span>.

<span dir="rtl">بالإضافة إلى ذلك، يجب علينا التعميم إلى جميع الحالات
الأخرى التي لم تظهر في الأمثلة. نفترض أن الحالات تظهر في الأمثلة وفقًا
لنفس التوزيع</span> $`\mu`$ <span dir="rtl">الذي نحاول تقليل **خطأ
القيمة**</span> **(VE)** <span dir="rtl">بناءً عليه كما هو موضح في
المعادلة (9.1). تعتبر استراتيجية جيدة في هذه الحالة محاولة تقليل الخطأ
على الأمثلة الملاحظة. تقوم طرق **الانحدار التدرجي العشوائي**</span>
**(SGD)** <span dir="rtl">بذلك عن طريق تعديل متجه الأوزان بعد كل مثال
بمقدار صغير في الاتجاه الذي يقلل الخطأ في ذلك المثال</span>:

``` math
w_{t + 1} = w_{t} - \frac{1}{2}\alpha\nabla\left\lbrack v_{\pi}\left( S_{t} \right) - \widehat{v}\left( S_{t},w_{t} \right) \right\rbrack^{2}
```

``` math
w_{t + 1} = w_{t} + \alpha\left\lbrack v_{\pi}\left( S_{t} \right) - \widehat{v}\left( S_{t},w_{t} \right) \right\rbrack\nabla\widehat{v}\left( S_{t},w_{t} \right)
```

<span dir="rtl">حيث</span> $`\alpha`$ <span dir="rtl">هو **معامل حجم
الخطوة**</span> **(Step-size Parameter)** <span dir="rtl">موجب،
و</span>$`\nabla f(w)`$<span dir="rtl">، لأي تعبير قياسي</span> $`f(w)`$
<span dir="rtl">يعتمد على **متجه**</span> **(Vector)**
<span dir="rtl">(هنا</span> $`w`$<span dir="rtl">)، يشير إلى
**متجه**</span> **(Vector)** <span dir="rtl">الأعمدة الذي يحتوي على
المشتقات الجزئية للتعبير بالنسبة إلى مكونات **المتجه**
</span>**(Vector)**<span dir="rtl">:</span>

``` math
\nabla f(w) = \left( \frac{\partial f(w)}{\partial w_{1}},\frac{\partial f(w)}{\partial w_{2}},\ldots,\frac{\partial f(w)}{\partial w_{d}} \right)^{\top}
```

<span dir="rtl">يُعرف هذا **متجه**</span> **(Vector)**
<span dir="rtl">المشتقات باسم **التدرج**</span> **(Gradient)**
<span dir="rtl">لـ</span> $`f`$ <span dir="rtl">بالنسبة إلى</span>
$`w`$<span dir="rtl">.</span> <span dir="rtl">تُعد طرق **الانحدار التدرجي
العشوائي**</span> **(Stochastic Gradient Descent - SGD)**
<span dir="rtl">طرقًا لـ "الانحدار التدرجي" لأنها تعتمد على الخطوة الكلية
في</span> $`wt`$ <span dir="rtl">التي تكون متناسبة مع **التدرج
السالب**</span> **<span dir="rtl">(</span>Negative
<span dir="rtl"></span>Gradient<span dir="rtl">)</span>**
<span dir="rtl">للخطأ المربع للمثال (المعادلة 9.4). هذا هو الاتجاه الذي
يتناقص فيه الخطأ بأسرع معدل. تُسمى طرق **الانحدار التدرجي**</span>
**(Gradient Descent)** <span dir="rtl">بـ "العشوائية</span>
(Stochastic)" <span dir="rtl">عندما يتم التحديث، كما في هذه الحالة، على
أساس مثال واحد فقط، قد يكون قد تم اختياره بشكل عشوائي. على مدار العديد
من الأمثلة، مع القيام بخطوات صغيرة، يكون التأثير الكلي هو تقليل متوسط
مقياس الأداء مثل **خطأ القيمة** </span>**(VE: Value
Error)**<span dir="rtl">.</span>

<span dir="rtl">قد لا يكون واضحًا على الفور لماذا يأخذ **الانحدار التدرجي
العشوائي**</span> **(SGD)** <span dir="rtl">خطوة صغيرة فقط في اتجاه
**التدرج** </span>**(Gradient)**<span dir="rtl">.</span>
<span dir="rtl">ألا يمكننا التحرك بشكل كامل في هذا الاتجاه والقضاء تمامًا
على الخطأ في المثال؟ في العديد من الحالات يمكن القيام بذلك، ولكن عادةً ما
يكون غير مرغوب فيه. تذكر أننا لا نسعى ولا نتوقع العثور على **دالة
قيمة**</span> **(Value Function)** <span dir="rtl">التي لديها خطأ صفري
لجميع  
**الحالات** </span>**(States)**<span dir="rtl">، بل نسعى للحصول على
تقريب يوازن بين الأخطاء في **الحالات** </span>**(States)**
<span dir="rtl">المختلفة. إذا قمنا بتصحيح كل مثال تمامًا في خطوة واحدة،
فلن نجد مثل هذا التوازن</span>.

<span dir="rtl">في الواقع، تفترض نتائج التقارب لطرق **الانحدار التدرجي
العشوائي**</span> **(SGD)** <span dir="rtl">أن</span> $`\alpha`$
<span dir="rtl">يتناقص بمرور الوقت. إذا تناقص بطريقة تلبي شروط **التقريب
العشوائي**</span> **(Stochastic Approximation)**
<span dir="rtl">القياسية (المعادلة 2.7)، فإن طريقة **الانحدار التدرجي
العشوائي**</span> **(SGD)** <span dir="rtl">(المعادلة 9.5) تضمن التقارب
إلى الحد الأمثل المحلي</span>.

<span dir="rtl">ننتقل الآن إلى الحالة التي يكون فيها الناتج المستهدف،
والذي يُرمز له هنا بـ</span> $`Ut \in R\ `$<span dir="rtl">، للمثال
التدريبي رقم</span> t<span dir="rtl">، أي</span> St​→Ut​<span dir="rtl">،
ليس هو القيمة الحقيقية</span> $`v\pi(St)`$ ​<span dir="rtl">، بل بعض
التقريبات العشوائية لها. على سبيل المثال، قد يكون</span> Ut
<span dir="rtl">نسخة مشوشة بضوضاء من</span>
$`v\pi(St)`$<span dir="rtl">، أو قد يكون أحد أهداف **التمهيد**
</span>**(Bootstrapping)** <span dir="rtl">باستخدام</span> $`v\hat{}`$
<span dir="rtl">المذكورة في القسم السابق. في هذه الحالات، لا يمكننا
إجراء التحديث الدقيق (المعادلة 9.5) لأن</span> $`v\pi(St)`$
<span dir="rtl">غير معروف، ولكن يمكننا تقريبها عن طريق استبدال</span>
$`Ut`$ <span dir="rtl">بدلاً من</span>
$`v\pi(St)`$<span dir="rtl">.</span> <span dir="rtl">ينتج عن ذلك الطريقة
العامة لـ **الانحدار التدرجي العشوائي**</span> **(SGD)**
<span dir="rtl">لتنبؤ **قيمة الحالة** </span>**(State-value
Prediction)<span dir="rtl">:</span>**

``` math
w_{t + 1} \doteq w_{t} + \alpha\left\lbrack U_{t} - \widehat{v}\left( S_{t},w_{t} \right) \right\rbrack\nabla\widehat{v}\left( S_{t},w_{t} \right)
```

<span dir="rtl">إذا كان</span> Ut <span dir="rtl">تقديرًا غير متحيز، أي
إذا كان</span> $`E\lbrack Ut \mid St = s\rbrack = v\pi(St)`$
<span dir="rtl"></span> ​ <span dir="rtl">لكل</span>
$`t`$<span dir="rtl">، فإن</span> $`wt`$ <span dir="rtl">مضمون للوصول
إلى الحد الأمثل المحلي وفقًا لشروط **التقريب العشوائي**</span>
**<span dir="rtl">(</span>Stochastic
<span dir="rtl"></span>Approximation<span dir="rtl">)
</span>**<span dir="rtl">التقليدية (المعادلة 2.7) مع تقليل</span>
$`\alpha`$<span dir="rtl">.</span>

<span dir="rtl">على سبيل المثال، افترض أن **الحالات**</span>
**(States)** <span dir="rtl">في الأمثلة هي **الحالات**</span>
**(States)** <span dir="rtl">الناتجة عن التفاعل (أو التفاعل المحاكى) مع
**البيئة**</span> **(Environment)** <span dir="rtl">باستخدام **السياسة**
</span>**(Policy)** π<span dir="rtl">.</span> <span dir="rtl">نظرًا لأن
القيمة الحقيقية للحالة هي القيمة المتوقعة للعائد الذي يتبعها، فإن هدف
مونت كارلو</span> $`Ut = Gt`$​<span dir="rtl">هو تعريفًا تقدير غير متحيز
للقيمة</span> $`v\pi(St)`$<span dir="rtl">.</span> <span dir="rtl">مع
هذا الاختيار، تتقارب الطريقة العامة لـ **الانحدار التدرجي
العشوائي**</span> **(SGD)** <span dir="rtl">(المعادلة 9.7) إلى تقريب
أمثل محلي للقيمة</span> $`v\pi(St)`$<span dir="rtl">.</span>
<span dir="rtl">وبالتالي، فإن نسخة **الانحدار التدرجي**</span>
**(Gradient Descent)** <span dir="rtl">من **تنبؤ قيمة الحالة**
</span>**(State-value Prediction)** <span dir="rtl">بأسلوب مونت كارلو
مضمونة لإيجاد حل أمثل محلي. يظهر **الكود الزائف**
</span>**(Pseudocode)** <span dir="rtl">لخوارزمية كاملة في المربع
أدناه</span>.

<span dir="rtl">خوارزمية مونت كارلو الانحدارية لتقدير</span>
$`\widehat{v} \approx v_{\pi}`$

<img src="./media/image97.png"
style="width:6.26806in;height:2.21042in" />

<span dir="rtl">لا يتم الحصول على نفس الضمانات إذا تم استخدام تقدير
**التعزيز الذاتي**</span> **(Bootstrap)** <span dir="rtl">لـ **دالة
القيمة**</span> **(Value Function)** $`v\pi(St)`$ <span dir="rtl">كهدف
**الحالة**</span> **(State)** $`Ut`$ <span dir="rtl">في (9.7). الأهداف
المستندة إلى **التعزيز الذاتي**</span> **(Bootstrap)**
<span dir="rtl">مثل **العوائد**</span> **(n-step returns)** $`Gt:t + n`$
​<span dir="rtl">أو هدف **البرمجة الديناميكية** </span>**(Dynamic
Programming Target)**
$`p(s',\ r|S\_ t,\ a)\lbrack r\  + \ \backslash gamma`$
$`\backslash hat\{ v\}(s',w\_ t)\rbrack`$ <span dir="rtl">تعتمد جميعها
على القيمة الحالية لمتجه الأوزان **متجه الأوزان  **
</span>**(Weight Vector)**
<span dir="rtl"></span>$`wt`$​<span dir="rtl">، مما يعني أنها ستكون
متحيزة ولن تنتج طريقة **التدرج التنازلي  **
</span>**(Gradient Descent)** <span dir="rtl">حقيقية. إحدى الطرق لفهم
هذا الأمر هي أن الخطوة الرئيسية من (9.4) إلى (9.5) تعتمد على أن يكون
الهدف مستقلًا عن</span> $`wt`$​<span dir="rtl">.</span> <span dir="rtl">لن
تكون هذه الخطوة صالحة إذا تم استخدام تقدير **التعزيز الذاتي**
</span>**(Bootstrap) <span dir="rtl"></span>**<span dir="rtl">بدلاً
من</span> $`v\pi(St)`$<span dir="rtl">.</span> <span dir="rtl">طرق
**التعزيز الذاتي**</span> **(Bootstrap)** <span dir="rtl">ليست في الواقع
أمثلة على **التدرج التنازلي الحقيقي** </span>**(True Gradient Descent)
<span dir="rtl"></span>**(Barnard, 1993)<span dir="rtl">.</span>
<span dir="rtl">فهي تأخذ في الاعتبار تأثير تغيير متجه الأوزان **متجه
الأوزان**</span> **(Weight Vector)** $`wt`$ <span dir="rtl">على التقدير،
لكنها تتجاهل تأثيره على الهدف. فهي تشمل جزءًا فقط من التدرج، وبناءً على
ذلك، نسميها طرق **التدرج شبه الكامل**</span>
**<span dir="rtl">(</span>Semi-Gradient
<span dir="rtl"></span>Methods<span dir="rtl">)</span>**.

<span dir="rtl">على الرغم من أن طرق **التدرج شبه الكامل**</span>
**(Semi-Gradient)** <span dir="rtl">(**التعزيز الذاتي**
</span>**(Bootstrap)**<span dir="rtl">)</span> <span dir="rtl">لا تتقارب
بشكل موثوق مثل طرق **التدرج الكامل** </span>**(Full Gradient
Methods)**<span dir="rtl">، إلا أنها تتقارب بشكل موثوق في حالات مهمة مثل
الحالة الخطية التي نناقشها في القسم التالي. بالإضافة إلى ذلك، فإنها تقدم
مزايا مهمة تجعلها في كثير من الأحيان مفضلة بوضوح. أحد أسباب ذلك هو أنها
عادةً ما تمكن من التعليم بشكل أسرع بكثير، كما رأينا في الفصول 6 و7. سبب
آخر هو أنها تتيح التعليم المستمر والمتصل بالإنترنت دون انتظار نهاية
الحلقة. هذا يمكنها من استخدامها في المشاكل المستمرة ويوفر مزايا حسابية.
إحدى الطرق النموذجية **للتدرج شبه الكامل**</span> **(Semi-Gradient
Methods)** <span dir="rtl">هي طريقة **التدرج شبه الكامل**
</span>**TD(0)**<span dir="rtl">، والتي تستخدم</span>
$`\ Ut = Rt + 1 + \gamma v\hat{}(S_{t + 1},w)\ `$<span dir="rtl">كهدف.</span>
**<span dir="rtl">الكود الزائف</span> (Pseudocode)**
<span dir="rtl">الكامل لهذه الطريقة مُعطى في المربع أدناه</span>.

<span dir="rtl">خوارزمية التدرج شبه الكامل</span> TD(0)
<span dir="rtl">لتقدير</span> (Semi-gradient TD(0) for Estimating)

<img src="./media/image98.png"
style="width:6.26806in;height:3.02014in" />

<span dir="rtl">تجميع الحالات</span> (State Aggregation)
<span dir="rtl">هو شكل بسيط من أشكال **التقريب العام للدالة**
</span>**(Generalizing Function Approximation)** <span dir="rtl">حيث يتم
تجميع **الحالات**</span> **(States)** <span dir="rtl">معًا، مع تقدير قيمة
واحدة (مكون واحد من **متجه الأوزان** </span>**(Weight Vector)**
$`w`$<span dir="rtl">)</span> <span dir="rtl">لكل مجموعة. تُقدر قيمة
**الحالة**</span> **(State)** <span dir="rtl">كمكون من مجموعة الحالات،
وعندما يتم تحديث **الحالة** </span>**(State)**<span dir="rtl">، يتم
تحديث هذا المكون فقط</span>. **<span dir="rtl">تجميع الحالات</span>
(State Aggregation)** <span dir="rtl">هو حالة خاصة من **التدرج التنازلي
العشوائي** </span>**(Stochastic Gradient Descent - SGD)**
<span dir="rtl">في المعادلة (9.7)، حيث يكون **التدرج**
</span>**(Gradient) <span dir="rtl"></span>**$`\nabla v\hat{}(St,wt)`$
<span dir="rtl">يساوي 1 لمكون مجموعة الحالة</span> $`St`$
<span dir="rtl">و0 للمكونات الأخرى</span>.

**<span dir="rtl"><u>مثال 9.1</u>: تجميع الحالات في مشية عشوائية من 1000
حالة</span>**

<span dir="rtl">لنعتبر نسخة من مهمة المشية العشوائية التي تحتوي على 1000
حالة (أمثلة 6.2 و7.1 في الصفحات 125 و144). الحالات مرقمة من 1 إلى 1000
من اليسار إلى اليمين، وتبدأ جميع الحلقات بالقرب من المركز، في **الحالة**
</span>**(State)** 500<span dir="rtl">.</span> <span dir="rtl">انتقالات
الحالات تكون من الحالة الحالية إلى واحدة من 100 حالة مجاورة على يسارها،
أو إلى واحدة من 100 حالة مجاورة على يمينها، مع احتمال متساوٍ. بالطبع، إذا
كانت **الحالة**</span> **(State)** <span dir="rtl">الحالية قريبة من
الحافة، فقد يكون هناك أقل من 100 جار على هذا الجانب. في هذه الحالة، كل
الاحتمالات التي كانت ستذهب إلى الجيران المفقودين تذهب إلى احتمال
الانتهاء على ذلك الجانب (مثلاً، **الحالة**</span> **(State)** 1
<span dir="rtl">لديها فرصة 0.5 للانتهاء على اليسار، و**الحالة**
</span>**(State)** <span dir="rtl"></span>950 <span dir="rtl">لديها فرصة
0.25 للانتهاء على اليمين). كما هو معتاد، فإن الانتهاء على اليسار ينتج
عنه مكافأة قدرها 1-، والانتهاء على اليمين ينتج عنه مكافأة قدرها 1+. جميع
الانتقالات الأخرى لديها مكافأة صفرية. نستخدم هذه المهمة كمثال مستمر في
هذا القسم</span>.

<span dir="rtl">يوضح **الشكل 9.1** **دالة القيمة الحقيقية**</span>
**(True Value Function)** $`v\pi`$ <span dir="rtl">لهذه المهمة. إنها
تقريبًا خط مستقيم، ولكن مع انحناء طفيف نحو الأفقي في آخر 100
**حالة**</span> **(State)** <span dir="rtl">في كل نهاية. أيضًا، يظهر
الشكل دالة القيمة التقريبية النهائية التي تعلمتها **خوارزمية مونت كارلو
الانحدارية**</span> **<span dir="rtl">(</span>Gradient
<span dir="rtl"></span>Monte-Carlo Algorithm<span dir="rtl">)
</span>**<span dir="rtl">مع تجميع الحالات بعد 100,000 حلقة بحجم
خطوة</span> $`\alpha = 2 \times 10 - 5`$<span dir="rtl">. لتجميع
الحالات، تم تقسيم 1000 **حالة**</span> **(State)** <span dir="rtl">إلى
10 مجموعات من 100 **حالة**</span> **(State)** <span dir="rtl">لكل منها
(أي، الحالات من 1 إلى 100 كانت مجموعة واحدة، الحالات من 101 إلى 200 كانت
مجموعة أخرى، وهكذا). التأثير المتدرج</span> (Staircase
Effect)<span dir="rtl">.</span>

<img src="./media/image99.png"
style="width:6.08386in;height:3.15027in" />

<span dir="rtl">**الشكل 9.1**: تقريب الدالة</span> (Function
Approximation) <span dir="rtl">عن طريق تجميع الحالات</span>
<span dir="rtl">(</span>State
<span dir="rtl"></span>Aggregation<span dir="rtl">) في مهمة المشية
العشوائية من 1000 حالة</span> <span dir="rtl">(</span>1000-state Random
Walk <span dir="rtl"></span>Task<span dir="rtl">)، باستخدام خوارزمية
مونت كارلو الانحدارية</span> (Gradient Monte Carlo Algorithm)
<span dir="rtl">(الصفحة 202)</span>.

**<span dir="rtl">الشكل</span> (Figure)** <span dir="rtl">الموضح هو
نموذج لتجميع الحالات</span> (State Aggregation)<span dir="rtl">؛ داخل كل
مجموعة، تكون **القيمة التقريبية**</span> **(Approximate Value)**
<span dir="rtl">ثابتة، وتتغير بشكل مفاجئ من مجموعة إلى أخرى. هذه القيم
التقريبية قريبة من الحد الأدنى العالمي لـ **دالة الخطأ التربيعي**</span>
**(VE)** <span dir="rtl">في المعادلة</span>
(9.1)<span dir="rtl">.</span>

<span dir="rtl">بعض تفاصيل القيم التقريبية يمكن تقديرها بشكل أفضل من
خلال الرجوع إلى **توزيع الحالات  
(**</span>**State
<span dir="rtl"></span>Distribution<span dir="rtl">)</span>** $`\mu`$
<span dir="rtl">لهذه المهمة، والذي يظهر في الجزء السفلي من
**الشكل**</span> **(Figure)** <span dir="rtl">مع مقياس على الجانب
الأيمن.</span> **<span dir="rtl">الحالة</span> (State)**
500<span dir="rtl">، في المركز، هي **الحالة**</span> **(State)**
<span dir="rtl">الأولى في كل حلقة، ولكن نادرًا ما تتم زيارتها مرة أخرى.
في المتوسط، يتم قضاء حوالي 1.37% من خطوات الوقت في **الحالة**</span>
**(State)** <span dir="rtl">الابتدائية. الحالات التي يمكن الوصول إليها
في خطوة واحدة من **الحالة** </span>**(State)**
<span dir="rtl">الابتدائية هي الثانية من حيث الزيارة، مع قضاء حوالي
0.17% من خطوات الوقت في كل منها. من هناك، ينخفض توزيع</span> $`\mu`$
<span dir="rtl">تقريبًا بشكل خطي، حيث يصل إلى حوالي 0.0147% في الحالات
القصوى 1 و1000. التأثير الأكثر وضوحًا لتوزيع</span> $`\mu`$
<span dir="rtl">هو على المجموعات الموجودة على أقصى اليسار، حيث تكون
قيمها مرتفعة بوضوح مقارنة بالمتوسط غير المرجح للقيم الحقيقية للحالات
داخل المجموعة، وعلى المجموعات الموجودة على أقصى اليمين، حيث تكون قيمها
منخفضة بوضوح. هذا يرجع إلى أن الحالات في هذه المناطق لها
**اختلال**</span> **(Asymmetry)** <span dir="rtl">كبير في توزيع الأوزان
من خلال</span> $`\mu`$<span dir="rtl">.</span> <span dir="rtl">على سبيل
المثال، في المجموعة الموجودة على أقصى اليسار، يتم توزيع وزن
**الحالة**</span> **(State)** 100 <span dir="rtl">بأكثر من ثلاثة أضعاف
قوة **الحالة** </span>**(State)** 1<span dir="rtl">.</span>
<span dir="rtl">وبالتالي فإن التقدير للمجموعة منحاز نحو القيمة
الحقيقية  
لـ **الحالة** </span>**(State)** 100<span dir="rtl">، والتي هي أعلى من
القيمة الحقيقية لـ **الحالة** </span>**(State)**
1<span dir="rtl">.</span>

**<u>9.4 <span dir="rtl">الطرق الخطية</span> (Linear Methods)</u>**

**<span dir="rtl">إحدى أهم الحالات الخاصة لتقريب الدالة</span> (Function
Approximation)** <span dir="rtl">هي الحالة التي تكون فيها **الدالة
التقريبية** </span>**(Approximate Function)**<span dir="rtl">،</span>
$`v\hat{}( \cdot ,w)`$<span dir="rtl">، **دالة خطية** </span>**(Linear
Function)** <span dir="rtl">بالنسبة لمتجه الأوزان **متجه الأوزان**
</span>**(Weight Vector)**<span dir="rtl">،</span>
$`w`$<span dir="rtl">.</span> <span dir="rtl">لكل **حالة**
</span>**(State)** $`s`$<span dir="rtl">، يوجد متجه قيم حقيقية</span>
$`x(s)`$ <span dir="rtl"></span> <span dir="rtl">يُعرَّف كما يلي</span>:
<span dir="rtl"></span>$`\mathbf{x(s) = (x1(s),x2(s),\ldots,xd(s))\top x}`$,<span dir="rtl">،
ويحتوي على نفس عدد المكونات مثل</span> $`w`$<span dir="rtl">.</span>
<span dir="rtl">تقوم **الطرق الخطية**</span>
**<span dir="rtl">(</span>Linear
<span dir="rtl"></span>Methods<span dir="rtl">)
</span>**<span dir="rtl">بتقريب **دالة القيمة للحالة**
</span>**(State-Value Function)** <span dir="rtl">بواسطة **حاصل الضرب
الداخلي**</span> **(Inner Product)** <span dir="rtl">بين</span> $`w`$
<span dir="rtl">و</span>$`x(s)`$<span dir="rtl">:</span>

``` math
v(s,w) = w^{\top}x(s) = \sum_{i = 1}^{d}{w_{i}x_{i}(s)}
```

<span dir="rtl">في هذه الحالة، يُقال إن **دالة القيمة التقريبية**</span>
**(Approximate Value Function)** <span dir="rtl">خطية بالنسبة للأوزان
**متجه الأوزان** </span>**(Weight Vector)**<span dir="rtl">، أو ببساطة
خطية. يُطلق على المتجه</span> $`x(s)`$ <span dir="rtl">اسم **متجه
الميزات**</span> **(Feature Vector)** <span dir="rtl">الذي يمثل
**الحالة** </span>**(State)** $`s`$<span dir="rtl">.</span>
<span dir="rtl">كل مكون</span> $`xi(s)`$ <span dir="rtl">من</span>
$`x(s)`$ <span dir="rtl">هو قيمة دالة</span>
$`x:S \rightarrow R`$<span dir="rtl">.</span> <span dir="rtl">نعتبر
**الميزة**</span> **(Feature)** <span dir="rtl">بمثابة الكلية لإحدى هذه
الدوال، ونطلق على قيمتها بالنسبة **لحالة** </span>**(State)
<span dir="rtl"></span>**s <span dir="rtl">اسم **ميزة الحالة**
</span>**(Feature of State)**<span dir="rtl">.</span>
<span dir="rtl">بالنسبة **للطرق الخطية  **
</span>**(Linear Methods)**<span dir="rtl">، تعتبر **الميزات**</span>
**(Features)** <span dir="rtl">بمثابة دوال أساسية لأنها تشكل أساسًا خطيًا
لمجموعة الدوال التقريبية. بناء متجهات ميزات</span> d
<span dir="rtl"></span>-<span dir="rtl">الأبعاد لتمثيل
**الحالات**</span> **(States)** <span dir="rtl">هو نفسه اختيار مجموعة من
الدوال الأساسية</span> d<span dir="rtl">.</span> <span dir="rtl">يمكن
تعريف **الميزات**</span> **(Features)** <span dir="rtl">بطرق عديدة
مختلفة؛ سنتناول بعض الإمكانيات في الأقسام التالية</span>.

<span dir="rtl">من الطبيعي استخدام تحديثات **التدرج التنازلي
العشوائي**</span> **<span dir="rtl">(</span>Stochastic Gradient Descent
– SGD<span dir="rtl">) </span>**<span dir="rtl">مع **التقريب الخطي
للدالة** </span>**(Linear Function
Approximation)**<span dir="rtl">.</span> **<span dir="rtl">التدرج</span>
(Gradient)** <span dir="rtl">لـ **دالة القيمة التقريبية**</span>
**(Approximate Value Function)** <span dir="rtl">بالنسبة لـ</span> $`w`$
<span dir="rtl">في هذه الحالة هو</span>:

``` math
\nabla\widehat{v}(s,w) = x(s)
```

<span dir="rtl">وبالتالي، في الحالة الخطية **تحديث التدرج التنازلي
العشوائي العام**</span> **(General SGD Update)** <span dir="rtl">في
المعادلة (9.7) يتبسط إلى شكل بسيط للغاية</span>:

``` math
w_{t + 1} = w_{t} + \alpha\left\lbrack U_{t} - \widehat{v}\left( S_{t},w_{t} \right) \right\rbrack x\left( S_{t} \right)
```

<span dir="rtl">نظرًا لكونها بسيطة للغاية، فإن حالة **التدرج التنازلي
العشوائي الخطي**</span> **(Linear SGD Case)** <span dir="rtl">هي واحدة
من الحالات الأكثر ملاءمة للتحليل الرياضي. تقريبًا جميع النتائج المفيدة
حول التقارب</span> (Convergence) <span dir="rtl">لنظم التعليم من جميع
الأنواع تعتمد على طرق **التقريب الخطي للدالة**</span>
**<span dir="rtl">(</span>Linear <span dir="rtl"></span>Function
Approximation<span dir="rtl">)</span>** <span dir="rtl">(أو طرق
أبسط)</span>.

<span dir="rtl">على وجه الخصوص، في الحالة الخطية يوجد فقط حد أمثل واحد
(أو في حالات محددة، مجموعة واحدة من الحلول المثلى المتماثلة)، وبالتالي
أي طريقة تضمن التقارب إلى أو بالقرب من **الحد الأمثل المحلي**
</span>**(Local Optimum)** <span dir="rtl">تضمن تلقائيًا التقارب إلى أو
بالقرب من **الحد الأمثل العالمي  
(**</span>**Global
<span dir="rtl"></span>Optimum<span dir="rtl">)</span>**.
<span dir="rtl">على سبيل المثال، **خوارزمية مونت كارلو الانحدارية  
(**</span>**Gradient Monte <span dir="rtl"></span>Carlo
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**
<span dir="rtl">التي تم تقديمها في القسم السابق تتقارب إلى **الحد الأمثل
العالمي**</span> **<span dir="rtl">(</span>Global
Optimum<span dir="rtl">) </span>**<span dir="rtl">لدالة الخطأ
التربيعي</span> (VE) <span dir="rtl">تحت **التقريب الخطي للدالة**</span>
**<span dir="rtl">(</span>Linear <span dir="rtl"></span>Function
Approximation<span dir="rtl">) </span>**<span dir="rtl">إذا تم
تقليل</span> $`\alpha`$ <span dir="rtl">بمرور الوقت وفقًا للشروط
المعتادة</span>.

**<span dir="rtl">خوارزمية التدرج شبه الكامل</span> TD(0) (Semi-Gradient
TD(0) Algorithm)** <span dir="rtl">التي تم تقديمها في القسم السابق
تتقارب أيضًا تحت **التقريب الخطي للدالة** </span>**(Linear Function
Approximation)**<span dir="rtl">، ولكن هذا لا يتبع من النتائج العامة لـ
**التدرج التنازلي العشوائي** </span>**(SGD)**<span dir="rtl">؛ بل يتطلب
الأمر نظرية منفصلة. المتجه الذي تتقارب إليه الأوزان ليس **الحد الأمثل
العالمي** </span>**(Global Optimum)**<span dir="rtl">، بل هو نقطة قريبة
من **الحد الأمثل المحلي** </span>**(Local
Optimum)**<span dir="rtl">.</span> <span dir="rtl">من المفيد النظر في
هذه الحالة المهمة بمزيد من التفصيل، خاصةً بالنسبة للحالة المستمرة.
التحديث في كل وقت</span> t <span dir="rtl">هو</span>:

``` math
w_{t + 1} = w_{t} + \alpha\left( R_{t + 1} + \gamma w_{t}^{\top}x_{t + 1} - w_{t}^{\top}x_{t} \right)x_{t}
```

``` math
w_{t + 1} = w_{t} + \alpha\left( R_{t + 1}x_{t} - x_{t}(x_{t} - \gamma x_{t + 1} \right)^{\top}w_{t})
```

<span dir="rtl">حيث استخدمنا هنا الاختصار الرمزي</span>
$`xt = x(St)`$<span dir="rtl">.</span> <span dir="rtl">بمجرد أن يصل
النظام إلى **الحالة المستقرة** </span>**(Steady
State)**<span dir="rtl">، يمكن كتابة **متجه الأوزان المتوقع
التالي**</span> **<span dir="rtl">(</span>Expected Next Weight
<span dir="rtl"></span>Vector<span dir="rtl">)
</span>**<span dir="rtl">لأي</span> $`wt`$ <span dir="rtl"></span>​
<span dir="rtl">معين كما يلي</span>:

``` math
E\left\lbrack w_{t + 1} \middle| w_{t} \right\rbrack = w_{t} + \alpha\left( b - Aw_{t} \right)
```

<span dir="rtl">عندما</span>

``` math
b = E\left\lbrack R_{t + 1}x_{t} \right\rbrack \in R^{d}\quad\text{and}\quad A = E\left\lbrack x_{t}\left( x_{t} - \gamma x_{t + 1} \right)^{\top} \right\rbrack \in R^{d} \times R^{d}
```

<span dir="rtl">من المعادلة (9.10) يتضح أنه، إذا تقارب النظام، فيجب أن
يتقارب إلى **متجه الأوزان**</span> **<span dir="rtl">(</span>Weight
<span dir="rtl"></span>Vector<span dir="rtl">)</span>** wTD
<span dir="rtl">حيث</span>:

``` math
b - Aw_{TD} = 0
```

``` math
\quad b = Aw_{TD}\quad
```

``` math
w_{TD} = A^{- 1}b
```

<span dir="rtl">تُسمى هذه الكمية **نقطة التثبيت** </span>**(TD Fixed
Point)**<span dir="rtl">.</span> <span dir="rtl">في الواقع، تتقارب
**خوارزمية التدرج شبه الكامل**</span> **TD (0)
<span dir="rtl">الخطي</span> (Linear Semi-Gradient TD (0))**
<span dir="rtl">إلى هذه النقطة. بعض النظريات التي تثبت تقاربها ووجود
**المعكوس**</span> **(Inverse)** <span dir="rtl">أعلاه موضحة في
الصندوق</span>.

**<span dir="rtl">إثبات تقارب خوارزمية التدرج شبه الكامل</span> TD (0)
<span dir="rtl">الخطي</span> <span dir="rtl">(</span>Proof of
Convergence of <span dir="rtl"></span>Linear TD
(0)<span dir="rtl">)</span>**

<span dir="rtl">ما هي الخصائص التي تضمن تقارب **خوارزمية التدرج شبه
الكامل**</span> **TD(0) <span dir="rtl">الخطي  
(</span>Linear TD(0) <span dir="rtl"></span>Algorithm<span dir="rtl">)
</span>**<span dir="rtl">في المعادلة (9.9)؟ يمكن اكتساب بعض الفهم عن
طريق إعادة كتابة المعادلة (9.10) كما يلي</span>:

``` math
E\left\lbrack w_{t + 1} \middle| w_{t} \right\rbrack = (I - \alpha A)w_{t} + \alpha b
```

<span dir="rtl">لاحظ أن **المصفوفة**</span> **(Matrix)** $`A`$
<span dir="rtl">تضرب **متجه الأوزان**</span> **(Weight Vector)** $`wt`$​
<span dir="rtl">وليس</span> $`b`$<span dir="rtl">؛ فقط</span> $`A`$
<span dir="rtl">هي المهمة لتحقيق التقارب. لاكتساب الفهم، لننظر في الحالة
الخاصة التي تكون فيها</span> $`A`$ <span dir="rtl">مصفوفة **قطرية**
</span>**(Diagonal Matrix)**<span dir="rtl">.</span> <span dir="rtl">إذا
كانت أي من العناصر القطرية سالبة، فإن العنصر القطري المقابل لـ</span>
$`I - \alpha A`$ <span dir="rtl">سيكون أكبر من واحد، وسيتم تضخيم العنصر
المقابل لـ</span> $`wt`$ <span dir="rtl"></span>​<span dir="rtl">، مما
سيؤدي إلى تباعد إذا استمر. من ناحية أخرى، إذا كانت جميع العناصر القطرية
لـ</span> $`A`$ <span dir="rtl">موجبة، فإنه يمكن اختيار</span>
$`\alpha`$ <span dir="rtl">أصغر من واحد مقسومًا على أكبرها، بحيث
يكون</span> $`I - \alpha A`$ **<span dir="rtl">مصفوفة قطرية</span>
(Diagonal Matrix)** <span dir="rtl">تحتوي جميع عناصرها القطرية بين 0 و1.
في هذه الحالة، يميل المصطلح الأول من التحديث إلى تقليص</span>
$`wt`$​<span dir="rtl">، ويتم ضمان الاستقرار</span>.

<span dir="rtl">بشكل عام، سيتم تقليل</span> $`wt`$ <span dir="rtl">نحو
الصفر كلما كانت</span> $`A`$ <span dir="rtl">موجبة **التعريف**
</span>**(Positive Definite)**<span dir="rtl">، مما يعني</span>
$`y\top Ay > 0y`$ <span dir="rtl">لأي **متجه حقيقي** </span>**(Real
Vector)** $`y \neq 0y`$<span dir="rtl">.</span>
**<span dir="rtl">إيجابية التعريف</span>
<span dir="rtl">(</span>Positive
<span dir="rtl"></span>Definiteness<span dir="rtl">)
</span>**<span dir="rtl">تضمن أيضًا وجود المعكوس</span>
$`A - 1`$<span dir="rtl">.</span> <span dir="rtl">بالنسبة لـ **خوارزمية
التدرج شبه الكامل** </span>**TD(0) <span dir="rtl">الخطي</span> (Linear
TD(0))**<span dir="rtl">، في الحالة المستمرة مع</span>
$`\gamma < 1\ `$<span dir="rtl">، يمكن كتابة مصفوفة</span> $`A`$
<span dir="rtl"></span> <span dir="rtl">في المعادلة (9.11) كما
يلي</span>:

``` math
A = \sum_{s}^{}{\mu(s)\sum_{a}^{}{\pi\left( a \middle| s \right)\sum_{r,s'}^{}{p\left( r,s' \middle| s,a \right)x(s)\left( x(s) - \gamma x\left( s' \right) \right)^{\top}}}} = \sum_{s}^{}{\mu(s)\sum_{s'}^{}{p\left( s' \middle| s \right)x(s)\left( x(s) - \gamma x\left( s' \right) \right)^{\top}}} = \sum_{s}^{}{\mu(s)x(s)\left( x(s) - \gamma\sum_{s'}^{}{p\left( s' \middle| s \right)x\left( s' \right)} \right)^{\top}} = X^{\top}D(I - \gamma P)X
```

<span dir="rtl">حيث</span> μ(s) <span dir="rtl">هو **توزيع الحالة
الثابت**</span> **(Stationary Distribution)** <span dir="rtl">تحت
السياسة</span> $`\pi`$<span dir="rtl">، و</span>$`p(s' \mid s)`$
<span dir="rtl">هو **احتمال الانتقال**</span> **(Transition
Probability)** <span dir="rtl">من الحالة</span> $`s`$
<span dir="rtl">إلى الحالة</span> $`s'`$ <span dir="rtl">تحت
السياسة</span> $`\pi`$<span dir="rtl">. **المصفوفة**</span> **(Matrix)**
P <span dir="rtl">هي مصفوفة بحجم</span> ∣S∣×∣S∣ <span dir="rtl">تحتوي
على هذه الاحتمالات، و</span>D <span dir="rtl">هي **مصفوفة قطرية**</span>
**(Diagonal Matrix)** <span dir="rtl">بحجم</span> ∣S∣×∣S∣
<span dir="rtl">تحتوي على</span> $`\mu(s)`$ <span dir="rtl">على قطرها،
و</span>X <span dir="rtl">هي **مصفوفة** </span>**(Matrix)**
<span dir="rtl">بحجم</span> ∣S∣×d\| <span dir="rtl">تحتوي على</span>
$`x(s)`$ <span dir="rtl">كصفوفها. من هنا، يتضح أن **المصفوفة الداخلية**
</span>**(Inner Matrix)** $`D(I - \gamma P)`$ <span dir="rtl">هي المفتاح
لتحديد **إيجابية التعريف** </span>**(Positive Definiteness)**
<span dir="rtl">للمصفوفة</span> A<span dir="rtl">.</span>

<span dir="rtl">بالنسبة لمصفوفة رئيسية من هذا النوع، يتم ضمان **إيجابية
التعريف**</span> **(Positive Definiteness)** <span dir="rtl">إذا كانت
جميع أعمدة المصفوفة تجمع إلى رقم غير سالب. تم إثبات ذلك بواسطة</span>
Sutton <span dir="rtl">(1988، صفحة 27) بناءً على نظريتين سبق تأسيسهما.
تقول النظرية الأولى إن أي مصفوفة</span> M <span dir="rtl">تكون موجبة
التعريف إذا وفقط إذا كانت **المصفوفة المتماثلة**</span> **(Symmetric
Matrix)** $`S = M + M\top`$ <span dir="rtl">موجبة التعريف (</span>Sutton
1988<span dir="rtl">، الملحق)</span>. <span dir="rtl">أما النظرية
الثانية فتقول إن أي مصفوفة متماثلة حقيقية</span> $`S`$
<span dir="rtl">تكون موجبة التعريف إذا كانت جميع مدخلاتها القطرية موجبة
وأكبر من مجموع القيم المطلقة للمدخلات خارج القطر المقابلة</span>
<span dir="rtl">(</span>Varga 1962<span dir="rtl">، صفحة 23)</span>.
<span dir="rtl">بالنسبة للمصفوفة الرئيسية لدينا</span>
$`D(I - \gamma P)`$<span dir="rtl">، فإن المدخلات القطرية موجبة
والمدخلات خارج القطر سالبة، لذلك كل ما علينا إثباته هو أن مجموع كل صف
زائد مجموع العمود المقابل هو موجب. مجموع الصفوف كلها موجب لأن</span>
$`P`$ <span dir="rtl">هي **مصفوفة عشوائية** </span>**(Stochastic
Matrix)** <span dir="rtl">و</span>$`\gamma < 1`$<span dir="rtl">.</span>
<span dir="rtl">لذا يبقى فقط إثبات أن مجموع الأعمدة غير سالب. لاحظ أن
**متجه الصف**</span> **(Row Vector)** <span dir="rtl">لمجموع الأعمدة لأي
مصفوفة</span> $`M`$ <span dir="rtl">يمكن كتابته  
كـ</span> 1⊤M<span dir="rtl">، حيث</span> 1 <span dir="rtl">هو **متجه
العمود**</span> **(Column Vector)** <span dir="rtl">الذي يحتوي على جميع
العناصر مساوية  
لـ 1. دع</span> $`\mu`$ <span dir="rtl">يرمز إلى **متجه الحالة
الثابتة**</span> **(Stationary Vector)** <span dir="rtl">بحجم</span>
∣S∣<span dir="rtl">  
لـ</span> $`\mu(s)`$<span dir="rtl">، حيث</span> $`\mu = P\top`$
<span dir="rtl">بحكم كون</span> $`\mu`$ <span dir="rtl">هو التوزيع
الثابت. إذن، مجموع الأعمدة للمصفوفة الرئيسية لدينا هو</span>:

``` math
1^{\top}D(I - \gamma P) = \mu^{\top}(I - \gamma P)
```

``` math
= \mu^{\top} - \gamma\mu^{\top}P
```

``` math
= \mu^{\top} - \gamma\mu\top
```

``` math
= (1 - \gamma)\mu^{\top}
```

<span dir="rtl">جميع مكونات هذا المتجه موجبة. وبالتالي، فإن **المصفوفة
الرئيسية**</span> **(Key Matrix)** <span dir="rtl">ومصفوفتها</span> A
<span dir="rtl">موجبتا التعريف، و**خوارزمية**</span> **TD(0)
<span dir="rtl">داخل السياسة</span> (On-Policy TD(0))**
<span dir="rtl">مستقرة. (تُحتاج إلى شروط إضافية وجدول زمني لتقليل</span>
$`\alpha`$ <span dir="rtl">بمرور الوقت لإثبات التقارب مع الاحتمالية
الواحد)</span>.

<span dir="rtl">عند **نقطة التثبيت**</span> **(TD Fixed
Point)**<span dir="rtl">، تم إثبات أيضًا (في الحالة المستمرة) أن **دالة
الخطأ التربيعي**</span> **(VE)** <span dir="rtl">تقع ضمن توسع محدود من
أدنى خطأ ممكن</span>:

``` math
\overline{\text{VE}}\left( w_{TD} \right) \leq \frac{1}{1 - \gamma}\min_{w}\overline{\text{VE}}(w)
```

<span dir="rtl">أي أن **الخطأ النهائي**</span> **(Asymptotic Error)**
<span dir="rtl">لطريقة</span> TD <span dir="rtl">لا يزيد عن</span> 1−γ
<span dir="rtl">مرة من أصغر خطأ ممكن، وهو الخطأ الذي يتم الوصول إليه في
الحد النهائي باستخدام طريقة مونت كارلو (</span>Monte
<span dir="rtl"></span>Carlo<span dir="rtl">).</span>
<span dir="rtl">نظرًا لأن</span> γ <span dir="rtl">غالبًا ما يكون قريبًا من
الواحد، يمكن أن يكون عامل التوسع هذا كبيرًا جدًا، وبالتالي هناك احتمال
كبير لفقدان الأداء النهائي مع طريقة</span> TD<span dir="rtl">.</span>
<span dir="rtl">من ناحية أخرى، تذكر أن طرق</span>TD
<span dir="rtl">تتميز بتقليل كبير في **التباين**</span> **(Variance)**
<span dir="rtl">مقارنة بطرق مونت كارلو، وبالتالي تكون أسرع، كما رأينا في
الفصول 6 و7. تحديد الطريقة الأفضل يعتمد على طبيعة **التقريب**</span>
**(Approximation)** <span dir="rtl">والمشكلة، وعلى مدى استمرارية
التعليم</span>.

<span dir="rtl">حد مشابه للمعادلة (9.14) ينطبق أيضًا على طرق **التعزيز
الذاتي**</span> **(Bootstrapping)** <span dir="rtl">الأخرى داخل
السياسة</span> (On-policy)<span dir="rtl">.</span> <span dir="rtl">على
سبيل المثال، **الطرق الخطية للتدرج شبه الكامل في  
البرمجة الديناميكية**</span> **(Linear Semi-Gradient DP)**
(<span dir="rtl">المعادلة 9.7 مع  
</span>$`Ut = \sum a\pi(a \mid St)\sum s',rp(s',r \mid St,a)\lbrack r + \gamma v\hat{}(s',wt)\rbrack`$
<span dir="rtl">مع التحديثات وفقًا لتوزيع داخل السياسة ستتقارب أيضًا إلى
**نقطة التثبيت** </span>**(TD Fixed Point)**<span dir="rtl">. **طرق
التدرج شبه الكامل ذات خطوة واحدة**</span> **(One-Step Semi-Gradient)**
<span dir="rtl">لتقدير قيمة الإجراءات، مثل **سارسا**
</span>**(Sarsa(0))** <span dir="rtl">التي ستتم مناقشتها في الفصل
التالي، تتقارب إلى نقطة تثبيت مماثلة وحد مماثل. بالنسبة للمهام التي
تتضمن حلقات، هناك حد مختلف قليلاً ولكنه مرتبط  
(راجع</span> Bertsekas
<span dir="rtl">و</span>Tsitsiklis<span dir="rtl">، 1996)</span>.
<span dir="rtl">هناك أيضًا بعض الشروط الفنية المتعلقة بالمكافآت،
والميزات، وتقليل **بارامتر حجم الخطوة** </span>**(Step-Size
Parameter)**<span dir="rtl">، والتي لم نذكرها هنا. يمكن العثور على
التفاصيل الكاملة في الورقة الأصلية</span>
<span dir="rtl">(</span>Tsitsiklis <span dir="rtl">و</span>Van
Roy<span dir="rtl">، 1997)</span>.

<span dir="rtl">الأمر الحاسم في نتائج التقارب هذه هو أن
**الحالات**</span> **(States)** <span dir="rtl">يتم تحديثها وفقًا لتوزيع
داخل السياسة</span> (On-Policy Distribution)<span dir="rtl">.</span>
<span dir="rtl">بالنسبة لتوزيعات التحديث الأخرى، قد تؤدي طرق **التعزيز
الذاتي**</span> **(Bootstrapping)** <span dir="rtl">باستخدام تقريب
الدالة إلى التباعد إلى اللانهاية. أمثلة على ذلك ومناقشة لطرق الحل
المحتملة يمكن العثور عليها في الفصل 11</span>.

**<span dir="rtl">مثال 9.2: التعزيز الذاتي في المشية العشوائية المكونة
من 1000 حالة</span>**

<span dir="rtl">تجميع الحالات هو حالة خاصة من **التقريب الخطي
للدالة**</span> **<span dir="rtl">(</span>Linear Function
<span dir="rtl"></span>Approximation<span dir="rtl">)</span>**<span dir="rtl">،
لذا دعونا نعود إلى المشية العشوائية المكونة من 1000 حالة لتوضيح بعض
الملاحظات التي تم ذكرها في هذا الفصل. يُظهر اللوح الأيسر من **الشكل 9.2**
**دالة القيمة النهائية** </span>**(Final Value Function)**
<span dir="rtl">التي تم تعلمها بواسطة **خوارزمية التدرج شبه
الكامل**</span> **TD(0) <span dir="rtl">(</span>Semi-Gradient TD(0)
Algorithm<span dir="rtl">)</span>** <span dir="rtl">(الصفحة 203)
باستخدام نفس تجميع الحالات كما في **مثال 9.1**</span>.
<span dir="rtl">نرى أن التقريب النهائي لـ</span> TD
<span dir="rtl">بالفعل أبعد عن القيم الحقيقية من تقريب مونت كارلو
المعروض في **الشكل 9.1**</span>.

<span dir="rtl">ومع ذلك، تحتفظ طرق</span> TD <span dir="rtl">بمزايا
كبيرة محتملة في **معدل التعليم** </span>**(Learning
Rate)**<span dir="rtl">، وتعمم طرق مونت كارلو، كما حققنا بالكامل مع
طرق</span> TD <span dir="rtl">ذات</span> -n<span dir="rtl">خطوة في
**الفصل 7**</span>. <span dir="rtl">يُظهر اللوح الأيمن من **الشكل 9.2**
نتائج باستخدام طريقة</span> TD <span dir="rtl">ذات</span>
-n<span dir="rtl">خطوة والتدرج شبه الكامل مع تجميع الحالات في المشية
العشوائية المكونة من 1000 حالة، وهي مشابهة بشكل لافت لتلك التي حصلنا
عليها سابقًا مع **الطرق الجدولية**</span> **(Tabular Methods)**
<span dir="rtl">والمشية العشوائية المكونة من 19 حالة (الشكل 7.2). للحصول
على نتائج مشابهة كميًا، قمنا بتبديل تجميع الحالات إلى 20 مجموعة من 50
حالة لكل مجموعة.  
كانت المجموعات العشرين بعد ذلك قريبة بشكل كمي</span>...

<img src="./media/image100.png"
style="width:6.26806in;height:2.13819in" />

**<span dir="rtl">الشكل 9.2: التعزيز الذاتي</span> (Bootstrapping)
<span dir="rtl">مع تجميع الحالات</span> (State Aggregation)
<span dir="rtl">في مهمة المشية العشوائية المكونة من 1000 حالة</span>
(1000-State Random Walk Task)<span dir="rtl">.</span>**  
**<span dir="rtl">اللوحة اليسرى</span> (Left)<span dir="rtl">:</span>**
<span dir="rtl">القيم النهائية لطرق **التدرج شبه الكامل  **
</span> **TD (Semi-Gradient TD)** <span dir="rtl">أسوأ من القيم النهائية
لمونت كارلو في **الشكل 9.1**</span>.  
**<span dir="rtl">اللوحة اليمنى</span> (Right)<span dir="rtl">:</span>**
<span dir="rtl">أداء طرق</span> -n<span dir="rtl">خطوة مع تجميع الحالات
مشابه بشكل لافت لأداء التمثيلات الجدولية</span> (Tabular
Representations) <span dir="rtl">(راجع **الشكل 7.2**)</span>.
<span dir="rtl">هذه البيانات هي متوسطات لـ 100 تشغيل</span>.

<span dir="rtl">إلى 19 حالة من المشكلة الجدولية</span> (tabular
problem)<span dir="rtl">.</span> <span dir="rtl">بشكل خاص، تذكر أن
الانتقالات</span> (transitions) <span dir="rtl">بين الحالات كانت تصل إلى
100 حالة إلى اليسار أو اليمين. الانتقال النموذجي</span> (typical
transition) <span dir="rtl">سيكون بعد ذلك 50 حالة إلى اليمين أو اليسار،
وهو مشابه من الناحية الكمية للانتقالات الأحادية الحالة</span>
(single-state state transitions) <span dir="rtl">لنظام الـ 19 حالة
الجدولي  
(</span>19-state tabular system<span dir="rtl">)</span>.
<span dir="rtl">لإكمال التوافق، نستخدم هنا نفس مقياس الأداء—متوسط غير
مرجح</span> (unweighted average) <span dir="rtl">للخطأ الجذري
التربيعي</span> (RMS error) <span dir="rtl">عبر جميع الحالات وعلى مدى
أول 10 حلقات</span> (episodes)—<span dir="rtl">بدلاً من هدف</span> (VE
objective) VE <span dir="rtl">كما هو أكثر ملاءمة عند استخدام تقريب
الدوال</span> (function approximation)<span dir="rtl">.</span>

<span dir="rtl">خوارزمية نصف التدرج</span> n-step TD (semi-gradient
n-step TD algorithm) <span dir="rtl">المستخدمة في المثال أعلاه هي
التمديد الطبيعي لخوارزمية</span> n-step TD
<span dir="rtl">الجدولية</span> <span dir="rtl">(</span>tabular n-step
TD <span dir="rtl"></span>algorithm<span dir="rtl">) المقدمة في الفصل 7
إلى تقريب الدوال النصف تدرجي</span>
<span dir="rtl">(</span>semi-gradient function
<span dir="rtl"></span>approximation<span dir="rtl">)</span>.
<span dir="rtl">الكود الزائف</span> (pseudocode) <span dir="rtl">مُعطى في
الصندوق أدناه</span>.

**<span dir="rtl">خوارزمية نصف التدرج</span> n-step TD
<span dir="rtl">لتقدير</span>**

``` math
```

<span dir="rtl">معادلة المفتاح لهذه الخوارزمية، التي تشابه المعادلة
(7.2)، هي</span>:

``` math
w_{t + n} = w_{t + n - 1} + \alpha\left\lbrack G_{t:t + n} - \widehat{v}\left( S_{t},w_{t + n - 1} \right) \right\rbrack\nabla\widehat{v}\left( S_{t},w_{t + n - 1} \right)
```

<span dir="rtl">حيث يتم تعميم العائد من نوع</span> n-step
<span dir="rtl">من (7.1) إلى</span>

``` math
G_{t:t + n} = R_{t + 1} + \gamma R_{t + 2} + \cdots + \gamma^{n - 1}R_{t + n} + \gamma^{n}\widehat{v}\left( S_{t + n},w_{t + n - 1} \right),\quad 0 \leq t \leq T - n.
```

**<u><span dir="rtl">التمرين 9.1</span>:</u>**

<span dir="rtl">أظهر أن **الطرق الجدولية**</span> **(Tabular Methods)**
<span dir="rtl">مثل التي عُرضت في الجزء الأول من هذا الكتاب هي حالة خاصة
من **تقريب الدوال**</span> **(Function Approximation)**
**<span dir="rtl">الخطية</span> (Linear)**<span dir="rtl">.</span>
<span dir="rtl">ما هي **المتجهات المميزة**</span> **(Feature Vectors)**
<span dir="rtl">في هذه الحالة؟</span>

**<u>9.5 <span dir="rtl">بناء المميزات</span> (Feature Construction)
<span dir="rtl">للطرق الخطية</span> (Linear Methods)</u>**

<span dir="rtl">الطرق الخطية</span> (Linear Methods)
<span dir="rtl">مثيرة للاهتمام بسبب ضماناتها في التلاشي</span>
<span dir="rtl">(</span>Convergence
<span dir="rtl"></span>Guarantees<span dir="rtl">)، ولكن أيضًا لأنها في
الممارسة العملية يمكن أن تكون فعالة جدًا من حيث البيانات والحساب</span>
(Data and Computation)<span dir="rtl">.</span> <span dir="rtl">سواء كان
هذا صحيحًا أم لا يعتمد بشكل حاسم على كيفية تمثيل الحالات</span> (States)
<span dir="rtl">من حيث المميزات</span> (Features)<span dir="rtl">، وهو
ما نستكشفه في هذا القسم الكبير. اختيار المميزات المناسبة للمهمة هو وسيلة
مهمة لإضافة المعرفة السابقة بالنطاق</span> <span dir="rtl">(</span>Prior
Domain <span dir="rtl"></span>Knowledge<span dir="rtl">) إلى أنظمة
التعليم المعزز</span> (Reinforcement Learning)<span dir="rtl">.</span>
<span dir="rtl">بديهياً، يجب أن تتوافق المميزات مع جوانب فضاء
الحالة</span> (State Space) <span dir="rtl">التي قد يكون التعميم عليها
مناسبًا. إذا كنا نقيم الكائنات الهندسية</span> (Geometric
Objects)<span dir="rtl">، على سبيل المثال، فقد نريد أن تكون هناك مميزات
لكل شكل أو لون أو حجم أو وظيفة محتملة. إذا كنا نقيم حالات روبوت
متنقل</span> (Mobile Robot)<span dir="rtl">، فقد نريد أن تكون هناك
مميزات للمواقع، ودرجات الطاقة المتبقية في البطارية، وقراءات السونار
الحديثة، وهكذا</span>.

<span dir="rtl">تتمثل إحدى القيود في الشكل الخطي</span> (Linear Form)
<span dir="rtl">في أنه لا يمكنه أخذ أي تفاعلات بين المميزات</span>
(Interactions Between Features) <span dir="rtl">في الاعتبار، مثل كون
وجود الميزة</span> $`\mathbf{i}`$ <span dir="rtl">مفيدًا فقط في غياب
الميزة</span> j<span dir="rtl">.</span> <span dir="rtl">على سبيل المثال،
في مهمة توازن العمود</span> (Pole-Balancing Task) <span dir="rtl">يمكن
أن تكون السرعة الزاوية العالية</span> (High Angular Velocity)
<span dir="rtl">إما جيدة أو سيئة حسب الزاوية</span>
(Angle)<span dir="rtl">.</span> <span dir="rtl">إذا كانت الزاوية عالية،
فإن السرعة الزاوية العالية تعني خطرًا وشيكًا بالسقوط - حالة سيئة</span>
(Bad State) - <span dir="rtl">بينما إذا كانت الزاوية منخفضة، فإن السرعة
الزاوية العالية تعني أن العمود يعيد وضعه - حالة جيدة  
(</span>Good <span dir="rtl"></span>State<span dir="rtl">)</span>.
<span dir="rtl">لا يمكن لدالة القيمة الخطية</span> (Linear Value
Function) <span dir="rtl">أن تمثل هذا إذا كانت مميزاتها ترمز بشكل منفصل
للزاوية والسرعة الزاوية. بدلاً من ذلك، تحتاج إلى مميزات أو ميزات إضافية
لتوليفات هذين البعدين الأساسيين للحالة</span>
<span dir="rtl">(</span>Combinations of These Two Underlying State
<span dir="rtl"></span>Dimensions<span dir="rtl">)</span>.
<span dir="rtl">في الأقسام الفرعية التالية، نعتبر مجموعة متنوعة من الطرق
العامة للقيام بذلك</span>.

**<u>9.5.1 <span dir="rtl">التحويلات متعددة الحدود</span>
<span dir="rtl">(</span>Polynomials<span dir="rtl">) </span></u>**

<span dir="rtl">تُعبر حالات العديد من المشكلات في البداية كأرقام، مثل
المواقع والسرعات في مهمة توازن القضيب</span> (Pole-Balancing Task)
<span dir="rtl">(المثال 3.4)، عدد السيارات في كل موقف في مشكلة تأجير
السيارات لجاك</span> (Jack’s Car Rental Problem) <span dir="rtl">(المثال
4.2)، أو رأس المال في مشكلة المقامر</span> (Gambler Problem)
<span dir="rtl">(المثال 4.3). في هذه الأنواع من المشكلات، يشترك التقدير
الوظيفي</span> (Function Approximation) <span dir="rtl">لتعلم
الآلة</span> (Reinforcement Learning) <span dir="rtl">كثيرًا مع المهام
المألوفة للتقريب</span> (Interpolation) <span dir="rtl">والانحدار</span>
(Regression)<span dir="rtl">.</span> <span dir="rtl">يمكن أيضًا استخدام
عائلات مختلفة من الميزات</span> (Features) <span dir="rtl">الشائعة
للتقريب والانحدار في تعلم الآلة. تشكل التحويلات المتعددة الحدود</span>
(Polynomials) <span dir="rtl">واحدة من أبسط العائلات المستخدمة للتقريب
والانحدار. على الرغم من أن ميزات التحويلات المتعددة الحدود الأساسية التي
نناقشها هنا لا تعمل بنفس كفاءة أنواع الميزات الأخرى في تعلم الآلة، إلا
أنها تقدم مقدمة جيدة لأنها بسيطة ومعروفة</span>.

<span dir="rtl">كمثال، افترض أن مشكلة تعلم الآلة تحتوي على حالات ذات
بعدين رقميين. بالنسبة لحالة تمثيلية واحدة</span>
$`\mathbf{s}`$<span dir="rtl">، دع أرقامه تكون</span>
$`s1\  \in \ \mathbb{R}`$
<span dir="rtl">و</span>$`s2\  \in \ \mathbb{R}`$. <span dir="rtl">قد
تختار تمثيل</span> $`s`$ <span dir="rtl">ببساطة من خلال بُعدي الحالة
الخاصين به، بحيث يكون</span> $`x(s)\  = \ (s1,\ s2)ᵀ`$<span dir="rtl">،
ولكن بعد ذلك لن تكون قادرًا على أخذ أي تفاعلات بين هذه الأبعاد في
الاعتبار. بالإضافة إلى ذلك، إذا كان كل من</span> $`s1`$
<span dir="rtl">و</span>$`s2`$ <span dir="rtl">صفرًا، فإن القيمة
التقريبية ستكون أيضًا صفرًا. يمكن تجاوز كلا القيدين بدلاً من ذلك من خلال
تمثيل</span> s <span dir="rtl">بواسطة متجه الميزات الرباعي
الأبعاد</span>
$`x(s)\  = \ (1,\ s1,\ s2,\ s1s2)ᵀ`$.<span dir="rtl">.</span>
<span dir="rtl">يسمح العنصر الأول 1 بتمثيل الدوال المائلة  
(</span>Affine <span dir="rtl"></span>Functions<span dir="rtl">) في
أرقام الحالة الأصلية، ويتيح عنصر المنتج النهائي،</span>
$`s1s2`$<span dir="rtl">، أخذ التفاعلات في الاعتبار. أو قد تختار استخدام
متجهات ميزات أعلى الأبعاد مثل  
</span> $`x(s)\  = \ (1,\ s1,\ s2,\ s1²s2²)ᵀ`$<span dir="rtl">لأخذ
التفاعلات الأكثر تعقيدًا في الاعتبار. تتيح مثل هذه المتجهات الميزات تقريب
الدوال الرباعية</span> (Quadratic Functions) <span dir="rtl">من أرقام
الحالة - على الرغم من أن التقدير لا يزال خطيًا في الأوزان</span>
(Weights) <span dir="rtl">التي يجب تعلمها. بتعميم هذا المثال من اثنين
إلى</span> $`k`$ <span dir="rtl">أرقام، يمكننا تمثيل التفاعلات المعقدة
للغاية بين أبعاد حالة المشكلة</span>.

<span dir="rtl">افترض أن كل حالة</span> $`s`$ <span dir="rtl">تتوافق
مع</span> $`k`$ <span dir="rtl">أرقام،</span>
$`s1,s2,\ldots,sk`$​<span dir="rtl">، مع كل</span>
$`si \in R`$<span dir="rtl">.</span> <span dir="rtl">بالنسبة لمساحة
الحالة ذات الأبعاد</span> $`k`$<span dir="rtl">، يمكن كتابة كل ميزة من
أساسيات متعددة الحدود من الرتبة</span> $`n`$
<span dir="rtl"></span>$`xi`$ <span dir="rtl">كما يلي</span>

``` math
x_{i}(s) = \prod_{j = 1}^{k}s_{i,j}
```

<span dir="rtl">حيث كل</span> $`ci,jc`$ <span dir="rtl">هو عدد صحيح في
المجموعة</span> {$`0,1,\ldots,n`$} <span dir="rtl">لعدد صحيح</span>
$`n \geq 0n`$<span dir="rtl">. تشكل هذه الميزات أساسيات متعددة الحدود من
الرتبة</span> $`n`$ <span dir="rtl">للأبعاد</span>
$`k`$<span dir="rtl">، والتي تحتوي على</span> $`(n + 1)k`$
<span dir="rtl"></span> <span dir="rtl">ميزات مختلفة</span>.

<span dir="rtl">الميزات ذات الرتبة الأعلى</span> (Higher-order
Polynomial Bases) <span dir="rtl">تسمح بتقريب أكثر دقة للدوال المعقدة.
ولكن نظرًا لأن عدد الميزات في أساسيات متعددة الحدود من الرتبة</span>
$`n`$ <span dir="rtl">ينمو بشكل أسي مع بعد</span> $`k`$
<span dir="rtl">للفضاء الطبيعي (إذا كان</span>
$`n > 0n\  > \ 0n > 0`$<span dir="rtl">)، من الضروري عمومًا اختيار مجموعة
فرعية منها لتقريب الدوال. يمكن القيام بذلك باستخدام المعتقدات السابقة
حول طبيعة الدالة التي يجب تقريبها، وبعض طرق الاختيار الآلي التي تم
تطويرها لتقريب متعددة الحدود يمكن تكييفها للتعامل مع الطبيعة التدريجية
وغير الثابتة للتعليم المعزز</span>.

<span dir="rtl"><u>**التمرين 9.2**:</u></span> <span dir="rtl">لماذا
تعرف المعادلة</span> (9.17) $`(n + 1)k`$ <span dir="rtl">ميزات متميزة
لأبعاد</span> $`k`$<span dir="rtl">؟</span>

<span dir="rtl"><u>**التمرين 9.3**:</u></span> <span dir="rtl">ما هي
قيم</span> $`n`$ <span dir="rtl">و</span> $`ci,j`$
<span dir="rtl"></span>​ <span dir="rtl">التي تنتج متجهات الميزات  
</span>$`x(s) = (1,s1,s2,s1s2,s12,s22,s1s22,s12s2,s12s22)\top`$
<span dir="rtl">؟</span>

**<u>9.5.2 <span dir="rtl">أساسيات فورييه</span> (Fourier Basis)</u>**

<span dir="rtl">طريقة أخرى لتقريب الدوال الخطية</span> (Linear Function
Approximation) <span dir="rtl">تعتمد على **سلسلة فورييه**
</span>**(Fourier Series)**<span dir="rtl">، التي تعبر عن الدوال الدورية
كجمع وزني لدوال الأساس</span> (Basis Functions) <span dir="rtl">من جيب
وجيب تمام بترددات مختلفة</span>. <span dir="rtl">(الدالة</span> $`f`$
<span dir="rtl">تكون دورية إذا كانت</span> $`f(x)\  = \ f(x\  + \ ⌧)`$
<span dir="rtl">لجميع</span> $`x`$ <span dir="rtl">ولفترة</span> $`⌧`$
<span dir="rtl">معينة)</span>. **<span dir="rtl">سلسلة فورييه</span>
(Fourier Series)** <span dir="rtl">و**تحويل فورييه  **
</span>**(Fourier Transform)
<span dir="rtl"></span>**<span dir="rtl">الأكثر عمومية يُستخدمان على نطاق
واسع في **العلوم التطبيقية** </span>**(Applied Sciences)**
<span dir="rtl">جزئياً لأن الدالة المراد تقريبها إذا كانت معروفة، فإن
أوزان دوال الأساس</span> (Basis Function Weights) <span dir="rtl">تُعطى
بصيغ بسيطة، وعلاوة على ذلك، مع ما يكفي من دوال الأساس يمكن تقريب أي دالة
بدقة كما هو مطلوب. في **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">، حيث تكون الدوال المراد تقريبها غير معروفة،
تعتبر دوال الأساس</span> (Basis Functions) <span dir="rtl">الخاصة  
بـ **فورييه**</span> **(Fourier)** <span dir="rtl">ذات اهتمام لأنها سهلة
الاستخدام ويمكن أن تؤدي بشكل جيد في مجموعة من مشكلات **التعليم
المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**.

<span dir="rtl">أولاً،</span> Consider <span dir="rtl">ن حالة البُعد
الواحد.</span> **<span dir="rtl">سلسلة فورييه</span> (Fourier Series)**
<span dir="rtl">التقليدية لدالة ذات بعد واحد والتي لها فترة</span> $`⌧`$
<span dir="rtl">تمثل الدالة كمزيج خطي لدوال الجيب وجيب التمام التي تكون
دورية بفترات تقسم</span> $`⌧`$ <span dir="rtl">بشكل متساوي (بمعنى آخر،
الترددات التي هي مضاعفات صحيحة لتردد أساسي</span>
$`1/⌧`$<span dir="rtl">). ولكن إذا كنت مهتمًا بتقريب دالة غير دورية محددة
على فترة محددة، يمكنك استخدام هذه الميزات الخاصة بـ **فورييه**</span>
**(Fourier)** <span dir="rtl">مع</span> $`⌧`$ <span dir="rtl">مُعينة لطول
الفترة. ستكون الدالة ذات الاهتمام حينها هي فترة واحدة فقط من المزيج
الدوري الخطي لدوال الجيب وجيب التمام</span>.

<span dir="rtl">علاوة على ذلك، إذا قمت بتعيين</span> $`⌧`$
<span dir="rtl">ليكون ضعف طول الفترة ذات الاهتمام وقيّدت الانتباه إلى
التقريب على النصف فترة \[0، ⌧/2\]، يمكنك استخدام فقط ميزات جيب التمام.
هذا ممكن لأنك تستطيع تمثيل أي دالة زوجية، أي، أي دالة تكون متماثلة حول
الأصل، باستخدام فقط دوال الأساس  
(</span>Basis <span dir="rtl"></span>Functions<span dir="rtl">)</span>
<span dir="rtl">الخاصة بالجيب التمام. لذا يمكن تقريبا أي دالة على نصف
الفترة \[0، ⌧/2\] بدقة كما هو مطلوب باستخدام ما يكفي من ميزات جيب
التمام. (قول "أي دالة" ليس دقيقًا تمامًا لأن الدالة يجب أن تكون سلوكية
رياضيًا، ولكننا نتخطى هذه التقنية هنا). بدلاً من ذلك، من الممكن استخدام
فقط ميزات الجيب، التي تكون دائمًا دوال فردية، أي دوال تكون مضادة للتناظر
حول الأصل. ولكن من الأفضل عمومًا الحفاظ على ميزات جيب التمام فقط لأن
"الدوال نصف الزوجية" تميل إلى أن تكون أسهل في التقريب من "الدوال نصف
الفردية" لأن الأخيرة غالبًا ما تكون غير مستمرة عند الأصل. بالطبع، لا يمنع
هذا من استخدام كل من ميزات الجيب وجيب التمام لتقريب الفترة على \[0،
⌧/2\]، وهو ما قد يكون مفيدًا في بعض الظروف</span>.

<span dir="rtl">اتّباعًا لهذه المنطق ومع تحديد ⌧ = 2 بحيث تُعرّف الميزات على
نصف فترة ⌧ \[0، 1\]، فإن دالة الأساس الخاصة بـ **فورييه**</span>
**(Fourier)** <span dir="rtl">من الدرجة</span> n <span dir="rtl">تتألف
من الميزات التالية</span>:

``` math
x_{i}(s) = \cos(i\pi s),\quad s \in \lbrack 0,1\rbrack,
```

<span dir="rtl">لـ</span> $`i\  = \ 0`$<span dir="rtl">،</span>
...<span dir="rtl">،</span> $`n`$. <span dir="rtl">تُظهر الشكل 9.3 ميزات
الكوسينوس أحادية الأبعاد</span> Fourier cosine features
xi<span dir="rtl">، لـ</span> i = 1<span dir="rtl">، 2، 3، 4؛ حيث</span>
$`x0`$ <span dir="rtl">هو دالة ثابتة</span>.

<img src="./media/image102.png"
style="width:6.26806in;height:1.28889in" />

<span dir="rtl">الشكل 9.3: ميزات الكوسينوس أحادية الأبعاد</span>
(Fourier cosine-basis features) $`xi`$​<span dir="rtl">،</span>
i=1<span dir="rtl">،</span>2<span dir="rtl">،</span>3<span dir="rtl">،</span>4i
= 1<span dir="rtl">، 2، 3، 4</span>i=1،2،3،4<span dir="rtl">، لتقريب
الدوال على الفترة \[0، 1\]. بعد</span> Konidaris et al.
(2011)<span dir="rtl">.</span>

<span dir="rtl">تنطبق نفس الفكرة على تقريب سلسلة الكوسينوس فورييه</span>
(Fourier cosine series) <span dir="rtl">في الحالة متعددة الأبعاد كما هو
موضح في الصندوق أدناه</span>.

<span dir="rtl">افترض أن كل حالة</span> $`s`$ <span dir="rtl">تتوافق مع
متجه من</span> $`k`$ <span dir="rtl">أرقام،</span>
$`s = (s1,s2,\ldots,sk)T`$<span dir="rtl">، حيث</span>
$`si \in \lbrack 0,1\rbrack`$<span dir="rtl">.</span>
<span dir="rtl">يمكن بعد ذلك كتابة الخاصية</span> - i <span dir="rtl">ية
في قاعدة الكوسين</span> (Cosine) <span dir="rtl">فورييه</span> (Fourier)
<span dir="rtl">من الرتبة</span> $`n`$ <span dir="rtl">كـ</span>:

``` math
x_{i}(s) = \cos\left( \pi s^{T}c_{i} \right),
```

<span dir="rtl">حيث</span> $`ci = (ci1,\ldots,cik)T`$<span dir="rtl">،
مع</span> $`cij \in \{ 0,\ldots,n\}`$ <span dir="rtl"></span>
<span dir="rtl">لـ</span> $`j = 1,\ldots,kj`$ <span dir="rtl"></span>
<span dir="rtl">و</span> $`0,\ldots,(n + 1)k`$<span dir="rtl">.</span>
<span dir="rtl">يُعرّف هذا ميزة لكل من</span> $`(n\  + \ 1)\hat{}k`$
<span dir="rtl">متجهات صحيحة ممكنة</span> $`ci`$​<span dir="rtl">.</span>
<span dir="rtl">الضرب الداخلي</span>$`cis\hat{}T\ \ `$​
<span dir="rtl">له تأثير تعيين عدد صحيح في</span> {$`0,\ldots,n`$}
<span dir="rtl"></span> <span dir="rtl">لكل بُعد من</span>
$`s`$<span dir="rtl">.</span> <span dir="rtl">كما في الحالة الأحادية
الأبعاد، هذا العدد الصحيح يحدد تردد الميزة على ذلك البُعد. يمكن بالطبع
تحريك وتغيير مقياس الميزات لتناسب مساحة الحالة المقيدة لتطبيق
معين</span>.

<span dir="rtl">كمثال، اعتبر الحالة</span> $`k = 2`$
<span dir="rtl">حيث</span> $`s = (s1,s2)T`$<span dir="rtl">، حيث
كل</span> $`ci = (ci1,ci2)T`$,<span dir="rtl">. تُظهر الشكل 9.4 مجموعة من
ست ميزات من دوال الأساس الكوسينية، كل منها مُميز بواسطة المتجه</span>
$`ci`$ <span dir="rtl"></span> <span dir="rtl">الذي يُعرّفها (محور</span>
$`s1`$ <span dir="rtl"></span>​ <span dir="rtl">هو المحور الأفقي
و</span>$`ci`$ <span dir="rtl"></span> <span dir="rtl">يُعرض كمتجه صف مع
حذف الفهرس</span> $`i`$<span dir="rtl">)</span>. <span dir="rtl">أي صفر
في</span> $`c`$ <span dir="rtl">يعني أن الميزة ثابتة على ذلك البُعد من
الحالة. لذا، إذا كان</span> $`c = (0,0)T`$<span dir="rtl">، فإن الميزة
ثابتة على كلا البعدين؛ إذا كان</span> $`c = (c1,0)T`$<span dir="rtl">،
فإن الميزة ثابتة على البُعد الثاني وتختلف على الأول بتردد يعتمد
على</span> $`c1`$​<span dir="rtl">؛ وبالمثل، بالنسبة لـ</span>
$`c = (0,c2)T`$<span dir="rtl">.</span> <span dir="rtl">عندما
يكون</span> $`c = (c1,c2)T`$ <span dir="rtl"></span> <span dir="rtl">مع
عدم كون أي من</span> $`cj = 0`$<span dir="rtl">، فإن الميزة تختلف على
كلا البعدين وتمثل تفاعلًا بين متغيري الحالة. تحدد قيم</span> $`c1`$
<span dir="rtl">و</span>$`c2`$ <span dir="rtl"></span>​
<span dir="rtl">التردد على كل بُعد، ونسبتهما تعطي اتجاه التفاعل</span>.

<img src="./media/image103.png"
style="width:6.26806in;height:3.45556in" />

<span dir="rtl">**الشكل 9.4**: مجموعة من ست ميزات كوسينية ثنائية
الأبعاد، كل منها مُميز بواسطة المتجه</span> $`ci`$ <span dir="rtl">الذي
يُعرّفها (محور</span> **s1** <span dir="rtl"></span>​ <span dir="rtl">هو
المحور الأفقي، و</span>$`\mathbf{ci}`$ <span dir="rtl"></span>
<span dir="rtl">يُعرض مع حذف الفهرس</span> $`i`$<span dir="rtl">)</span>.

<span dir="rtl">بعد</span> Konidaris et al.
<span dir="rtl">(</span>2011<span dir="rtl">)</span>.

<span dir="rtl">عند استخدام **ميزات جيب التمام فورييه**</span>
**(Fourier Cosine Features)** <span dir="rtl">مع خوارزمية تعلم مثل
**الفصل** </span>**(Chapter)** 9.7<span dir="rtl">، **التعليم شبه
المتدرج** </span>**(Semigradient)** **TD(0)**<span dir="rtl">، أو
**سارسا شبه المتدرج**</span> **(Semi-gradient Sarsa)**<span dir="rtl">،
قد يكون من المفيد استخدام **بارامتر خطوة**</span>
**<span dir="rtl">(</span>Step-size
<span dir="rtl"></span>Parameter<span dir="rtl">)
</span>**<span dir="rtl">مختلف لكل ميزة. إذا كان</span> $`\alpha`$
<span dir="rtl">هو **بارامتر خطوة الأساسي**</span>
**<span dir="rtl">(</span>Basic Step-size
Parameter<span dir="rtl">)</span>**<span dir="rtl">، فإن **كونيداريس**
</span>**(Konidaris)**<span dir="rtl">، **أوسنتوسكي**
</span>**(Osentoski)**<span dir="rtl">، و**توماس** </span>**(Thomas)**
(2011) <span dir="rtl">يقترحون تعيين **بارامتر خطوة**</span>
**(Step-size Parameter)** <span dir="rtl">للميزة</span> $`xi`$​
<span dir="rtl">كالتالي</span>:
$`\alpha i = \alpha(ci1 - 1)2 + \cdots + (cik - 1)2`$
<span dir="rtl">إلا إذا كانت جميع القيم</span>
$`cij = 0`$<span dir="rtl">، في هذه الحالة</span>
$`\alpha i = \alpha`$<span dir="rtl">.</span>

<span dir="rtl">يمكن أن تنتج **ميزات جيب التمام فورييه**</span>
**(Fourier Cosine Features)** <span dir="rtl">مع **سارسا**</span>
**(Sarsa)** <span dir="rtl">أداءً جيدًا مقارنةً بالعديد من مجموعات **دوال
الأساس**</span> **(Basis Functions)** <span dir="rtl">الأخرى، بما في ذلك
**الدوال متعددة الحدود**</span> **(Polynomial Functions)**
<span dir="rtl">و**دوال الأساس الشعاعي**</span>
**<span dir="rtl">(</span>Radial Basis
<span dir="rtl"></span>Functions<span dir="rtl">)</span>**.
<span dir="rtl">ولكن ليس من المستغرب أن تواجه **ميزات فورييه**</span>
**(Fourier Features)** <span dir="rtl">مشاكل مع **الانقطاعات**</span>
**(Discontinuities)** <span dir="rtl">نظرًا لصعوبة تجنب **الطنين**</span>
**(Ringing)** <span dir="rtl">حول نقاط **الانقطاع**
</span>**(Discontinuity)** <span dir="rtl">ما لم يتم تضمين **دوال
أساس**</span> **(Basis Functions)** <span dir="rtl">بترددات عالية
جدًا</span>.

<span dir="rtl">يتزايد عدد **الميزات**</span> **(Features)**
<span dir="rtl">في **قاعدة فورييه**</span> **(Fourier Basis)**
<span dir="rtl">من الدرجة</span> $`n`$ <span dir="rtl">بشكل أسي مع بعد
**فضاء الحالة** </span>**(State Space)**<span dir="rtl">، ولكن إذا كان
هذا **البعد**</span> **(Dimension)** <span dir="rtl">صغيرًا بما فيه
الكفاية (مثل</span> $`k \leq 5`$<span dir="rtl">)، فيمكن اختيار</span>
$`n`$ <span dir="rtl">بحيث يتم استخدام جميع **ميزات فورييه**</span>
**(Fourier Features)** <span dir="rtl">من الدرجة</span>
$`n`$<span dir="rtl">.</span> <span dir="rtl">وهذا يجعل اختيار
**الميزات**</span> **(Features)** <span dir="rtl">تلقائيًا إلى حد ما. ومع
ذلك، بالنسبة **لفضاءات الحالة**</span> **(State Spaces)**
<span dir="rtl">ذات الأبعاد العالية، يكون من الضروري اختيار مجموعة فرعية
من هذه **الميزات** </span>**(Features)**<span dir="rtl">.</span>
<span dir="rtl">يمكن القيام بذلك باستخدام **الاعتقادات المسبقة**</span>
**(Prior Beliefs)** <span dir="rtl">حول طبيعة **الدالة**</span>
**(Function)** <span dir="rtl">التي سيتم تقريبها، ويمكن تكييف بعض طرق
**الاختيار التلقائي**</span> **<span dir="rtl">(</span>Automated
<span dir="rtl"></span>Selection<span dir="rtl">)
</span>**<span dir="rtl">للتعامل مع الطبيعة التزايدية وغير المستقرة
**للتعليم المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**.
<span dir="rtl">ميزة **ميزات قاعدة فورييه**</span> **(Fourier Basis
Features)** <span dir="rtl">في هذا الصدد هي أنه من السهل اختيار
**الميزات**</span> **(Features)** <span dir="rtl">من خلال تعيين
**متجهات**</span> **(Vectors)** $`ci`$​ <span dir="rtl">لمراعاة
**التفاعلات** </span>**(Interactions)** <span dir="rtl">المشتبه بها بين
**متغيرات الحالة**</span> **(State Variables)** <span dir="rtl">وتحديد
القيم في **متجهات** </span>**(Vectors)** <span dir="rtl"></span>$`cj`$​
<span dir="rtl">بحيث يمكن للتقريب تصفية **المكونات ذات التردد
العالي**</span> **<span dir="rtl">(</span>High Frequency
<span dir="rtl"></span>Components<span dir="rtl">)</span>**
<span dir="rtl">التي تعتبر **ضوضاء**
</span>**(Noise)**<span dir="rtl">.</span> <span dir="rtl">من ناحية
أخرى، نظرًا لأن **ميزات فورييه** </span>**(Fourier Features)**
<span dir="rtl">تكون غير صفرية في جميع أنحاء **فضاء الحالة**</span>
**(State Space)** <span dir="rtl">(مع استثناء القليل من النقاط الصفرية)،
فإنها تمثل **خصائص عالمية**</span> **(Global Properties)**
**<span dir="rtl">للحالات</span> (States)**<span dir="rtl">، مما قد يجعل
من الصعب إيجاد طرق جيدة لتمثيل **الخصائص المحلية**</span>
**<span dir="rtl">(</span>Local
<span dir="rtl"></span>Properties<span dir="rtl">)</span>**.

<img src="./media/image104.png"
style="width:6.10886in;height:3.35862in" />

<span dir="rtl">**الشكل 9.5**:</span> **<span dir="rtl">قاعدة
فورييه</span> (Fourier Basis)** <span dir="rtl">مقابل **متعددات
الحدود**</span> **(Polynomials)** <span dir="rtl">على **المشي العشوائي
ذو الـ 1000 حالة**</span> **(1000-state Random Walk)**.
<span dir="rtl">يظهر في الشكل **منحنيات التعليم**</span> **(Learning
Curves)** <span dir="rtl">لطريقة **تدرج مونت كارلو** </span>**(Gradient
Monte Carlo Method)** <span dir="rtl">باستخدام **قاعدتي فورييه ومتعددات
الحدود**</span> **(Fourier and Polynomial Bases)** <span dir="rtl">من
الدرجة 5، 10، و20. تم تقريبًا تحسين **بارامترات حجم الخطوة**</span>
**(Step-size Parameters)** <span dir="rtl">لكل حالة</span>:
<span dir="rtl"></span>$`\alpha = 0.0001`$ **<span dir="rtl">لقاعدة
متعددات الحدود</span> (Polynomial Basis)**
<span dir="rtl">و</span>$`\alpha = 0.00005`$ **<span dir="rtl">لقاعدة
فورييه</span> (Fourier Basis)**<span dir="rtl">. **مقياس الأداء**</span>
**(Performance Measure)** <span dir="rtl">(المحور</span>
$`y`$<span dir="rtl">)</span> <span dir="rtl">هو **جذر متوسط مربع خطأ
القيمة**</span> **(Root Mean Squared Value Error)** <span dir="rtl">كما
هو موضح في الفصل 9.1</span>.

<u>**9.5**.**3 <span dir="rtl">الترميز الخشن</span> (Coarse Coding)**
</u>

<span dir="rtl">ضع في اعتبارك مهمة يكون فيها التمثيل الطبيعي لمجموعة
**الحالات**</span> **(State Set)** <span dir="rtl">هو **فضاء ثنائي
الأبعاد**</span> **(Two-Dimensional Space)** <span dir="rtl">مستمر. أحد
أنواع التمثيل لهذه الحالة يتكون من **ميزات** </span>**(Features)**
<span dir="rtl">تمثل دوائر في **فضاء الحالة** </span>**(State
Space)**<span dir="rtl">، كما هو موضح على اليمين. إذا كانت
**الحالة**</span> **(State)** <span dir="rtl">داخل دائرة، فإن
**الميزة**</span> **(Feature)** <span dir="rtl">المقابلة تأخذ القيمة 1
ويُقال إنها موجودة؛ أما إذا كانت **الميزة**</span> **(Feature)**
<span dir="rtl">غير موجودة، فإنها تأخذ القيمة 0 ويُقال إنها غائبة. هذا
النوع من **الميزات** </span>**(Features)** <span dir="rtl">ذو القيمة 1-0
يسمى **ميزة ثنائية** </span>**(Binary Feature)**<span dir="rtl">.</span>
<span dir="rtl">عند إعطاء **حالة** </span>**(State)**
<span dir="rtl">معينة، تشير **الميزات الثنائية**</span> **(Binary
Features)** <span dir="rtl">الموجودة إلى الدوائر التي تقع **الحالة**
</span>**(State)** <span dir="rtl">داخلها، وبالتالي تعطي **ترميزًا
خشنًا**</span> **(Coarse Coding)** <span dir="rtl">لموقعها. تمثيل
**الحالة**</span> **(State)** **<span dir="rtl">بميزات</span>
(Features)** <span dir="rtl">تتداخل بهذه الطريقة (على الرغم من أنها ليست
بحاجة لأن تكون دوائر أو ثنائية) يعرف باسم **الترميز الخشن**
</span>**(Coarse Coding)**<span dir="rtl">.</span>

<img src="./media/image105.png"
style="width:3.23361in;height:3.01693in" />

<span dir="rtl">**الشكل** **9.6**:</span> **<span dir="rtl">الترميز
الخشنؤ</span>(Coarse Coding)**<span dir="rtl">.</span>
<span dir="rtl">يعتمد التعميم من **الحالة**</span> **(State)** $`s`$
<span dir="rtl">إلى **الحالة** </span>**(State)** $`s'`$
<span dir="rtl">على عدد **الميزات**</span> **(Features)**
<span dir="rtl">التي تتداخل حقول استقبالها (في هذه الحالة، الدوائر). هذه
**الحالات**</span> **(States)** <span dir="rtl">لها ميزة واحدة مشتركة،
لذلك سيكون هناك تعميم طفيف بينها</span>.

<span dir="rtl">بافتراض تقريب دالة القيمة باستخدام **الانحدار التدرجي
الخطي**</span> **<span dir="rtl">(</span>Linear Gradient-Descent
<span dir="rtl"></span>Function
Approximation<span dir="rtl">)</span>**<span dir="rtl">، ضع في اعتبارك
تأثير حجم وكثافة الدوائر. تتوافق كل دائرة مع **وزن**</span> **(Weight)**
<span dir="rtl">واحد (أحد مكونات</span>
$`\mathbf{w}`$<span dir="rtl">)</span> <span dir="rtl">الذي يتأثر بعملية
التعليم. إذا قمنا بالتدريب في **حالة** </span>**(State)**
<span dir="rtl">واحدة، وهي نقطة في الفضاء، فإن أوزان جميع الدوائر التي
تتقاطع مع تلك **الحالة**</span> **(State)** <span dir="rtl">ستتأثر.
وبالتالي، وفقًا للمعادلة (9.8)، ستتأثر **دالة القيمة التقريبية**
</span>**(Approximate Value Function)** <span dir="rtl">في جميع
**الحالات**</span> **(States)** <span dir="rtl">داخل اتحاد الدوائر، مع
تأثير أكبر كلما زاد عدد الدوائر التي يشترك فيها نقطة مع **الحالة**
</span>**(State)**<span dir="rtl">، كما هو موضح في **الشكل** 9.6. إذا
كانت الدوائر صغيرة، فإن **التعميم** </span>**(Generalization)**
<span dir="rtl">سيكون على مسافة قصيرة، كما هو موضح في **الشكل** 9.7
(اليسار)، بينما إذا كانت كبيرة، فسيكون التعميم على مسافة كبيرة، كما هو
موضح في **الشكل** 9.7 (الوسط)</span>.

<img src="./media/image106.png"
style="width:6.26806in;height:2.36597in" />

<span dir="rtl">**الشكل 9.7**:</span> **<span dir="rtl">التعميم</span>
(Generalization)** <span dir="rtl">في طرق تقريب الدوال الخطية</span>
<span dir="rtl">(</span>**Linear Function
<span dir="rtl"></span>Approximation Methods**<span dir="rtl">) يتحدد
بأحجام وأشكال حقول استقبال **الميزات**
</span>**(Features)**<span dir="rtl">.</span> <span dir="rtl">جميع هذه
الحالات الثلاث لديها تقريبًا نفس العدد والكثافة من **الميزات**
</span>**(Features)**<span dir="rtl">.</span>

<span dir="rtl">علاوة على ذلك، فإن شكل **الميزات**</span> **(Features)**
<span dir="rtl">سيحدد طبيعة **التعميم**
</span>**(Generalization)**<span dir="rtl">.</span> <span dir="rtl">على
سبيل المثال، إذا لم تكن دائرية بشكل صارم، بل كانت ممدودة في اتجاه واحد،
فإن **التعميم**</span> **(Generalization)** <span dir="rtl">سيتأثر
بطريقة مشابهة، كما هو موضح في **الشكل** **9.7** (اليمين)</span>.

**<span dir="rtl">الميزات</span> (Features)** <span dir="rtl">ذات حقول
الاستقبال الكبيرة تعطي تعميمًا واسعًا، لكنها قد تبدو أيضًا وكأنها تحد من
الدالة المتعلمة إلى تقريب خشن، غير قادرة على إجراء تمييزات أدق من عرض
حقول الاستقبال. ولكن، لحسن الحظ، ليس هذا هو الحال. صحيح أن
**التعميم**</span> **(Generalization)** <span dir="rtl">الأولي من نقطة
إلى أخرى يتم التحكم فيه بحجم وشكل حقول الاستقبال، لكن الدقة، أي أدق
تمييز ممكن في النهاية، يتحكم فيها بشكل أكبر العدد الإجمالي **للميزات**
</span>**(Features)**<span dir="rtl">.</span>

### <span dir="rtl">**<u>المثال 9.3</u>**: خشونة الترميز الخشن</span> (Coarseness of Coarse Coding)

<span dir="rtl">يوضح هذا المثال تأثير حجم حقول الاستقبال في **الترميز
الخشن**</span> **(Coarse Coding)** <span dir="rtl">على التعليم. تم
استخدام تقريب دالة القيمة الخطي</span> **(Linear Function
Approximation)** <span dir="rtl">المعتمد على **الترميز الخشن**</span>
**(Coarse Coding)** <span dir="rtl">والمعادلة (9.7) لتعلم دالة موجة
مربعة أحادية البعد (مبينة في الجزء العلوي من الشكل 9.8). استخدمت قيم هذه
الدالة كأهداف</span> $`Ut`$​<span dir="rtl">.</span> <span dir="rtl">مع
بُعد واحد فقط، كانت حقول الاستقبال عبارة عن فترات بدلاً من دوائر. تم تكرار
التعليم بثلاثة أحجام مختلفة للفترات: ضيقة، متوسطة، وواسعة، كما هو موضح
في الجزء السفلي من الشكل. كانت جميع الحالات الثلاث تحتوي على نفس كثافة
**الميزات** </span>**(Features)**<span dir="rtl">، حوالي 50 على امتداد
الدالة التي يتم تعلمها. تم توليد أمثلة التدريب بشكل عشوائي على هذا
الامتداد. كان بارامتر حجم الخطوة</span>
$`\alpha = n0.2`$​<span dir="rtl">، حيث</span> $`n`$ <span dir="rtl">هو
عدد **الميزات**</span> **(Features)** <span dir="rtl">التي كانت موجودة
في وقت واحد. يظهر الشكل 9.8 الدوال المتعلمة في جميع الحالات الثلاث خلال
عملية التعليم. لاحظ أن عرض **الميزات**</span> **(Features)**
<span dir="rtl">كان له تأثير قوي في بداية التعليم. مع **الميزات**</span>
**(Features)** <span dir="rtl">العريضة، كان **التعميم**</span>
**(Generalization)** <span dir="rtl">يميل إلى أن يكون واسعًا؛ مع
**الميزات**</span> **(Features)** <span dir="rtl">الضيقة، تم تغيير
الجيران القريبين فقط من كل نقطة تدريب، مما جعل الدالة المتعلمة أكثر
تقلبًا. ومع ذلك، تأثرت الدالة النهائية المتعلمة بشكل طفيف فقط بعرض
**الميزات** </span>**(Features)**<span dir="rtl">. يميل شكل حقل
الاستقبال إلى أن يكون له تأثير قوي على **التعميم**
</span>**(Generalization)** <span dir="rtl">ولكن تأثيره يكون ضئيلًا على
جودة الحل النهائية</span>.

<img src="./media/image107.png"
style="width:6.26806in;height:3.29792in" />

<span dir="rtl">**<u>الشكل 9.8</u>**: مثال على تأثير عرض
**الميزات**</span> **(Features)** <span dir="rtl">القوي على **التعميم
الأولي**</span> **<span dir="rtl">(</span>Initial
<span dir="rtl"></span>Generalization<span dir="rtl">)</span>**
<span dir="rtl">(الصف الأول) وتأثيره الضعيف على **الدقة
النهائية**</span> **<span dir="rtl">(</span>Asymptotic
<span dir="rtl"></span>Accuracy<span dir="rtl">)</span>**
<span dir="rtl">(الصف الأخير)</span>.

**<u>9.5.4 <span dir="rtl">ترميز التجزئة</span>
<span dir="rtl">(</span>Tile Coding<span dir="rtl">)</span></u>**

**<span dir="rtl">ترميز التجزئة</span> (Tile Coding)**
<span dir="rtl">هو شكل من أشكال **الترميز الخشن**</span> **(Coarse
Coding)** <span dir="rtl">لفضاءات متعددة الأبعاد مستمرة يتميز بالمرونة
والكفاءة الحسابية. قد يكون هذا هو التمثيل الأكثر عملية **للميزات**
</span>**(Features)** <span dir="rtl">للحواسيب الرقمية التسلسلية
الحديثة</span>.

<span dir="rtl">في **ترميز التجزئة** </span>**(Tile
Coding)**<span dir="rtl">، يتم تجميع حقول استقبال **الميزات**</span>
**(Features)** <span dir="rtl">في تقسيمات لفضاء **الحالة**
</span>**(State Space)**<span dir="rtl">.</span> <span dir="rtl">يُطلق
على كل تقسيم من هذه التقسيمات اسم "تجزئة"</span>
(Tiling)<span dir="rtl">، ويُطلق على كل عنصر من عناصر هذا التقسيم اسم
"بلاطة"</span> (Tile)<span dir="rtl">.</span> <span dir="rtl">على سبيل
المثال، أبسط تقسيم لفضاء **حالة ثنائي الأبعاد**</span>
**(Two-Dimensional State Space)** <span dir="rtl">هو شبكة موحدة كما هو
موضح على الجانب الأيسر من **الشكل** **9.9**. في هذه الحالة، تكون
البلاطات أو حقول الاستقبال مربعات بدلاً من الدوائر كما في **الشكل**
**9.6**. إذا تم استخدام هذه التجزئة الواحدة فقط، فإن **الحالة**</span>
**(State)** <span dir="rtl">المشار إليها بالنقطة البيضاء سيتم تمثيلها
بواسطة **ميزة**</span> **(Feature)** <span dir="rtl">واحدة وهي البلاطة
التي تقع داخلها؛ سيكون **التعميم**</span> **(Generalization)**
<span dir="rtl">كاملاً لجميع **الحالات**</span> **(States)**
<span dir="rtl">داخل نفس البلاطة وغير موجود **للحالات**</span>
**(States)** <span dir="rtl">خارجها. مع وجود تجزئة واحدة فقط، لن يكون
لدينا **ترميز خشن  
(**</span>**Coarse
<span dir="rtl"></span>Coding<span dir="rtl">)</span>**
<span dir="rtl">ولكن مجرد حالة من تجميع **الحالات** </span>**(State
Aggregation)**<span dir="rtl">.</span>

<img src="./media/image108.png"
style="width:6.26806in;height:2.2375in" />

<span dir="rtl">**<u>الشكل 9.9</u>**: تجزئات شبكية متعددة ومتداخلة على
فضاء ثنائي الأبعاد محدود. هذه التجزئات متحاذية بشكل متساوٍ في كل
بُعد</span>.

<span dir="rtl">للحصول على قوة **الترميز الخشن**</span> **(Coarse
Coding)** <span dir="rtl">يتطلب وجود **حقول استقبال  
(**</span>**Receptive <span dir="rtl"></span>Fields<span dir="rtl">)
</span>**<span dir="rtl">متداخلة، وبحكم التعريف، فإن **بلاطات
التجزئة**</span> **(Tiles of a Partition)** <span dir="rtl">لا تتداخل.
للحصول على **ترميز خشن**</span> **(Coarse Coding)**
<span dir="rtl">حقيقي باستخدام **ترميز التجزئة  
(**</span>**Tile
<span dir="rtl"></span>Coding<span dir="rtl">)</span>**<span dir="rtl">،
يتم استخدام **تجزئات متعددة** </span>**(Multiple
Tilings)**<span dir="rtl">، كل منها متحاذٍ بمقدار جزء من عرض **البلاطة**
</span>**(Tile Width)**<span dir="rtl">.</span> <span dir="rtl">حالة
بسيطة مع أربع **تجزئات**</span> **(Tilings)** <span dir="rtl">تظهر على
الجانب الأيمن من **الشكل** **9.9**. كل **حالة**
</span>**(State)**<span dir="rtl">، مثل تلك المشار إليها بالنقطة
البيضاء، تقع في **بلاطة** </span>**(Tile)** <span dir="rtl">واحدة فقط في
كل من **التجزئات الأربع** </span>**(Four
Tilings)**<span dir="rtl">.</span> <span dir="rtl">هذه **البلاطات
الأربع  
(**</span>**Four <span dir="rtl"></span>Tiles<span dir="rtl">)</span>**
<span dir="rtl">تمثل أربع **ميزات**</span> **(Features)**
<span dir="rtl">تصبح نشطة عندما تحدث **الحالة**
</span>**(State)**<span dir="rtl">.</span> <span dir="rtl">تحديدًا، يحتوي
**متجه الميزات**</span> **(Feature Vector)** $`x(s)`$
<span dir="rtl">على مكون واحد لكل **بلاطة**</span> **(Tile)**
<span dir="rtl">في كل **تجزئة**
</span>**(Tiling)**<span dir="rtl">.</span> <span dir="rtl">في هذا
المثال، هناك</span> $`4 \times 4\  = \ 64`$ <span dir="rtl">مكونًا،
جميعها ستكون 0 باستثناء الأربعة المقابلة للبلاطات التي تقع
**الحالة**</span> **(State)** <span dir="rtl">داخلها. يُظهر **الشكل**
**9.10** ميزة **التجزئات المتعددة المتحاذية** </span>**(Multiple Offset
Tilings)** <span dir="rtl">(**الترميز الخشن**</span> **(Coarse Coding)**
(<span dir="rtl">على **تجزئة واحدة**</span>
**<span dir="rtl">(</span>Single
<span dir="rtl"></span>Tiling<span dir="rtl">)</span>**
<span dir="rtl">في مثال **المشي العشوائي ذو الـ 1000 حالة**
</span>**(1000-state Random Walk)**<span dir="rtl">.</span>

<span dir="rtl">ميزة عملية فورية لـ **ترميز التجزئة**</span> **(Tile
Coding)** <span dir="rtl">هي أنه، نظرًا لأنه يعمل مع **التجزئات**
</span>**(Partitions)**<span dir="rtl">، فإن العدد الإجمالي
**للميزات**</span> **(Features)** <span dir="rtl">النشطة في وقت واحد هو
نفسه لأي **حالة** </span>**(State)**<span dir="rtl">. توجد
**ميزة**</span> **(Feature)** <span dir="rtl">واحدة فقط في كل **تجزئة**
</span>**(Tiling)**<span dir="rtl">، لذا فإن العدد الإجمالي **للميزات**
</span>**(Features)** <span dir="rtl">النشطة دائمًا هو نفسه عدد
**التجزئات** </span>**(Tilings)**<span dir="rtl">.</span>
<span dir="rtl">يتيح هذا تعيين **بارامتر حجم الخطوة**
</span>**(Step-size Parameter)**<span dir="rtl">،</span>
$`\alpha`$<span dir="rtl">، بطريقة سهلة وبديهية. على سبيل المثال،
اختيار</span> $`\alpha = 1n\ `$​<span dir="rtl">، حيث</span> $`n`$
<span dir="rtl">هو عدد **التجزئات**
</span>**(Tilings)**<span dir="rtl">، يؤدي إلى تعلم تام في تجربة واحدة.
إذا تم التدريب على المثال</span> $`s \rightarrow vs`$<span dir="rtl">،
فإن التقدير الجديد</span> $`v\hat{}(s,wt + 1) = v`$<span dir="rtl">، بغض
النظر عن التقدير السابق</span> $`v\hat{}(s,wt)`$<span dir="rtl">. عادةً
ما يرغب المرء في التغيير بشكل أبطأ من هذا، للسماح **بالتعميم**</span>
**(Generalization)** <span dir="rtl">والتغير العشوائي في القيم
المستهدفة. على سبيل المثال، قد تختار</span>
$`\alpha = 10n1`$​<span dir="rtl">، وفي هذه الحالة يتحرك التقدير **للحالة
المدربة**</span> **(Trained State)** <span dir="rtl">عُشر الطريق نحو
الهدف في تحديث واحد، وسيتم تحريك **الحالات المجاورة**</span>
**(Neighboring States)** <span dir="rtl">أقل، بشكل يتناسب مع عدد
**البلاطات**</span> **(Tiles)** <span dir="rtl">المشتركة بينها</span>.

<img src="./media/image109.png"
style="width:5.60049in;height:3.06693in" />

<span dir="rtl">**<u>الشكل 9.10</u>**: لماذا نستخدم **الترميز الخشن**
</span>**(Coarse Coding)**<span dir="rtl">.</span> <span dir="rtl">يظهر
في الشكل **منحنيات التعليم**</span> **(Learning Curves)**
<span dir="rtl">على مثال **المشي العشوائي ذو الـ 1000 حالة**</span>
**<span dir="rtl">  
(</span>1000-state <span dir="rtl"></span>Random
Walk<span dir="rtl">)</span>** <span dir="rtl">لخوارزمية **تدرج مونت
كارلو**</span> **<span dir="rtl">(</span>Gradient Monte Carlo
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**
<span dir="rtl">مع **تجزئة واحدة**</span> **(Single Tiling)**
<span dir="rtl">ومع **تجزئات متعددة** </span>**(Multiple
Tilings)**<span dir="rtl">.</span> <span dir="rtl">تم التعامل مع فضاء
**الحالات الـ 1000**</span> **(1000 States)** <span dir="rtl">كأنه بُعد
واحد مستمر، مغطى ببلاطات عرض كل منها 200 حالة. كانت **التجزئات
المتعددة**</span> **(Multiple Tilings)** <span dir="rtl">متحاذية من
بعضها البعض بمقدار 4 حالات. تم ضبط **بارامتر حجم الخطوة**</span>
**(Step-size Parameter)** <span dir="rtl">بحيث يكون معدل التعليم الأولي
في الحالتين هو نفسه،</span> α=0.0001 <span dir="rtl">للتجزئة **الواحدة**
</span>**(Single Tiling)** <span dir="rtl">و</span>α=0.0001/50
<span dir="rtl">للـ **50 تجزئة** </span>**(50
Tilings)**<span dir="rtl">.</span>

<span dir="rtl">يحقق **ترميز التجزئة**</span> **(Tile Coding)**
<span dir="rtl">أيضًا مزايا حسابية من خلال استخدام **متجهات الميزات
الثنائية** </span>**(Binary Feature Vectors)**<span dir="rtl">.</span>
<span dir="rtl">نظرًا لأن كل مكون إما 0 أو 1، فإن المجموع الموزون الذي
يشكل **دالة القيمة التقريبية**</span> **(Approximate Value Function)**
<span dir="rtl">(المعادلة 9.8) يكون حسابه شبه تافه. بدلًا من إجراء</span>
$`d`$ <span dir="rtl">عملية ضرب وجمع، يتم ببساطة حساب مؤشرات **الميزات
النشطة  
(**</span>**Active <span dir="rtl"></span>Features<span dir="rtl">)
</span>**<span dir="rtl">وعددها</span> $`\ n \ll dn`$<span dir="rtl">ومن
ثم جمع المكونات المقابلة في **متجه الوزن  
(**</span>**Weight
<span dir="rtl"></span>Vector<span dir="rtl">)</span>**.

<span dir="rtl">يحدث **التعميم**</span> **(Generalization)**
<span dir="rtl">إلى **حالات**</span> **(States)** <span dir="rtl">أخرى
غير الحالة التي تم تدريبها إذا كانت تلك **الحالات**</span> **(States)**
<span dir="rtl">تقع داخل أي من **البلاطات**</span> **(Tiles)**
<span dir="rtl">نفسها، بشكل يتناسب مع عدد **البلاطات**
</span>**(Tiles)** <span dir="rtl">المشتركة بينها. حتى اختيار كيفية
تحاذي **التجزئات**</span> **(Tilings)** <span dir="rtl">من بعضها البعض
يؤثر على **التعميم** </span>**(Generalization)**<span dir="rtl">.</span>
<span dir="rtl">إذا كانت **التجزئات**</span> **(Tilings)**
<span dir="rtl">متحاذية بشكل منتظم في كل بُعد، كما كانت في **الشكل** 9.9،
فإن **الحالات المختلفة**</span> **(Different States)**
<span dir="rtl">يمكن أن تتعمم بطرق مختلفة نوعيًا، كما هو موضح في النصف
العلوي من **الشكل** 9.11. كل واحدة من الأشكال الثمانية الفرعية تظهر نمط
**التعميم**</span> **(Generalization)** <span dir="rtl">من **حالة
مدربة**</span> **(Trained State)** <span dir="rtl">إلى نقاط مجاورة. في
هذا المثال، هناك ثماني **تجزئات** </span>**(Tilings)**<span dir="rtl">،
وبالتالي 64 **منطقة فرعية داخل بلاطة**</span>
**<span dir="rtl">(</span>Subregions <span dir="rtl"></span>within a
Tile<span dir="rtl">) </span>**<span dir="rtl">تتعمم بطرق مختلفة، لكن
جميعها وفقًا لأحد هذه الأنماط الثمانية. لاحظ كيف تؤدي **التحاذيات
المنتظمة**</span> **(Uniform Offsets)** <span dir="rtl">إلى تأثير قوي
على القطر في العديد من الأنماط. يمكن تجنب هذه التشوهات إذا كانت
**التجزئات**</span> **(Tilings)** <span dir="rtl">متحاذية بشكل غير
متماثل، كما هو موضح في النصف السفلي من الشكل. أنماط **التعميم**</span>
**(Generalization Patterns)** <span dir="rtl">السفلية أفضل لأنها جميعها
متمركزة بشكل جيد على **الحالة المدربة**</span> **(Trained State)**
<span dir="rtl">دون أي تماثلات واضحة</span>.

<img src="./media/image110.png"
style="width:6.26806in;height:4.53889in" />

<span dir="rtl">**<u>الشكل 9.11</u>**: لماذا يُفضل استخدام **التحاذيات
غير المتماثلة**</span> **(Asymmetrical Offsets)** <span dir="rtl">في
**ترميز التجزئة** </span>**(Tile Coding)**<span dir="rtl">.</span>
<span dir="rtl">يظهر في الشكل قوة **التعميم**</span>
**(Generalization)** <span dir="rtl">من **حالة مدربة** </span>**(Trained
State)**<span dir="rtl">، المشار إليها بعلامة زائد سوداء صغيرة، إلى
**الحالات القريبة  
(**</span>**Nearby
<span dir="rtl"></span>States<span dir="rtl">)</span>**<span dir="rtl">،
في حالة استخدام ثماني **تجزئات**
</span>**(Tilings)**<span dir="rtl">.</span> <span dir="rtl">إذا كانت
**التجزئات** </span>**(Tilings)** <span dir="rtl">متحاذية بشكل منتظم (في
الجزء العلوي)، تظهر تشوهات قطرية وتباينات كبيرة في **التعميم**
</span>**(Generalization)**<span dir="rtl">، بينما مع **التجزئات
المتحاذية بشكل غير متماثل**</span>
**<span dir="rtl">(</span>Asymmetrically <span dir="rtl"></span>Offset
Tilings<span dir="rtl">) </span>**<span dir="rtl">يكون
**التعميم**</span> **(Generalization)** <span dir="rtl">أكثر كروية
ومتجانسًا</span>.

**<span dir="rtl">التجزئات</span> (Tilings)** <span dir="rtl">في جميع
الحالات تكون متحاذية من بعضها البعض بجزء من عرض **البلاطة**
</span>**(Tile Width)** <span dir="rtl">في كل بعد. إذا كان</span> $`w`$
<span dir="rtl">يمثل عرض البلاطة و</span>$`n`$ <span dir="rtl">عدد
**التجزئات** </span>**(Tilings)**<span dir="rtl">، فإن</span> $`wn`$
<span dir="rtl">هو وحدة أساسية. داخل المربعات الصغيرة</span> $`wn`$
<span dir="rtl">على الجوانب، جميع **الحالات**</span> **(States)**
<span dir="rtl">تنشط نفس **البلاطات**
</span>**(Tiles)**<span dir="rtl">، ولديها نفس تمثيل **الميزة**
</span>**(Feature Representation)**<span dir="rtl">، ونفس القيمة
التقريبية. إذا تم تحريك **حالة**</span> **(State)**
<span dir="rtl">بمقدار</span> $`wn`$ <span dir="rtl">في أي اتجاه
ديكارتي، فإن تمثيل **الميزة  
(**</span>**Feature
<span dir="rtl"></span>Representation<span dir="rtl">)</span>**
<span dir="rtl">يتغير بمكون/بلاطة واحدة</span>.

**<span dir="rtl">التجزئات المنتظمة</span> (Uniformly Offset Tilings)**
<span dir="rtl">تكون متحاذية من بعضها البعض بالضبط بمقدار هذه المسافة
الوحدوية. بالنسبة لفضاء ثنائي الأبعاد، نقول إن كل **تجزئة**</span>
**(Tiling)** <span dir="rtl">متحاذية باتجاه **متجه الإزاحة**
</span>**(Displacement Vector)** (1, 1)<span dir="rtl">، مما يعني أنها
متحاذية من **التجزئة السابقة** </span>**(Previous Tiling)**
<span dir="rtl">بمقدار</span> $`wn`$ <span dir="rtl">مضروبًا بهذا المتجه.
بهذه المصطلحات، تكون **التجزئات غير المتماثلة**</span> **(Asymmetrically
Offset Tilings)** <span dir="rtl">الموضحة في الجزء السفلي من الشكل 9.11
متحاذية باتجاه **متجه الإزاحة** </span>**(Displacement Vector)** (1,
3)<span dir="rtl">.</span>

<span dir="rtl">أُجريت دراسات موسعة على تأثير **متجهات الإزاحة
المختلفة**</span> **<span dir="rtl">(</span>Different Displacement
<span dir="rtl"></span>Vectors<span dir="rtl">)
</span>**<span dir="rtl">على **التعميم**</span> **(Generalization)**
<span dir="rtl">في **ترميز التجزئة**</span> **(Tile Coding)**
<span dir="rtl">(</span>Parks and <span dir="rtl"></span>Militzer, 1991;
An, 1991; An, Miller and Parks, 1991; Miller, An, Glanz and Carter,
1990<span dir="rtl">)، حيث قيّموا تجانسها وميلها نحو التشوهات القطرية مثل
تلك التي تظهر مع **متجهات الإزاحة**</span> **(Displacement Vectors)**
(1, 1). <span dir="rtl">استنادًا إلى هذا العمل، يوصي</span> **Miller
<span dir="rtl">و</span>Glanz** (1996) <span dir="rtl">باستخدام **متجهات
الإزاحة**</span> **(Displacement Vectors)** <span dir="rtl">التي تتكون
من الأعداد الفردية الأولى. على وجه الخصوص، بالنسبة لفضاء مستمر من
بعد</span> $`k`$<span dir="rtl">، فإن الاختيار الجيد هو استخدام الأعداد
الفردية الأولى (1, 3, 5, 7, ...) مع ضبط</span> $`n`$
<span dir="rtl">(عدد **التجزئات**
</span>**(Tilings)**<span dir="rtl">)</span> <span dir="rtl">على قوة
عددية صحيحة من 2 أكبر من أو تساوي</span> 4k<span dir="rtl">.</span>
<span dir="rtl">هذا ما قمنا به لإنتاج **التجزئات**</span> **(Tilings)**
<span dir="rtl">في النصف السفلي من **الشكل** 9.11، حيث</span>
$`k = 2`$<span dir="rtl">،</span> $`n = 23 \geq 4k`$<span dir="rtl">،
و**متجه الإزاحة** </span>**(Displacement Vector)** <span dir="rtl">هو
(1, 3). في حالة ثلاثية الأبعاد، فإن **التجزئات الأربعة الأولى**</span>
**(First Four Tilings)** <span dir="rtl">ستكون متحاذية في المجموع من
موضع أساسي بمقدار (0, 0, 0)، (1, 3, 5)، (2, 6, 10)، و(3, 9, 15).
البرمجيات المفتوحة المصدر التي يمكنها إنشاء **التجزئات**</span>
**(Tilings)** <span dir="rtl">بكفاءة مثل هذه لأي قيمة</span> k
<span dir="rtl">متاحة بسهولة</span>.

<span dir="rtl">عند اختيار استراتيجية **التجزئة**
</span>**(Tiling)**<span dir="rtl">، يتعين عليك تحديد عدد
**التجزئات**</span> **(Tilings)** <span dir="rtl">وشكل **البلاطات**
</span>**(Tiles)**<span dir="rtl">.</span> <span dir="rtl">يحدد عدد
**التجزئات**</span> **(Tilings)** <span dir="rtl">مع حجم **البلاطات**
</span>**(Tiles)** <span dir="rtl">الدقة أو النعومة **للتقريب النهائي**
</span>**(Asymptotic Approximation)**<span dir="rtl">، كما هو الحال في
**الترميز الخشن**</span> **<span dir="rtl">(</span>Coarse
<span dir="rtl"></span>Coding<span dir="rtl">)
</span>**<span dir="rtl">الموضح في **الشكل** 9.8. سيحدد شكل
**البلاطات**</span> **(Tiles)** <span dir="rtl">طبيعة **التعميم**
</span>**(Generalization)** <span dir="rtl">كما في **الشكل** 9.7. ستعمم
**البلاطات المربعة**</span> **(Square Tiles)** <span dir="rtl">بشكل
متساوٍ تقريبًا في كل بُعد كما هو موضح في الشكل 9.11 (الجزء السفلي). ستعزز
**البلاطات الممدودة** </span>**(Elongated Tiles)** <span dir="rtl">على
طول بُعد واحد، مثل **تجزئات الشريط**</span> **(Stripe Tilings)**
<span dir="rtl">في **الشكل** 9.12 (الوسط)، **التعميم**</span>
**(Generalization)** <span dir="rtl">على طول ذلك البُعد. تكون
**التجزئات**</span> **(Tilings)** <span dir="rtl">في **الشكل** 9.12
(الوسط) أيضًا أكثر كثافة وأرفع على اليسار، مما يعزز التمييز على طول البُعد
الأفقي عند القيم الأدنى على طول ذلك البُعد</span>.
**<span dir="rtl">تجزئة الشريط القطري</span> (Diagonal Stripe Tiling)**
<span dir="rtl">في الشكل 9.12 (اليمين) ستعزز **التعميم**</span>
**(Generalization)** <span dir="rtl">على طول قطر واحد. في الأبعاد
الأعلى، تتوافق **الأشرطة المحاذية للمحور**</span> **(Axis-Aligned
Stripes)** <span dir="rtl">مع تجاهل بعض الأبعاد في بعض **التجزئات**
</span>**(Tilings)**<span dir="rtl">، أي إلى شرائح فوق مسطحة</span>
(Hyperplanar Slices)<span dir="rtl">.</span> <span dir="rtl">من الممكن
أيضًا وجود **تجزئات غير منتظمة**</span> **(Irregular Tilings)**
<span dir="rtl">مثل تلك الموضحة في الشكل 9.12 (اليسار)، على الرغم من
أنها نادرة في الممارسة وعادة ما تكون خارج نطاق البرمجيات
القياسية</span>.

<img src="./media/image111.png"
style="width:6.26806in;height:1.97569in" />

<span dir="rtl">**<u>الشكل 9.12</u>**: ليست هناك حاجة لأن تكون
**التجزئات**</span> **(Tilings)** <span dir="rtl">على شكل شبكات. يمكن أن
تكون ذات أشكال عشوائية وغير منتظمة، ومع ذلك تكون في العديد من الحالات
فعالة من الناحية الحسابية</span>.

<span dir="rtl">في الممارسة العملية، غالبًا ما يكون من المرغوب استخدام
**بلاطات**</span> **(Tiles)** <span dir="rtl">بأشكال مختلفة في
**تجزئات** </span>**(Tilings)** <span dir="rtl">مختلفة. على سبيل المثال،
قد يستخدم البعض **تجزئات شريطية عمودية  
(**</span>**Vertical <span dir="rtl"></span>Stripe
Tilings<span dir="rtl">)</span>** <span dir="rtl">وأخرى **شريطية أفقية**
</span>**(Horizontal Stripe Tilings)**<span dir="rtl">.</span>
<span dir="rtl">هذا من شأنه أن يشجع **التعميم**</span>
**(Generalization)** <span dir="rtl">على طول أي من البعدين. ومع ذلك،
باستخدام **التجزئات الشريطية**</span> **(Stripe Tilings)**
<span dir="rtl">وحدها، لا يمكن تعلم أن اقترانًا معينًا من الإحداثيات
الأفقية والعمودية له قيمة مميزة (ما يتم تعلمه لها سوف ينتشر إلى
**الحالات**</span> **(States)** <span dir="rtl">ذات الإحداثيات الأفقية
والعمودية نفسها). لتحقيق ذلك، يحتاج المرء إلى **بلاطات مستطيلة
متقاربة**</span> **<span dir="rtl">(</span>Conjunctive Rectangular
<span dir="rtl"></span>Tiles<span dir="rtl">)</span>**
<span dir="rtl">مثل تلك الموضحة في **الشكل** 9.9. مع وجود **تجزئات
متعددة** </span>**(Multiple Tilings)** <span dir="rtl">بعضها أفقي،
وبعضها عمودي، وبعضها متقاطع—يمكن الحصول على كل شيء: تفضيل للتعميم على
طول كل بُعد، ومع ذلك القدرة على تعلم قيم محددة للاقترانات</span>
<span dir="rtl">(راجع</span> Sutton, 1996 <span dir="rtl">لمزيد من
الأمثلة)</span>. <span dir="rtl">يحدد اختيار **التجزئات**</span>
**(Tilings)** **<span dir="rtl">التعميم</span>
(Generalization)**<span dir="rtl">، وحتى يتم أتمتة هذا الاختيار بفعالية،
من المهم أن يتيح **ترميز التجزئة**</span> **(Tile Coding)**
<span dir="rtl">إجراء هذا الاختيار بمرونة وبطريقة منطقية للأشخاص</span>.

<span dir="rtl">حيلة مفيدة أخرى لتقليل متطلبات الذاكرة هي
**التجزئة**</span> **(Hashing)**—<span dir="rtl">وهو ضغط عشوائي زائف
متسق **لتجزئة كبيرة**</span> **(Large Tiling)** <span dir="rtl">إلى
مجموعة أصغر بكثير من **البلاطات**
</span>**(Tiles)**<span dir="rtl">.</span> <span dir="rtl">ينتج عن
**التجزئة** </span>**(Hashing)** **<span dir="rtl">بلاطات</span>
(Tiles)** <span dir="rtl">تتكون من مناطق غير متجاورة ومنفصلة موزعة
عشوائيًا في جميع أنحاء **فضاء الحالة** </span>**(State
Space)**<span dir="rtl">، لكنها لا تزال</span>
<img src="./media/image112.png"
style="width:1.35903in;height:1.35903in" /><span dir="rtl">تشكل تقسيمًا
شاملاً. على سبيل المثال، قد تتكون **بلاطة**</span> **(Tile)**
<span dir="rtl">واحدة من **البلاطات الفرعية الأربعة  
(**</span>**Four
<span dir="rtl"></span>Subtiles<span dir="rtl">)</span>**
<span dir="rtl">الموضحة إلى اليمين. من خلال **التجزئة**
</span>**(Hashing)**<span dir="rtl">، غالبًا ما يتم تقليل متطلبات الذاكرة
بعوامل كبيرة مع فقدان طفيف في الأداء. هذا ممكن لأن الدقة العالية مطلوبة
في جزء صغير فقط من **فضاء الحالة** </span>**(State
Space)**<span dir="rtl">.</span> <span dir="rtl">تحررنا
**التجزئة**</span> **(Hashing)** <span dir="rtl">من لعنة الأبعاد بمعنى
أن متطلبات الذاكرة لا تحتاج أن تكون أسية في عدد الأبعاد، بل تحتاج فقط أن
تتناسب مع الاحتياجات الفعلية للمهمة. تتضمن تطبيقات **ترميز
التجزئة**</span> **(Tile Coding)** <span dir="rtl">مفتوحة المصدر عادة
**تجزئة فعالة** </span>**(Efficient Hashing)**<span dir="rtl">.</span>

### **<u><span dir="rtl">تمرين 9.4</span>:</u>** <span dir="rtl">افترض أننا نعتقد أن أحد البعدين **للحالة**</span> **(State)** <span dir="rtl">من المرجح أن يكون له تأثير أكبر على **دالة القيمة**</span> **(Value Function)** <span dir="rtl">من الآخر، وأن **التعميم**</span> **(Generalization)** <span dir="rtl">يجب أن يكون بشكل أساسي عبر هذا البعد وليس على طوله. ما نوع **التجزئات**</span> **(Tilings)** <span dir="rtl">التي يمكن استخدامها للاستفادة من هذه المعرفة المسبقة؟</span>

<u>**9.5.5** **<span dir="rtl">دوال الأساس الشعاعي</span> (Radial Basis
Functions)**</u>

**<span dir="rtl">دوال الأساس الشعاعي</span> (Radial Basis Functions)**
(RBFs) <span dir="rtl">هي التعميم الطبيعي **للترميز الخشن**</span>
**(Coarse Coding)** <span dir="rtl">إلى **ميزات ذات قيم مستمرة**
</span>**(Continuous-Valued Features)**<span dir="rtl">. بدلاً من أن تكون
كل **ميزة**</span> **(Feature)** <span dir="rtl">إما 0 أو 1، يمكن أن
تكون أي شيء في المدى \[0, 1\]، مما يعكس درجات مختلفة لوجود **الميزة**
</span>**(Feature)**<span dir="rtl">.</span>
**<span dir="rtl">ميزة</span> RBF** <span dir="rtl">النموذجية،</span>
$`xi`$​<span dir="rtl">، لها استجابة غاوسية (على شكل جرس) تعتمد فقط على
المسافة بين **الحالة** </span>**(State)**<span dir="rtl">،</span>
$`s`$<span dir="rtl">، و**الحالة المركزية للميزة  
(**</span>**Feature’s <span dir="rtl"></span>Prototypical or Center
State<span dir="rtl">)</span>**<span dir="rtl">،</span> $`ci`$
​<span dir="rtl">، وبالنسبة إلى عرض **الميزة  
(**</span>**Feature’s
<span dir="rtl"></span>Width<span dir="rtl">)</span>**<span dir="rtl">،</span>
$`\sigma i`$<span dir="rtl">:</span>

``` math
x_{i}(s) = \exp\left( - \frac{\left| \left| s - c_{i} \right| \right|^{2}}{2\sigma_{i}^{2}} \right)
```

<span dir="rtl">يمكن بالطبع اختيار **المعيار**</span> **(Norm)**
<span dir="rtl">أو **مقياس المسافة**</span> **(Distance Metric)**
<span dir="rtl">بالطريقة التي تبدو أكثر ملاءمة **للحالات**</span>
**(States)** <span dir="rtl">والمهمة قيد البحث. يوضح الشكل أدناه مثالًا
أحادي البعد باستخدام **مقياس المسافة الإقليدية** </span>**(Euclidean
Distance Metric)**<span dir="rtl">.</span>

<img src="./media/image113.png"
style="width:4.64207in;height:1.26678in" />

<span dir="rtl">الشكل 9.13</span>: **<span dir="rtl">دوال الأساس الشعاعي
أحادية البعد</span> <span dir="rtl">(</span>One-Dimensional Radial Basis
<span dir="rtl"></span>Functions<span dir="rtl">)</span>**.

<span dir="rtl">الميزة الأساسية **لدوال الأساس الشعاعي**</span>
**(RBFs)** <span dir="rtl">مقارنةً **بالميزات الثنائية**</span> **(Binary
Features)** <span dir="rtl">هي أنها تنتج دوال تقريبية تتغير بسلاسة
وقابلة **للاشتقاق** </span>**(Differentiable)**<span dir="rtl">.</span>
<span dir="rtl">على الرغم من أن هذا يبدو جذابًا، إلا أنه في معظم الحالات
لا يحمل أهمية عملية كبيرة. ومع ذلك، تم إجراء دراسات واسعة حول دوال
الاستجابة التدريجية مثل **دوال الأساس الشعاعي**</span> **(RBFs)**
<span dir="rtl">في سياق **ترميز التجزئة**</span>
**<span dir="rtl">(</span>Tile <span dir="rtl"></span>Coding)** (An,
1991; Miller et al., 1991; An et al., 1991; Lane, Handelman and Gelfand,
1992<span dir="rtl">)</span>. <span dir="rtl">تتطلب جميع هذه الطرق زيادة
كبيرة في التعقيد الحسابي (مقارنةً بـ **ترميز التجزئة**</span> **(Tile
Coding)**<span dir="rtl">)</span> <span dir="rtl">وغالبًا ما تؤدي إلى
تقليل الأداء عندما يكون هناك أكثر من بُعدين للحالة. في الأبعاد العالية،
تصبح حواف **البلاطات**</span> **(Tiles)** <span dir="rtl">أكثر أهمية،
وقد ثبت أنه من الصعب الحصول على تنشيطات **بلاطات**</span> **(Tiles)**
<span dir="rtl">متدرجة جيدًا بالقرب من الحواف</span>.

**<span dir="rtl">شبكة دوال الأساس الشعاعي</span> (RBF Network)**
<span dir="rtl">هي **مقرب دالة خطي**</span>
**<span dir="rtl">(</span>Linear Function
<span dir="rtl"></span>Approximator<span dir="rtl">)
</span>**<span dir="rtl">يستخدم **دوال الأساس الشعاعي**</span>
**(RBFs)** <span dir="rtl">كميزات له. يتم تعريف التعليم من خلال
المعادلات (9.7) و(9.8)، تمامًا كما هو الحال في **مقربات الدوال الخطية
الأخرى**</span> **<span dir="rtl">(</span>Other Linear
<span dir="rtl"></span>Function Approximators<span dir="rtl">)</span>**.
<span dir="rtl">بالإضافة إلى ذلك، فإن بعض طرق التعليم لـ **شبكات دوال
الأساس الشعاعي**</span> **(RBF Networks)** <span dir="rtl">تغير أيضًا
مراكز وعروض الميزات، مما يجعلها ضمن نطاق **مقربات الدوال غير الخطية**
</span>**(Nonlinear Function Approximators)**<span dir="rtl">.</span>
<span dir="rtl">قد تتمكن الطرق غير الخطية من **ملاءمة**</span> **(Fit)**
<span dir="rtl">الدوال المستهدفة بدقة أكبر. الجانب السلبي لـ **شبكات
دوال الأساس الشعاعي** </span>**(RBF Networks)**<span dir="rtl">، وخاصة
**الشبكات غير الخطية** </span>**(Nonlinear RBF
Networks)**<span dir="rtl">، هو زيادة التعقيد الحسابي، وغالبًا ما تتطلب
ضبطًا يدويًا أكبر قبل أن يكون التعليم قويًا وفعالًا</span>.

### <u>**9.6 <span dir="rtl">اختيار</span>** **<span dir="rtl">بارامترات حجم الخطوة يدويًا</span> (Selecting Step-Size Parameters Manually)**</u>

<span dir="rtl">معظم طرق **الانحدار التدرجي العشوائي**</span> **(SGD)**
<span dir="rtl">تتطلب من المصمم اختيار **بارامتر حجم الخطوة**
</span>**(Step-Size Parameter)** <span dir="rtl">المناسب</span>
$`\alpha`$<span dir="rtl">.</span> <span dir="rtl">من الناحية المثالية،
ينبغي أن يتم هذا الاختيار بشكل آلي، وفي بعض الحالات تم تحقيق ذلك، ولكن
في معظم الحالات لا يزال من الشائع تعيينه يدويًا. للقيام بذلك، وللفهم
الأفضل **للخوارزميات** </span>**(Algorithms)**<span dir="rtl">، من
المفيد تطوير شعور بديهي لدور **بارامتر حجم الخطوة** </span>**(Step-Size
Parameter)**<span dir="rtl">.</span> <span dir="rtl">هل يمكننا بشكل عام
القول كيف يجب تعيينه؟</span>

<span dir="rtl">للأسف، الاعتبارات النظرية لا تقدم الكثير من المساعدة.
نظرية **التقريب العشوائي**</span> **<span dir="rtl">(</span>Stochastic
<span dir="rtl"></span>Approximation<span dir="rtl">)
</span>**<span dir="rtl">تقدم لنا شروطًا (المعادلة 2.7) على تسلسل **حجم
الخطوة**</span> **<span dir="rtl">(</span>Step-Size
<span dir="rtl"></span>Sequence<span dir="rtl">)
</span>**<span dir="rtl">المتناقص ببطء التي تكفي لضمان التقارب، لكن هذه
الشروط تميل إلى أن تؤدي إلى تعلم بطيء للغاية. الخيار التقليدي</span>
$`\alpha t = 1t`$​<span dir="rtl">، الذي ينتج **متوسطات العينة**
</span>**(Sample Averages)** <span dir="rtl">في طرق **مونت كارلو
الجدولية** </span>**(Tabular MC Methods)**<span dir="rtl">، ليس مناسبًا
لطرق **التعليم الزمني التفاضلي** </span>**(TD
Methods)**<span dir="rtl">، أو للمشكلات غير المستقرة، أو لأي طريقة
تستخدم **تقريب الدوال (**</span>**Function
<span dir="rtl"></span>Approximation<span dir="rtl">)</span>**.
<span dir="rtl">بالنسبة للطرق الخطية، هناك طرق **المربعات الصغرى
التكرارية (**</span>**Recursive <span dir="rtl"></span>Least-Squares
Methods<span dir="rtl">)</span>** <span dir="rtl">التي تعين **مصفوفة حجم
الخطوة المثلى (**</span>**Optimal Matrix <span dir="rtl"></span>Step
Size<span dir="rtl">)</span>**<span dir="rtl">، ويمكن تمديد هذه الطرق
لتشمل **التعليم الزمني التفاضلي (**</span>**Temporal-Difference
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">كما في طريقة</span> **LSTD
<span dir="rtl"></span>**<span dir="rtl">الموصوفة في الفصل 9.8، لكن هذه
تتطلب</span> $`O(d2)`$ <span dir="rtl">من **بارامترات حجم الخطوة**
</span>**(Step-Size Parameters)**<span dir="rtl">، أي</span> $`d`$
<span dir="rtl">مرة أكثر من **البارامترات**</span> **(Parameters)**
<span dir="rtl">التي نتعلمها. لهذا السبب نستبعد استخدامها في المشكلات
الكبيرة حيث تكون **تقريب الدوال**</span>
**<span dir="rtl">(</span>Function
<span dir="rtl"></span>Approximation<span dir="rtl">)
</span>**<span dir="rtl">أكثر أهمية</span>.

<span dir="rtl">لاكتساب شعور بديهي حول كيفية تعيين **بارامتر حجم
الخطوة**</span> **(Step-Size Parameter)** <span dir="rtl">يدويًا، من
الأفضل العودة لحظة إلى الحالة الجدولية. هناك يمكننا أن نفهم أن **حجم
خطوة**</span> **(Step Size)** <span dir="rtl">مقداره</span> α=1
<span dir="rtl">سيؤدي إلى القضاء التام على خطأ العينة بعد هدف واحد (انظر
المعادلة 2.4 مع **حجم خطوة** </span>**(Step Size)**
<span dir="rtl">واحد). كما نوقش في الصفحة 201، نريد عادةً أن نتعلم أبطأ
من ذلك. في الحالة الجدولية، **حجم خطوة**</span> **(Step Size)**
<span dir="rtl">مقداره</span> $`\alpha = 110`$ <span dir="rtl"></span>​
<span dir="rtl">سيستغرق حوالي 10 تجارب للتقارب تقريبًا إلى متوسط الهدف،
وإذا أردنا التعليم في 100 تجربة، فسنستخدم</span>
$`\alpha = 1100`$​<span dir="rtl">.</span> <span dir="rtl">بشكل عام، إذا
كان</span> $`\alpha = 1\tau`$​<span dir="rtl">، فإن التقدير الجدولي
**لحالة**</span> **(State)** <span dir="rtl">سيتقارب إلى متوسط أهدافه،
مع تأثير أكبر على الأهداف الأحدث، بعد حوالي</span> τ
<span dir="rtl">تجربة مع **الحالة**
</span>**(State)**<span dir="rtl">.</span>

<span dir="rtl">مع **تقريب الدوال العام** </span>**(General Function
Approximation)**<span dir="rtl">، لا يوجد مفهوم واضح لعدد التجارب مع
**حالة معينة** </span>**(State)**<span dir="rtl">، حيث يمكن أن تكون كل
**حالة**</span> **(State)** <span dir="rtl">مشابهة أو مختلفة عن جميع
الحالات الأخرى بدرجات مختلفة. ومع ذلك، هناك قاعدة مشابهة تؤدي إلى سلوك
مشابه في حالة **تقريب الدوال الخطي** </span>**(Linear Function
Approximation)**<span dir="rtl">.</span> <span dir="rtl">افترض أنك ترغب
في التعليم في حوالي</span> $`\tau`$ <span dir="rtl">تجربة مع **متجه
الميزات**</span> **(Feature Vector)** <span dir="rtl">الذي يظل ثابتًا
بشكل كبير. في هذه الحالة، قاعدة جيدة لضبط **بارامتر حجم الخطوة**</span>
**(Step-Size Parameter)** <span dir="rtl">في طرق **الانحدار التدرجي
العشوائي الخطي**</span> **(Linear SGD Methods)**
<span dir="rtl">هي</span>:

``` math
\alpha = \left( \tau E\left\lbrack x^{\top}\mathbf{x} \right\rbrack \right)^{- 1}
```

<span dir="rtl">حيث</span> $`x`$ <span dir="rtl">هو **متجه ميزات
عشوائي**</span> **(Random Feature Vector)** <span dir="rtl">يتم اختياره
من نفس التوزيع مثل **متجهات الإدخال**</span> **(Input Vectors)**
<span dir="rtl">في **الانحدار التدرجي العشوائي**
</span>**(SGD)**<span dir="rtl">.</span> <span dir="rtl">تعمل هذه
الطريقة بشكل أفضل إذا لم تتفاوت **متجهات الميزات**</span> **(Feature
Vectors)** <span dir="rtl">بشكل كبير في الطول؛ من الناحية المثالية
يكون</span> $`x\top`$ <span dir="rtl">ثابتًا</span>.

### <span dir="rtl"><u>**تمرين** **9.5**</u></span>

<span dir="rtl">افترض أنك تستخدم **ترميز التجزئة**</span> **(Tile
Coding)** <span dir="rtl">لتحويل فضاء حالة مستمر ذو سبعة أبعاد إلى
**متجهات ميزات ثنائية**</span> **(Binary Feature Vectors)**
<span dir="rtl">لتقدير **دالة قيمة الحالة**</span>
**<span dir="rtl">(</span>State Value
<span dir="rtl"></span>Function<span dir="rtl">)</span>**
<span dir="rtl"></span>$`v\hat{}(s,w) \approx v\pi(s)`$<span dir="rtl">.</span>
<span dir="rtl">تعتقد أن الأبعاد لا تتفاعل بشكل كبير، لذا قررت استخدام
ثماني **تجزئات**</span> **(Tilings)** <span dir="rtl">لكل بُعد بشكل منفصل
(تجزئات شريطية)، ليصبح المجموع</span> ×8 = 56<span dir="rtl">7 **تجزئة**
</span>**(Tilings)**<span dir="rtl">.</span> <span dir="rtl">بالإضافة
إلى ذلك، في حالة وجود بعض التفاعلات الثنائية بين الأبعاد، تأخذ جميع
أزواج الأبعاد</span> <span dir="rtl">(</span>(72)=21
<span dir="rtl"></span> <span dir="rtl">زوجًا من الأبعاد) وتجزئ كل زوج
معًا باستخدام **بلاطات مستطيلة (**</span>**Rectangular
<span dir="rtl"></span>Tiles<span dir="rtl">)</span>**.
<span dir="rtl">قمت بعمل تجزئتين لكل زوج من الأبعاد، مما يجعل المجموع
الكلي  
21</span> × <span dir="rtl"></span> 2 + 56 =
98**<span dir="rtl">تجزئة</span> (Tilings)**<span dir="rtl">.</span>
<span dir="rtl">بالنظر إلى هذه **متجهات الميزات (**</span>**Feature
<span dir="rtl"></span>Vectors<span dir="rtl">)</span>**<span dir="rtl">،
تشتبه في أنه لا يزال عليك التخلص من بعض الضوضاء، لذا قررت أنك تريد أن
يكون التعليم تدريجيًا، ويستغرق حوالي 10 تقديمات بنفس **متجه
الميزات**</span> **(Feature Vector)** <span dir="rtl">قبل أن يقترب
التعليم من حده النهائي. ما هو **بارامتر حجم الخطوة**</span> **(Step-Size
Parameter)** α <span dir="rtl">الذي يجب أن تستخدمه؟ ولماذا؟</span>

**<u>9.7 <span dir="rtl">تقريب الدوال غير الخطية</span> (Nonlinear
Function Approximation): <span dir="rtl">الشبكات العصبية
الاصطناعية</span> (Artificial Neural Networks)</u>**

**<span dir="rtl">الشبكات العصبية الاصطناعية</span> (Artificial Neural
Networks - ANNs)** <span dir="rtl">تُستخدم على نطاق واسع في **تقريب
الدوال غير الخطية** </span>**(Nonlinear Function
Approximation)**<span dir="rtl">.</span> <span dir="rtl">الشبكة العصبية
الاصطناعية</span> (ANN) <span dir="rtl">هي شبكة من الوحدات المتصلة
ببعضها البعض والتي تمتلك بعض خصائص الخلايا العصبية، المكونات الرئيسية في
الأنظمة العصبية. تمتلك الشبكات العصبية الاصطناعية تاريخًا طويلًا، وكانت
آخر التطورات في تدريب الشبكات العصبية ذات الطبقات العميقة (التعليم
العميق) مسؤولة عن بعض القدرات الأكثر إثارة للإعجاب في أنظمة تعلم الآلة،
بما في ذلك أنظمة التعليم المعزز. في **الفصل** 16 نصف العديد من الأمثلة
المثيرة للإعجاب لأنظمة التعليم المعزز التي تستخدم تقريب الدوال بالشبكات
العصبية الاصطناعية</span> (ANN)<span dir="rtl">.</span>

<span dir="rtl">يوضح **الشكل** 9.14 شبكة عصبية اصطناعية **موجهة للأمام**
</span>**(Feedforward ANN)**<span dir="rtl">، مما يعني أنه لا توجد حلقات
في الشبكة، أي أنه لا توجد مسارات داخل الشبكة يمكن من خلالها لمخرجات وحدة
معينة أن تؤثر على مدخلاتها. تحتوي الشبكة في الشكل على **طبقة مخرجات**
</span>**(Output Layer)** <span dir="rtl">تتكون من وحدتي إخراج، و**طبقة
مدخلات**</span> **(Input Layer)** <span dir="rtl">تحتوي على أربع وحدات
إدخال، وطبقتين "مخفية</span> (Hidden Layers)"<span dir="rtl">، وهي طبقات
ليست طبقة مدخلات ولا طبقة مخرجات. يتم ربط وزن ذو قيمة حقيقية مع كل رابط.
**الوزن**</span> **(Weight)** <span dir="rtl">يرتبط بشكل تقريبي بفعالية
الاتصال التشابكي في شبكة عصبية حقيقية (انظر الفصل 15.1). إذا كانت الشبكة
العصبية الاصطناعية تحتوي على حلقة واحدة على الأقل في اتصالاتها، فإنها
تعتبر شبكة عصبية اصطناعية **تكرارية**</span> **(Recurrent ANN)**
<span dir="rtl">وليست موجهة للأمام. على الرغم من استخدام كل من الشبكات
العصبية الموجهة للأمام والتكرارية في **التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">، فإننا هنا ننظر فقط
في حالة الشبكة الموجهة للأمام الأبسط</span>.

<img src="./media/image114.png"
style="width:4.93058in;height:3.75663in" />

<span dir="rtl">**<u>الشكل 9.14</u>:** شبكة عصبية اصطناعية **موجهة
للأمام**</span> **(Feedforward ANN)** <span dir="rtl">عامة تحتوي على
أربع وحدات إدخال، وحدتي إخراج، وطبقتين مخفيتين</span>.

**<span dir="rtl">الوحدات</span> (Units)** <span dir="rtl">(الدوائر في
الشكل 9.14) عادةً ما تكون **وحدات شبه خطية**</span>
**<span dir="rtl">(</span>Semi-Linear
<span dir="rtl"></span>Units<span dir="rtl">)</span>**<span dir="rtl">،
مما يعني أنها تحسب مجموعًا موزونًا من إشارات المدخلات الخاصة بها، ثم تطبق
على النتيجة **دالة غير خطية**</span> **(Nonlinear Function)**
<span dir="rtl">تسمى **دالة التنشيط** </span>**(Activation Function)**
<span dir="rtl">لإنتاج مخرجات الوحدة أو **التنشيط**</span>
**(Activation)** <span dir="rtl">الخاص بها. تُستخدم **دوال تنشيط  
(**</span>**Activation Functions<span dir="rtl">)</span>**
<span dir="rtl">مختلفة، لكنها عادةً ما تكون على شكل حرف</span>
$`S`$<span dir="rtl">، أو **دوال سيغمودية** </span>**(Sigmoid
Functions)** <span dir="rtl">مثل **دالة اللوغاريتم الطبيعي**
</span>**(Logistic Function)<span dir="rtl">  
</span>** $`f(x) = 11 + e - x`$ ​<span dir="rtl">، على الرغم من أنه
أحيانًا تُستخدم **دالة التنشيط القطعي  
(**</span>**Rectifier Nonlinearity<span dir="rtl">)</span>**
$`f(x) = max(0,x)`$ **<span dir="rtl">دالة الخطوة</span> (Step
Function)** <span dir="rtl">مثل</span> $`f(x) = 1`$
<span dir="rtl"></span> <span dir="rtl">إذا كانت</span>
$`x \geq \theta x`$<span dir="rtl">، و0 في الحالة الأخرى، تنتج **وحدة
ثنائية**</span> **(Binary Unit)** <span dir="rtl">مع حد</span>
θ<span dir="rtl">.الوحدات في **طبقة المدخلات**</span> **(Input Layer)**
<span dir="rtl">للشبكة تكون مختلفة قليلاً حيث يتم ضبط تنشيطها على القيم
المقدمة خارجيًا والتي تكون مدخلات الدالة التي تقوم الشبكة
بتقريبها</span>.

**<span dir="rtl">تنشيط</span> (Activation)** <span dir="rtl">كل وحدة
إخراج في **شبكة عصبية اصطناعية موجهة للأمام**</span> **(Feedforward
ANN)** <span dir="rtl">هو **دالة غير خطية**</span> **(Nonlinear
Function)** <span dir="rtl">لأنماط التنشيط عبر وحدات المدخلات للشبكة.
يتم تقييس هذه الدوال بواسطة **أوزان اتصالات الشبكة**</span>
**<span dir="rtl">(</span>Connection
<span dir="rtl"></span>Weights<span dir="rtl">)</span>**.
<span dir="rtl">يمكن **لشبكة عصبية اصطناعية بدون طبقات مخفية**</span>
**<span dir="rtl">(</span>ANN with No Hidden
<span dir="rtl"></span>Layers<span dir="rtl">)
</span>**<span dir="rtl">أن تمثل فقط جزءًا صغيرًا جدًا من الدوال الممكنة
بين المدخلات والمخرجات. ومع ذلك، يمكن **لشبكة عصبية اصطناعية تحتوي على
طبقة مخفية واحدة**</span> **<span dir="rtl">(</span>ANN with a Single
<span dir="rtl"></span>Hidden Layer<span dir="rtl">)
</span>**<span dir="rtl">تحتوي على عدد كافٍ من **الوحدات
السيغمودية**</span> **(Sigmoid Units)** <span dir="rtl">أن تقرب أي
**دالة مستمرة**</span> **(Continuous Function)** <span dir="rtl">على
نطاق مضغوط من **فضاء المدخلات  
(**</span>**Input <span dir="rtl"></span>Space<span dir="rtl">)
</span>**<span dir="rtl">الخاص بالشبكة بأي درجة من الدقة</span>
<span dir="rtl">(</span>Cybenko<span dir="rtl">، 1989)</span>.
<span dir="rtl">ينطبق هذا أيضًا على **دوال التنشيط غير الخطية**</span>
**(Nonlinear Activation Functions)** <span dir="rtl">الأخرى التي تفي
بشروط معتدلة، لكن **اللاخطية**</span> **(Nonlinearity)**
<span dir="rtl">أساسية: إذا كانت جميع الوحدات في **شبكة عصبية اصطناعية
متعددة الطبقات**</span> **(Multi-Layer Feedforward ANN)**
<span dir="rtl">تحتوي على **دوال تنشيط خطية** </span>**(Linear
Activation Functions)**<span dir="rtl">، فإن الشبكة بأكملها تعادل **شبكة
بدون طبقات مخفية** </span>**(Network with No Hidden Layers)**
<span dir="rtl">(لأن الدوال الخطية للدوال الخطية تكون بنفسها
خطية)</span>.

<span dir="rtl">على الرغم من خاصية "التقريب الشامل</span> (Universal
Approximation)" **<span dir="rtl">لشبكات عصبية اصطناعية تحتوي على طبقة
مخفية واحدة</span> (One-Hidden-Layer ANNs)**<span dir="rtl">، فإن
التجربة والنظرية تظهران أن تقريب **الدوال المعقدة**</span> **(Complex
Functions)** <span dir="rtl">المطلوبة للعديد من مهام **الذكاء
الاصطناعي** </span>**(Artificial Intelligence)** <span dir="rtl">يصبح
أسهل—وقد يتطلب بالفعل</span>—**<span dir="rtl">تجريدات</span>
(Abstractions)** <span dir="rtl">هي **تركيبات هرمية**</span>
**(Hierarchical Compositions)** <span dir="rtl">من طبقات متعددة من
التجريدات ذات المستوى الأدنى، أي **التجريدات**</span> **(Abstractions)**
<span dir="rtl">التي تنتجها **المعماريات العميقة  
(**</span>**Deep <span dir="rtl"></span>Architectures<span dir="rtl">)
</span>**<span dir="rtl">مثل **الشبكات العصبية الاصطناعية التي تحتوي على
طبقات مخفية متعددة** </span>**(ANNs with Many Hidden
Layers)**<span dir="rtl">. (انظر</span> Bengio<span dir="rtl">، 2009،
لمراجعة شاملة)</span>. <span dir="rtl">تحسب **الطبقات المتعاقبة**</span>
**(Successive Layers)** <span dir="rtl">من **شبكة عصبية اصطناعية عميقة  
(**</span>**Deep <span dir="rtl"></span>ANN<span dir="rtl">)</span>**
<span dir="rtl">تمثيلات متزايدة التجريد من</span>
**“<span dir="rtl">المدخلات الأولية</span> (Raw Input)**
“<span dir="rtl">للشبكة، حيث توفر كل وحدة **ميزة**</span> **(Feature)**
<span dir="rtl">تساهم في **تمثيل هرمي**</span> **(Hierarchical
Representation)** **<span dir="rtl">لدالة المدخلات والمخرجات
العامة</span> (Overall Input-Output Function)**
<span dir="rtl">للشبكة</span>.

<span dir="rtl">لذلك، يعد تدريب **الطبقات المخفية**</span> **(Hidden
Layers)** **<span dir="rtl">لشبكة عصبية اصطناعية</span> (ANN)**
<span dir="rtl">طريقة لإنشاء **ميزات**</span> **(Features)**
<span dir="rtl">مناسبة لمشكلة معينة تلقائيًا، بحيث يمكن إنتاج **التمثيلات
الهرمية** </span>**(Hierarchical Representations)** <span dir="rtl">دون
الاعتماد بشكل حصري على **الميزات المصنوعة يدويًا** </span>**(Hand-Crafted
Features)**<span dir="rtl">.</span> <span dir="rtl">لقد كان هذا تحديًا
مستمرًا **للذكاء الاصطناعي  
(**</span>**Artificial
<span dir="rtl"></span>Intelligence<span dir="rtl">)</span>**
<span dir="rtl">ويفسر سبب تلقي **خوارزميات التعليم**</span> **(Learning
Algorithms)** **<span dir="rtl">للشبكات العصبية الاصطناعية التي تحتوي
على طبقات مخفية</span> (ANNs with Hidden Layers)**
<span dir="rtl">الكثير من الاهتمام على مر السنين. تتعلم **الشبكات
العصبية الاصطناعية**</span> **(ANNs)** <span dir="rtl">عادةً بطريقة
**التدرج العشوائي**</span> **(Stochastic Gradient Method)**
<span dir="rtl">(القسم 9.3). يتم تعديل كل **وزن** </span>**(Weight)**
<span dir="rtl">في اتجاه يهدف إلى تحسين الأداء العام للشبكة كما يُقاس
بواسطة **دالة هدف** </span>**(Objective Function)** <span dir="rtl">إما
أن يتم تقليلها أو تعظيمها. في حالة **التعليم تحت الإشراف**
</span>**(Supervised Learning)** <span dir="rtl">الأكثر شيوعًا، تكون
**دالة الهدف**</span> **(Objective Function)** <span dir="rtl">هي
**الخطأ المتوقع** </span>**(Expected Error)**<span dir="rtl">، أو
**الخسارة** </span>**(Loss)**<span dir="rtl">، على مجموعة من **أمثلة
التدريب الموسومة** </span>**(Labeled Training
Examples)**<span dir="rtl">.</span> <span dir="rtl">في **التعليم
المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**<span dir="rtl">،
يمكن **للشبكات العصبية الاصطناعية**</span> **(ANNs)**
<span dir="rtl">استخدام **أخطاء**</span> **TD** <span dir="rtl">لتعلم
**دوال القيمة** </span>**(Value Functions)**<span dir="rtl">، أو يمكنها
السعي لتعظيم **المكافأة المتوقعة**</span> **(Expected Reward)**
<span dir="rtl">كما في **خوارزمية الحزمة التدرجية**</span> **(Gradient
Bandit)** <span dir="rtl">(القسم 2.8) أو **خوارزمية تدرج السياسة**
</span>**(Policy-Gradient Algorithm)** <span dir="rtl">(الفصل 13). في
جميع هذه الحالات، من الضروري تقدير كيفية تأثير تغيير في كل **وزن
اتصال**</span> **(Connection Weight)** <span dir="rtl">على الأداء العام
للشبكة، بعبارة أخرى، تقدير **المشتق الجزئي**</span> **(Partial
Derivative)** **<span dir="rtl">لدالة الهدف</span> (Objective
Function)** <span dir="rtl">بالنسبة لكل وزن، مع القيم الحالية لجميع
أوزان الشبكة</span>. **<span dir="rtl">التدرج</span> (Gradient)**
<span dir="rtl">هو **متجه**</span> **(Vector)** <span dir="rtl">هذه
**المشتقات الجزئية** </span>**(Partial
Derivatives)**<span dir="rtl">.</span>

<span dir="rtl">الطريقة الأكثر نجاحًا للقيام بذلك بالنسبة **للشبكات
العصبية الاصطناعية التي تحتوي على طبقات مخفية**</span> **(ANNs with
Hidden Layers)** (<span dir="rtl">بشرط أن تحتوي الوحدات على **دوال تنشيط
قابلة للاشتقاق**</span> **(Differentiable Activation Functions)**)
<span dir="rtl">هي **خوارزمية الانتشار العكسي**
</span>**(Backpropagation Algorithm)**<span dir="rtl">، التي تتألف من
**تمريرات متناوبة للأمام والخلف** </span>**(Alternating Forward and
Backward Passes)** <span dir="rtl">عبر الشبكة. تحسب كل **تمريرة للأمام**
</span>**(Forward Pass)** **<span dir="rtl">تنشيط</span> (Activation)**
<span dir="rtl">كل وحدة بالنظر إلى **التنشيطات الحالية  
(**</span>**Current
<span dir="rtl"></span>Activations<span dir="rtl">)</span>**
**<span dir="rtl">لوحدات المدخلات</span> (Input Units)**
<span dir="rtl">للشبكة. بعد كل **تمريرة للأمام  
(**</span>**Forward
<span dir="rtl"></span>Pass<span dir="rtl">)</span>**<span dir="rtl">،
تحسب **التمريرة الخلفية**</span> **(Backward Pass)**
<span dir="rtl">بكفاءة **مشتقًا جزئيًا  
(**</span>**Partial
<span dir="rtl"></span>Derivative<span dir="rtl">)</span>**
<span dir="rtl">لكل وزن. ((كما هو الحال في **خوارزميات التعليم الأخرى
باستخدام التدرج العشوائي** </span>**(Other Stochastic Gradient Learning
Algorithms)**<span dir="rtl">، يعد **متجه هذه المشتقات الجزئية**</span>
**(Vector of These Partial Derivatives)** <span dir="rtl">تقديرًا للتدرج
الحقيقي)). في القسم 15.10، نناقش طرقًا لتدريب **الشبكات العصبية
الاصطناعية التي تحتوي على طبقات مخفية  
(**</span>**ANNs with <span dir="rtl"></span>Hidden
Layers<span dir="rtl">)</span>** <span dir="rtl">باستخدام مبادئ
**التعليم المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">بدلاً من **الانتشار العكسي**
</span>**(Backpropagation)**<span dir="rtl">.</span>
<span dir="rtl">تكون هذه الطرق أقل كفاءة من **خوارزمية الانتشار العكسي**
</span>**(Backpropagation Algorithm)**<span dir="rtl">، لكنها قد تكون
أقرب إلى كيفية تعلم **الشبكات العصبية الحقيقية** </span>**(Real Neural
Networks)**<span dir="rtl">.</span>

<span dir="rtl">يمكن أن تنتج **خوارزمية الانتشار العكسي**</span>
**(Backpropagation Algorithm)** <span dir="rtl">نتائج جيدة **للشبكات
الضحلة**</span> **(Shallow Networks)** <span dir="rtl">التي تحتوي على 1
أو 2 من **الطبقات المخفية**</span> **<span dir="rtl">(</span>Hidden
<span dir="rtl"></span>Layers<span dir="rtl">)</span>**<span dir="rtl">،
لكنها قد لا تعمل بشكل جيد **للشبكات العصبية الاصطناعية الأعمق**
</span>**(Deeper ANNs)**<span dir="rtl">. في الواقع، قد يؤدي تدريب
**شبكة تحتوي على** </span>$`\mathbf{k + 1}`$ **<span dir="rtl">من
الطبقات المخفية  
(</span>Network <span dir="rtl"></span>with** $`\mathbf{k + 1}`$
**Hidden Layers<span dir="rtl">) </span>**<span dir="rtl">إلى أداء أسوأ
من تدريب **شبكة تحتوي على** </span>$`\mathbf{k}`$ **<span dir="rtl">من
الطبقات المخفية</span> (Network with k Hidden Layers)**<span dir="rtl">،
على الرغم من أن **الشبكة الأعمق** </span>**(Deeper Network)**
<span dir="rtl">يمكنها تمثيل جميع الدوال التي يمكن **للشبكة الضحلة  
(**</span>**Shallower
<span dir="rtl"></span>Network<span dir="rtl">)</span>**
<span dir="rtl">تمثيلها (</span>Bengio<span dir="rtl">، 2009)</span>.
<span dir="rtl">ليس من السهل شرح نتائج مثل هذه، ولكن هناك عدة عوامل
مهمة. أولاً، العدد الكبير **للأوزان**</span> **(Weights)**
<span dir="rtl">في **شبكة عصبية اصطناعية عميقة نموذجية**</span>
**(Typical Deep ANN)** <span dir="rtl">يجعل من الصعب تجنب مشكلة
**الإفراط في التخصيص** </span>**(Overfitting)**<span dir="rtl">، أي
مشكلة الفشل في **التعميم بشكل صحيح**</span> **(Generalization)**
<span dir="rtl">على الحالات التي لم يتم تدريب الشبكة عليها. ثانيًا، لا
يعمل **الانتشار العكسي**</span> **(Backpropagation)**
<span dir="rtl">بشكل جيد **للشبكات العصبية الاصطناعية العميقة**</span>
**(Deep ANNs)** <span dir="rtl">لأن **المشتقات الجزئية  
(**</span>**Partial
<span dir="rtl"></span>Derivatives<span dir="rtl">)</span>**
<span dir="rtl">المحسوبة بواسطة **التمريرات الخلفية**</span> **(Backward
Passes)** <span dir="rtl">إما تتناقص بسرعة نحو جانب المدخلات من الشبكة،
مما يجعل التعليم بواسطة **الطبقات العميقة  
(**</span>**Deep <span dir="rtl"></span>Layers<span dir="rtl">)</span>**
<span dir="rtl">بطيئًا للغاية، أو أن **المشتقات الجزئية**</span>
**(Partial Derivatives)** <span dir="rtl">تتزايد بسرعة نحو جانب المدخلات
من الشبكة، مما يجعل التعليم غير مستقر. تعود الأساليب المستخدمة للتعامل
مع هذه المشكلات إلى تحقيق العديد من النتائج المثيرة للإعجاب التي حققتها
الأنظمة التي تستخدم **الشبكات العصبية الاصطناعية العميقة**
</span>**(Deep ANNs)**<span dir="rtl">.</span>

**<span dir="rtl">الإفراط في التخصيص</span> (Overfitting)**
<span dir="rtl">هو مشكلة تواجه أي طريقة لتقريب الدوال التي تقوم بتعديل
الدوال ذات درجات الحرية العديدة استنادًا إلى بيانات تدريب محدودة. تكون
هذه المشكلة أقل حدة في **التعليم المعزز عبر الإنترنت**</span> **(Online
Reinforcement Learning)** <span dir="rtl">الذي لا يعتمد على مجموعات
بيانات تدريب محدودة، ولكن التعميم الفعّال لا يزال مسألة مهمة</span>.
<span dir="rtl"></span>

**<span dir="rtl">الإفراط في التخصيص</span> (Overfitting)**
<span dir="rtl">هو مشكلة عامة **للشبكات العصبية الاصطناعية**
</span>**(ANNs)**<span dir="rtl">، ولكنه يشكل مشكلة خاصة **للشبكات
العصبية الاصطناعية العميقة**</span> **(Deep ANNs)**
<span dir="rtl">لأنها تميل إلى امتلاك عدد كبير جدًا من **الأوزان**
</span>**(Weights)**<span dir="rtl">.</span> <span dir="rtl">تم تطوير
العديد من الأساليب لتقليل **الإفراط في التخصيص**
</span>**(Overfitting)**<span dir="rtl">.</span> <span dir="rtl">تشمل
هذه الأساليب إيقاف التدريب عندما يبدأ الأداء في الانخفاض على بيانات
التحقق التي تختلف عن بيانات التدريب</span> <span dir="rtl">(**التحقق
المتقاطع** </span>**Cross-Validation**<span dir="rtl">)، وتعديل **دالة
الهدف**</span> **(Objective Function)** <span dir="rtl">لتقليل تعقيد
التقريب</span> <span dir="rtl">(**التنظيم**</span>
**Regularization–**<span dir="rtl">)، وإدخال تبعيات بين
**الأوزان**</span> **(Weights)** <span dir="rtl">لتقليل عدد درجات الحرية
(مثل **مشاركة الأوزان** </span>**- Weight
Sharing**<span dir="rtl">)</span>.

<span dir="rtl">طريقة فعّالة بشكل خاص لتقليل **الإفراط في
التخصيص**</span> **(Overfitting)** <span dir="rtl">في **الشبكات العصبية
الاصطناعية العميقة**</span> **(Deep ANNs)** <span dir="rtl">هي طريقة
**الإسقاط**</span> **(Dropout Method)** <span dir="rtl">التي
قدمها</span> **Srivastava, Hinton, Krizhevsky, Sutskever, and
Salakhutdinov** <span dir="rtl">(</span>2014<span dir="rtl">).</span>
<span dir="rtl">أثناء التدريب، يتم إزالة الوحدات من الشبكة بشكل عشوائي
(إسقاطها) مع روابطها. يمكن اعتبار هذا بمثابة تدريب عدد كبير من الشبكات
"الرقيقة". جمع نتائج هذه الشبكات الرقيقة أثناء الاختبار هو طريقة لتحسين
أداء التعميم. تقوم طريقة **الإسقاط**</span> **(Dropout Method)**
<span dir="rtl">بتقريب هذا الجمع بكفاءة من خلال ضرب كل وزن صادر عن وحدة
في احتمال بقاء تلك الوحدة أثناء التدريب. وجد</span> **Srivastava et
al.** <span dir="rtl">أن هذه الطريقة تحسن بشكل كبير من أداء التعميم.
تشجع هذه الطريقة **الوحدات المخفية**</span>
**<span dir="rtl">(</span>Hidden
<span dir="rtl"></span>Units<span dir="rtl">)
</span>**<span dir="rtl">الفردية على تعلم ميزات تعمل جيدًا مع مجموعات
عشوائية من الميزات الأخرى. هذا يزيد من تنوع الميزات التي تشكلها
**الوحدات المخفية**</span> **(Hidden Units)** <span dir="rtl">بحيث لا
تتخصص الشبكة بشكل مفرط في الحالات التي نادرًا ما تحدث</span>.

<span dir="rtl">قام</span> **Hinton, Osindero, and Teh** (2006)
<span dir="rtl">بخطوة كبيرة نحو حل مشكلة تدريب **الطبقات
العميقة**</span> **(Deep Layers)** <span dir="rtl">في **الشبكات العصبية
الاصطناعية العميقة**</span> **(Deep ANN)** <span dir="rtl">في عملهم مع
**شبكات الاعتقاد العميق** </span>**(Deep Belief
Networks)**<span dir="rtl">، وهي شبكات ذات طبقات مرتبطة ارتباطًا وثيقًا
**بالشبكات العصبية الاصطناعية العميقة**</span> **(Deep ANNs)**
<span dir="rtl">التي نناقشها هنا. في طريقتهم، يتم تدريب **الطبقات
الأعمق**</span> **(Deepest Layers)** <span dir="rtl">واحدة تلو الأخرى
باستخدام خوارزمية تعلم غير مراقب. دون الاعتماد على **دالة الهدف
الشاملة** </span>**(Overall Objective Function)**<span dir="rtl">، يمكن
**للتعليم الغير الخاضع للاشراف**</span> **(Unsupervised Learning)**
<span dir="rtl">استخراج الميزات التي تلتقط الانتظامات الإحصائية في سلسلة
المدخلات. يتم تدريب **الطبقة الأعمق**</span> **(Deepest Layer)**
<span dir="rtl">أولاً، ثم باستخدام المدخلات التي توفرها هذه الطبقة
المدربة، يتم تدريب **الطبقة الأعمق التالية  
(**</span>**Next Deepest
<span dir="rtl"></span>Layer<span dir="rtl">)</span>**<span dir="rtl">،
وهكذا، حتى يتم تعيين **الأوزان**</span> **(Weights)** <span dir="rtl">في
جميع، أو العديد، من طبقات الشبكة على قيم تعمل الآن كقيم أولية **للتعليم
الخاضع للإشراف** </span>**(Supervised Learning)**<span dir="rtl">. يتم
بعد ذلك تحسين الشبكة بواسطة **الانتشار العكسي**</span>
**(Backpropagation)** <span dir="rtl">بالنسبة **لدالة الهدف الشاملة**
</span>**(Overall Objective Function)**<span dir="rtl">.</span>
<span dir="rtl">تُظهر الدراسات أن هذا النهج يعمل عمومًا بشكل أفضل من
**الانتشار العكسي**</span> **(Backpropagation)**
<span dir="rtl">باستخدام **أوزان تم تعيينها بشكل عشوائي**
</span>**(Randomly Initialized Weights)**<span dir="rtl">.</span>
<span dir="rtl">قد يكون الأداء الأفضل للشبكات التي تم تدريبها باستخدام
أوزان تم تعيينها بهذه الطريقة ناتجًا عن العديد من العوامل، ولكن أحد
الأفكار هو أن هذه الطريقة تضع الشبكة في منطقة من **فضاء الأوزان**</span>
**(Weight Space)** <span dir="rtl">يمكن أن تحقق فيها خوارزمية تعتمد على
التدرج تقدمًا جيدًا</span>.

**<span dir="rtl">التطبيع على دفعات</span> (Batch Normalization)**
(**Ioffe and Szegedy, 2015**) <span dir="rtl">هو تقنية أخرى تجعل من
السهل تدريب **الشبكات العصبية الاصطناعية العميقة** </span>**(Deep
ANNs)**<span dir="rtl">.</span> <span dir="rtl">لقد عُرف منذ فترة طويلة
أن تعلم الشبكات العصبية الاصطناعية أسهل إذا تم **تطبيع المدخلات  
(**</span>**Input
<span dir="rtl"></span>Normalization<span dir="rtl">)</span>**
<span dir="rtl">للشبكة، على سبيل المثال، من خلال ضبط كل **متغير مدخلات  
(**</span>**Input
<span dir="rtl"></span>Variable<span dir="rtl">)</span>**
<span dir="rtl">ليكون متوسطه صفريًا وذو تباين موحد</span>.
**<span dir="rtl">التطبيع على دفعات  
(</span>Batch
<span dir="rtl"></span>Normalization<span dir="rtl">)</span>**
<span dir="rtl">لتدريب **الشبكات العصبية الاصطناعية العميقة**</span>
**(Deep ANNs)** <span dir="rtl">يقوم بتطبيع مخرجات **الطبقات
العميقة**</span> **(Deep Layers)** <span dir="rtl">قبل تغذيتها إلى
**الطبقة التالية**</span> **<span dir="rtl">(</span>Following
<span dir="rtl"></span>Layer<span dir="rtl">)</span>**.
<span dir="rtl">استخدم</span> **Ioffe and Szegedy
<span dir="rtl"></span>(2015)** <span dir="rtl">إحصاءات من مجموعات
فرعية، أو  
"دفعات صغيرة</span> (Mini-Batches)"<span dir="rtl">، من أمثلة التدريب
لتطبيع هذه الإشارات بين الطبقات لتحسين معدل تعلم **الشبكات العصبية
الاصطناعية العميقة** </span>**(Deep ANNs)<span dir="rtl">.</span>**

<span dir="rtl">تقنية أخرى مفيدة لتدريب **الشبكات العصبية الاصطناعية
العميقة**</span> **(Deep ANNs)** <span dir="rtl">هي **التعليم العميق
بالمتبقي**</span> **(Deep Residual Learning)** <span dir="rtl">التي
قدمها</span> **He, Zhang, Ren, and Sun
<span dir="rtl"></span>**<span dir="rtl">في عام 2016. أحيانًا يكون من
الأسهل تعلم كيفية اختلاف دالة ما عن **الدالة الهوية**</span>
**<span dir="rtl">(</span>Identity
<span dir="rtl"></span>Function<span dir="rtl">)
</span>**<span dir="rtl">بدلاً من تعلم الدالة نفسها. بعد ذلك، يمكن إضافة
هذا الاختلاف، أو **دالة المتبقي**</span>
**<span dir="rtl">(</span>Residual
<span dir="rtl"></span>Function<span dir="rtl">)</span>**<span dir="rtl">،
إلى المدخلات لإنتاج الدالة المطلوبة. في **الشبكات العصبية الاصطناعية
العميقة** </span>**(Deep ANNs)**<span dir="rtl">، يمكن جعل كتلة من
**الطبقات**</span> **(Layers)** <span dir="rtl">تتعلم **دالة المتبقي  
(**</span>**Residual
<span dir="rtl"></span>Function<span dir="rtl">)</span>**
<span dir="rtl">ببساطة عن طريق إضافة **اتصالات اختصارية أو تخطي  
(**</span>**Shortcut <span dir="rtl"></span>or Skip
Connections<span dir="rtl">)</span>** <span dir="rtl">حول الكتلة. تقوم
هذه **الاتصالات** </span>**(Connections)** <span dir="rtl">بإضافة
المدخلات إلى الكتلة إلى مخرجاتها، ولا يلزم وجود **أوزان إضافية**
</span>**(Additional Weights)**<span dir="rtl">.</span>
<span dir="rtl">قام</span> **He et al. (2016)
<span dir="rtl"></span>**<span dir="rtl">بتقييم هذه الطريقة باستخدام
**الشبكات العصبية الالتفافية العميقة**</span> **<span dir="rtl">  
(</span>Deep <span dir="rtl"></span>Convolutional
Networks<span dir="rtl">)</span>** <span dir="rtl">مع **اتصالات
تخطي**</span> **(Skip Connections)** <span dir="rtl">حول كل زوج من
**الطبقات المتجاورة** </span>**(Adjacent Layers)**<span dir="rtl">،
ووجدوا تحسينات كبيرة على الشبكات التي لا تحتوي على **اتصالات
التخطي**</span> **(Skip Connections)** <span dir="rtl">في مهام تصنيف
الصور القياسية. تم استخدام كل من **التطبيع على دفعات**</span> **(Batch
Normalization)** <span dir="rtl">و**التعليم العميق بالمتبقي**</span>
**<span dir="rtl">  
(</span>Deep <span dir="rtl"></span>Residual
Learning<span dir="rtl">)</span>** <span dir="rtl">في تطبيق **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">على
لعبة</span> **Go <span dir="rtl"></span>**<span dir="rtl">التي نناقشها
في الفصل 16</span>.

<span dir="rtl">نوع آخر من **الشبكات العصبية الاصطناعية العميقة**</span>
**(Deep ANN)** <span dir="rtl">الذي أثبت نجاحه الكبير في التطبيقات، بما
في ذلك التطبيقات المثيرة للإعجاب في **التعليم المعزز  
(**</span>**Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**
<span dir="rtl">(الفصل 16)، هو **الشبكة العصبية الالتفافية العميقة  
(**</span>**Deep <span dir="rtl"></span>Convolutional
Network<span dir="rtl">)</span>**. <span dir="rtl">هذا النوع من الشبكات
متخصص في معالجة البيانات ذات الأبعاد العالية التي يتم ترتيبها في
**مصفوفات مكانية** </span>**(Spatial Arrays)**<span dir="rtl">، مثل
الصور. استوحي هذا النوع من الشبكات من كيفية عمل **المعالجة البصرية
المبكرة**</span> **(Early Visual Processing)** <span dir="rtl">في
الدماغ</span> <span dir="rtl">(</span>LeCun, Bottou, Bengio and
Haffner<span dir="rtl">، 1998)</span>. <span dir="rtl">بسبب بنيتها
الخاصة، يمكن تدريب **الشبكة العصبية الالتفافية العميقة**</span> **(Deep
Convolutional Network)** <span dir="rtl">باستخدام **الانتشار
العكسي**</span> **(Backpropagation)** <span dir="rtl">دون اللجوء إلى
الأساليب مثل تلك التي تم وصفها أعلاه لتدريب **الطبقات العميقة**
</span>**(Deep Layers)**<span dir="rtl">.</span>

<span dir="rtl">يوضح **الشكل** 9.15 بنية **الشبكة العصبية الالتفافية
العميقة**</span> **<span dir="rtl">(</span>Deep Convolutional
<span dir="rtl"></span>Network<span dir="rtl">)</span>**.
<span dir="rtl">هذا المثال، من</span> LeCun et al.
(1998)<span dir="rtl">، تم تصميمه للتعرف على الأحرف المكتوبة بخط اليد.
يتكون من **طبقات التلافيف والتخفيض المتناوبة**</span>
**<span dir="rtl">(</span>Alternating Convolutional and
<span dir="rtl"></span>Subsampling
Layers<span dir="rtl">)</span>**<span dir="rtl">، يليها عدة **طبقات
نهائية متصلة بالكامل**</span> **<span dir="rtl">(</span>Fully Connected
<span dir="rtl"></span>Final Layers<span dir="rtl">)</span>**.
<span dir="rtl">تنتج كل **طبقة تلافيف**</span> **(Convolutional Layer)**
<span dir="rtl">عددًا من **خرائط الميزات** </span>**(Feature
Maps)**<span dir="rtl">.</span> **<span dir="rtl">خريطة الميزة</span>
(Feature Map)** <span dir="rtl">هي نمط من النشاط على **مصفوفة الوحدات**
</span>**(Array of Units)**<span dir="rtl">، حيث تقوم كل وحدة بنفس
العملية على البيانات في **حقل استقبالها** </span>**(Receptive
Field)**<span dir="rtl">، وهو الجزء من البيانات الذي "ترى" فيه **الطبقة
السابقة  
(**</span>**Preceding
<span dir="rtl"></span>Layer<span dir="rtl">)</span>**
<span dir="rtl">(أو من المدخلات الخارجية في حالة الطبقة الالتفافية
الأولى). تكون **الوحدات في خريطة الميزة**</span> **(Units in the Feature
Map)** <span dir="rtl">متطابقة مع بعضها البعض باستثناء أن **حقول
استقبالها** </span>**(Receptive Fields)**<span dir="rtl">، والتي تكون
كلها بنفس الحجم والشكل، تتحرك إلى مواقع مختلفة على **مصفوفات البيانات
الواردة** </span>**(Incoming Data Arrays)**<span dir="rtl">.</span>
<span dir="rtl">تشترك **الوحدات في نفس خريطة الميزة**</span> **(Units in
the Same Feature Map)** <span dir="rtl">في نفس **الأوزان**
</span>**(Weights)**<span dir="rtl">.</span> <span dir="rtl">هذا يعني أن
**خريطة الميزة**</span> **(Feature Map)** <span dir="rtl">تكتشف نفس
الميزة بغض النظر عن مكان وجودها في **مصفوفة المدخلات** </span>**(Input
Array)**<span dir="rtl">.</span> <span dir="rtl">في الشبكة الموضحة في
الشكل 9.15، على سبيل المثال، تنتج **الطبقة الالتفافية الأولى**</span>
**(First Convolutional Layer)** 6 **<span dir="rtl">خرائط ميزات</span>
(Feature Maps)**<span dir="rtl">، كل منها يتكون من</span> **28 × 28
<span dir="rtl">وحدة</span> (28 × 28 Units)**<span dir="rtl">.</span>
<span dir="rtl">كل وحدة في كل **خريطة ميزة** </span>**(Feature Map)**
<span dir="rtl">لديها **حقل استقبال**</span> **(Receptive Field)**
<span dir="rtl">بحجم</span> **5 × 5**<span dir="rtl">، وتتداخل هذه
**حقول الاستقبال** </span>**(Receptive Fields)** <span dir="rtl">(في هذه
الحالة بأربعة أعمدة وأربعة صفوف)</span>.

<img src="./media/image115.png"
style="width:6.26806in;height:1.92083in" />

**<span dir="rtl">الشكل 9.15</span>**: **<span dir="rtl">الشبكة العصبية
الالتفافية العميقة</span> (Deep Convolutional
Network)**<span dir="rtl">.</span> <span dir="rtl">أُعيد نشره بإذن
من</span> **Proceedings of the IEEE**<span dir="rtl">، من **التعليم
القائم على التدرج المطبق على التعرف على المستندات**
</span>**(Gradient-based Learning Applied to Document
Recognition)**<span dir="rtl">،</span> LeCun, Bottou, Bengio, and
Haffner<span dir="rtl">، المجلد 86، 1998؛ الإذن مُنح عبر **مركز تصفية
حقوق الطبع والنشر** </span>**(Copyright Clearance Center,
Inc.)**<span dir="rtl">.</span>

<span dir="rtl">نتيجة لذلك، يتم تحديد كل واحدة من **خرائط الميزات
الست**</span> **(6 Feature Maps)** <span dir="rtl">بواسطة 25 **وزنًا
قابلًا للتعديل**</span> **(Adjustable Weights)**
<span dir="rtl">فقط.</span> **<span dir="rtl">طبقات التخفيض</span>
(Subsampling Layers)** <span dir="rtl">في **الشبكة العصبية الالتفافية
العميقة**</span> **(Deep Convolutional Network)** <span dir="rtl">تقلل
من الدقة المكانية **لخرائط الميزات** </span>**(Feature
Maps)**<span dir="rtl">.</span> <span dir="rtl">تتكون كل **خريطة
ميزة**</span> **(Feature Map)** <span dir="rtl">في **طبقة التخفيض**
</span>**(Subsampling Layer)** <span dir="rtl">من وحدات تقوم بعملية
المتوسط على **حقل استقبال**</span> **<span dir="rtl">(</span>Receptive
<span dir="rtl"></span>Field<span dir="rtl">)</span>**
<span dir="rtl">من الوحدات في **خرائط الميزات**</span> **(Feature
Maps)** **<span dir="rtl">للطبقة الالتفافية السابقة</span>
<span dir="rtl">(</span>Preceding <span dir="rtl"></span>Convolutional
Layer<span dir="rtl">)</span>**. <span dir="rtl">على سبيل المثال، كل
وحدة في كل من **خرائط الميزات الست  
(**</span>**6 <span dir="rtl"></span>Feature Maps<span dir="rtl">)
</span>**<span dir="rtl">في **طبقة التخفيض الأولى**</span> **(First
Subsampling Layer)** <span dir="rtl">للشبكة الموضحة في **الشكل** 9.15
تقوم بعملية المتوسط على **حقل استقبال**</span> **(Receptive Field)**
<span dir="rtl">بحجم  
2 × 2 غير متداخل على إحدى **خرائط الميزات**</span> **(Feature Maps)**
<span dir="rtl">التي أنتجتها **الطبقة الالتفافية الأولى**
</span>**(First Convolutional Layer)**<span dir="rtl">، مما ينتج عنه ستة
**خرائط ميزات** </span>**(Feature Maps)** <span dir="rtl">بحجم 14 ×
14</span>. **<span dir="rtl">طبقات التخفيض</span> (Subsampling Layers)**
<span dir="rtl">تقلل من حساسية الشبكة للمواقع المكانية **للميزات
المكتشفة** </span>**(Features Detected)**<span dir="rtl">، أي أنها تساعد
في جعل استجابات الشبكة غير متغيرة مكانيًا. هذا مفيد لأن **الميزة المكتشفة
في مكان واحد في الصورة**</span> **<span dir="rtl">(</span>Feature
Detected at <span dir="rtl"></span>One Place in an
Image<span dir="rtl">) </span>**<span dir="rtl">من المحتمل أن تكون مفيدة
في أماكن أخرى أيضًا</span>.

<span dir="rtl">تساهم التقدمات في تصميم وتدريب **الشبكات العصبية
الاصطناعية**</span> **(ANNs)**—<span dir="rtl">والتي ذكرنا منها القليل
فقط—في **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">.</span> <span dir="rtl">على الرغم من أن
**نظرية التعليم المعزز الحالية**</span> **(Current Reinforcement
Learning Theory)** <span dir="rtl">تقتصر في الغالب على الطرق التي تستخدم
**تقريب الدوال الجدولي أو الخطي**</span>
**<span dir="rtl">(</span>Tabular or Linear Function Approximation
<span dir="rtl"></span>Methods<span dir="rtl">)</span>**<span dir="rtl">،
إلا أن الأداء المثير للإعجاب في تطبيقات **التعليم المعزز
البارزة**</span> **<span dir="rtl">(</span>Notable
<span dir="rtl"></span>Reinforcement Learning
Applications<span dir="rtl">) </span>**<span dir="rtl">يعود في كثير من
نجاحاته إلى **تقريب الدوال غير الخطية**</span> **(Nonlinear Function
Approximation)** <span dir="rtl">بواسطة **الشبكات العصبية الاصطناعية
متعددة الطبقات** </span>**(Multi-Layer ANNs)**<span dir="rtl">.</span>
<span dir="rtl">نناقش عدة من هذه التطبيقات في **الفصل** 16</span>.

<u>**9.8** <span dir="rtl">أقل **المربعات لتعلم الفرق الزمني**</span>
**(Least-Squares TD - LSTD)**</u>

<span dir="rtl">كل الطرق التي ناقشناها حتى الآن في هذا الفصل تتطلب
حسابات لكل خطوة زمنية تتناسب مع عدد **البارامترات**
</span>**(Parameters)**<span dir="rtl">.</span> <span dir="rtl">ومع ذلك،
مع المزيد من الحسابات، يمكن تحقيق نتائج أفضل. في هذا القسم، نقدم طريقة
**لتقريب الدوال الخطي**</span> **(Linear Function Approximation)**
<span dir="rtl">التي يمكن القول بأنها الأفضل التي يمكن تحقيقها في هذه
الحالة</span>.

<span dir="rtl">كما أشرنا في **القسم** 9.4، فإن</span> **TD(0)
<span dir="rtl"></span> <span dir="rtl">مع تقريب الدوال الخطي</span>
<span dir="rtl">(</span>TD(0) with Linear
<span dir="rtl"></span>Function
<span dir="rtl"></span>Approximation<span dir="rtl">)
</span>**<span dir="rtl">يتقارب **بشكل غير متناهي**</span>
**(Asymptotically)** <span dir="rtl">(مع **أحجام خطوة تتناقص بشكل
مناسب** </span>**(Appropriately Decreasing Step
Sizes)**<span dir="rtl">)</span> <span dir="rtl">إلى **نقطة الثبات لتعلم
الفرق الزمني** </span>**(TD Fixed Point)**<span dir="rtl">:</span>

``` math
w_{\text{T}} = A^{- 1}b
```

<span dir="rtl">عندما</span>

``` math
A = E\left\lbrack x_{t}\left( x_{t} - \gamma x_{t + 1} \right)^{\top} \right\rbrack
```

<span dir="rtl">و</span>

``` math
b = E\left\lbrack R_{t + 1}x_{t} \right\rbrack
```

<span dir="rtl">لماذا، قد يتساءل المرء، يجب علينا حساب هذا الحل بشكل
تكراري؟ هذا يعتبر إهدارًا للبيانات! أليس من الممكن تحقيق نتيجة أفضل من
خلال حساب تقديرات لـ</span> $`\mathbf{A}`$
<span dir="rtl">و</span>$`\mathbf{b}`$<span dir="rtl">**،** ثم حساب
**نقطة الثبات لتعلم الفرق الزمني**</span> **(TD Fixed Point)**
<span dir="rtl">مباشرةً؟ خوارزمية **أقل المربعات لتعلم الفرق الزمني  
(**</span>**Least-Squares TD –
LSTD<span dir="rtl">)</span>**<span dir="rtl">، والمعروفة عادةً بـ</span>
**LSTD**<span dir="rtl">، تقوم بذلك بالضبط. فهي تقوم بتشكيل **التقديرات
الطبيعية** </span>**(Natural Estimates)**<span dir="rtl">:</span>

``` math
\widehat{A_{t}} = \sum_{k = 0}^{t - 1}{x_{k}\left( x_{k} - \gamma x_{k + 1} \right)^{\top}} + \epsilon I,\quad\ 
```

<span dir="rtl">و</span>

``` math
\widehat{b_{t}} = \sum_{k = 0}^{t - 1}{R_{k + 1}x_{k}}
```

<span dir="rtl">حيث إن</span> I <span dir="rtl">هي **مصفوفة الوحدة**
</span>**(Identity Matrix)**<span dir="rtl">،
و</span>$`\epsilon I`$<span dir="rtl">، لبعض</span> $`\epsilon > 0`$
<span dir="rtl">صغيرة، تضمن أن</span> $`A\hat{}t`$ <span dir="rtl">تكون
دائمًا قابلة **للانعكاس** </span>**(Invertible)**<span dir="rtl">.</span>
<span dir="rtl">قد يبدو أن هذه **التقديرات الطبيعية  
(**</span>**Natural
<span dir="rtl"></span>Estimates<span dir="rtl">)</span>**
<span dir="rtl">يجب أن تُقسم على</span> $`t`$<span dir="rtl">، وبالفعل
يجب ذلك؛ كما هي معرفة هنا، فإن هذه في الواقع تقديرات لـ</span> $`t`$
<span dir="rtl">مضروبًا في</span> $`A`$ <span dir="rtl">و</span>$`t`$
<span dir="rtl">مضروبًا في</span> $`b`$<span dir="rtl">.</span>
<span dir="rtl">ومع ذلك، فإن عوامل</span> $`t`$ <span dir="rtl">الإضافية
تلغي بعضها البعض عندما تستخدم **خوارزمية**</span> **LSTD**
<span dir="rtl">هذه التقديرات لتقدير **نقطة الثبات لتعلم الفرق الزمني  
(**</span>**TD <span dir="rtl"></span>Fixed
Point<span dir="rtl">)</span>** <span dir="rtl">كما يلي</span>:

``` math
w_{t} = \widehat{A_{t}^{- 1}\widehat{b_{t}}}
```

<span dir="rtl">هذه الخوارزمية هي الشكل الأكثر كفاءة في استخدام البيانات
**لتعلم الفرق الزمني الخطي  
(**</span>**Linear
<span dir="rtl"></span>TD(0)<span dir="rtl">)</span>**<span dir="rtl">،
لكنها أيضًا أكثر تكلفة من الناحية الحسابية. تذكر أن **خوارزمية**</span>
**TD(0) <span dir="rtl">شبه التدرج</span> (Semi-Gradient TD(0))**
<span dir="rtl">تتطلب ذاكرة وحسابات لكل خطوة تكون فقط</span>
$`\mathbf{O(d)}`$<span dir="rtl">.</span>

<span dir="rtl">ما مدى تعقيد **خوارزمية**
</span>**LSTD**<span dir="rtl">؟ كما هو مكتوب أعلاه، يبدو أن التعقيد
يزداد مع</span> $`\mathbf{t}`$<span dir="rtl">، ولكن يمكن تنفيذ
التقديرين في المعادلة (9.20) بشكل تزايدي باستخدام التقنيات التي غطيناها
سابقًا (مثل تلك الموجودة في الفصل 2) بحيث يمكن القيام بها في **زمن ثابت
لكل خطوة** </span>**(Constant Time per Step)**<span dir="rtl">.</span>
<span dir="rtl">ومع ذلك، فإن التحديث لـ</span> $`\mathbf{A\hat{}t}`$
<span dir="rtl">سيتضمن **ضرب متجه عمودي بمتجه أفقي**</span> **(Outer
Product)** <span dir="rtl">وبالتالي سيكون **تحديث مصفوفة**
</span>**(Matrix Update)**<span dir="rtl">؛ سيكون التعقيد الحسابي
له</span> $`\mathbf{O(d2)}`$<span dir="rtl">، وبالطبع الذاكرة المطلوبة
للاحتفاظ **بمصفوفة** </span>$`\mathbf{A\hat{}t}`$
<span dir="rtl"></span>**​** <span dir="rtl">ستكون</span>
$`\mathbf{O(d2)}`$<span dir="rtl">.</span>

<span dir="rtl">مشكلة محتملة أكبر هي أن حسابنا النهائي في المعادلة
(9.21) يستخدم **معكوس مصفوفة  
(**</span>**Inverse <span dir="rtl"></span>of**
$`\mathbf{A\hat{}t}`$<span dir="rtl">**)**، والتعقيد الحسابي **لحساب
المعكوس العام  
(**</span>**General <span dir="rtl"></span>Inverse
Computation<span dir="rtl">)</span>** <span dir="rtl">هو</span>
$`\mathbf{O(d3)}`$<span dir="rtl">.</span> <span dir="rtl">لحسن الحظ،
يمكن أيضًا تحديث **معكوس مصفوفة بالشكل الخاص الذي لدينا**
</span>**(Inverse of a Matrix of Our Special Form)**
—<span dir="rtl">مجموع من **الضربات الخارجية**</span> **(Outer
Products)** —<span dir="rtl">بشكل تزايدي باستخدام حسابات</span>
$`\mathbf{O(d2)}`$ <span dir="rtl">فقط، كما هو موضح لاحقًا</span>.

``` math
\widehat{A_{t}^{- 1}} = \left( \widehat{A_{t - 1}} + x_{t}\left( x_{t} - \gamma x_{t + 1} \right)^{\top} \right)^{- 1}
```

``` math
\widehat{A_{t}^{- 1}} = \widehat{A_{t - 1}^{- 1}} - \frac{\widehat{A_{t - 1}^{- 1}x_{t}}\left( x_{t} - \gamma x_{t + 1} \right)^{\top}\widehat{A_{t - 1}^{- 1}}}{1 + \left( x_{t} - \gamma x_{t + 1} \right)^{\top}\widehat{A_{t - 1}^{- 1}x_{t}}}
```

<span dir="rtl">لـ</span> $`t > 0`$<span dir="rtl">، مع</span>
$`A\hat{}0 = \epsilon I`$<span dir="rtl">.</span> <span dir="rtl">على
الرغم من أن الهوية (9.22)، والمعروفة بـ **صيغة شيرمان-موريسون**
</span>**(Sherman-Morrison Formula)**<span dir="rtl">، تبدو معقدة
ظاهريًا، إلا أنها تتضمن فقط عمليات ضرب بين **المتجهات والمصفوفات**</span>
**(Vector-Matrix)** <span dir="rtl">و**المتجهات**
</span>**(Vector-Vector)**<span dir="rtl">، وبالتالي يكون تعقيدها
الحسابي</span> $`\mathbf{O(d2)}`$ <span dir="rtl">فقط. لذا، يمكننا تخزين
**المصفوفة المعكوسة** </span>**(Inverse Matrix)**
$`\mathbf{A\hat{}t - 1}`$**​**<span dir="rtl">، وصيانتها باستخدام
المعادلة (9.22)، ثم استخدامها في المعادلة (9.21)، وكل هذا باستخدام ذاكرة
وحسابات لكل خطوة بزمن</span> $`\mathbf{O(d2)}`$ <span dir="rtl">فقط.
الخوارزمية الكاملة موضحة في الصندوق في الصفحة التالية</span>.

<span dir="rtl">بالطبع،</span> $`\mathbf{O(d2)}`$ <span dir="rtl">لا
يزال أكثر تكلفة بكثير من</span> $`\mathbf{O(d)}`$ <span dir="rtl">الخاصة
بـ **الـ**</span> **TD <span dir="rtl">شبه التدرجي  
(</span>Semi-Gradient TD<span dir="rtl">)</span>**. <span dir="rtl">ما
إذا كانت الكفاءة الأكبر في استخدام البيانات لـ **خوارزمية**
</span>**LSTD** <span dir="rtl">تستحق هذه التكلفة الحسابية يعتمد على
حجم</span> $`\mathbf{d}`$<span dir="rtl">، ومدى أهمية التعليم السريع،
وتكلفة الأجزاء الأخرى من النظام. يُشاع أحيانًا أن **خوارزمية**</span>
**LSTD** <span dir="rtl">لا تتطلب **بارامتر حجم الخطوة**</span>
**<span dir="rtl">(</span>Step-Size
<span dir="rtl"></span>Parameter<span dir="rtl">)</span>**<span dir="rtl">،
ولكن ربما يتم المبالغة في هذه الميزة. على الرغم من أن
**خوارزمية**</span> **LSTD** <span dir="rtl">لا تتطلب **حجم الخطوة**
</span>**(Step Size)**<span dir="rtl">، إلا أنها تتطلب</span>
$`\epsilon`$<span dir="rtl">؛ إذا تم اختيار</span> $`\epsilon`$
<span dir="rtl">صغير جدًا، فقد تتغير سلسلة المعكوسات بشكل كبير، وإذا تم
اختيار</span> $`\epsilon`$ <span dir="rtl"></span> <span dir="rtl">كبير
جدًا، فإن التعليم يكون بطيئًا. بالإضافة إلى ذلك، فإن عدم وجود **بارامتر
حجم الخطوة**</span> **(Step-Size Parameter)** <span dir="rtl">في
**خوارزمية**</span> **LSTD** <span dir="rtl">يعني أنها لا تنسى أبدًا. قد
يكون هذا مرغوبًا في بعض الأحيان، لكنه يمثل مشكلة إذا تغيرت</span>
$`\mathbf{\pi}`$ **<span dir="rtl">السياسة المستهدفة</span> (Target
Policy** $`\mathbf{\pi}`$**)** <span dir="rtl">كما يحدث في **التعليم
المعزز  
(**</span>**Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**
<span dir="rtl">و**تحسين السياسة العامة**
</span>**(GPI)**<span dir="rtl">.</span> <span dir="rtl">في تطبيقات
التحكم، عادةً ما يتعين دمج **خوارزمية**</span> **LSTD**
<span dir="rtl">مع آلية أخرى لتحفيز النسيان، مما يلغي أي ميزة أولية لعدم
الحاجة إلى **بارامتر حجم الخطوة** </span>**(Step-Size
Parameter)**<span dir="rtl">.</span>

<img src="./media/image116.png"
style="width:6.26806in;height:3.61806in" />

**<u>9.9 <span dir="rtl">تقريب الدوال المستند إلى الذاكرة</span>
(Memory-based Function Approximation)</u>**

<span dir="rtl">حتى الآن، ناقشنا النهج **المعتمد على المعلمات**</span>
**(Parametric Approach)** <span dir="rtl">لتقريب **دوال القيمة**
</span>**(Value Functions)**<span dir="rtl">.</span> <span dir="rtl">في
هذا النهج، يقوم **خوارزمية التعليم**</span> **(Learning Algorithm)**
<span dir="rtl">بتعديل معلمات شكل دالة معينة تهدف إلى تقريب **دالة
القيمة**</span> **(Value Function)** <span dir="rtl">عبر **فضاء الحالة**
</span>**(State Space)** <span dir="rtl">للمشكلة بأكملها. كل
تحديث،</span> $`s \rightarrow g`$<span dir="rtl">، يعتبر مثالًا تدريبيًا
يستخدمه **خوارزمية التعليم** </span>**(Learning Algorithm)**
<span dir="rtl">لتغيير المعلمات بهدف تقليل **خطأ التقريب**</span>
**<span dir="rtl">(</span>Approximation
<span dir="rtl"></span>Error<span dir="rtl">)</span>**.
<span dir="rtl">بعد التحديث، يمكن تجاهل المثال التدريبي (على الرغم من
أنه قد يتم حفظه ليتم استخدامه مرة أخرى). عندما تكون هناك حاجة إلى **قيمة
تقريبية**</span> **(Approximate Value)** <span dir="rtl">لحالة معينة
(والتي سنسميها **الحالة المستعلم عنها)** </span>**(Query
State)**<span dir="rtl">، يتم ببساطة تقييم الدالة عند تلك الحالة
باستخدام أحدث المعلمات التي أنتجتها **خوارزمية التعليم**
</span>**(Learning Algorithm)**<span dir="rtl">.</span>

**<span dir="rtl">طرق تقريب الدوال المستندة إلى الذاكرة</span>
<span dir="rtl">(</span>Memory-based Function Approximation
<span dir="rtl"></span>Methods<span dir="rtl">)
</span>**<span dir="rtl">تختلف بشكل كبير. فهي ببساطة تحفظ الأمثلة
التدريبية في الذاكرة عند وصولها (أو على الأقل تحفظ مجموعة فرعية من
الأمثلة) دون تحديث أي معلمات. ثم، عندما تكون هناك حاجة إلى **تقدير قيمة
لحالة مستعلم عنها** </span>**(Value Estimate for the Query
State)**<span dir="rtl">، يتم استرجاع مجموعة من الأمثلة من الذاكرة
واستخدامها لحساب تقدير قيمة لتلك الحالة. يُطلق على هذا النهج أحيانًا
**التعليم الكسول**</span> **(Lazy Learning)** <span dir="rtl">لأن معالجة
الأمثلة التدريبية تؤجل حتى يتم استعلام النظام لتقديم مخرجات</span>.

**<span dir="rtl">طرق تقريب الدوال المستندة إلى الذاكرة</span>
<span dir="rtl">(</span>Memory-based Function Approximation
<span dir="rtl"></span>Methods<span dir="rtl">)
</span>**<span dir="rtl">هي أمثلة رئيسية على **الطرق غير المعتمدة على
المعلمات**</span> **<span dir="rtl">(</span>Nonparametric
<span dir="rtl"></span>Methods<span dir="rtl">)</span>**.
<span dir="rtl">على عكس **الطرق المعتمدة على المعلمات**
</span>**(Parametric Methods)**<span dir="rtl">، فإن شكل الدالة المقربة
لا يقتصر على فئة ثابتة من الدوال ذات المعلمات، مثل **الدوال
الخطية**</span> **<span dir="rtl">(</span>Linear
<span dir="rtl"></span>Functions<span dir="rtl">)</span>**
<span dir="rtl">أو **المتعددات الحدودية**
</span>**(Polynomials)**<span dir="rtl">، بل يتم تحديده بواسطة الأمثلة
التدريبية نفسها، جنبًا إلى جنب مع بعض الوسائل للجمع بينها لإخراج قيم
مقدرة للحالات المستعلم عنها. ومع تراكم المزيد من الأمثلة التدريبية في
الذاكرة، يُتوقع أن تنتج **الطرق غير المعتمدة على المعلمات**</span>
**(Nonparametric Methods)** <span dir="rtl">تقريبًا أكثر دقة لأي **دالة
هدف** </span>**(Target Function)**<span dir="rtl">.</span>

<span dir="rtl">هناك العديد من **الطرق المستندة إلى الذاكرة**</span>
**(Memory-based Methods)** <span dir="rtl">المختلفة التي تعتمد على كيفية
اختيار الأمثلة التدريبية المخزنة وكيفية استخدامها للاستجابة لاستعلام ما.
هنا، نركز على **طرق التعليم المحلي**</span> **(Local-Learning Methods)**
<span dir="rtl">التي تقوم بتقريب **دالة القيمة**</span>
**<span dir="rtl">(</span>Value
<span dir="rtl"></span>Function<span dir="rtl">)
</span>**<span dir="rtl">فقط في **الحي المجاور للحالة المستعلم عنها
الحالية**</span> **<span dir="rtl">(</span>Neighborhood of the
<span dir="rtl"></span>Current Query State<span dir="rtl">)</span>**.
<span dir="rtl">تسترجع هذه الطرق مجموعة من الأمثلة التدريبية من الذاكرة
والتي تُعتبر حالاتها الأقرب إلى **الحالة المستعلم عنها** </span>**(Query
State)**<span dir="rtl">، حيث تعتمد الصلة عادةً على **المسافة بين
الحالات** </span>**(Distance Between States)**<span dir="rtl">:</span>
<span dir="rtl">كلما كانت حالة المثال التدريبي أقرب إلى **الحالة
المستعلم عنها** </span>**(Query State)**<span dir="rtl">، كلما اعتبرت
أكثر صلة، ويمكن تعريف المسافة بطرق عديدة مختلفة. بعد إعطاء **الحالة
المستعلم عنها**</span> **(Query State)** <span dir="rtl">قيمة، يتم تجاهل
**التقريب المحلي** </span>**(Local
Approximation)**<span dir="rtl">.</span>

<span dir="rtl">أبسط مثال على **النهج المستند إلى الذاكرة**</span>
**(Memory-based Approach)** <span dir="rtl">هو **طريقة الجار الأقرب**
</span>**(Nearest Neighbor Method)**<span dir="rtl">، التي ببساطة تجد
المثال في الذاكرة الذي تكون حالته الأقرب إلى **الحالة المستعلم
عنها**</span> **(Query State)** <span dir="rtl">وتعيد قيمة ذلك المثال
كقيمة تقريبية **للحالة المستعلم عنها** </span>**(Query
State)**<span dir="rtl">.</span> <span dir="rtl">بعبارة أخرى، إذا كانت
**الحالة المستعلم عنها** </span>**(Query State)**
<span dir="rtl">هي</span> $`s`$<span dir="rtl">، وكان</span>
$`s' \rightarrow g`$ <span dir="rtl">هو المثال في الذاكرة حيث تكون
حالة</span> s′ <span dir="rtl">هي الأقرب إلى</span>
$`s`$<span dir="rtl">، فإن</span> $`g`$ <span dir="rtl">تُعاد كالقيمة
التقريبية لـ</span> $`s`$<span dir="rtl">.</span> <span dir="rtl">هناك
طرق أكثر تعقيدًا قليلاً مثل **طرق المتوسط المرجح**</span>
**<span dir="rtl">(</span>Weighted <span dir="rtl"></span>Average
Methods<span dir="rtl">) </span>**<span dir="rtl">التي تسترجع مجموعة من
أمثلة الجار الأقرب وتعيد **متوسطًا مرجحًا (**</span>**Weighted
<span dir="rtl"></span>Average<span dir="rtl">)</span>**
<span dir="rtl">لقيمهم المستهدفة، حيث تتناقص الأوزان عادةً بزيادة المسافة
بين حالاتهم و**الحالة المستعلم عنها** </span>**(Query
State)**<span dir="rtl">.</span> **<span dir="rtl">الانحدار المرجح
محليًا</span> <span dir="rtl">(</span>Locally Weighted
<span dir="rtl"></span>Regression<span dir="rtl">)</span>**
<span dir="rtl">مشابه، لكنه يقوم بتقريب **سطح**</span> **(Surface)**
<span dir="rtl">لقيم مجموعة من الحالات الأقرب باستخدام **طريقة تقريب
معلمية** </span>**(Parametric Approximation Method)**
<span dir="rtl">تقلل من **مقياس خطأ مرجح**</span>
**<span dir="rtl">(</span>Weighted Error
<span dir="rtl"></span>Measure<span dir="rtl">)</span>**
<span dir="rtl">مثل (9.1)، حيث تعتمد الأوزان على المسافات من **الحالة
المستعلم عنها**</span> **<span dir="rtl">(</span>Query
<span dir="rtl"></span>State<span dir="rtl">)</span>**.
<span dir="rtl">القيمة المعادة هي **تقييم السطح المحلي المناسب  
(**</span>**Evaluation of the Locally-Fitted
Surface<span dir="rtl">)</span>** <span dir="rtl">عند **الحالة المستعلم
عنها  
(**</span>**Query
<span dir="rtl"></span>State<span dir="rtl">)</span>**<span dir="rtl">،
وبعد ذلك يتم تجاهل **السطح المحلي التقريبي**</span>
**<span dir="rtl">(</span>Local Approximation
<span dir="rtl"></span>Surface<span dir="rtl">)</span>**.

<span dir="rtl">باعتبارها طرقًا غير معلمية</span> (Nonparametric
Methods)<span dir="rtl">، تتمتع **الطرق المستندة إلى الذاكرة**
</span>**(Memory-based Methods)** <span dir="rtl">بميزة على **الطرق
المعلمية**</span> **(Parametric Methods)** <span dir="rtl">في عدم تقييد
التقريبات بأشكال دوال محددة مسبقًا. هذا يسمح بتحسين الدقة مع تراكم المزيد
من البيانات. تتمتع **طرق التقريب المحلي المستندة إلى الذاكرة**</span>
**<span dir="rtl">(</span>Memory-based Local Approximation
<span dir="rtl"></span>Methods<span dir="rtl">)
</span>**<span dir="rtl">بخصائص أخرى تجعلها مناسبة جدًا **للتعليم
المعزز** </span>**(Reinforcement Learning)**<span dir="rtl">. نظرًا
لأهمية **تجميع المسارات**</span> **(Trajectory Sampling)**
<span dir="rtl">في **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**<span dir="rtl">،
كما نوقش في **القسم** 8.6، يمكن **لطرق التقريب المحلي المستندة إلى
الذاكرة** </span>**(Memory-based Local Methods)** <span dir="rtl">تركيز
**تقريب الدالة** </span>**(Function Approximation)** <span dir="rtl">على
الأحياء المحلية **للحالات**</span> **(States)** <span dir="rtl">أو
**أزواج الحالة–الإجراء**</span> **(State-Action Pairs)**
<span dir="rtl">التي تمت زيارتها في **المسارات الفعلية أو المحاكاة**
</span>**(Real or Simulated Trajectories)**<span dir="rtl">.</span>
<span dir="rtl">قد لا تكون هناك حاجة إلى **تقريب عالمي**</span>
**(Global Approximation)** <span dir="rtl">لأن العديد من مناطق **فضاء
الحالة**</span> **(State Space)** <span dir="rtl">قد لا يتم الوصول إليها
أبدًا (أو نادرًا ما يتم الوصول إليها). بالإضافة إلى ذلك، تسمح **الطرق
المستندة إلى الذاكرة**</span> **(Memory-based Methods)**
<span dir="rtl">بأن يكون لتجربة الوكيل تأثير فوري نسبيًا على **تقديرات
القيمة**</span> **(Value Estimates)** <span dir="rtl">في **حي الحالة
الحالية**</span> **<span dir="rtl">(</span>Neighborhood
<span dir="rtl"></span>of the Current
State<span dir="rtl">)</span>**<span dir="rtl">، على عكس حاجة **الطريقة
المعلمية**</span> **(Parametric Method)** <span dir="rtl">لتعديل معلمات
**التقريب العالمي**</span> **(Global Approximation)**
<span dir="rtl">بشكل تدريجي</span>.

<span dir="rtl">تجنب **التقريب العالمي**</span> **(Global
Approximation)** <span dir="rtl">هو أيضًا وسيلة لمواجهة **لعنة الأبعاد**
</span>**(Curse of Dimensionality)**<span dir="rtl">. على سبيل المثال،
بالنسبة لفضاء حالة ذي</span> $`k`$ <span dir="rtl">أبعاد، تتطلب **طريقة
الجدول**</span> **(Tabular Method)** <span dir="rtl">التي تخزن **تقريبًا
عالميًا**</span> **(Global Approximation)** <span dir="rtl">ذاكرة **تزداد
بشكل أسي مع** </span>$`\mathbf{k}`$
<span dir="rtl"></span>**(Exponential in**
$`\mathbf{k}`$**)**<span dir="rtl">.</span> <span dir="rtl">من ناحية
أخرى، في تخزين الأمثلة **لطريقة مستندة إلى الذاكرة**</span>
**(Memory-based Method)**<span dir="rtl">، يتطلب كل مثال ذاكرة تتناسب
مع</span> $`k`$<span dir="rtl">، والذاكرة المطلوبة لتخزين، على سبيل
المثال،</span> $`n`$ <span dir="rtl">أمثلة **تتناسب خطيًا مع**
</span>$`\mathbf{n}`$ <span dir="rtl"></span>**(Linear in**
$`\mathbf{n}`$**)**<span dir="rtl">.</span> <span dir="rtl">لا يوجد شيء
**يتزايد بشكل أسي مع** </span>$`\mathbf{k}`$
<span dir="rtl">**أو**</span> $`\mathbf{n}`$
<span dir="rtl"></span>**(Exponential in k or**
$`\mathbf{n}`$**)**<span dir="rtl">.</span> <span dir="rtl">بالطبع،
القضية الحاسمة المتبقية هي ما إذا كانت **الطريقة المستندة إلى
الذاكرة**</span> **(Memory-based Method)** <span dir="rtl">يمكنها
الإجابة على الاستفسارات بسرعة كافية لتكون مفيدة لوكيل. وهناك مسألة ذات
صلة هي كيف يتدهور الأداء مع زيادة حجم الذاكرة. قد يستغرق العثور على
**الأقرباء في قاعدة بيانات كبيرة**</span>
**<span dir="rtl">(</span>Nearest Neighbors in a Large
<span dir="rtl"></span>Database<span dir="rtl">)
</span>**<span dir="rtl">وقتًا طويلاً بحيث يكون غير عملي في العديد من
التطبيقات</span>.

**<span dir="rtl">مؤيدو الطرق المستندة إلى الذاكرة</span> (Proponents of
Memory-based Methods)** <span dir="rtl">قد طوروا طرقًا لتسريع عملية البحث
عن **الجار الأقرب** </span>**(Nearest Neighbor
Search)**<span dir="rtl">.</span> <span dir="rtl">استخدام **الحواسيب
المتوازية**</span> **(Parallel Computers)** <span dir="rtl">أو **الأجهزة
المتخصصة** </span>**(Special Purpose Hardware)** <span dir="rtl">هو أحد
الأساليب؛ وهناك أسلوب آخر يتمثل في استخدام **بنى بيانات متعددة الأبعاد  
(**</span>**Multi-Dimensional Data Structures<span dir="rtl">)</span>**
<span dir="rtl">خاصة لتخزين بيانات التدريب. إحدى **بنى البيانات**
</span>**(Data Structures)** <span dir="rtl">التي تمت دراستها لهذا
التطبيق هي **شجرة** </span>$`\mathbf{k - d}`$
<span dir="rtl"></span>**(**$`\mathbf{k - d\ }`$**Tree)**<span dir="rtl">،
وهي اختصار لـ **شجرة الأبعاد** </span>$`\mathbf{k}`$
<span dir="rtl"></span>**(**$`\mathbf{k}`$**-dimensional
Tree)**<span dir="rtl">، التي تقوم بتقسيم **فضاء الأبعاد**
</span>$`\mathbf{k}`$**<span dir="rtl">  
(</span>k-dimensional Space<span dir="rtl">)
</span>**<span dir="rtl">بشكل متكرر إلى مناطق مرتبة كعقد في **شجرة
ثنائية  
(**</span>**Binary
<span dir="rtl"></span>Tree<span dir="rtl">)</span>**.
<span dir="rtl">اعتمادًا على كمية البيانات وكيفية توزيعها على **فضاء
الحالة** </span>**(State Space)**<span dir="rtl">، يمكن أن يساعد البحث
عن الجار الأقرب باستخدام **أشجار**</span> $`\mathbf{k - d}`$
<span dir="rtl">في استبعاد مناطق كبيرة من الفضاء بسرعة أثناء البحث عن
الجيران، مما يجعل عمليات البحث ممكنة في بعض المشكلات التي قد تستغرق فيها
عمليات البحث البدائية وقتًا طويلًا</span>.

<span dir="rtl">يتطلب **الانحدار المرجح محليًا**</span> **(Locally
Weighted Regression)** <span dir="rtl">أيضًا طرقًا سريعة لإجراء حسابات
**الانحدار المحلي**</span> **(Local Regression)** <span dir="rtl">التي
يجب تكرارها للإجابة على كل استفسار. وقد قام الباحثون بتطوير العديد من
الطرق لمعالجة هذه المشكلات، بما في ذلك **طرق لنسيان**
</span>**(Forgetting Methods)** <span dir="rtl">الإدخالات من أجل الحفاظ
على حجم قاعدة البيانات ضمن الحدود. يشير **قسم التعليقات الببليوغرافية
والتاريخية**</span> **(Bibliographic and Historical Comments)**
<span dir="rtl">في نهاية هذا الفصل إلى بعض الأدبيات ذات الصلة، بما في
ذلك مجموعة من الأوراق التي تصف تطبيقات **التعليم المستند إلى
الذاكرة**</span> **(Memory-based Learning)** <span dir="rtl">في
**التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">.</span>

**<u>9.10 <span dir="rtl">تقريب الدوال المستند إلى النواة</span>
(Kernel-based Function Approximation)</u>**

**<span dir="rtl">الطرق المستندة إلى الذاكرة</span> (Memory-based
Methods)** <span dir="rtl">مثل **طريقة المتوسط المرجح**
</span>**(Weighted Average Method)** <span dir="rtl">و**طريقة الانحدار
المرجح محليًا**</span> **<span dir="rtl">(</span>Locally Weighted
<span dir="rtl"></span>Regression Method<span dir="rtl">)</span>**
<span dir="rtl">التي تم وصفها أعلاه تعتمد على تعيين أوزان للأمثلة</span>
$`s' \rightarrow g`$ <span dir="rtl">في قاعدة البيانات استنادًا إلى
المسافة بين</span> $`s'`$ <span dir="rtl">والحالة **المستعلم عنها**
</span>**(Query State)** $`\mathbf{s}`$<span dir="rtl">.</span>
<span dir="rtl">الدالة التي تعين هذه الأوزان تُسمى **دالة النواة**
</span>**(Kernel Function)**<span dir="rtl">، أو ببساطة **النواة**
</span>**(Kernel)**<span dir="rtl">.</span> <span dir="rtl">على سبيل
المثال، في **طرق المتوسط المرجح**</span> **(Weighted Average)**
<span dir="rtl">و**الانحدار المرجح محليًا**</span>
**<span dir="rtl">(</span>Locally Weighted
<span dir="rtl"></span>Regressions<span dir="rtl">)</span>**<span dir="rtl">،
تقوم **دالة النواة** </span>$`\mathbf{k:R \rightarrow R}`$
<span dir="rtl"></span> <span dir="rtl">بتعيين أوزان للمسافات بين
الحالات. بشكل عام، لا يجب أن تعتمد الأوزان على المسافات فقط؛ يمكن أن
تعتمد على أي مقياس آخر للتشابه بين الحالات. في هذه الحالة، تكون **دالة
النواة** </span>$`\mathbf{k:S \times S \rightarrow R}`$<span dir="rtl">،
بحيث يمثل</span> $`k(s,s')`$ <span dir="rtl">الوزن المُعطى للبيانات
حول</span> $`s'`$ <span dir="rtl">وتأثيرها في الإجابة عن الاستفسارات
المتعلقة بـ</span> $`s`$<span dir="rtl">.</span>

<span dir="rtl">من منظور مختلف قليلاً،</span> $`\mathbf{k(s,s')}`$
<span dir="rtl">هو مقياس لقوة التعميم من</span> $`\mathbf{s'}`$
<span dir="rtl">إلى</span> $`\mathbf{s}`$<span dir="rtl">.</span>
<span dir="rtl">تعبر **دوال النواة  
(**</span>**Kernel
<span dir="rtl"></span>Functions<span dir="rtl">)</span>**
<span dir="rtl">رقميًا عن مدى صلة المعرفة حول أي حالة بأي حالة أخرى. على
سبيل المثال، تعادل قوة التعميم **للتشفير المتنوع**</span> **(Tile
Coding)** <span dir="rtl">الموضحة في **الشكل** 9.11 **دوال نواة
مختلفة**</span> **(Different Kernel Functions)** <span dir="rtl">ناتجة
عن إزاحات متنوعة وغير متناظرة **للتشفير المتنوع** </span>**(Tile
Offsets)**<span dir="rtl">.</span> <span dir="rtl">على الرغم من أن
**التشفير المتنوع**</span> **(Tile Coding)** <span dir="rtl">لا يستخدم
بشكل صريح **دالة نواة**</span> **(Kernel Function)** <span dir="rtl">في
عمله، إلا أنه يقوم بالتعميم وفقًا لأحدها. في الواقع، كما سنناقش أدناه،
يمكن دائمًا وصف قوة التعميم الناتجة عن **تقريب الدوال المعلمية
الخطية**</span> **<span dir="rtl">(</span>Linear Parametric
<span dir="rtl"></span>Function Approximation<span dir="rtl">)
</span>**<span dir="rtl">بواسطة **دالة نواة** </span>**(Kernel
Function)**<span dir="rtl">.</span>

**<span dir="rtl">الانحدار بالنواة</span> (Kernel Regression)**
<span dir="rtl">هو الطريقة المستندة إلى الذاكرة التي تحسب **متوسطًا مرجحًا
بالنواة**</span> **(Kernel Weighted Average)** <span dir="rtl">للأهداف
لجميع الأمثلة المخزنة في الذاكرة، وتعيّن النتيجة **للحالة المستعلم عنها**
</span>**(Query State)**<span dir="rtl">.</span> <span dir="rtl">إذا
كانت</span> $`\mathbf{D}`$ <span dir="rtl">هي مجموعة الأمثلة المخزنة،
وكان</span> $`\mathbf{g(s')}`$ <span dir="rtl">يمثل الهدف لحالة</span>
$`\mathbf{s'}`$ <span dir="rtl">في مثال مخزن، فإن **الانحدار
بالنواة**</span> **(Kernel Regression)** <span dir="rtl">يقرب **دالة
الهدف** </span>**(Target Function)**<span dir="rtl">، في هذه الحالة
**دالة قيمة**</span> **(Value Function)** <span dir="rtl">تعتمد
على</span> $`\mathbf{D}`$<span dir="rtl">، كما يلي</span>:

``` math
v(s,D) = \sum_{s' \in D}^{}{k\left( s,s' \right)g\left( s' \right)}
```

<span dir="rtl">الطريقة **المتوسط المرجح**</span> **(Weighted Average)**
<span dir="rtl">التي تم وصفها أعلاه هي حالة خاصة حيث تكون</span>
$`\mathbf{k(s,s')}`$ <span dir="rtl">غير صفرية فقط عندما تكون</span>
$`\mathbf{s}`$ <span dir="rtl">و</span>$`*s'`$ <span dir="rtl">قريبتين
من بعضهما بحيث لا يتعين حساب المجموع على كل</span>
$`\mathbf{D}`$<span dir="rtl">.</span>

<span dir="rtl">واحدة من **الدوال النواة الشائعة**</span> **(Common
Kernels)** <span dir="rtl">هي **دالة الأساس الشعاعي الغاوسي**
</span>**(Gaussian Radial Basis Function - RBF)** <span dir="rtl">التي
تُستخدم في **تقريب الدوال بالأساس الشعاعي**</span> **(RBF Function
Approximation)** <span dir="rtl">كما تم وصفه في **القسم** 9.5.5. في
الطريقة الموضحة هناك،</span> **RBFs** <span dir="rtl">هي ميزات يكون
**مراكزها وعرضها**</span> **(Centers and Widths)** <span dir="rtl">إما
محددة من البداية، مع التركيز على المناطق التي من المتوقع أن تقع فيها
العديد من الأمثلة، أو يتم تعديلها بطريقة ما أثناء التعليم. باستثناء
الطرق التي تعدل **المراكز والعروض** </span>**(Centers and
Widths)**<span dir="rtl">، هذه طريقة خطية معلمية تكون
**معلماتها**</span> **(Parameters)** <span dir="rtl">هي **أوزان كل**
</span>**RBF**<span dir="rtl">، التي يتم تعلمها عادة بواسطة **التدرج
العشوائي**</span> **(Stochastic Gradient)** <span dir="rtl">أو **التدرج
شبه العشوائي**</span> **<span dir="rtl">(</span>Semi-Gradient
<span dir="rtl"></span>Descent<span dir="rtl">)</span>**.
<span dir="rtl">شكل التقريب هو **مجموعة خطية**</span> **(Linear
Combination)** <span dir="rtl">من</span> **RBFs
<span dir="rtl"></span>**<span dir="rtl">المحددة مسبقًا</span>.
**<span dir="rtl">الانحدار بالنواة باستخدام نواة</span> RBF (Kernel
Regression with an RBF Kernel)** <span dir="rtl">يختلف عن هذا في
طريقتين. أولاً، هو **مستند إلى الذاكرة**
</span>**(Memory-based)**<span dir="rtl">:</span> **RBFs**
<span dir="rtl">متمركزة على **حالات الأمثلة المخزنة** </span>**(States
of the Stored Examples)**<span dir="rtl">.</span> <span dir="rtl">ثانيًا،
هو **غير معلمي** </span>**(Nonparametric)**<span dir="rtl">:</span>
<span dir="rtl">لا توجد معلمات للتعليم؛ الاستجابة للاستفسار معطاة
بواسطة</span> (9.23)<span dir="rtl">.</span>

<span dir="rtl">بالطبع، هناك العديد من القضايا التي يجب معالجتها للتطبيق
العملي **للانحدار بالنواة  
(**</span>**Kernel
<span dir="rtl"></span>Regression<span dir="rtl">)</span>**<span dir="rtl">،
وهي قضايا تتجاوز نطاق مناقشتنا الموجزة. ومع ذلك، يتضح أن أي **طريقة
انحدار معلمية خطية**</span> **(Linear Parametric Regression Method)**
<span dir="rtl">مثل تلك التي وصفناها في **القسم** 9.4، مع **الحالات
الممثلة بواسطة متجهات الميزات  **
</span>$`\mathbf{x(s) = (x1(s),x2(s),...,xd(s))T}`$**​**<span dir="rtl">،
يمكن إعادة صياغتها كـ **انحدار بالنواة (**</span>**Kernel
<span dir="rtl"></span>Regression<span dir="rtl">)
</span>**<span dir="rtl">حيث تكون</span>$`\mathbf{k(s,s')}`$
<span dir="rtl">هي **الضرب الداخلي**</span> **(Inner Product)**
<span dir="rtl">لتمثيلات **متجهات الميزات**</span>
**<span dir="rtl">(</span>Feature <span dir="rtl"></span>Vector
Representations<span dir="rtl">)</span>** <span dir="rtl">لـ</span>
$`\mathbf{s}`$ <span dir="rtl"></span>
<span dir="rtl">و</span>$`s'*`$<span dir="rtl">، أي</span>:

``` math
k\left( s,s' \right) = x(s)^{\top}x\left( s' \right)
```

**<span dir="rtl">الانحدار بالنواة</span> (Kernel Regression)**
<span dir="rtl">باستخدام **دالة النواة هذه** </span>**(This Kernel
Function)** <span dir="rtl">ينتج نفس التقريب الذي ستنتجه طريقة معلمية
خطية إذا استخدمت هذه **متجهات الميزات  
(**</span>**Feature
<span dir="rtl"></span>Vectors<span dir="rtl">)</span>**
<span dir="rtl">وتعلمت باستخدام نفس بيانات التدريب</span>.

<span dir="rtl">نحن نتجاوز التبرير الرياضي لهذا، والذي يمكن العثور عليه
في أي كتاب حديث عن **التعليم الآلي** </span>**(Machine
Learning)**<span dir="rtl">، مثل كتاب</span> **Bishop
(2006)**<span dir="rtl">، ونشير ببساطة إلى تأثير مهم. بدلاً من إنشاء
ميزات **لمقربات الدوال المعلمية الخطية**</span>
**<span dir="rtl">(</span>Linear Parametric Function
<span dir="rtl"></span>Approximators<span dir="rtl">)</span>**<span dir="rtl">،
يمكن بدلاً من ذلك إنشاء **دوال نواة**</span> **(Kernel Functions)**
<span dir="rtl">مباشرة دون الرجوع على الإطلاق إلى **متجهات الميزات**
</span>**(Feature Vectors)**<span dir="rtl">. ليس كل **دوال النواة  
(**</span>**Kernel
<span dir="rtl"></span>Functions<span dir="rtl">)</span>**
<span dir="rtl">يمكن التعبير عنها كـ **ضرب داخلي لمتجهات الميزات  
(**</span>**Inner Products <span dir="rtl"></span>of Feature
Vectors<span dir="rtl">)</span>** <span dir="rtl">كما في المعادلة
(9.24)، ولكن **دالة النواة** </span>**(Kernel Function)**
<span dir="rtl">التي يمكن التعبير عنها بهذه الطريقة يمكن أن تقدم فوائد
كبيرة مقارنة بالطريقة المعلمية المكافئة. بالنسبة للعديد من مجموعات
**متجهات الميزات** </span>**(Feature Vectors)**<span dir="rtl">، فإن
المعادلة (9.24) لديها شكل دالي مضغوط يمكن تقييمه دون أي حساب يحدث في
**الفضاء ذو الأبعاد** </span>$`\mathbf{d}`$**  
<span dir="rtl">(</span>d-dimensional Feature
Space<span dir="rtl">)</span>**. <span dir="rtl">في هذه الحالات، يكون
**الانحدار بالنواة  
(**</span>**Kernel
<span dir="rtl"></span>Regression<span dir="rtl">)</span>**
<span dir="rtl">أقل تعقيدًا بكثير من استخدام طريقة معلمية خطية مباشرة مع
**الحالات الممثلة بواسطة هذه المتجهات** </span>**(States Represented by
These Feature Vectors)**<span dir="rtl">.</span> <span dir="rtl">هذا هو
ما يُعرف باسم</span> **"<span dir="rtl">خدعة النواة</span> (Kernel
Trick)"** <span dir="rtl">التي تسمح بالعمل بفعالية في **البعد العالي
لفضاء الميزات الواسع**</span> **(High-Dimension of an Expansive Feature
Space)** <span dir="rtl">بينما يجري العمل فعليًا فقط مع مجموعة الأمثلة
التدريبية المخزنة</span>. **<span dir="rtl">خدعة النواة</span> (Kernel
Trick)** <span dir="rtl">هي أساس العديد من طرق **التعليم الآلي**
</span>**(Machine Learning)**<span dir="rtl">، وقد أظهر الباحثون كيف
يمكن أن تستفيد منها أحيانًا **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">.</span>

<u>**9.11** **<span dir="rtl">نظرة أعمق على التعليم داخل السياسة:
الاهتمام والتركيز</span> <span dir="rtl">  
</span>(Looking Deeper at On-policy Learning: Interest and
Emphasis)**</u>

<span dir="rtl">الخوارزميات التي تناولناها حتى الآن في هذا الفصل تعاملت
مع جميع الحالات التي تم مواجهتها بشكل متساوٍ، كما لو كانت كلها ذات أهمية
متساوية. ومع ذلك، في بعض الحالات، قد نكون أكثر اهتمامًا ببعض الحالات
مقارنة بأخرى. في المشاكل الحلقية ذات الخصم</span>
<span dir="rtl">(</span>Discounted Episodic
<span dir="rtl"></span>Problems<span dir="rtl">)، على سبيل المثال، قد
نكون أكثر اهتمامًا بتقييم دقيق للحالات المبكرة في الحلقة أكثر من الحالات
المتأخرة حيث قد يكون الخصم قد جعل المكافآت أقل أهمية بالنسبة لقيمة
الحالة الابتدائية. أو، إذا كانت **دالة قيمة الإجراء**</span>
**(Action-Value Function)** <span dir="rtl">قيد التعليم، فقد يكون من
الأقل أهمية تقييم الإجراءات السيئة بدقة والتي تكون قيمتها أقل بكثير من
**الإجراء الجشع** </span>**(Greedy Action)**<span dir="rtl">. موارد
**تقريب الدوال**</span> **(Function Approximation)**
<span dir="rtl">دائمًا ما تكون محدودة، وإذا تم استخدامها بطريقة أكثر
استهدافًا، فقد يتحسن الأداء</span>.

<span dir="rtl">أحد الأسباب التي جعلتنا نعامل جميع الحالات التي نواجهها
بشكل متساوٍ هو أننا نقوم بالتحديث وفقًا **لتوزيع داخل السياسة**
</span>**(On-policy Distribution)**<span dir="rtl">، حيث تتوفر نتائج
نظرية أقوى بالنسبة لطرق **شبه التدرج** </span>**(Semi-Gradient
Methods)**<span dir="rtl">.</span> <span dir="rtl">تذكر أن **توزيع داخل
السياسة**</span> **<span dir="rtl">(</span>On-policy
<span dir="rtl"></span>Distribution<span dir="rtl">)
</span>**<span dir="rtl">تم تعريفه على أنه **توزيع الحالات**</span>
**(Distribution of States)** <span dir="rtl">التي يتم مواجهتها في
**نموذج ماركوف لاتخاذ القرارات**</span> **(MDP)** <span dir="rtl">أثناء
اتباع **السياسة المستهدفة** </span>**(Target
Policy)**<span dir="rtl">.</span> <span dir="rtl">الآن سنقوم بتعميم هذا
المفهوم بشكل كبير. بدلاً من وجود **توزيع داخل السياسة  
(**</span>**On-policy
<span dir="rtl"></span>Distribution<span dir="rtl">)
</span>**<span dir="rtl">واحد فقط لـ **نموذج ماركوف لاتخاذ القرارات**
</span>**(MDP)**<span dir="rtl">، سيكون لدينا العديد من هذه التوزيعات.
سيكون لها جميعًا شيء مشترك وهو أنها **توزيع للحالات  
(**</span>**Distribution <span dir="rtl"></span>of
States<span dir="rtl">) </span>**<span dir="rtl">التي يتم مواجهتها في
**المسارات**</span> **(Trajectories)** <span dir="rtl">أثناء اتباع
**السياسة المستهدفة** </span>**(Target Policy)**<span dir="rtl">، لكنها
ستختلف في كيفية **بدء هذه المسارات**
</span>**(Initiated)**<span dir="rtl">، بطريقة ما</span>.

<span dir="rtl">الآن نقدم بعض المفاهيم الجديدة. أولاً، نقدم **مقياسًا
عدديًا غير سالب**</span> **<span dir="rtl">(</span>Non-negative Scalar
<span dir="rtl"></span>Measure<span dir="rtl">)</span>**<span dir="rtl">،
وهو **متغير عشوائي**</span> **(Random Variable)** <span dir="rtl">يسمى
**الاهتمام** </span>**(Interest)**<span dir="rtl">، يشير إلى الدرجة التي
نهتم فيها بتقييم الحالة (أو **زوج الحالة–الإجراء)**</span>
**(State-Action Pair)** <span dir="rtl">بدقة  
عند الزمن</span> $`\mathbf{t}`$<span dir="rtl">.</span>
<span dir="rtl">إذا لم نهتم بالحالة على الإطلاق، فيجب أن يكون
**الاهتمام**</span> **(Interest)** <span dir="rtl">صفرًا؛ إذا كنا نهتم
بشكل كامل، فقد يكون **الاهتمام**</span> **(Interest)**
<span dir="rtl">واحدًا، على الرغم من أنه يُسمح له رسميًا بأخذ أي قيمة غير
سالبة. يمكن ضبط **الاهتمام**</span> **(Interest)** <span dir="rtl">بأي
طريقة سببية؛ على سبيل المثال، قد يعتمد على **المسار**</span>
**(Trajectory)** <span dir="rtl">حتى الزمن</span> $`\mathbf{t}`$
<span dir="rtl">أو على **المعلمات المتعلمة**</span> **(Learned
Parameters)** <span dir="rtl">عند الزمن</span>
$`\mathbf{t}`$<span dir="rtl">.</span> <span dir="rtl">ثم يتم تعريف
**التوزيع**</span> **(**$`\mathbf{\mu}`$**)** <span dir="rtl">في
**معادلة تقييم الدالة**</span> **(Value Equation)** (9.1)
<span dir="rtl">على أنه **توزيع الحالات**</span> **(Distribution of
States)** <span dir="rtl">التي تم مواجهتها أثناء اتباع **السياسة
المستهدفة** </span>**(Target Policy)**<span dir="rtl">، مرجحًا بالاهتمام.
ثانيًا، نقدم **متغير عشوائي عددي غير سالب آخر  
(**</span>**Another <span dir="rtl"></span>Non-negative Scalar Random
Variable<span dir="rtl">) </span>**<span dir="rtl">يسمى **التركيز**
</span>**(Emphasis)** <span dir="rtl">ويرمز له بـ</span>
**Mt​**<span dir="rtl">.</span> <span dir="rtl">هذا العدد المضاعف يقوم
بضرب التحديث التعليمي وبالتالي يقوم بتأكيد أو تخفيض الأهمية التي تم
إيلاؤها للتعليم في الزمن</span> $`\mathbf{t}`$<span dir="rtl">.</span>
**<span dir="rtl">قاعدة التعليم العامة للنموذج ذو الخطوات</span>**
$`\mathbf{n}`$ **<span dir="rtl">(</span>General
<span dir="rtl"></span>n-step Learning
Rule<span dir="rtl">)</span>**<span dir="rtl">، والتي تستبدل معادلة
(9.15)، هي</span>:

``` math
w_{t + n} = w_{t + n - 1} + \alpha M_{t}\left\lbrack G_{t:t + n} - \widehat{v}\left( S_{t},w_{t + n - 1} \right) \right\rbrack\nabla\widehat{v}\left( S_{t},w_{t + n - 1} \right),\quad 0 \leq t < T
```

<span dir="rtl">مع **العائد** </span>$`\mathbf{n}`$ **<span dir="rtl">ذو
الخطوات</span> (n-step Return)** <span dir="rtl">المعطى في المعادلة
(9.16) و**التركيز**</span> **(Emphasis)** <span dir="rtl">الذي يتم
تحديده بشكل متكرر من **الاهتمام**</span> **(Interest)**
<span dir="rtl">بواسطة</span>:

``` math
M_{t} = I_{t} + \gamma^{n}M_{t - n},\quad 0 \leq t < T
```

<span dir="rtl">مع</span> $`Mt = 0`$ <span dir="rtl">لجميع</span>
$`t < 0`$<span dir="rtl">.</span> <span dir="rtl">تعتبر هذه المعادلات
مشمولة بحالة **مونت كارلو** </span>**(Monte Carlo)**<span dir="rtl">،
حيث</span> $`Gt:t + n = Gt`$​<span dir="rtl">، ويتم إجراء جميع التحديثات
في نهاية الحلقة، و</span>$`n = T - t`$<span dir="rtl">،</span>
$`Mt = I`$​<span dir="rtl">.</span>

<span dir="rtl">المثال 9.4 يوضح كيف يمكن للاهتمام والتركيز أن يؤدي إلى
تقديرات أكثر دقة للقيمة</span>.

<span dir="rtl">مثال 9.4</span>: **<span dir="rtl">الاهتمام
والتركيز</span> (Interest and Emphasis)**

<span dir="rtl">لفهم الفوائد المحتملة لاستخدام **الاهتمام والتركيز**
</span>**(Interest and Emphasis)**<span dir="rtl">، تأمل في **عملية
مكافأة ماركوف**</span> **(Markov Reward Process)**
<span dir="rtl">المكونة من أربع حالات كما هو موضح أدناه</span>:

<img src="./media/image117.png"
style="width:6.17553in;height:1.5168in" />

<span dir="rtl">تبدأ الحلقات في **الحالة**</span> **(State)**
<span dir="rtl">الموجودة في أقصى اليسار، ثم تنتقل حالة واحدة إلى اليمين،
مع **مكافأة**</span> **(Reward)** <span dir="rtl">قدرها 1+ في كل خطوة
حتى الوصول إلى **الحالة النهائية** </span>**(Terminal
State)**<span dir="rtl">. **القيمة الحقيقية**</span> **(True Value)**
<span dir="rtl">للحالة الأولى هي 4، وللحالة الثانية 3، وهكذا كما هو موضح
أسفل كل حالة. هذه هي **القيم الحقيقية**</span> **(True
Values)**<span dir="rtl">؛ والقيم المقدرة يمكن أن تقترب منها فقط لأنها
مقيدة **بالمعلمات** </span>**(Parameters)**<span dir="rtl">.</span>
<span dir="rtl">هناك مكونان في **متجه المعلمات** </span>**(Parameter
Vector)** $`\mathbf{w = (w1,w2)T}`$<span dir="rtl">، وتمثل كل
**حالة**</span> **(State)** <span dir="rtl">البارامترية بشكل محدد كما هو
مكتوب داخل كل حالة</span>. **<span dir="rtl">القيم المقدرة</span>
(Estimated Values)** <span dir="rtl">للحالتين الأولى والثانية تعتمد
على</span> $`\mathbf{w1}`$ <span dir="rtl"></span>**​**
<span dir="rtl">فقط وبالتالي يجب أن تكون متساوية على الرغم من أن
**قيمهما الحقيقية**</span> **(True Values)** <span dir="rtl">مختلفة.
وبالمثل، تعتمد **القيم المقدرة**</span> **(Estimated Values)**
<span dir="rtl">للحالتين الثالثة والرابعة على</span> $`\mathbf{w2}`$
<span dir="rtl"></span>**​** <span dir="rtl">فقط ويجب أن تكون متساوية على
الرغم من أن **قيمهما الحقيقية**</span> **(True Values)**
<span dir="rtl">مختلفة. لنفترض أننا مهتمون بتقييم **الحالة**
</span>**(State)** <span dir="rtl">الموجودة في أقصى اليسار بدقة؛ نعطيها
**اهتمامًا**</span> **(Interest)** <span dir="rtl">بقيمة 1 بينما نعطي
جميع **الحالات**</span> **(States)** <span dir="rtl">الأخرى
**اهتمامًا**</span> **(Interest)** <span dir="rtl">بقيمة 0، كما هو موضح
أعلى **الحالات** </span>**(States)**<span dir="rtl">.</span>

<span dir="rtl">أولاً، دعنا نأخذ بعين الاعتبار تطبيق **خوارزميات مونت
كارلو التدرجية**</span> **<span dir="rtl">(</span>Gradient Monte Carlo
<span dir="rtl"></span>Algorithms<span dir="rtl">)
</span>**<span dir="rtl">على هذه المشكلة</span>.
**<span dir="rtl">الخوارزميات</span> (Algorithms)** <span dir="rtl">التي
تم تقديمها سابقًا في هذا الفصل والتي لا تأخذ في الاعتبار
**الاهتمام**</span> **(Interest)** <span dir="rtl">و**التركيز**</span>
**(Emphasis)** <span dir="rtl">(في المعادلة (9.7) والصندوق في الصفحة
202) ستتقارب (لمعلمات خطوة متناقصة) إلى **متجه المعلمات**</span>
**<span dir="rtl">(</span>Parameter
<span dir="rtl"></span>Vector<span dir="rtl">)
</span>**$`\mathbf{w1 = \ (3.5,1.5)}`$<span dir="rtl">، والذي يعطي
**الحالة الأولى**</span> **(First State)** —<span dir="rtl">الوحيدة التي
نهتم بها—قيمة قدرها 3.5 (أي بين **القيمتين الحقيقيتين**</span> **(True
Values)** **<span dir="rtl">للحالتين الأولى والثانية  
(</span>First and Second States**<span dir="rtl">)</span>.
<span dir="rtl">من ناحية أخرى، الطرق التي تم تقديمها في هذا القسم والتي
تستخدم **الاهتمام**</span> **(Interest)**
<span dir="rtl">و**التركيز**</span> **(Emphasis)**
<span dir="rtl">ستتعلم **قيمة**</span> **(Value)**
**<span dir="rtl">الحالة الأولى  
(</span>First State<span dir="rtl">)</span>** <span dir="rtl">بشكل دقيق؛
سيتقارب</span> $`\mathbf{w1}`$ <span dir="rtl">إلى 4 بينما لن يتم
تحديث</span> $`\mathbf{w2}`$ <span dir="rtl"></span>**​**
<span dir="rtl">أبدًا لأن **التركيز** </span>**(Emphasis)**
<span dir="rtl">صفر في جميع **الحالات**</span> **(States)**
<span dir="rtl">باستثناء **الحالة**</span> **(State)**
<span dir="rtl">الموجودة في أقصى اليسار</span>.

<span dir="rtl">الآن دعنا نأخذ بعين الاعتبار تطبيق **طرق**</span> **TD
<span dir="rtl">شبه التدرجية ذات الخطوتين</span>
<span dir="rtl">(</span>Two-step Semi-Gradient TD
Methods<span dir="rtl">)</span>**. <span dir="rtl">الطرق من القسم السابق
من هذا الفصل بدون **اهتمام** </span>**(Interest)**
<span dir="rtl">و**تركيز**</span> **(Emphasis)** <span dir="rtl">(في
المعادلتين (9.15) و(9.16) والصندوق في الصفحة 209) ستتقارب أيضًا
إلى</span> $`\mathbf{w1 = (3.5,1.5)}`$<span dir="rtl">، بينما الطرق التي
تستخدم **الاهتمام**</span> **(Interest)** <span dir="rtl">و**التركيز**
</span>**(Emphasis)** <span dir="rtl">ستتقارب إلى</span>
$`\mathbf{w1 = (4,2)}`$<span dir="rtl">.</span> <span dir="rtl">الأخيرة
تنتج **القيم الصحيحة تمامًا**</span> **<span dir="rtl">(</span>Exactly
<span dir="rtl"></span>Correct Values<span dir="rtl">) للحالة
الأولى</span> (First State)** <span dir="rtl">و**للحالة الثالثة**
</span>**(Third State)** <span dir="rtl">(التي تعتمد **الحالة
الأولى**</span> **(First State)** <span dir="rtl">على قيمتها من خلال
**التمهيد** </span>**(Bootstrapping)**<span dir="rtl">)</span>
<span dir="rtl">بينما لا تقوم بأي تحديثات تتعلق **بالحالتين الثانية أو
الرابعة (**</span>**Second or <span dir="rtl"></span>Fourth
States<span dir="rtl">)</span>**.

<u>**9.12 <span dir="rtl">الملخص</span>** (**Summary**)</u>

<span dir="rtl">أنظمة **التعليم المعزز**</span> **(Reinforcement
Learning)** <span dir="rtl">يجب أن تكون قادرة على **التعميم**
</span>**(Generalization)** <span dir="rtl">لكي تكون قابلة للتطبيق في
مجال **الذكاء الاصطناعي  
(**</span>**Artificial
<span dir="rtl"></span>Intelligence<span dir="rtl">)</span>**
<span dir="rtl">أو في التطبيقات الهندسية الكبيرة. لتحقيق ذلك، يمكن
استخدام أي من مجموعة واسعة من الطرق الموجودة **لتقريب دالة التعليم تحت
الإشراف**</span> **<span dir="rtl">(</span>Supervised-Learning
<span dir="rtl"></span>Function Approximation<span dir="rtl">)
</span>**<span dir="rtl">ببساطة عن طريق التعامل مع كل تحديث كأنه مثال
تدريبي</span>.

<span dir="rtl">ربما تكون الطرق الأكثر ملاءمة للتعليم تحت الإشراف هي تلك
التي تستخدم **تقريب الدوال المعلمية** </span>**(Parameterized Function
Approximation)**<span dir="rtl">، حيث تكون **السياسة**</span>
**(Policy)** <span dir="rtl">بارامتر بواسطة **متجه الوزن**
</span>**(Weight Vector)** $`\mathbf{w}`$<span dir="rtl">.</span>
<span dir="rtl">على الرغم من أن **متجه الوزن** </span>**(Weight
Vector)** <span dir="rtl">يحتوي على العديد من المكونات، إلا أن **فضاء
الحالة**</span> **(State Space)** <span dir="rtl">أكبر بكثير، ويجب علينا
أن نكتفي بحل تقريبي. لقد قمنا بتعريف **خطأ القيمة المتوسط
التربيعي**</span> **<span dir="rtl">(</span>Mean Squared Value
<span dir="rtl"></span>Error<span dir="rtl">)</span> VE(w)**
<span dir="rtl">كقياس للخطأ في القيم</span> $`v\pi(s)`$
<span dir="rtl">لمتجه الوزن</span> $`w`$ <span dir="rtl">تحت **توزيع
داخل السياسة  
(**</span>**On-policy Distribution<span dir="rtl">)</span>**
$`\mathbf{\mu}`$. <span dir="rtl">يعطي</span> **VE
<span dir="rtl"></span>**<span dir="rtl">طريقة واضحة لترتيب مختلف
**تقريبات دالة القيمة**</span> **(Value-Function Approximations)**
<span dir="rtl">في حالة **داخل السياسة** </span>**(On-policy
Case)**<span dir="rtl">.</span>

<span dir="rtl">لإيجاد **متجه وزن جيد** </span>**(Good Weight
Vector)**<span dir="rtl">، فإن الطرق الأكثر شيوعًا هي **التدرج العشوائي
التنازلي**</span> **(Stochastic Gradient Descent - SGD)**
<span dir="rtl">وتعديلاته. في هذا الفصل، ركزنا على حالة **داخل
السياسة**</span> **(On-policy Case)** <span dir="rtl">مع **سياسة ثابتة**
</span>**(Fixed Policy)**<span dir="rtl">، والمعروفة أيضًا باسم **تقييم
السياسة أو التنبؤ** </span>**(Policy Evaluation or
Prediction)**<span dir="rtl">؛ **خوارزمية التعليم الطبيعية لهذه
الحالة**</span> **(Natural Learning Algorithm)** <span dir="rtl">هي
**طريقة**</span> **TD <span dir="rtl">شبه التدرجية ذات</span>
n<span dir="rtl">-الخطوات</span> (n-step Semi-Gradient
TD)**<span dir="rtl">، والتي تشمل **خوارزميات مونت كارلو
التدرجية**</span> **<span dir="rtl">(</span>Gradient
<span dir="rtl"></span>Monte Carlo Algorithms<span dir="rtl">)
</span>**<span dir="rtl">و</span>**TD(0) <span dir="rtl">شبه
التدرجية</span> (Semi-Gradient TD(0))** <span dir="rtl">كحالات خاصة
عندما</span> $`n = 1`$ <span dir="rtl"></span>
<span dir="rtl">و</span>$`\ n = 1\ `$<span dir="rtl">على التوالي.
طرق</span> **TD <span dir="rtl">شبه التدرجية</span>
<span dir="rtl">(</span>Semi-Gradient TD
<span dir="rtl"></span>Methods<span dir="rtl">)
</span>**<span dir="rtl">ليست طرق تدرج حقيقية. في مثل هذه الطرق القائمة
على **التمهيد**</span> **<span dir="rtl">(</span>Bootstrapping
<span dir="rtl"></span>Methods)**<span dir="rtl">) بما في ذلك **البرمجة
الديناميكية** </span>**(Dynamic Programming - DP**)<span dir="rtl">،
يظهر **متجه الوزن**</span> **(Weight Vector)** <span dir="rtl">في هدف
التحديث، ومع ذلك، لا يتم أخذ ذلك في الاعتبار في حساب التدرج - وبالتالي
فهي **طرق شبه تدرجية** </span>**(Semi-Gradient
Methods)**<span dir="rtl">.</span> <span dir="rtl">وبذلك، لا يمكنهم
الاعتماد على نتائج **التدرج العشوائي التنازلي الكلاسيكية**
</span>**(Classical SGD Results)**<span dir="rtl">.</span>

<span dir="rtl">ومع ذلك، يمكن الحصول على نتائج جيدة باستخدام **الطرق شبه
التدرجية**</span> **<span dir="rtl">(</span>Semi-Gradient
<span dir="rtl"></span>Methods<span dir="rtl">)
</span>**<span dir="rtl">في حالة **تقريب الدالة الخطية**
</span>**(Linear Function Approximation)**<span dir="rtl">، حيث تكون
**القيم المقدرة**</span> **(Value Estimates)** <span dir="rtl">مجموعات
**للميزات**</span> **(Features)** <span dir="rtl">مضروبة في **الأوزان
المقابلة** </span>**(Corresponding Weights)**<span dir="rtl">.</span>
<span dir="rtl">الحالة الخطية هي الأكثر فهمًا من الناحية النظرية وتعمل
بشكل جيد في الممارسة العملية عند تزويدها بالميزات المناسبة. يعد اختيار
**الميزات**</span> **(Features)** <span dir="rtl">واحدًا من أهم الطرق
لإضافة المعرفة المسبقة عن المجال إلى **أنظمة** **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement <span dir="rtl"></span>Learning
Systems<span dir="rtl">)</span>**. <span dir="rtl">يمكن اختيار الميزات
كـ **متعددات الحدود** </span>**(Polynomials)**<span dir="rtl">، ولكن هذه
الحالة تعمم بشكل سيئ في بيئة **التعليم عبر الإنترنت**</span> **(Online
Learning Setting)** <span dir="rtl">التي تعتبر عادة في **التعليم
المعزز** </span>**(Reinforcement Learning)**<span dir="rtl">.</span>
<span dir="rtl">من الأفضل اختيار الميزات وفقًا **لأساس فورييه**
</span>**(Fourier Basis)**<span dir="rtl">، أو وفقًا لشكل من **الترميز
الخشن**</span> **(Coarse Coding)** <span dir="rtl">مع **حقول مستقبلة
متداخلة نادرة** </span>**(Sparse Overlapping Receptive
Fields)**<span dir="rtl">.</span> **<span dir="rtl">التشفير القِطعي  
(</span>Tile <span dir="rtl"></span>Coding<span dir="rtl">)</span>**
<span dir="rtl">هو شكل من **الترميز الخشن**</span> **(Coarse Coding)**
<span dir="rtl">الذي يكون فعالًا من حيث **الحسابات**</span>
**(Computationally Efficient)** <span dir="rtl">ومرنًا بشكل خاص.</span>
**<span dir="rtl">دوال الأساس الشعاعي  
</span> <span dir="rtl">(</span>Radial <span dir="rtl"></span>Basis
Functions – RBFs<span dir="rtl">) </span>**<span dir="rtl">مفيدة للمهام
أحادية أو ثنائية الأبعاد التي يكون فيها الاستجابة المتغيرة بسلاسة
مهمة</span>. **<span dir="rtl">خوارزمية</span> LSTD** <span dir="rtl">هي
**الطريقة الأكثر كفاءة في استخدام البيانات** </span>**(Most
Data-Efficient Linear TD Prediction Method)**<span dir="rtl">، لكنها
تتطلب حسابات تتناسب مع مربع عدد الأوزان، في حين أن جميع الطرق الأخرى
تعقيدها خطي بالنسبة لعدد الأوزان. تشمل الطرق غير الخطية **الشبكات
العصبية الاصطناعية** </span>**(Artificial Neural Networks - ANNs)**
<span dir="rtl">المدربة بواسطة **الانتشار الخلفي**</span>
**(Backpropagation)** <span dir="rtl">وتعديلات</span>
**SGD**<span dir="rtl">؛ وقد أصبحت هذه الطرق شائعة جدًا في السنوات
الأخيرة تحت اسم **التعليم المعزز العميق**</span>
**<span dir="rtl">(</span>Deep Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**.

**<span dir="rtl">طريقة</span> TD <span dir="rtl">شبه التدرجية ذات
الخطوات</span>** $`\mathbf{n}`$ **(Linear Semi-Gradient n-step TD)**
<span dir="rtl">مضمونة للتقارب تحت الظروف القياسية، لكل</span>
$`\mathbf{n}`$<span dir="rtl">، إلى **خطأ قيمة**</span> **(VE)**
<span dir="rtl">الذي يكون ضمن حد للخطأ الأمثل (الذي يتحقق بشكل غير مباشر
بواسطة **طرق مونت كارلو** </span>**(Monte Carlo
Methods**)<span dir="rtl">.</span> <span dir="rtl">هذا الحد دائمًا ما
يكون أكثر إحكامًا بالنسبة لـ</span> $`\mathbf{n}`$ <span dir="rtl">الأعلى
ويقترب من الصفر مع</span>
$`\mathbf{n \rightarrow \infty}`$<span dir="rtl">. ومع ذلك، في الممارسة
العملية، ينتج عن</span> $`\mathbf{n}`$ <span dir="rtl">المرتفع جدًا تعلم
بطيء جدًا، وعادةً ما يكون **التمهيد بدرجة معينة  **
</span>**(**$`\mathbf{n\  < \ 1}`$**)** <span dir="rtl">مفضلًا، تمامًا كما
رأينا في مقارنة **الطرق الجدولية ذات الخطوات** </span>$`\mathbf{n}`$
**<span dir="rtl">  
(</span>Tabular n-step
<span dir="rtl"></span>Methods<span dir="rtl">)</span>**
<span dir="rtl">في الفصل 7 وفي مقارنة **طرق**</span> **TD
<span dir="rtl">الجدولية وطرق مونت كارلو</span> (Tabular TD and Monte
Carlo Methods)** <span dir="rtl">في الفصل 6</span>.

<span dir="rtl">الفصل العاشر</span>:

**<span dir="rtl">التحكم داخل السياسة مع التقريب</span>
<span dir="rtl">(</span>On-policy <span dir="rtl"></span>Control with
Approximation<span dir="rtl">)</span>**

<span dir="rtl">في هذا الفصل، نعود إلى مشكلة **التحكم**
</span>**(Control Problem)**<span dir="rtl">، الآن مع **التقريب
المعلمي** </span>**(Parametric Approximation)** **<span dir="rtl">لدالة
قيمة الإجراء</span> (Action-Value Function) q^\*(s,a)
q^​(s,a,w)≈q∗(s,a)**<span dir="rtl">، حيث يكون</span> **w∈Rdw
<span dir="rtl"></span>** <span dir="rtl">هو **متجه وزن محدود الأبعاد  
(**</span>**Finite-Dimensional Weight Vector<span dir="rtl">)</span>**.
<span dir="rtl">نواصل التركيز على حالة **داخل السياسة  
(**</span>**On-policy Case<span dir="rtl">)</span>**<span dir="rtl">،
تاركين **الطرق خارج السياسة**</span> **(Off-policy Methods)**
<span dir="rtl">للفصل 11. يتميز هذا الفصل **بخوارزمية سارسا شبه
التدرجية** </span>**(Semi-Gradient Sarsa Algorithm)**<span dir="rtl">،
الامتداد الطبيعي **لخوارزمية**</span> **TD(0) <span dir="rtl">شبه
التدرجية</span> (Semi-Gradient TD(0))** <span dir="rtl">(الفصل الماضي)
إلى **قيم الإجراء**</span> **(Action Values)** <span dir="rtl">وإلى
**التحكم داخل السياسة** </span>**(On-policy
Control)**<span dir="rtl">.</span> <span dir="rtl">في حالة الحلقات، يكون
التمديد مباشرًا، ولكن في حالة الاستمرار، يتعين علينا أن نتراجع قليلاً
ونعيد فحص كيفية استخدامنا **للخصم**</span> **(Discounting)**
<span dir="rtl">لتعريف **السياسة المثلى** </span>**(Optimal
Policy)**<span dir="rtl">.</span> <span dir="rtl">والمفاجئ أنه بمجرد أن
يكون لدينا تقريب فعلي للدوال، علينا التخلي عن الخصم والانتقال إلى صياغة
جديدة للمشكلة تُعرف بـ **صياغة المكافأة المتوسطة**</span>
**(Average-Reward Formulation)** <span dir="rtl">مع **دوال قيمة تفاضلية
جديدة** </span>**(Differential Value
Functions)**<span dir="rtl">.</span>

<span dir="rtl">نبدأ أولاً في حالة الحلقات، حيث نقوم بتمديد الأفكار
المتعلقة **بتقريب الدوال**</span> **<span dir="rtl">(</span>Function
<span dir="rtl"></span>Approximation<span dir="rtl">)
</span>**<span dir="rtl">التي قدمناها في الفصل الماضي من **قيم
الحالات**</span> **(State Values)** <span dir="rtl">إلى **قيم الإجراء**
</span>**(Action Values)**<span dir="rtl">. ثم نقوم بتمديدها إلى
**التحكم**</span> **(Control)** <span dir="rtl">باتباع النمط العام
**للتحسين المتدرج داخل السياسة** </span>**(On-policy
GPI)**<span dir="rtl">، باستخدام سياسة "-الجشع</span> (-Greedy)"
<span dir="rtl">لاختيار الإجراءات. نعرض النتائج لخوارزمية **سارسا
الخطية** </span>**n <span dir="rtl">ذات الخطوات</span> (n-step Linear
Sarsa)** <span dir="rtl">على مشكلة **السيارة الجبلية**
</span>**(Mountain Car Problem)**<span dir="rtl">.</span>
<span dir="rtl">بعد ذلك، ننتقل إلى حالة الاستمرار ونعيد تطوير هذه
الأفكار لحالة **المكافأة المتوسطة**</span> **(Average-Reward Case)**
<span dir="rtl">مع **القيم التفاضلية** </span>**(Differential
Values)**<span dir="rtl">.</span>

<span dir="rtl">تمديد **طرق التنبؤ شبه التدرجي**</span> **(Semi-gradient
Prediction Methods)** <span dir="rtl">من الفصل 9 إلى **قيم
الإجراء**</span> **(Action Values)** <span dir="rtl">أمر بسيط. في هذه
الحالة، تكون **دالة قيمة الإجراء التقريبية** </span>**(Approximate
Action-Value Function) <span dir="rtl"></span>
q^≈**$`q_{\pi}`$<span dir="rtl">ممثلة كـ **صيغة دالية بارامتر**
</span>**(Parameterized Functional Form)** <span dir="rtl">مع **متجه
الوزن** </span>**(Weight Vector) w**<span dir="rtl">.</span>
<span dir="rtl">بينما كنا نعتبر سابقًا **أمثلة تدريبية عشوائية**</span>
**(Random Training Examples)** <span dir="rtl">من الشكل</span>
St→Ut​<span dir="rtl">، فإننا الآن نعتبر أمثلة من الشكل</span>
St,At→Ut<span dir="rtl">.</span>

<span dir="rtl">يمكن أن يكون **هدف التحديث**</span> **(Update Target)
Ut** <span dir="rtl">أي تقريب لـ</span> $`q_{\pi}`$
**(St​,At​)**<span dir="rtl">، بما في ذلك القيم **المسترجعة
المعتادة**</span> **(Backed-up Values)** <span dir="rtl">مثل **عائد مونت
كارلو الكامل  
(**</span>**Full <span dir="rtl"></span>Monte Carlo
Return<span dir="rtl">)</span> Gt** <span dir="rtl">أو أي من **عوائد
سارسا ذات الخطوات**</span>**n <span dir="rtl">  
(</span>n-step Sarsa
<span dir="rtl"></span>Returns<span dir="rtl">)</span>**
<span dir="rtl">(المعادلة 7.4). التحديث العام بواسطة **الانحدار
التدرجي** </span>**(Gradient-Descent Update)** <span dir="rtl">لتنبؤ
**قيمة الإجراء**</span> **(Action-Value Prediction)**
<span dir="rtl">هو:</span>

``` math
w_{t + 1} = w_{t} + \alpha\left\lbrack U_{t} - \widehat{q}\left( S_{t},A_{t},w_{t} \right) \right\rbrack\nabla\widehat{q}\left( S_{t},A_{t},w_{t} \right).
```

<span dir="rtl">على سبيل المثال، التحديث لطريقة **سارسا ذات الخطوة
الواحدة**</span> **(One-step Sarsa Method)** <span dir="rtl">هو</span>:

``` math
w_{t + 1} = w_{t} + \alpha\left\lbrack R_{t + 1} + \gamma\widehat{q}\left( S_{t + 1},A_{t + 1},w_{t} \right) - \widehat{q}\left( S_{t},A_{t},w_{t} \right) \right\rbrack\nabla\widehat{q}\left( S_{t},A_{t},w_{t} \right).
```

<span dir="rtl">نسمي هذه الطريقة **سارسا شبه التدرجية ذات الخطوة الواحدة
للحلقات (**</span>**Episodic Semi-gradient One-step
Sarsa<span dir="rtl">)</span>**. <span dir="rtl">بالنسبة **لسياسة
ثابتة** </span>**(Constant Policy)**<span dir="rtl">، تتقارب هذه الطريقة
بنفس الطريقة التي تتقارب بها</span> **TD(0)**<span dir="rtl">، مع نفس
نوع **حد الخطأ**</span> **(Error Bound)** <span dir="rtl">الموضح في
المعادلة</span> (9.14)<span dir="rtl">.</span>

<span dir="rtl">لتكوين **طرق التحكم** </span>**(Control
Methods)**<span dir="rtl">، نحتاج إلى ربط **طرق تنبؤ قيمة الإجراء  
(**</span>**Action-Value Prediction Methods<span dir="rtl">)
</span>**<span dir="rtl">بتقنيات لتحسين **السياسة (**</span>**Policy
<span dir="rtl"></span>Improvement<span dir="rtl">)
</span>**<span dir="rtl">واختيار **الإجراء** </span>**(Action
Selection)**<span dir="rtl">.</span> <span dir="rtl">التقنيات المناسبة
التي يمكن تطبيقها على **الإجراءات المستمرة** </span>**(Continuous
Actions)**<span dir="rtl">، أو على الإجراءات من مجموعات كبيرة من
**المجموعات المتقطعة** </span>**(Discrete Sets)**<span dir="rtl">، هي
موضوع بحث مستمر ولم يتم التوصل إلى حل واضح حتى الآن. من ناحية أخرى، إذا
كانت مجموعة **الإجراءات**</span> **(Action Set)** <span dir="rtl">متقطعة
وليست كبيرة جدًا، فيمكننا استخدام التقنيات التي تم تطويرها بالفعل في
الفصول السابقة. بمعنى أنه لكل إجراء ممكن</span> a <span dir="rtl">متاح
في **الحالة الحالية** </span>**(Current State) St ​**<span dir="rtl">،
يمكننا حساب</span> q^(St,a,wt)<span dir="rtl">،</span>
<span dir="rtl">ثم إيجاد **الإجراء الجشع** </span>**(Greedy Action)**
At∗​=argmaxa​q^​(St​,a,wt​)<span dir="rtl">.</span> <span dir="rtl">يتم بعد
ذلك **تحسين السياسة** </span>**(Policy Improvement)**
<span dir="rtl">(في حالة **داخل السياسة**</span> **(On-policy Case)**
<span dir="rtl">التي تم تناولها في هذا الفصل) عن طريق تغيير **سياسة
التقدير**</span> **(Estimation Policy)** <span dir="rtl">إلى **تقريب مرن
للسياسة الجشعة** </span>**(Soft Approximation of the Greedy Policy)**
<span dir="rtl">مثل **السياسة الجشعة**</span> **"-
<span dir="rtl">(</span>-Greedy Policy<span dir="rtl">)</span>**.
<span dir="rtl">يتم اختيار **الإجراءات**</span> **(Actions)**
<span dir="rtl">وفقًا لهذه السياسة نفسها. يتم تقديم **الكود
الزائف**</span> **(Pseudocode)** <span dir="rtl">للخوارزمية الكاملة في
الصندوق</span>.

<span dir="rtl">خوارزمية سارسا</span> (Sarsa) <span dir="rtl">نصف
التدرج</span> (Semi-gradient) <span dir="rtl">الحلقية</span> (Episodic)
<span dir="rtl">لتقدير</span>

<img src="./media/image118.png"
style="width:6.26806in;height:3.37569in" />

**<span dir="rtl">مثال 10.1: مهمة السيارة الجبلية</span> (Mountain Car
Task)**

<span dir="rtl">اعتبر مهمة قيادة سيارة ذات قوة محرك ضعيفة صعودًا على طريق
جبلي شديد الانحدار، كما هو موضح في الرسم التوضيحي في الجزء العلوي الأيسر
من الشكل 10.1. الصعوبة تكمن في أن الجاذبية أقوى من محرك السيارة، وحتى
عند استخدام القوة القصوى، لا يمكن للسيارة التسارع للصعود على المنحدر
الشديد. الحل الوحيد هو الابتعاد أولاً عن الهدف والصعود على المنحدر
المقابل في اليسار. ثم، عند تطبيق القوة القصوى، يمكن للسيارة أن تبني قوة
كافية من القصور الذاتي لتحملها للصعود على المنحدر الشديد على الرغم من
أنها تبطئ طوال الطريق. هذا مثال بسيط على مهمة التحكم المستمر</span>
(Continuous Control Task) <span dir="rtl">حيث يجب أن تزداد الأمور سوءًا
بمعنى معين (الابتعاد عن الهدف) قبل أن تتحسن. العديد من منهجيات
التحكم</span> (Control Methodologies) <span dir="rtl">تواجه صعوبات كبيرة
مع مهام من هذا النوع ما لم يتم تعزيزها بشكل صريح من قبل مصمم
بشري</span>.

<img src="./media/image119.png"
style="width:6.26806in;height:3.75903in" />

<span dir="rtl">الشكل 10.1: مهمة السيارة الجبلية</span> (Mountain Car
Task) <span dir="rtl">(اللوحة العلوية اليسرى) ودالة التكلفة حتى
الوصول</span> (Cost-to-go Function) (−maxa​q^​(s,a,w))
<span dir="rtl">التي تم تعلمها خلال إحدى الجولات</span>.

<span dir="rtl">المكافأة</span> (Reward) <span dir="rtl">في هذه المشكلة
هي</span> −1-1−1 <span dir="rtl">في جميع خطوات الزمن حتى تتحرك السيارة
متجاوزة موضع الهدف في قمة الجبل، مما ينهي الحلقة</span> (Episode).
<span dir="rtl">هناك ثلاث إجراءات</span> (Actions)
<span dir="rtl">ممكنة: تسريع كامل للأمام</span> (+1) (+1)
(+1)<span dir="rtl">، تسريع كامل للخلف</span> (−1) (-1)
(−1)<span dir="rtl">، وعدم التسريع (0). تتحرك السيارة وفقًا لفيزياء
مبسطة. يتم تحديث موضعها</span> (xt) <span dir="rtl"></span>
<span dir="rtl">وسرعتها</span> (x˙t) <span dir="rtl"></span>
<span dir="rtl">بواسطة المعادلة التالية</span>:

``` math
x_{t + 1} = \text{bound}\left\lbrack x_{t} + \dot{x_{t + 1}} \right\rbrack
```

``` math
\dot{x_{t + 1}} = \text{bound}\left\lbrack \dot{x_{t}} + 0.001A_{t} - 0.0025\cos\left( 3x_{t} \right) \right\rbrack
```

<span dir="rtl">حيث أن عملية التقييد</span> (Bound Operation)
<span dir="rtl">تفرض أن</span> 0.5−1.2≤xt+1​≤0.5 <span dir="rtl">و</span>
−0.07≤x˙t+1≤0.07-0.07<span dir="rtl">.</span> <span dir="rtl">بالإضافة
إلى ذلك، عندما يصل</span> xt+1 <span dir="rtl"></span>​
<span dir="rtl">إلى الحد الأيسر، يتم إعادة ضبط السرعة</span> x˙t+1
<span dir="rtl">إلى الصفر. وعندما يصل إلى الحد الأيمن، يتم تحقيق الهدف
وتنتهي الحلقة</span> (Episode)<span dir="rtl">. بدأت كل حلقة من موضع
عشوائي</span> xt∈\[−0.6,−0.4) <span dir="rtl">وبسرعة صفرية. لتحويل
متغيرات الحالة المستمرة</span> (Continuous State Variables)
<span dir="rtl">إلى ميزات ثنائية</span> (Binary
Features)<span dir="rtl">، استخدمنا تجزئة الشبكة</span> (Grid-tilings)
<span dir="rtl">كما هو موضح في الشكل 9.9. استخدمنا 8 تجزئات، حيث يغطي كل
تجزئة 1/8 من المسافة المحدودة في كل بعد، مع تعويضات غير متناظرة  
(</span>Asymmetrical <span dir="rtl"></span>Offsets<span dir="rtl">) كما
هو موصوف في القسم 9.5.4.1. بعد ذلك، تم دمج متجهات الميزات</span> x(s,a)
<span dir="rtl">التي تم إنشاؤها بواسطة ترميز التجزئة</span> (Tile
Coding) <span dir="rtl">خطيًا مع متجه البارامترات لتقريب دالة القيمة
للإجراء</span> (Action-value Function)<span dir="rtl">:</span>

``` math
q(s,a,w) = w^{\top}x(s,a) = \sum_{i = 1}^{d}w_{i} \cdot x_{i}(s,a)
```

<span dir="rtl">لكل زوج من الحالة</span> (State)<span dir="rtl">،</span>
s<span dir="rtl">، والإجراء</span> (Action)<span dir="rtl">،</span>
a<span dir="rtl">.</span> <span dir="rtl">يُظهر الشكل 10.1 ما يحدث عادة
أثناء تعلم حل هذه المهمة باستخدام هذا الشكل من تقريب الدوال</span>
(Function Approximation)<span dir="rtl">.</span> <span dir="rtl">يظهر في
الشكل السالب لدالة القيمة</span> (Negative of the Value Function)
<span dir="rtl">(دالة التكلفة حتى الوصول</span> (Cost-to-go
Function)<span dir="rtl">) التي تم تعلمها في جولة واحدة. كانت قيم
الإجراءات الأولية جميعها تساوي الصفر، وهو ما كان يعتبر متفائلًا (حيث أن
جميع القيم الحقيقية سلبية في هذه المهمة)، مما تسبب في حدوث استكشاف مكثف
حتى مع أن معامل الاستكشاف</span> (Exploration
Parameter)<span dir="rtl">،  
</span>ϵ<span dir="rtl">، كان 0. يمكن رؤية ذلك في اللوحة العلوية الوسطى
من الشكل، والمعنونة بـ "الخطوة 428". في هذا الوقت لم تكتمل حتى حلقة
واحدة، لكن السيارة كانت تتأرجح ذهابًا وإيابًا في الوادي، متبعة مسارات
دائرية في فضاء الحالة</span> (State Space)<span dir="rtl">.</span>
<span dir="rtl">جميع الحالات التي تمت زيارتها بشكل متكرر قيمت على أنها
أسوأ من الحالات غير المستكشفة، لأن المكافآت الفعلية كانت أسوأ مما كان
متوقعًا (بشكل غير واقعي). هذا يستمر في دفع العميل</span> (Agent)
<span dir="rtl">بعيدًا عن الأماكن التي زارها مسبقًا، لاستكشاف حالات جديدة،
حتى يتم العثور على حل</span>.

<span dir="rtl">يُظهر الشكل 10.2 عدة منحنيات تعلم</span> (Learning
Curves) <span dir="rtl">لخوارزمية سارسا</span> (Sarsa)
<span dir="rtl">نصف التدرج</span> (Semi-gradient) <span dir="rtl">في هذه
المشكلة، مع أحجام خطوات مختلفة</span> (Various Step
Sizes)<span dir="rtl">.</span>

<img src="./media/image120.png"
style="width:6.26806in;height:2.63264in" />

<span dir="rtl">الشكل 10.2: منحنيات التعليم</span> (Learning Curves)
<span dir="rtl">لمهمة السيارة الجبلية</span> (Mountain Car)
<span dir="rtl">باستخدام خوارزمية سارسا</span> (Sarsa)
<span dir="rtl">نصف التدرج</span> (Semi-gradient) <span dir="rtl">مع
تقريب الدوال باستخدام ترميز التجزئة</span> (Tile-coding Function
Approximation) <span dir="rtl">واختيار الإجراءات بطريقة جشع</span>
ϵ<span dir="rtl">  
(</span>ϵ-Greedy Action Selection<span dir="rtl">).</span>

**<u>10.2 <span dir="rtl">خوارزمية سارسا</span> (Sarsa)
<span dir="rtl">نصف التدرج</span> (Semi-gradient)
<span dir="rtl">بخطوات</span> (n-step)</u>**

<span dir="rtl">يمكننا الحصول على نسخة بخطوات</span> n
<span dir="rtl">من خوارزمية سارسا</span> (Sarsa) <span dir="rtl">نصف
التدرج  
(</span>Semi-gradient<span dir="rtl">) الحلقية</span> (Episodic)
<span dir="rtl">من خلال استخدام عائد بخطوات</span> (n-step Return)
<span dir="rtl">كهدف التحديث في معادلة تحديث سارسا نصف التدرج</span>
<span dir="rtl">(</span>Semi-gradient Sarsa Update
Equation<span dir="rtl">) (المعادلة 10.1). العائد بخطوات</span> n
<span dir="rtl">يعمم فورًا من شكله الجدولي</span> (Tabular Form)
<span dir="rtl">(المعادلة 7.4) إلى شكل تقريب الدوال</span> (Function
Approximation Form)<span dir="rtl">:</span>

``` math
G_{t:t + n} = R_{t + 1} + \gamma R_{t + 2} + \cdots + \gamma^{n - 1}R_{t + n} + \gamma^{n}\widehat{q}\left( S_{t + n},A_{t + n},w_{t + n - 1} \right),\quad t + n < T
```

``` math
G_{t:t + n} = G_{t}\text{ if }t + n \geq T,\text{ as usual. The n-step update equation is}
```

``` math
w_{t + n} = w_{t + n - 1} + \alpha\left\lbrack G_{t:t + n} - \widehat{q}\left( S_{t},A_{t},w_{t + n - 1} \right) \right\rbrack\nabla\widehat{q}\left( S_{t},A_{t},w_{t + n - 1} \right),\quad 0 \leq t < T.
```

<span dir="rtl">يرد الكود الزائف الكامل في المربع أدناه.</span>

<span dir="rtl">خوارزمية سارسا</span> (Sarsa) <span dir="rtl">نصف
التدرج</span> (Semi-gradient) <span dir="rtl">الحلقية</span> (Episodic)
<span dir="rtl">بخطوات</span> n <span dir="rtl">لتقدير</span>

<img src="./media/image121.png"
style="width:6.26806in;height:5.14444in" />

<span dir="rtl">كما رأينا سابقًا، يكون الأداء أفضل إذا تم استخدام مستوى
وسيط من عملية **التدعيم**</span> (bootstrapping) <span dir="rtl">الذي
يتوافق مع</span> n <span dir="rtl">أكبر من 1. يوضح الشكل 10.3 كيف يميل
هذا الخوارزم إلى التعليم بسرعة أكبر والحصول على أداء أفضل على المدى
البعيد عندما يكون</span> n=8 <span dir="rtl">مقارنةً بـ</span> n=1
<span dir="rtl">في مهمة **السيارة الجبلية**</span> (Mountain
Car)<span dir="rtl">. يظهر الشكل 10.4 نتائج دراسة أكثر تفصيلًا حول تأثير
**البارامترات**</span> α <span dir="rtl">و</span>n <span dir="rtl">على
معدل التعليم في هذه المهمة</span>.

<img src="./media/image122.png"
style="width:6.30965in;height:2.802in" />

<span dir="rtl">الشكل 10.3: أداء **سارسا**</span> **(Sarsa)**
<span dir="rtl">باستخدام خطوة واحدة مقابل 8 خطوات شبه التدرج في مهمة  
**السيارة الجبلية**</span> **(Mountain Car)**. <span dir="rtl">تم
استخدام أحجام خطوات مناسبة</span>: α=0.5/8 <span dir="rtl">عندما</span>
n=1 <span dir="rtl">و</span>α=0.3/8 <span dir="rtl">عندما</span>
n=8<span dir="rtl">.  
</span><img src="./media/image123.png"
style="width:6.26806in;height:2.79028in" />

<span dir="rtl">الشكل 10.4: تأثير</span> α <span dir="rtl">و</span>n
<span dir="rtl">على الأداء المبكر لخوارزمية **سارسا**</span> **(Sarsa)**
<span dir="rtl">شبه التدرجية مع</span> n <span dir="rtl">خطوات، وتقدير
الدالة باستخدام **ترميز البلاط**</span> **(Tile-Coding Function
Approximation)** <span dir="rtl">في مهمة **السيارة الجبلية**
</span>**(Mountain Car)**<span dir="rtl">.</span>
<span dir="rtl">كالعادة، قدم مستوى وسيط من **التدعيم**
</span>**(Bootstrapping)** <span dir="rtl"></span>(n=4)
<span dir="rtl">الأداء الأفضل. هذه النتائج تمثل قيمًا مختارة لـ</span> α
<span dir="rtl">على مقياس لوغاريتمي، ومن ثم تم توصيلها بخطوط مستقيمة.
تراوحت الأخطاء المعيارية من 0.5 (أقل من عرض الخط) عندما</span> n=1
<span dir="rtl">إلى حوالي 4 عندما</span> n=16<span dir="rtl">، لذلك فإن
التأثيرات الرئيسية كلها ذات دلالة إحصائية</span>.
<span dir="rtl"></span>

<span dir="rtl">التمرين 10.1: لم نقم بدراسة أو تقديم **الكود
الزائف**</span> **(Pseudocode)** <span dir="rtl">لأي من طرق **مونت
كارلو** </span>**(Monte Carlo)** <span dir="rtl">في هذا الفصل. كيف يمكن
أن تكون هذه الطرق؟ لماذا من المنطقي عدم تقديم **الكود الزائف**</span>
**(Pseudocode)** <span dir="rtl">لها؟ كيف سيكون أداؤها في مهمة **السيارة
الجبلية  
(**</span>**Mountain
<span dir="rtl"></span>Car<span dir="rtl">)</span>**<span dir="rtl">؟</span>

<span dir="rtl">التمرين 10.2: قدم **الكود الزائف**</span>
**(Pseudocode)** <span dir="rtl">لـ **سارسا المتوقع**</span> **(Expected
Sarsa)** <span dir="rtl">بخطوة واحدة شبه التدرج للتحكم</span>.

<span dir="rtl">التمرين 10.3: لماذا تحتوي النتائج الموضحة في الشكل 10.4
على أخطاء معيارية أعلى عند</span> n <span dir="rtl">الكبير مقارنةً
بـ</span> n <span dir="rtl">الصغير؟</span>

10.3 <span dir="rtl">متوسط المكافأة: إعداد مشكلة جديدة للمهام
المستمرة</span> <span dir="rtl">(</span>Average Reward: A New
<span dir="rtl"></span>Problem Setting for Continuing
Tasks<span dir="rtl">)</span>

<span dir="rtl">نقدم الآن إعدادًا ثالثًا كلاسيكيًا—جنبًا إلى جنب مع
الإعدادات الحلقية والمخفضة—لتحديد الهدف في مشاكل اتخاذ القرار
الماركوفية</span> **(Markov Decision Problems -
MDPs)**<span dir="rtl">.</span> <span dir="rtl">كما هو الحال في الإعداد
المخفض، ينطبق إعداد **متوسط المكافأة**</span> **(Average Reward)**
<span dir="rtl">على المشاكل المستمرة، وهي المشاكل التي تستمر فيها
التفاعلات بين العميل</span> **(Agent)** <span dir="rtl">والبيئة</span>
**(Environment)** <span dir="rtl">إلى الأبد دون انقطاع أو حالات بداية
ونهاية. على عكس ذلك الإعداد، لا يوجد هنا تخفيض—أي أن العميل يهتم
بالمكافآت المؤجلة تمامًا كما يهتم بالمكافآت الفورية. يعد إعداد **متوسط
المكافأة**</span> **<span dir="rtl">(</span>Average
Reward<span dir="rtl">) </span>**<span dir="rtl">واحدًا من الإعدادات
الرئيسية التي يتم اعتبارها عادةً في نظرية البرمجة الديناميكية التقليدية،
وأقل شيوعًا في **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">.</span> <span dir="rtl">كما سنناقش في القسم
التالي، فإن الإعداد المخفض يواجه مشاكل مع **تقريب الدوال**
</span>**(Function Approximation)**<span dir="rtl">، وبالتالي هناك حاجة
إلى إعداد **متوسط المكافأة**</span> **(Average Reward)**
<span dir="rtl">ليحل محله</span>.

<span dir="rtl">في إعداد **متوسط المكافأة** </span>**(Average
Reward)**<span dir="rtl">، يتم تعريف جودة السياسة</span> **(Policy)**
<span dir="rtl"></span>π <span dir="rtl">كمعدل المكافأة المتوسط، أو
ببساطة متوسط المكافأة، أثناء اتباع تلك السياسة، والذي نرمز له بـ</span>
r(π)<span dir="rtl">:</span>

``` math
r(\pi) = \lim_{h \rightarrow \infty}\frac{1}{h}\sum_{t = 1}^{h}{E\left\lbrack R_{t}\mid S_{0},A_{0:t - 1} \sim \pi \right\rbrack}
```

``` math
\underset{t \rightarrow \infty}{= lim}E\left\lbrack R_{t}\mid S_{0},A_{0:t - 1} \sim \pi \right\rbrack
```

``` math
\underset{t \rightarrow \infty}{= lim}E\left\lbrack R_{t}\mid S_{0},A_{0:t - 1} \sim \pi \right\rbrack
```

<span dir="rtl">حيث أن التوقعات تكون مشروطة بالحالة الابتدائية</span> S0
<span dir="rtl">وعلى الأفعال اللاحقة</span> A0,A1,…,At−1,
<span dir="rtl">التي تم اتخاذها وفقًا للسياسة</span>
π<span dir="rtl">.</span> **<span dir="rtl">التوزيع الثابت</span>
(Steady-State Distribution)** μπ <span dir="rtl">هو التوزيع الذي يُرمز له
كما يلي</span>:

μπ(s)=t→∞lim​Pr{St​=s∣A0:t−1​∼π}

<span dir="rtl">والذي يُفترض أنه موجود لأي سياسة</span> π
<span dir="rtl">وأنه مستقل عن</span> S0 <span dir="rtl"></span>​.
<span dir="rtl">تُعرف هذه الفرضية حول **مشكلة اتخاذ القرار
الماركوفية**</span> **(MDP)** <span dir="rtl">باسم **الإرجودية**
</span>**(Ergodicity)**<span dir="rtl">.</span> <span dir="rtl">تعني
الإرجودية أنه بغض النظر عن الحالة التي يبدأ فيها **مشكلة اتخاذ القرار
الماركوفية**</span> **(MDP)** <span dir="rtl">أو أي قرار مبكر يتخذه
العميل، سيكون له تأثير مؤقت فقط؛ على المدى الطويل، يعتمد التوقع للبقاء
في حالة معينة على السياسة واحتمالات الانتقال الخاصة بـ **مشكلة اتخاذ
القرار الماركوفية** </span>**(MDP)**<span dir="rtl">.</span>
<span dir="rtl">الإرجودية كافية لضمان وجود الحدود في المعادلات المذكورة
أعلاه</span>.

<span dir="rtl">هناك تمايزات دقيقة يمكن تمييزها بين أنواع مختلفة من
الأمثلية في حالة الاستمرار غير المخفضة. ومع ذلك، لأغراض عملية، قد يكون
من الكافي ببساطة ترتيب السياسات وفقًا لمتوسط المكافأة لكل خطوة زمنية،
بمعنى آخر، وفقًا لقيم</span> r(π)<span dir="rtl">.</span>
<span dir="rtl">هذه القيمة هي في الأساس متوسط المكافأة تحت
السياسة</span> π<span dir="rtl">، كما هو مُقترح في المعادلة (10.7). على
وجه الخصوص، نعتبر جميع السياسات التي تحقق القيمة القصوى لـ</span> r(π)
<span dir="rtl">على أنها سياسات مثالية</span>.

<span dir="rtl">لاحظ أن **التوزيع الثابت**</span> **(Steady-State
Distribution)** <span dir="rtl">هو التوزيع الخاص الذي، إذا قمت باختيار
الأفعال وفقًا للسياسة</span> π<span dir="rtl">، فإنك ستظل في نفس التوزيع.
أي،</span>

``` math
\sum_{s}^{}{\mu^{\pi}(s)\sum_{a}^{}{\pi\left( a\mid s \right)p\left( s'\mid s,a \right)}} = \mu^{\pi}\left( s' \right)
```

<span dir="rtl">في إعداد **متوسط المكافأة** </span>**(Average
Reward)**<span dir="rtl">، تُعرّف العوائد من حيث الفروقات بين المكافآت
ومتوسط المكافأة</span>:<span dir="rtl">  
</span>

``` math
G_{t} = R_{t + 1} - r(\pi) + R_{t + 2} - r(\pi) + R_{t + 3} - r(\pi) +
```

<span dir="rtl">هذا ما يُعرف بالعائد التفاضلي</span> (Differential
Return)<span dir="rtl">، وتُعرف **دوال القيمة التفاضلية**
</span>**(Differential Value Functions)** <span dir="rtl">المقابلة لها
بنفس الطريقة، وسنستخدم نفس الترميز لها كما استخدمنا دائمًا</span>:

``` math
v^{\pi}(s) = E^{\pi}\left\lbrack G_{t}\mid S_{t} = s \right\rbrack
```

<span dir="rtl">و</span>

``` math
q^{\pi}(s,a) = E^{\pi}\left\lbrack G_{t}\mid S_{t} = s,A_{t} = a \right\rbrack
```

<span dir="rtl">وبالمثل بالنسبة لـ</span> v∗
<span dir="rtl">و</span>q∗<span dir="rtl">.</span>
**<span dir="rtl">دوال القيمة التفاضلية</span> (Differential Value
Functions)** <span dir="rtl">لديها أيضًا معادلات بيلمان</span> (Bellman
Equations)<span dir="rtl">، لكنها تختلف قليلاً عن تلك التي رأيناها سابقًا.
ببساطة، نقوم بإزالة جميع عوامل التخفيض</span> (γ)
<span dir="rtl">واستبدال جميع المكافآت بالفارق بين المكافأة ومتوسط
المكافأة الحقيقي</span>.

``` math
v^{\pi}(s) = \sum_{a}^{}{\pi\left( a\mid s \right)\sum_{r,s'}^{}{p\left( s',r\mid s,a \right)\left\lbrack r - r(\pi) + v^{\pi}\left( s' \right) \right\rbrack}}
```

``` math
q^{\pi}(s,a) = \sum_{r,s'}^{}{p\left( s',r\mid s,a \right)\left\lbrack r - r(\pi) + \sum_{a'}^{}{\pi\left( a'\mid s' \right)q^{\pi}\left( s',a' \right)} \right\rbrack}
```

``` math
v^{*}(s) = \max_{a}{\sum_{r,s'}^{}{p\left( s',r\mid s,a \right)\left\lbrack r - \max_{\pi}\ r(\pi) + v^{*}\left( s' \right) \right\rbrack}}
```

``` math
q^{*}(s,a) = \sum_{r,s'}^{}{p\left( s',r\mid s,a \right)\left\lbrack r - \max_{\pi}\ r(\pi) + \max_{a'}q^{*}\left( s',a' \right) \right\rbrack}
```

<span dir="rtl">(انظر المعادلات (3.14)، تمرين 3.17، (3.19)، و</span>
(3.20)<span dir="rtl">). هناك أيضًا صيغة تفاضلية لأخطاء</span> TD
<span dir="rtl">الاثنين</span>:

``` math
\delta_{t} = R_{t + 1} - \overline{R_{t}} + \widehat{v}\left( S_{t + 1},w_{t} \right) - \widehat{v}\left( S_{t},w_{t} \right)
```

<span dir="rtl">و</span>

``` math
\delta_{t} = R_{t + 1} - \overline{R_{t}} + \widehat{q}\left( S_{t + 1},A_{t + 1},w_{t} \right) - \widehat{q}\left( S_{t},A_{t},w_{t} \right)
```

<span dir="rtl">حيث أن</span> Rˉt <span dir="rtl"></span>​
<span dir="rtl">هو تقدير عند الزمن</span> t <span dir="rtl">لمتوسط
المكافأة</span> r(π)<span dir="rtl">.</span> <span dir="rtl">باستخدام
هذه التعريفات البديلة، يمكن تطبيق معظم خوارزمياتنا والعديد من النتائج
النظرية على إعداد متوسط المكافأة دون تغيير</span>.

<span dir="rtl">على سبيل المثال، يتم تعريف نسخة متوسط المكافأة من
**سارسا شبه التدرجية**</span> **<span dir="rtl">(</span>Semi-Gradient
<span dir="rtl"></span>Sarsa<span dir="rtl">)</span>**
<span dir="rtl">تمامًا كما في المعادلة (10.2)، باستثناء استخدام النسخة
التفاضلية من خطأ</span> TD<span dir="rtl">.</span> <span dir="rtl">أي،
عن طريق</span>:

``` math
w_{t + 1} = w_{t} + \alpha\delta_{t}\nabla\widehat{q}\left( S_{t},A_{t},w_{t} \right)
```

<span dir="rtl">التمرين 10.4: قدم **الكود الزائف**</span>
**(Pseudocode)** <span dir="rtl">للنسخة التفاضلية من</span> **Q-learning
<span dir="rtl">شبه التدرجية</span> (Semi-Gradient
Q-learning)**<span dir="rtl">.</span>

<span dir="rtl">التمرين 10.5: ما هي المعادلات المطلوبة (إلى جانب
المعادلة 10.10) لتحديد النسخة التفاضلية  
من</span> **TD(0)**<span dir="rtl">؟</span>

**<span dir="rtl">سارسا شبه التدرج التفاضلية</span> (Differential
Semi-Gradient Sarsa)** <span dir="rtl">لتقدير</span>

<img src="./media/image124.png"
style="width:6.26806in;height:3.1375in" />

<span dir="rtl">التمرين 10.6: اعتبر عملية مكافأة ماركوف التي تتكون من
حلقة تضم ثلاث حالات:</span> A<span dir="rtl">،</span> B<span dir="rtl">،
و</span>C<span dir="rtl">، حيث تتم الانتقالات بين الحالات بشكل حتمي حول
الحلقة. يتم تلقي مكافأة قدرها 1+ عند الوصول إلى الحالة</span>
A<span dir="rtl">، بينما تكون المكافأة 0 في الحالات الأخرى. ما هي القيم
التفاضلية للحالات الثلاث؟</span>

<span dir="rtl">**المثال 10.2: مهمة إدارة وصول في طابور** هذه مهمة قرار
تتعلق بإدارة الوصول إلى مجموعة من 10 خوادم. يصل العملاء إلى طابور واحد
بأربعة مستويات من الأولوية المختلفة. إذا تم منح العميل الوصول إلى خادم،
فإنه يدفع مكافأة قدرها 1، 2، 4، أو 8 للخادم، بناءً على أولويته، حيث يدفع
العملاء ذوو الأولوية الأعلى أكثر. في كل خطوة زمنية، يتم قبول العميل في
مقدمة الطابور (يُخصص لأحد الخوادم) أو رفضه (يتم إزالته من الطابور بدون
مكافأة). في كلتا الحالتين، في الخطوة الزمنية التالية، يتم النظر في
العميل التالي في الطابور. لا يفرغ الطابور أبدًا، وتكون أولويات العملاء في
الطابور موزعة بشكل عشوائي ومتساوي. بالطبع، لا يمكن خدمة العميل إذا لم
يكن هناك خادم متاح؛ في هذه الحالة، يتم رفض العميل دائمًا. كل خادم مشغول
يصبح متاحًا مع احتمال</span> p=0.06 <span dir="rtl">في كل خطوة زمنية. على
الرغم من أننا وصفنا هذا السيناريو بشكل محدد، دعنا نفترض أن إحصائيات
الوصول والمغادرة غير معروفة. المهمة هي تحديد ما إذا كان يجب قبول أو رفض
العميل التالي في كل خطوة بناءً على أولويته وعدد الخوادم المتاحة، بهدف
تعظيم المكافأة طويلة المدى دون تخفيض</span>.

<span dir="rtl">في هذا المثال، ندرس حلاً جدوليًا لهذه المشكلة. على الرغم
من عدم وجود تعميم بين الحالات، يمكننا مع ذلك اعتبارها في سياق **تقريب
الدوال**</span> **(Function Approximation)** <span dir="rtl">العام حيث
يعمم هذا السياق على السياق الجدولي. وبالتالي، لدينا تقدير تفاضلي لقيمة
الإجراء لكل زوج من الحالة (عدد الخوادم المتاحة وأولوية العميل في مقدمة
الطابور) والإجراء (القبول أو الرفض). يوضح الشكل 10.5 الحل الذي تم العثور
عليه بواسطة **سارسا شبه التدرج التفاضلية**</span>
**<span dir="rtl">(</span>Differential Semi-Gradient
<span dir="rtl"></span>Sarsa<span dir="rtl">)
</span>**<span dir="rtl">مع البارامترات</span>
α=0.01<span dir="rtl">،</span> β=0.01<span dir="rtl">،
و</span>ϵ=0.1<span dir="rtl">.</span> <span dir="rtl">كانت قيم الإجراءات
الأولية و</span>Rˉ <span dir="rtl">صفرًا</span>.

<img src="./media/image125.png"
style="width:6.26806in;height:4.46181in" />

<span dir="rtl">الشكل 10.5: السياسة ودالة القيمة التي تم العثور عليها
بواسطة **سارسا شبه التدرج التفاضلية بخطوة واحدة**</span> **(Differential
Semi-Gradient One-Step Sarsa)** <span dir="rtl">في مهمة إدارة الوصول إلى
الطابور بعد 2 مليون خطوة. الانخفاض على الجانب الأيمن من الرسم البياني
ربما يكون بسبب نقص البيانات؛ العديد من هذه الحالات لم تُختبر أبدًا. القيمة
التي تم تعلمها لـ</span> Rˉ <span dir="rtl">كانت حوالي 2.31</span>.

**<span dir="rtl">التمرين 10.7</span>:** <span dir="rtl">افترض وجود
**مشكلة اتخاذ القرار الماركوفية**</span> **(MDP)** <span dir="rtl">التي
تحت أي سياسة تنتج سلسلة محددة من المكافآت: +1، 0، +1، 0، +1، 0، ... التي
تستمر إلى الأبد. تقنيًا، هذا غير مسموح به لأنه ينتهك **الإرجودية**
</span>**(Ergodicity)**<span dir="rtl">؛ لا يوجد توزيع ثابت محدود</span>
μπ <span dir="rtl">والحد الموضح في المعادلة (10.7) لا يوجد. ومع ذلك، فإن
متوسط المكافأة الموضح في المعادلة (10.6) مُعرف جيدًا؛ ما هو؟ الآن فكر في
حالتين في هذه **مشكلة اتخاذ القرار الماركوفية**
</span>**(MDP)**<span dir="rtl">.</span> <span dir="rtl">من
الحالة</span> A<span dir="rtl">، تكون سلسلة المكافآت كما هو موضح أعلاه،
بدءًا من +1، بينما من الحالة</span> B<span dir="rtl">، تبدأ سلسلة
المكافآت بـ 0 ثم تستمر بـ +1، 0، +1، 0، ... العائد التفاضلي الموضح في
المعادلة (10.9) غير معرف جيدًا لهذه الحالة لأن الحد لا يوجد. لإصلاح هذا،
يمكن للمرء أن يُعرّف قيمة الحالة بشكل مختلف كالتالي</span>:

``` math
v^{\pi}(s) = \lim_{\gamma \rightarrow 1}{\lim_{h \rightarrow \infty}{\sum_{t = 0}^{h}{\gamma^{t}\left( E^{\pi}\left\lbrack R_{t + 1}\mid S_{0} = s \right\rbrack - r(\pi) \right)}}}
```

**<span dir="rtl">السؤال</span>:** <span dir="rtl">تحت هذا التعريف، ما
هي قيم الحالات</span> A
<span dir="rtl">و</span>B<span dir="rtl">؟</span>

**<span dir="rtl">التمرين 10.8</span>:** <span dir="rtl">الكود الزائف في
المربع على الصفحة 251 يقوم بتحديث</span> Rˉt
<span dir="rtl">باستخدام</span> δt <span dir="rtl"></span>​
<span dir="rtl">كخطأ بدلاً من ببساطة</span> $`R_{t + 1}\ `$​
<span dir="rtl">كلا الخطأين يعملان، ولكن استخدام</span> δt
<span dir="rtl">أفضل. لفهم السبب، فكر في **عملية مكافأة ماركوف**</span>
**(MRP)** <span dir="rtl">المكونة من ثلاث حالات من التمرين 10.6. يجب أن
يتجه تقدير متوسط المكافأة نحو قيمته الحقيقية البالغة</span>
13\frac​<span dir="rtl">.</span> <span dir="rtl">افترض أنه كان هناك
بالفعل وتم تثبيته هناك. ما هي سلسلة الأخطاء</span> $`R_{t + 1}`$
<span dir="rtl"></span> <span dir="rtl">التي قد تحدث؟ وما هي سلسلة
الأخطاء</span> δt <span dir="rtl">(باستخدام المعادلة</span>
(10.10)<span dir="rtl">)؟ أي سلسلة أخطاء ستنتج تقديرًا أكثر استقرارًا
لمتوسط المكافأة إذا تم السماح للتقدير بالتغير استجابةً لهذه الأخطاء؟
ولماذا؟</span>

**<u>10.4 <span dir="rtl">إهمال الإعداد المخفض</span> (Deprecating the
Discounted Setting)</u>**

<span dir="rtl">لقد كانت صياغة المشكلة المستمرة والمخفضة مفيدة جدًا في
الحالة الجدولية، حيث يمكن التعرف على العوائد من كل حالة بشكل منفصل
وتقييمها. لكن في الحالة التقريبية، هناك تساؤل حول ما إذا كان يجب استخدام
هذه الصياغة للمشكلة على الإطلاق. لفهم السبب، فكر في سلسلة لانهائية من
العوائد بدون بداية أو نهاية، وبدون حالات محددة بوضوح. قد يتم تمثيل
الحالات فقط بواسطة **متجهات الميزات** </span>**(Feature
Vectors)**<span dir="rtl">، والتي قد لا تميز بين الحالات بشكل جيد. في
حالة خاصة، قد تكون جميع **متجهات الميزات**</span> **(Feature Vectors)**
<span dir="rtl">متشابهة. في هذه الحالة، لديك فعليًا فقط سلسلة المكافآت
(والأفعال)، ويجب تقييم الأداء بناءً على هذه السلسلة فقط. كيف يمكن القيام
بذلك؟ إحدى الطرق هي حساب متوسط المكافآت على فترة زمنية طويلة—وهذا هو
مفهوم إعداد **متوسط المكافأة**</span> **<span dir="rtl">(</span>Average
<span dir="rtl"></span>Reward<span dir="rtl">)</span>**.
<span dir="rtl">كيف يمكن استخدام التخفيض؟ في كل خطوة زمنية يمكننا قياس
العائد المخفض. بعض العوائد ستكون صغيرة وبعضها كبير، لذا مرة أخرى يجب
علينا حساب متوسطها على فترة زمنية كافية. في الإعداد المستمر، لا توجد
بدايات أو نهايات، ولا خطوات زمنية خاصة، لذا لا يوجد شيء آخر يمكن القيام
به. ومع ذلك، إذا قمت بذلك، فستجد أن متوسط العوائد المخفضة يتناسب مع
متوسط المكافأة. في الواقع، بالنسبة للسياسة</span> π<span dir="rtl">، فإن
متوسط العوائد المخفضة دائمًا هو</span> r(π)/(1−γ)<span dir="rtl">، أي أنه
في الأساس **متوسط المكافأة** </span>**(Average Reward)**
r(π)<span dir="rtl">.</span> <span dir="rtl">على وجه الخصوص، ترتيب جميع
السياسات في إعداد العائد المخفض المتوسط سيكون هو نفسه بالضبط كما في
إعداد **متوسط المكافأة**</span> **<span dir="rtl">(</span>Average
<span dir="rtl"></span>Reward<span dir="rtl">)</span>**.
<span dir="rtl">وبالتالي، فإن معدل التخفيض</span> γ <span dir="rtl">ليس
له تأثير على صياغة المشكلة. يمكن أن يكون في الواقع صفرًا ولن يتغير
الترتيب</span>.

<span dir="rtl">يُثبت هذا الأمر المدهش في المربع على الصفحة التالية، لكن
الفكرة الأساسية يمكن رؤيتها عبر حجة تماثلية. كل خطوة زمنية هي بالضبط مثل
كل خطوة أخرى. مع التخفيض، ستظهر كل مكافأة مرة واحدة بالضبط في كل موضع في
بعض العوائد. ستظهر المكافأة في الخطوة الزمنية</span> t
<span dir="rtl">بدون تخفيض في العائد الخاص بالخطوة</span>
t−1<span dir="rtl">، وتخفيض مرة واحدة في العائد الخاص بالخطوة</span>
t−2<span dir="rtl">، وتخفيض 9 مرة في العائد الخاص بالخطوة</span>
t−1000<span dir="rtl">.</span> <span dir="rtl">الوزن على المكافأة في
الخطوة</span> t <span dir="rtl">هو بذلك</span>
1+γ+γ2+γ3+⋯=1/(1−γ)<span dir="rtl">. وبما أن جميع الحالات متشابهة، فإنها
جميعًا تكون محملة بهذا الوزن، وبالتالي فإن متوسط العوائد سيكون هذا مضروبًا
في **متوسط المكافأة** </span>**(Average Reward)**<span dir="rtl">،
أو</span> r(π)/(1−γ)<span dir="rtl">.</span>

<span dir="rtl">يُظهر هذا المثال والحجة العامة في المربع أنه إذا قمنا
بتحسين القيمة المخفضة على توزيع السياسة المتبعة، فإن التأثير سيكون
مطابقًا لتحسين متوسط المكافأة غير المخفضة؛ لن يكون لمعدل التخفيض</span> γ
<span dir="rtl">أي تأثير. وهذا يشير بقوة إلى أن التخفيض ليس له دور في
تعريف مشكلة التحكم مع **تقريب الدوال** </span>**(Function
Approximation)**<span dir="rtl">. ومع ذلك، يمكن المضي قدمًا واستخدام
التخفيض في طرق الحل. يتحول **معامل التخفيض**</span> **(Discounting
Parameter) γ** <span dir="rtl">من كونه **معاملًا للمشكلة** إلى **معامل
لطريقة الحل**</span>! <span dir="rtl">ولكن في هذه الحالة، للأسف، لن نكون
مضمونين لتحسين **متوسط المكافأة  
(**</span>**Average Reward<span dir="rtl">)</span>** <span dir="rtl">(أو
القيمة المخفضة المكافئة على توزيع السياسة المتبعة)</span>.

<span dir="rtl">عدم جدوى التخفيض في المشاكل المستمرة</span>
<span dir="rtl">(</span>The Futility of Discounting in Continuing
<span dir="rtl"></span>Problems<span dir="rtl">)</span>

``` math
\text{Perhaps discounting can be saved by choosing an objective that sums discounted values over the distribution with which states occur under the policy: }J(\pi) = \sum_{s}^{}{\mu^{\pi}(s)v^{\pi}(s)\text{ (where }v^{\pi\text{ is the discounted value function)}}} = \sum_{s}^{}{\mu^{\pi}(s)\sum_{a}^{}{\pi\left( a\mid s \right)\sum_{s'}^{}{\sum_{r}^{}{p\left( s',r\mid s,a \right)\left\lbrack r + \gamma v^{\pi}\left( s' \right) \right\rbrack\text{ (Bellman Eq.)}}}}} = r(\pi) + \sum_{s}^{}{\mu^{\pi}(s)\sum_{a}^{}{\pi\left( a\mid s \right)\sum_{s'}^{}{\sum_{r}^{}{p\left( s',r\mid s,a \right)\gamma v^{\pi}\left( s' \right)\text{ (from (10.7))}}}}} = r(\pi) + \gamma\sum_{s'}^{}{v^{\pi}\left( s' \right)\sum_{s}^{}{\mu^{\pi}(s)\sum_{a}^{}{\pi\left( a\mid s \right)p\left( s'\mid s,a \right)\text{ (from (3.4))}}}} = r(\pi) + \gamma\sum_{s'}^{}{v^{\pi}\left( s' \right)\mu^{\pi}\left( s' \right)\text{ (from (10.8))}} = r(\pi) + \gamma J(\pi) = r(\pi) + \gamma r(\pi) + \gamma^{2}J(\pi) = r(\pi) + \gamma r(\pi) + \gamma^{2}r(\pi) + \gamma^{3}r(\pi) + \cdots = \frac{1}{1 - \gamma}r(\pi).\text{The proposed discounted objective orders policies identically to the undiscounted (average reward) objective. The discount rate }\gamma\text{ does not influence the ordering!}
```

<span dir="rtl">المشكلة الجذرية في صعوبات إعداد التحكم المخفض هي أننا مع
**تقريب الدوال  
(**</span>**Function
<span dir="rtl"></span>Approximation<span dir="rtl">)
</span>**<span dir="rtl">فقدنا **نظرية تحسين السياسة**</span>
**<span dir="rtl">(</span>Policy Improvement
<span dir="rtl"></span>Theorem<span dir="rtl">)
</span>**<span dir="rtl">(انظر القسم 4.2). لم يعد صحيحًا أنه إذا غيرنا
السياسة لتحسين القيمة المخفضة لحالة واحدة، فنحن مضمونون بتحسين السياسة
الشاملة بأي معنى مفيد. كان هذا الضمان مفتاحًا لنظرية أساليب التحكم في
**التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">.</span> <span dir="rtl">ومع **تقريب
الدوال**</span> **<span dir="rtl">  
(</span>Function <span dir="rtl"></span>Approximation<span dir="rtl">)
</span>**<span dir="rtl">فقدنا هذا الضمان</span>!

<span dir="rtl">في الواقع، غياب **نظرية تحسين السياسة**</span> **(Policy
Improvement Theorem)** <span dir="rtl">يمثل أيضًا فجوة نظرية في إعدادات
**المكافأة الكلية الحلقية**</span> **(Total-Episodic)**
<span dir="rtl">ومتوسط **المكافأة**</span>
**<span dir="rtl">(</span>Average
<span dir="rtl"></span>Reward<span dir="rtl">)</span>**.
<span dir="rtl">بمجرد أن ندخل في **تقريب الدوال** </span>**(Function
Approximation)**<span dir="rtl">، لا يمكننا ضمان التحسين في أي إعداد. في
الفصل 13، نقدم فئة بديلة من خوارزميات التعليم المعزز تعتمد على
**السياسات البارامترية** </span>**(Parameterized
Policies)**<span dir="rtl">، وهناك لدينا ضمان نظري يسمى  
</span>**"<span dir="rtl">نظرية تدرج السياسة</span> (Policy-Gradient
Theorem)"** <span dir="rtl">التي تلعب دورًا مشابهًا لنظرية تحسين السياسة.
ولكن بالنسبة للطرق التي تتعلم قيم الأفعال، يبدو أننا حاليًا بدون ضمان
لتحسين محلي  
(ربما النهج الذي اتبعه بيركنز وبريكاب (2003) قد يوفر جزءًا من الإجابة).
نحن نعلم أن</span> **"-<span dir="rtl">التفضيل الجشع</span>
('-Greedification)"** <span dir="rtl">قد يؤدي أحيانًا إلى سياسة أقل
كفاءة، حيث قد تتذبذب السياسات بين سياسات جيدة بدلاً من أن تتقارب</span>
<span dir="rtl">(جوردون، 1996</span>a<span dir="rtl">)</span>.
<span dir="rtl">هذا مجال يحتوي على العديد من الأسئلة النظرية
المفتوحة</span>.

<u>10.5 **<span dir="rtl">سارسا شبه التدرج التفاضلية بخطوات</span> n
<span dir="rtl">(</span>Differential Semi-Gradient n-Step
Sarsa<span dir="rtl">)</span>**</u>

<span dir="rtl">للتعميم إلى التدعيم بخطوات</span> n<span dir="rtl">،
نحتاج إلى نسخة بخطوات</span> n <span dir="rtl">من خطأ</span>
**TD**<span dir="rtl">.</span> <span dir="rtl">نبدأ بتعميم العائد  
بخطوات</span> n <span dir="rtl">(المعادلة 7.4) إلى شكله التفاضلي، مع
**تقريب الدوال** </span>**(Function
Approximation)**<span dir="rtl">:</span>

``` math
G_{t:t + n} = R_{t + 1} - \overline{R_{t + n - 1}} + \cdots + R_{t + n} - \overline{R_{t + n - 1}} + \widehat{q}\left( S_{t + n},A_{t + n},w_{t + n - 1} \right)
```

``` math
\text{where }\overline{R}\text{ is an estimate of }r(\pi),\, n \geq 1,\text{ and }t + n < T.\text{ If }t + n \geq T,\text{ then we define }G_{t:t + n} = G_{t}\text{ as usual. The n-step TD error is then:}
```

``` math
\delta_{t} = G_{t:t + n} - \widehat{q}\left( S_{t},A_{t},w \right)
```

<span dir="rtl">بعد ذلك، يمكننا تطبيق تحديث **سارسا شبه
التدرجية**</span> **(Semi-Gradient Sarsa)** <span dir="rtl">المعتاد
الخاص بنا كما هو موضح في المعادلة (10.12). يتم تقديم **الكود
الزائف**</span> **(Pseudocode)** <span dir="rtl">للخوارزمية الكاملة في
المربع</span>.

**<span dir="rtl">سارسا شبه التدرج التفاضلية بخطوات</span> n
<span dir="rtl"></span>(Differential Semi-Gradient n-Step Sarsa)**
<span dir="rtl">لتقدير</span>

<img src="./media/image126.png"
style="width:6.26806in;height:3.60347in" />

**<span dir="rtl">التمرين 10.9</span>:** <span dir="rtl">في خوارزمية
**سارسا شبه التدرج التفاضلية بخطوات** </span>**n**<span dir="rtl">، يجب
أن يكون **معامل حجم الخطوة**</span> **(Step-Size Parameter)**
<span dir="rtl">للمكافأة المتوسطة</span> β <span dir="rtl">صغيرًا جدًا حتى
يصبح</span> Rˉ <span dir="rtl">تقديرًا جيدًا على المدى الطويل للمكافأة
المتوسطة. للأسف، سيظل</span> Rˉ <span dir="rtl">متحيزًا بقيمته الأولية
لعدة خطوات، مما قد يجعل التعليم غير فعال. بدلاً من ذلك، يمكن استخدام
متوسط عينة المكافآت المرصودة لـ</span> Rˉ<span dir="rtl">.</span>
<span dir="rtl">سيقوم هذا في البداية بالتكيف بسرعة ولكن في النهاية سيقوم
أيضًا بالتكيف ببطء. مع التغيير البطيء للسياسة، سيتغير أيضًا</span>
Rˉ<span dir="rtl">؛ وهذا يجعل طرق متوسط العينة غير مناسبة بسبب إمكانية
عدم الاستقرار طويل الأمد. في الواقع، يعتبر **معامل حجم الخطوة**</span>
**(Step-Size Parameter)** <span dir="rtl">للمكافأة المتوسطة مكانًا مثاليًا
لاستخدام حيلة **حجم الخطوة الثابتة غير المتحيزة**</span>
**<span dir="rtl">(</span>Unbiased Constant-Step-Size
<span dir="rtl"></span>Trick<span dir="rtl">)
</span>**<span dir="rtl">من التمرين 2.7</span>.

**<span dir="rtl">المطلوب</span>:** <span dir="rtl">وصف التغييرات
المحددة اللازمة على الخوارزمية الموضحة في المربع لخوارزمية **سارسا شبه
التدرج التفاضلية بخطوات** </span>**n
<span dir="rtl"></span>**<span dir="rtl">لاستخدام هذه الحيلة</span>.

**<u>10.6 <span dir="rtl">الملخص</span> (Summary)</u>**

<span dir="rtl">في هذا الفصل، قمنا بتوسيع أفكار **تقريب الدوال
البارامترية**</span> **<span dir="rtl">(</span>Parameterized Function
<span dir="rtl"></span>Approximation<span dir="rtl">)
</span>**<span dir="rtl">والانحدار شبه التدرجي التي تم تقديمها في الفصل
السابق لتشمل التحكم. كان التوسع مباشرًا في حالة المشاكل الحلقية، ولكن في
حالة المشاكل المستمرة كان علينا تقديم صياغة جديدة بالكامل للمشكلة تعتمد
على تعظيم **متوسط المكافأة**</span> **(Average Reward)**
<span dir="rtl">لكل خطوة زمنية. من المدهش أن الصياغة المخفضة لا يمكن
تطبيقها في التحكم عند وجود التقريبات. في الحالة التقريبية، لا يمكن تمثيل
معظم السياسات بواسطة دالة القيمة. السياسات العشوائية المتبقية تحتاج إلى
ترتيب، وتوفر **متوسط المكافأة**</span> **(Average Reward) r(π)**
<span dir="rtl">طريقة فعالة لتحقيق ذلك</span>.

<span dir="rtl">تشمل صياغة **متوسط المكافأة**</span> **(Average
Reward)** <span dir="rtl">نسخًا تفاضلية جديدة من **دوال القيمة**
</span>**(Value Functions)**<span dir="rtl">، **معادلات بيلمان**
</span>**(Bellman Equations)**<span dir="rtl">، وأخطاء</span>
**TD**<span dir="rtl">، ولكن جميع هذه النسخ موازية للنسخ القديمة،
والتغييرات المفاهيمية صغيرة. هناك أيضًا مجموعة جديدة موازية من
الخوارزميات التفاضلية لحالة **متوسط المكافأة** </span>**(Average
Reward)**<span dir="rtl">.</span>

<span dir="rtl">الفصل الحادي عشر:</span>

<span dir="rtl">طرق خارج السياسة مع التقريب</span>
<span dir="rtl">(</span>Off-policy Methods <span dir="rtl"></span>with
Approximation<span dir="rtl">)</span>

<span dir="rtl">لقد تناول هذا الكتاب طرق التعليم داخل السياسة</span>
(On-policy) <span dir="rtl">وخارج السياسة</span> (Off-policy)
<span dir="rtl">منذ الفصل الخامس كبديلين للتعامل مع الصراع بين
الاستغلال</span> (Exploitation) <span dir="rtl">والاستكشاف</span>
(Exploration) <span dir="rtl">الذي يعتبر جزءًا أساسيًا من أشكال التكرار
العام للسياسات</span> <span dir="rtl">(</span>Generalized Policy
<span dir="rtl"></span>Iteration<span dir="rtl">)</span>.
<span dir="rtl">أما الفصلان السابقان فقد تناولا حالة داخل السياسة مع
تقريب الدوال</span> <span dir="rtl">(</span>Function
<span dir="rtl"></span>Approximation<span dir="rtl">)، وفي هذا الفصل
سنتناول حالة خارج السياسة مع تقريب الدوال. يتضح أن التمديد إلى تقريب
الدوال يكون مختلفًا بشكل كبير وأكثر صعوبة في التعليم خارج السياسة مقارنةً
بالتعليم داخل السياسة. إن الطرق الجدولية</span> (Tabular Methods)
<span dir="rtl">خارج السياسة التي تم تطويرها في الفصلين السادس والسابع
يمكن أن تمتد بسهولة إلى الخوارزميات شبه التدرجية</span>
<span dir="rtl">(</span>Semi-Gradient
<span dir="rtl"></span>Algorithms<span dir="rtl">)، لكن هذه الخوارزميات
لا تتقارب بالصلابة نفسها كما هو الحال في التدريب داخل السياسة</span>.

<span dir="rtl">في هذا الفصل، سنستكشف مشاكل التقارب، ونلقي نظرة أقرب على
نظرية تقريب الدوال الخطية</span> (Linear Function
Approximation)<span dir="rtl">، ونقدم مفهومًا جديدًا للتعليمية</span>
(Learnability)<span dir="rtl">، ثم نناقش خوارزميات جديدة بضمانات تقارب
أقوى لحالة خارج السياسة. في النهاية، سنحصل على طرق محسنة، لكن النتائج
النظرية لن تكون قوية كما هي في حالة التعليم داخل السياسة، ولا النتائج
التجريبية مرضية بالقدر نفسه. على طول الطريق، سنكتسب فهمًا أعمق للتقريب في
التعليم المعزز</span> <span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">) لكل من التعليم داخل
السياسة وخارج السياسة</span>.

<span dir="rtl">تذكر أنه في التعليم خارج السياسة نسعى إلى تعلم دالة
قيمة</span> (Value Function) <span dir="rtl">لسياسة الهدف</span> (Target
Policy) π<span dir="rtl">، معطاةً بيانات من سياسة سلوك مختلفة</span>
b<span dir="rtl">.</span> <span dir="rtl">في حالة التنبؤ  
(</span>Prediction <span dir="rtl"></span>Case<span dir="rtl">)، تكون
كلا السياستين ثابتتين ومعطاتين، ونسعى لتعلم إما قيم الحالة  
(</span>State <span dir="rtl"></span>Values<span dir="rtl">)</span> v^π
<span dir="rtl">أو قيم الإجراءات</span> (Action Values)
q^π<span dir="rtl">.</span> <span dir="rtl">أما في حالة التحكم  
(</span>Control <span dir="rtl"></span>Case<span dir="rtl">)، فتتعلم قيم
الإجراءات، وتغير كلا السياستين عادةً خلال التعليم — حيث تكون</span> π
<span dir="rtl">هي السياسة الجشعة</span> (Greedy Policy)
<span dir="rtl">بالنسبة إلى</span> q^​<span dir="rtl">، وتكون</span> b
<span dir="rtl">شيئًا أكثر استكشافية مثل السياسة الجشعة مع نسبة
استكشاف</span> (ε-Greedy Policy) <span dir="rtl">بالنسبة إلى</span>
q^​<span dir="rtl">.</span>

<span dir="rtl">يمكن تقسيم تحدي التعليم خارج السياسة</span> (Off-policy
Learning) <span dir="rtl">إلى جزئين: جزء ينشأ في حالة الجدولية</span>
(Tabular Case) <span dir="rtl">وجزء ينشأ فقط مع تقريب الدوال</span>
(Function Approximation)<span dir="rtl">.</span> <span dir="rtl">الجزء
الأول من التحدي يتعلق **بهدف التحديث**</span> **(Target of the Update)**
— <span dir="rtl">الذي لا يجب الخلط بينه وبين **سياسة الهدف**</span>
**(Target Policy)** — <span dir="rtl">والجزء الثاني يتعلق **بتوزيع
التحديثات  
(**</span>**Distribution <span dir="rtl"></span>of the
Updates<span dir="rtl">)</span>**. <span dir="rtl">التقنيات المتعلقة
**بأخذ العينات الهامة  
(**</span>**Importance <span dir="rtl"></span>Sampling<span dir="rtl">)
</span>**<span dir="rtl">التي تم تطويرها في الفصلين الخامس والسابع
تتعامل مع الجزء الأول؛ قد تزيد هذه من **التباين**</span> **(Variance)**
<span dir="rtl">ولكنها ضرورية في جميع الخوارزميات الناجحة، سواء كانت
جدولية أو تقريبية. يتم التعامل بسرعة مع تمديد هذه التقنيات إلى تقريب
الدوال في القسم الأول من هذا الفصل</span>.

<span dir="rtl">هناك حاجة إلى شيء إضافي للتعامل مع الجزء الثاني من تحدي
التعليم خارج السياسة مع تقريب الدوال لأن **توزيع التحديثات**</span>
**(Update Distribution)** <span dir="rtl">في حالة خارج السياسة لا يتوافق
مع **التوزيع داخل السياسة** </span>**(On-policy
Distribution)**<span dir="rtl">.</span> <span dir="rtl">يعد **التوزيع
داخل السياسة**</span> **<span dir="rtl">(</span>On-policy
<span dir="rtl"></span>Distribution<span dir="rtl">)
</span>**<span dir="rtl">مهمًا **لاستقرار**</span> **(Stability)**
**<span dir="rtl">الطرق شبه التدرجية</span>
<span dir="rtl">(</span>Semi-Gradient
<span dir="rtl"></span>Methods<span dir="rtl">)</span>**.
<span dir="rtl">تم استكشاف نهجين عامين للتعامل مع هذا. الأول هو استخدام
**طرق أخذ العينات الهامة**</span> **(Importance Sampling Methods)**
<span dir="rtl">مرة أخرى، هذه المرة لتشويه **توزيع التحديثات**</span>
**(Update Distribution)** <span dir="rtl">وإعادته إلى **توزيع داخل
السياسة** </span>**(On-policy Distribution)**<span dir="rtl">، مما يضمن
**تقارب**</span> **(Convergence)** **<span dir="rtl">الطرق شبه
التدرجية  
</span> (Semi-Gradient Methods)** <span dir="rtl">(في الحالة الخطية).
النهج الآخر هو تطوير **طرق التدرج الحقيقية**</span> **(True Gradient
Methods)** <span dir="rtl">التي لا تعتمد على أي توزيع خاص **للاستقرار**
</span>**(Stability)<span dir="rtl">.</span>** <span dir="rtl">نقدم طرقًا
تعتمد على كلا النهجين. هذه منطقة بحث متقدمة، وليس من الواضح أي من هذه
النهجين هو الأكثر فعالية في الممارسة</span>.

**<u>11.1 <span dir="rtl">الطرق شبه التدرجية</span> (Semi-gradient
Methods)</u>**

<span dir="rtl">نبدأ بوصف كيفية تمديد الطرق التي تم تطويرها في الفصول
السابقة لحالة خارج السياسة</span>
<span dir="rtl">(</span>Off-policy<span dir="rtl">) إلى تقريب
الدوال</span> (Function Approximation) <span dir="rtl">كطرق شبه
تدرجية</span> <span dir="rtl">(</span>Semi-gradient
<span dir="rtl"></span>Methods<span dir="rtl">)</span>.
<span dir="rtl">هذه الطرق تعالج الجزء الأول من تحدي التعليم خارج السياسة
(تغيير أهداف التحديثات) ولكن ليس الجزء الثاني (تغيير توزيع التحديثات).
وبناءً على ذلك، قد تتباعد هذه الطرق في بعض الحالات، ومن هذه الناحية لا
تعتبر موثوقة بشكل كامل، لكنها مع ذلك تُستخدم بنجاح في كثير من الأحيان.
تذكر أن هذه الطرق مضمونة الاستقرار</span> (Stable) <span dir="rtl">وغير
متحيزة بشكل غير تدريجي</span> (Asymptotically Unbiased)
<span dir="rtl">في الحالة الجدولية</span> (Tabular
Case)<span dir="rtl">، التي تتوافق مع حالة خاصة من تقريب الدوال. لذا قد
يكون من الممكن دمجها مع طرق اختيار الميزات</span> (Feature Selection)
<span dir="rtl">بطريقة يمكن من خلالها ضمان استقرار النظام المشترك. في كل
الأحوال، هذه الطرق بسيطة وبالتالي تعد نقطة جيدة للبدء</span>.

<span dir="rtl">في الفصل السابع، وصفنا مجموعة متنوعة من الخوارزميات
الجدولية خارج السياسة  
(</span>Tabular Off-policy Algorithms<span dir="rtl">)</span>.
<span dir="rtl">لتحويلها إلى شكل شبه تدرجي</span> (Semi-gradient
Form)<span dir="rtl">، نستبدل ببساطة التحديث لمصفوفة</span>
<span dir="rtl">(</span>V <span dir="rtl">أو</span>
Q<span dir="rtl">)</span> <span dir="rtl">إلى تحديث لمتجه الوزن</span>
(w)<span dir="rtl">، باستخدام **دالة القيمة التقريبية**</span> **v^
(Approximate Value Function)** <span dir="rtl">أو</span> q^​
<span dir="rtl">وتدرجها. العديد من هذه الخوارزميات تستخدم نسبة أخذ
العينات الهامة لكل خطوة</span> <span dir="rtl">(</span>Per-step
Importance Sampling Ratio<span dir="rtl">)</span>:

``` math
\rho_{t} = \rho_{t:t} = \frac{\pi\left( A_{t} \middle| S_{t} \right)}{b\left( A_{t} \middle| S_{t} \right)},\quad
```

<span dir="rtl">على سبيل المثال، خوارزمية قيمة الحالة ذات الخطوة الواحدة
هي شبه التدرج لخوارزمية خارج السياسة</span> TD(0)<span dir="rtl">، وهي
تمامًا مثل خوارزمية داخل السياسة المقابلة (الصفحة 203) باستثناء
إضافة</span> ρt<span dir="rtl">.</span>

``` math
w_{t + 1} = w_{t} + \alpha\rho_{t}\left( r + \gamma\widehat{v}\left( S_{t + 1},w_{t} \right) - \widehat{v}\left( S_{t},w_{t} \right) \right)
```

<span dir="rtl">حيث يتم تعريف</span> t <span dir="rtl">بشكل مناسب بناءً
على ما إذا كانت المشكلة حلقية ومخصومة، أو مستمرة وغير مخصومة باستخدام
**متوسط المكافأة** </span>**(Average Reward)**<span dir="rtl">.</span>

``` math
\delta_{t} = R_{t + 1} + \gamma\widehat{v}\left( S_{t + 1},w_{t} \right) - \widehat{v}\left( S_{t},w_{t} \right)
```

<span dir="rtl">او</span>

``` math
\delta_{t} = R_{t + 1} - \overline{R_{t}} + \widehat{v}\left( S_{t + 1},w_{t} \right) - \widehat{v}\left( S_{t},w_{t} \right)
```

<span dir="rtl">بالنسبة لقيم الإجراءات، فإن خوارزمية الخطوة الواحدة هي
"سارسا المتوقعة شبه التدرجية</span>" <span dir="rtl">  
(</span>Semi-gradient Expected Sarsa<span dir="rtl">)</span>

``` math
w_{t + 1} \doteq w_{t} + \alpha\delta_{t}\nabla\widehat{q}\left( S_{t},A_{t},w_{t} \right),\quad
```

``` math
\delta_{t} \doteq R_{t + 1} + \gamma\sum_{a}^{}{\pi\left( a \middle| S_{t + 1} \right)\widehat{q}\left( S_{t + 1},a,w_{t} \right)} - \widehat{q}\left( S_{t},A_{t},w_{t} \right),\quad\text{or}\quad
```

``` math
\delta_{t} \doteq R_{t + 1} - \overline{R_{t}} + \sum_{a}^{}{\pi\left( a \middle| S_{t + 1} \right)\widehat{q}\left( S_{t + 1},a,w_{t} \right)} - \widehat{q}\left( S_{t},A_{t},w_{t} \right).
```

<span dir="rtl">لاحظ أن هذه الخوارزمية لا تستخدم أخذ العينات
الهامة</span> (Importance Sampling)<span dir="rtl">.</span>
<span dir="rtl">في الحالة الجدولية</span> (Tabular
Case)<span dir="rtl">، من الواضح أن هذا مناسب لأن الإجراء الوحيد المأخوذ
في العينة هو</span> At​<span dir="rtl">، وفي تعلم قيمته لا نحتاج إلى
النظر في أي إجراءات أخرى. أما مع تقريب الدوال  
(</span>Function <span dir="rtl"></span>Approximation<span dir="rtl">)،
فالأمر أقل وضوحًا لأننا قد نرغب في توزيع وزن مختلف على أزواج
الحالة-الإجراء</span> (State–Action Pairs) <span dir="rtl">المختلفة
بمجرد أن تساهم جميعها في نفس التقريب الإجمالي. يتطلب الحل المناسب لهذه
المسألة فهمًا أعمق لنظرية تقريب الدوال في التعليم المعزز</span>
(Reinforcement Learning)<span dir="rtl">.</span>

<span dir="rtl">في التعميمات المتعددة الخطوات لهذه الخوارزميات، تشمل كل
من خوارزميات قيمة الحالة</span>
<span dir="rtl">(</span>State-value<span dir="rtl">) وقيمة
الإجراء</span> (Action-value) <span dir="rtl">أخذ العينات الهامة</span>
(Importance Sampling)<span dir="rtl">. على سبيل المثال، النسخة ذات
الخطوات المتعددة</span> (n-step) <span dir="rtl">من "سارسا المتوقعة شبه
التدرجية</span>" <span dir="rtl">(</span>Semi-gradient Expected
Sarsa<span dir="rtl">) هي</span>:

``` math
w_{t + n} = w_{t + n - 1} + \alpha\rho_{t + 1}\cdots\rho_{t + n - 1}\left\lbrack G_{t:t + n} - \widehat{q}\left( S_{t},A_{t},w_{t + n - 1} \right) \right\rbrack\nabla\widehat{q}\left( S_{t},A_{t},w_{t + n - 1} \right)
```

<span dir="rtl">مع</span>

``` math
w_{t + n} = w_{t + n - 1} + \alpha\left\lbrack G_{t:t + n} - \widehat{q}\left( S_{t},A_{t},w_{t + n - 1} \right) \right\rbrack\nabla\widehat{q}\left( S_{t},A_{t},w_{t + n - 1} \right),
```

``` math
G_{t:t + n} = \widehat{q}\left( S_{t},A_{t},w_{t - 1} \right) + \sum_{k = t}^{t + n - 1}{\delta_{k}\prod_{i = t + 1}^{k}{\gamma\pi\left( A_{i} \middle| S_{i} \right)}}
```

<span dir="rtl">بناءً على ما ورد في الصفحة، يتم تعريف المتغير</span> t
<span dir="rtl">كما هو الحال في خوارزمية **سارسا المتوقعة
(**</span>**Expected SARSA<span dir="rtl">)</span>**.
<span dir="rtl">وفي الفصل السابع، قمنا بتقديم خوارزمية تجمع بين مختلف
خوارزميات تقدير قيمة الإجراء</span> (Action-value
algorithms)<span dir="rtl">، وهي</span>: **n-step
Q(λ)**<span dir="rtl">.</span> <span dir="rtl">نترك للقارئ مهمة استنتاج
الشكل شبه التدرجي</span> (Semi-gradient form) <span dir="rtl">لهذه
الخوارزمية، وكذلك الشكل الخاص بخوارزمية تقدير قيمة الحالة متعددة
الخطوات</span> (n-step state-value algorithm)<span dir="rtl">، كتمرين
تطبيقي</span>.

**<span dir="rtl">تمرين 11.1</span>:** <span dir="rtl">حوّل معادلة</span>
"TD <span dir="rtl">خارج السياسة ذو الخطوات المتعددة</span> (n-step
Off-policy TD)" <span dir="rtl">(المعادلة 7.9) إلى الشكل شبه التدرجي.
أعطِ تعريفات للعائد لكل من الحالات الحلقية</span> (Episodic)
<span dir="rtl">والمستمرة</span> (Continuing)<span dir="rtl">.</span>

**<span dir="rtl">تمرين 11.2</span>:** <span dir="rtl">حوّل
معادلات</span> "n-step Q(λ)" <span dir="rtl">(المعادلتين 7.11 و7.17) إلى
الشكل شبه التدرجي. أعطِ تعريفات تغطي كل من الحالات الحلقية</span>
(Episodic) <span dir="rtl">والمستمرة</span> (Continuing)

**<u>11.2 <span dir="rtl">أمثلة على التباعد خارج السياسة</span>
(Off-policy Divergence)</u>**

<span dir="rtl">في هذا القسم، نبدأ بمناقشة الجزء الثاني من تحدي التعليم
خارج السياسة</span> (Off-policy Learning) <span dir="rtl">مع تقريب
الدوال</span> (Function Approximation)<span dir="rtl">، وهو أن توزيع
التحديثات لا يتطابق مع توزيع داخل السياسة</span> (On-policy
Distribution)<span dir="rtl">.</span> <span dir="rtl">نصف بعض الأمثلة
المضادة</span> (Counterexamples) <span dir="rtl">التي توضح حالات يكون
فيها التعليم خارج السياسة غير مستقر ويتباعد باستخدام الخوارزميات شبه
التدرجية</span> (Semi-gradient) <span dir="rtl">وغيرها من الخوارزميات
البسيطة</span>.

<span dir="rtl">لبناء الفهم، من الأفضل أن نبدأ بمثال بسيط جدًا. افترض
أنه، ربما كجزء من عملية اتخاذ القرار الماركوفية</span> (MDP)
<span dir="rtl">أكبر، هناك حالتان تكون قيمتهما المقدرة على شكل دالة
من</span> w <span dir="rtl">و2</span>w<span dir="rtl">، حيث يتكون متجه
البارامتر</span> w <span dir="rtl">من مكون واحد فقط</span>
w<span dir="rtl">.</span> <span dir="rtl">يحدث هذا تحت تقريب الدوال
الخطي</span> <span dir="rtl">(</span>Linear Function
<span dir="rtl"></span>Approximation<span dir="rtl">) إذا كانت متجهات
الميزات للحالتين عبارة عن أرقام بسيطة (متجهات ذات مكون واحد)، في هذه
الحالة 11</span>1 <span dir="rtl">و</span>222<span dir="rtl">.</span>
<span dir="rtl">في الحالة الأولى، يوجد إجراء واحد متاح فقط، وينتج عنه
انتقال محدد إلى الحالة الثانية مع مكافأة قدرها 0</span>.

<img src="./media/image127.png"
style="width:1.96684in;height:0.60005in" />

<span dir="rtl">حيث تعبر التعبيرات داخل الدائرتين عن قيم الحالتين. افترض
أن</span> w=10 <span dir="rtl">في البداية. سيكون الانتقال حينها من حالة
ذات قيمة مقدرة بـ 10 إلى حالة ذات قيمة مقدرة بـ 20. سيبدو هذا كأنه
انتقال جيد، وسيتم زيادة</span> w <span dir="rtl">لرفع القيمة المقدرة
للحالة الأولى. إذا كان</span> γ <span dir="rtl">قريبًا من 1، فسيكون
خطأ</span> TD <span dir="rtl">قريبًا من 10، وإذا كان</span>
α=0.1<span dir="rtl">، فسيتم زيادة</span> w <span dir="rtl">إلى حوالي 11
في محاولة لتقليل خطأ</span> TD<span dir="rtl">.</span>
<span dir="rtl">ومع ذلك، ستزداد القيمة المقدرة للحالة الثانية أيضًا إلى
حوالي 22. إذا حدث الانتقال مرة أخرى، فسيكون من حالة ذات قيمة مقدرة تقارب
11 إلى حالة ذات قيمة مقدرة تقارب 22، مما يؤدي إلى خطأ</span> TD
<span dir="rtl">يقارب 11 — وهو أكبر، وليس أصغر، من السابق. سيبدو أكثر
كأن الحالة الأولى مقدرة بأقل من قيمتها الحقيقية، وستزداد قيمتها مرة
أخرى، هذه المرة إلى حوالي 12.1. يبدو هذا سيئًا، وفي الواقع مع المزيد من
التحديثات سيزداد</span> w <span dir="rtl">ويتباعد إلى اللانهاية</span>.

<span dir="rtl">لفهم ذلك بشكل نهائي، علينا أن ننظر بعناية أكبر إلى سلسلة
التحديثات. خطأ</span> TD <span dir="rtl">في الانتقال بين الحالتين
هو</span>:

``` math
\delta_{t} = R_{t + 1} + \gamma\widehat{v}\left( S_{t + 1},w_{t} \right) - \widehat{v}\left( S_{t},w_{t} \right) = 0 + \gamma \cdot 2w_{t} - w_{t} = (2\gamma - 1)w_{t}
```

<span dir="rtl">والتحديث في خوارزمية شبه التدرج</span> TD(0)
<span dir="rtl">خارج السياسة (من المعادلة</span> (11.2)<span dir="rtl">)
هو</span>:

``` math
w_{t + 1} = w_{t} + \alpha\rho_{t}\delta_{t}\nabla\widehat{v}\left( S_{t},w_{t} \right) = w_{t} + \alpha \cdot 1 \cdot (2\gamma - 1)w_{t} \cdot 1 = \left\lbrack 1 + \alpha(2\gamma - 1) \right\rbrack w_{t}
```

<span dir="rtl">لاحظ أن نسبة أخذ العينات الهامة</span> ρt
<span dir="rtl">هي 1 في هذا الانتقال لأن هناك إجراءً واحدًا فقط متاحًا من
الحالة الأولى، لذا فإن احتمالات اختياره تحت سياسة الهدف</span> (Target
Policy) <span dir="rtl">وسياسة السلوك</span> (Behavior Policy)
<span dir="rtl">يجب أن تكون كلاهما 1. في التحديث الأخير أعلاه، تكون
البارامتر الجديدة هي البارامتر القديمة مضروبة في ثابت قياسي</span>
1+α(2γ−1)<span dir="rtl">.</span> <span dir="rtl">إذا كان هذا الثابت
أكبر من 1، فإن النظام غير مستقر وسيصل</span> w <span dir="rtl">إلى
اللانهاية الإيجابية أو السلبية اعتمادًا على قيمته الأولية. هنا يكون هذا
الثابت أكبر من 1 كلما كانت</span> γ\>0.5<span dir="rtl">.</span>
<span dir="rtl">لاحظ أن الاستقرار لا يعتمد على حجم الخطوة المحدد، طالما
أن</span> α\>0<span dir="rtl">.</span> <span dir="rtl">يمكن أن تؤثر
أحجام الخطوات الصغيرة أو الكبيرة على المعدل الذي يصل فيه</span> w
<span dir="rtl">إلى اللانهاية، ولكن ليس على ما إذا كان سيصل إليها أم
لا</span>.

<span dir="rtl">المفتاح في هذا المثال هو أن الانتقال الواحد يحدث بشكل
متكرر دون أن يتم تحديث</span> w <span dir="rtl">على انتقالات أخرى. هذا
ممكن في التدريب خارج السياسة لأن سياسة السلوك قد تختار إجراءات في تلك
الانتقالات الأخرى التي لن تختارها سياسة الهدف أبدًا. بالنسبة لهذه
الانتقالات، ستكون</span> ρt <span dir="rtl">صفرًا ولن يتم إجراء أي تحديث.
ومع ذلك، في التدريب داخل السياسة، تكون</span> ρt
<span dir="rtl">دائمًا 1. في كل مرة يحدث فيها انتقال من حالة</span> w
<span dir="rtl">إلى حالة</span> 2w <span dir="rtl">مما يزيد من</span>
w<span dir="rtl">، يجب أن يحدث أيضًا انتقال من حالة</span>
2w<span dir="rtl">.</span> <span dir="rtl">هذا الانتقال سيقلل</span> w
<span dir="rtl">ما لم يكن الانتقال إلى حالة تكون قيمتها أعلى (لأن</span>
1γ\<1 <span dir="rtl">من</span> 2w<span dir="rtl">، ومن ثم يجب أن تتبع
تلك الحالة بحالة ذات قيمة أعلى، وإلا سيتم تقليل</span> w
<span dir="rtl">مرة أخرى. يمكن لكل حالة دعم الحالة التي قبلها فقط من
خلال إنشاء توقع أعلى. في النهاية، يجب أن يتم الوفاء بالوعد. في حالة داخل
السياسة، يجب الوفاء بوعد المكافأة المستقبلية ويتم إبقاء النظام تحت
السيطرة. ولكن في حالة خارج السياسة، يمكن تقديم الوعد ومن ثم، بعد اتخاذ
إجراء لن تختاره سياسة الهدف أبدًا، يتم تجاهله والتخلي
عنه</span>.<span dir="rtl">  
هذا المثال البسيط يوضح الكثير من أسباب لماذا يمكن أن يؤدي التدريب خارج
السياسة إلى التباعد، لكنه ليس مقنعًا تمامًا لأنه ليس مكتملًا — فهو مجرد جزء
من</span> MDP <span dir="rtl">كامل. هل يمكن أن يكون هناك نظام كامل يعاني
من عدم الاستقرار؟ مثال بسيط وكامل على التباعد هو المثال المضاد لبيرد  
(</span>Baird's <span dir="rtl"></span>Counterexample<span dir="rtl">).
فكر في</span> MDP <span dir="rtl">ذو الحلقات السبع وحالتين من العمل
الموضح في الشكل 11.1. يأخذ الإجراء المنقط النظام إلى إحدى الحالات الستة
العلوية باحتمالية متساوية، في حين يأخذ الإجراء الصلب النظام إلى الحالة
السابعة. تختار سياسة السلوك</span> b <span dir="rtl">الإجراء المنقط
والإجراء الصلب باحتمالات</span> $`\frac{6}{7}`$
<span dir="rtl">و</span>$`\frac{1}{7}`$<span dir="rtl">، بحيث يكون توزيع
الحالة التالية تحتها متجانسًا (نفسه لجميع الحالات غير النهائية)، وهو أيضًا
توزيع البدء لكل حلقة. سياسة الهدف</span> π <span dir="rtl">دائمًا تأخذ
الإجراء الصلب، وبالتالي فإن توزيع داخل السياسة (لـ</span> π
<span dir="rtl">يكون متركزًا في الحالة السابعة. المكافأة تكون صفرًا في
جميع الانتقالات. معدل الخصم هو</span> γ= 0.99<span dir="rtl">.</span>

<span dir="rtl">فكر في تقدير **قيمة الحالة**</span> **(State-value)**
<span dir="rtl">تحت التقدير الخطي المشار إليه بالتعبير الموضح في كل
دائرة حالة. على سبيل المثال، القيمة المقدرة لأقصى اليسار هي</span>
2w1+w82w_1 + w_82w1​+w8​<span dir="rtl">، حيث يشير الرقم السفلي إلى المكون
في **متجه الوزن**</span> **(Weight Vector)**
<span dir="rtl">الكلي</span> w∈R8w<span dir="rtl">؛ هذا يتوافق مع **متجه
الميزات**</span> **(Feature Vector)** <span dir="rtl">للحالة الأولى الذي
يكون</span> x(1)=(2,0,0,0,0,0,0,1)⊤<span dir="rtl">.</span>
<span dir="rtl">المكافأة هي صفر في جميع الانتقالات، لذا فإن **دالة
القيمة الحقيقية** </span>**(True Value Function)**
<span dir="rtl">هي</span> vπ(s)=0 <span dir="rtl">لكل</span>
s<span dir="rtl">، ويمكن تقريبها بدقة إذا كان</span>
w=0<span dir="rtl">.</span> <span dir="rtl">في الواقع، هناك العديد من
الحلول، حيث أن هناك مكونات أكثر في **متجه الوزن  
(**</span>**Weight Vector<span dir="rtl">)</span>**
<span dir="rtl"></span>(8) <span dir="rtl">مما توجد حالات غير نهائية
(7). بالإضافة إلى ذلك، فإن مجموعة **متجهات الميزات** </span>**(Feature
Vectors)**<span dir="rtl">،</span> {x(s):$`s \in S`$ }<span dir="rtl">،
هي مجموعة مستقلة خطيًا. بكل هذه الطرق، يبدو أن هذه المهمة هي حالة مناسبة
لـ **تقريب الدوال الخطية** </span>**(Linear Function
Approximation)**<span dir="rtl">.  
</span>

<img src="./media/image128.png"
style="width:6.26806in;height:3.16458in" />

<span dir="rtl">الشكل 11.1</span>: **<span dir="rtl">المثال المضاد
لبيرد</span> (Baird’s Counterexample)**<span dir="rtl">.</span>
<span dir="rtl">دالة قيمة الحالة التقريبية</span> (Approximate
State-value Function) <span dir="rtl">لهذه العملية ماركوف تأخذ الشكل
الموضح بالتعبيرات الخطية داخل كل حالة. عادة ما يؤدي الإجراء الصلب إلى
الحالة السابعة، بينما يؤدي الإجراء المنقط عادةً إلى واحدة من الحالات الست
الأخرى، كل منها باحتمالية متساوية. المكافأة دائمًا تكون صفرًا</span>.

<span dir="rtl">إذا قمنا بتطبيق **شبه التدرج**</span> **(Semi-gradient)
TD(0)** <span dir="rtl">على هذه المشكلة (المعادلة 11.2)، فإن
**الأوزان**</span> **(Weights)** <span dir="rtl">تتباعد إلى اللانهاية،
كما هو موضح في الشكل 11.2 (الجهة اليسرى). يحدث هذا الانعدام في الاستقرار
مع أي حجم خطوة إيجابي، بغض النظر عن مدى صغره. في الواقع، يحدث ذلك حتى
إذا تم إجراء تحديث متوقع كما في **البرمجة الديناميكية**</span>
**<span dir="rtl">(</span>Dynamic Programming –
<span dir="rtl"></span>DP<span dir="rtl">)</span>**<span dir="rtl">، كما
هو موضح في الشكل 11.2 (الجهة اليمنى). أي أنه إذا تم تحديث **متجه
الوزن**</span> **<span dir="rtl">(</span>Weight
<span dir="rtl"></span>Vector<span dir="rtl">)</span>**<span dir="rtl">،</span>
wk​<span dir="rtl">، لجميع **الحالات**</span> **(States)**
<span dir="rtl">في نفس الوقت بطريقة **شبه التدرج**
</span>**(Semi-gradient)**<span dir="rtl">، باستخدام **الهدف المستند إلى
التوقعات**</span> **(Expectation-based Target)** <span dir="rtl">في
**البرمجة الديناميكية** </span>**(DP)**<span dir="rtl">:</span>

``` math
w_{k + 1} = w_{k} + \frac{\alpha}{|S|}\sum_{s}^{}{\left( E_{\pi}\left\lbrack R_{t + 1} + \gamma\widehat{v}\left( S_{t + 1},w_{k} \right)\mid S_{t} = s \right\rbrack - \widehat{v}\left( s,w_{k} \right) \right)\nabla\widehat{v}\left( s,w_{k} \right)}
```

<span dir="rtl">في هذه الحالة، لا يوجد **عشوائية**</span>
**(Randomness)** <span dir="rtl">ولا **تزامن غير متزامن**
</span>**(Asynchrony)**<span dir="rtl">، تمامًا كما في **تحديث البرمجة
الديناميكية**</span> **(Dynamic Programming - DP)**
<span dir="rtl">التقليدي. الطريقة تقليدية باستثناء استخدامها لـ **تقريب
الدالة شبه التدرجي**</span> **<span dir="rtl">(</span>Semi-gradient
Function <span dir="rtl"></span>Approximation<span dir="rtl">)</span>**.
<span dir="rtl">ومع ذلك، لا يزال النظام غير مستقر</span>.

<span dir="rtl">إذا قمنا بتغيير **توزيع تحديثات البرمجة
الديناميكية**</span> **(DP Update Distribution)** <span dir="rtl">في
**المثال المضاد لبيرد**</span> **(Baird's Counterexample)**
<span dir="rtl">فقط، من **التوزيع الموحد (**</span>**Uniform
<span dir="rtl"></span>Distribution<span dir="rtl">)
</span>**<span dir="rtl">إلى **توزيع داخل السياسة**</span> **(On-policy
Distribution)** <span dir="rtl">(الذي يتطلب عادةً **التحديث غير
المتزامن** </span>**(Asynchronous Updating)**<span dir="rtl">)، فإن
**التقارب** </span>**(Convergence)** <span dir="rtl">مضمون إلى حل مع خطأ
محدود حسب المعادلة</span> (9.14)<span dir="rtl">.</span>

<span dir="rtl">هذا المثال ملفت للنظر لأن طريقتي</span> **TD
<span dir="rtl">و</span>DP** <span dir="rtl">المستخدمتين هما بلا شك أبسط
وأفضل طرق **الاستدلال التمهيدي**</span> **(Bootstrapping)**
<span dir="rtl">فهمًا، والطريقة الخطية، **شبه النزول**
</span>**(Semi-descent Method)** <span dir="rtl">المستخدمة هي بلا شك
أبسط وأفضل أنواع **تقريب الدوال**</span> **(Function Approximation)**
<span dir="rtl">فهمًا. يوضح المثال أن حتى أبسط تركيبة من **الاستدلال
التمهيدي**</span> **(Bootstrapping)** <span dir="rtl">و**تقريب الدوال**
</span>**(Function Approximation)** <span dir="rtl">يمكن أن تكون غير
مستقرة إذا لم يتم **التحديث**</span> **(Updating)** <span dir="rtl">وفقًا
لـ **توزيع داخل السياسة** </span>**(On-policy
Distribution)**<span dir="rtl">.</span>

<img src="./media/image129.png"
style="width:6.26806in;height:3.24653in" />

<span dir="rtl">الشكل 11.2: توضيح عدم الاستقرار في **المثال المضاد
لبيرد** </span>**(Baird's Counterexample)**<span dir="rtl">. يظهر في
الشكل تطور مكونات **متجه البارامتر**</span> **(Parameter Vector)** w
<span dir="rtl">للخوارزميتين **شبه التدرج** </span>**(Semi-gradient
Algorithms)**<span dir="rtl">. كان حجم الخطوة</span>
α=0.01<span dir="rtl">، وكانت الأوزان الابتدائية</span>
w=(1,1,1,1,1,1,10,1)⊤ <span dir="rtl"></span>

<span dir="rtl">هناك أيضًا أمثلة مضادة مشابهة لـ **المثال المضاد
لبيرد**</span> **(Baird's Counterexample)** <span dir="rtl">تظهر التباعد
في</span> **Q-learning <span dir="rtl"></span>**<span dir="rtl">هذا يثير
القلق لأن</span> **Q-learning
<span dir="rtl"></span>**<span dir="rtl">يتمتع بأفضل ضمانات التقارب بين
جميع طرق التحكم</span> (Control Methods)<span dir="rtl">.</span>
<span dir="rtl">لقد بُذلت جهود كبيرة في محاولة العثور على علاج لهذه
المشكلة أو الحصول على بعض الضمانات الأضعف، ولكن لا تزال قابلة للعمل. على
سبيل المثال، قد يكون من الممكن ضمان تقارب</span> **Q-learning
<span dir="rtl"></span>**<span dir="rtl">طالما أن **سياسة
السلوك**</span> **(Behavior Policy)** <span dir="rtl">قريبة بما فيه
الكفاية من **سياسة الهدف** </span>**(Target Policy)**<span dir="rtl">،
مثل عندما تكون **السياسة الجشعة مع نسبة استكشاف** </span>**(ε-greedy
policy)**<span dir="rtl">.</span> <span dir="rtl">على حد علمنا، لم يتم
العثور على حالات تباعد لـ  
</span>**Q-learning <span dir="rtl"></span>**<span dir="rtl">في هذه
الحالة، ولكن لم يتم إجراء تحليل نظري لذلك. في بقية هذا القسم، نقدم عدة
أفكار أخرى تم استكشافها</span>.

<span dir="rtl">افترض أنه بدلاً من أخذ خطوة واحدة فقط نحو العائد المتوقع
لخطوة واحدة في كل تكرار، كما في **المثال المضاد لبيرد**
</span>**(Baird's Counterexample)**<span dir="rtl">، نقوم فعليًا بتغيير
**دالة القيمة**</span> **<span dir="rtl">(</span>Value
<span dir="rtl"></span>Function<span dir="rtl">)
</span>**<span dir="rtl">بالكامل إلى أفضل تقريب بأقل مربعات. هل سيحل هذا
مشكلة عدم الاستقرار؟ بالطبع، إذا كانت **متجهات الميزات**
</span>**(Feature Vectors)**<span dir="rtl">،</span> {x(s): $`s \in S`$
}<span dir="rtl">، تشكل مجموعة مستقلة خطيًا، كما هو الحال في **المثال
المضاد لبيرد** </span>**(Baird's Counterexample)**<span dir="rtl">، لأن
التقريب الدقيق سيكون ممكنًا في كل تكرار وتتحول الطريقة إلى **البرمجة
الديناميكية الجدولية القياسية**</span>
**<span dir="rtl">(</span>Standard Tabular
<span dir="rtl"></span>DP<span dir="rtl">).</span>**
<span dir="rtl">ولكن بالطبع، النقطة هنا هي النظر في الحالة عندما لا يكون
الحل الدقيق ممكنًا. في هذه الحالة، لا يتم ضمان الاستقرار حتى عند تكوين
أفضل تقريب في كل تكرار، كما هو موضح في المثال</span>.

<img src="./media/image130.png"
style="width:1.34722in;height:1.12917in" />**<u><span dir="rtl">المثال
11.1</span>: <span dir="rtl">المثال المضاد لتسيتسيكليس وفان روي</span>
<span dir="rtl">(</span>Tsitsiklis and Van Roy’s
<span dir="rtl"></span>Counterexample<span dir="rtl">)</span></u>**

<span dir="rtl">هذا المثال يوضح أن **تقريب الدوال الخطية**</span>
**<span dir="rtl">(</span>Linear Function
<span dir="rtl"></span>Approximation<span dir="rtl">)
</span>**<span dir="rtl">لن يعمل مع **البرمجة الديناميكية**</span>
**(DP)** <span dir="rtl">حتى لو تم العثور على حل المربعات الصغرى</span>
(Least-squares Solution) <span dir="rtl">في كل خطوة. يتم تكوين المثال
المضاد من خلال تمديد مثال "من</span> w <span dir="rtl">إلى</span> 2w
<span dir="rtl">(من القسم السابق) بإضافة حالة نهائية، كما هو موضح في
الجهة اليمنى. كما في السابق، القيمة المقدرة للحالة الأولى هي</span>
w<span dir="rtl">، والقيمة المقدرة للحالة الثانية هي</span>
2w<span dir="rtl">.</span> <span dir="rtl">المكافأة هي صفر في جميع
الانتقالات، لذا فإن **القيم الحقيقية**</span>
**<span dir="rtl">(</span>True
<span dir="rtl"></span>Values<span dir="rtl">)
</span>**<span dir="rtl">في كلتا الحالتين هي صفر، مما يمكن تمثيله بدقة
عند</span> w=0<span dir="rtl">.</span> <span dir="rtl">إذا قمنا
بتعيين</span> wk+1 <span dir="rtl"></span>​ <span dir="rtl">في كل خطوة
لتقليل **خطأ التقييم**</span> **(VE)** <span dir="rtl">بين القيمة
المقدرة والعائد المتوقع لخطوة واحدة، فسنحصل على</span>:

``` math
w_{k + 1} = \arg{\min_{w \in R}{\sum_{s \in S}^{}\left( \widehat{v}(s,w) - E_{\pi}\left\lbrack R_{t + 1} + \gamma\widehat{v}\left( S_{t + 1},w_{k} \right)\mid S_{t} = s \right\rbrack \right)^{2}}}
```

``` math
w_{k + 1} = \arg{\min_{w \in R}\left( w - \gamma^{2}w_{k} \right)}^{2} + \left( 2w - (1 - \epsilon)\gamma^{2}w_{k} \right)^{2}
```

``` math
= \frac{6 - 4\epsilon}{5}\gamma^{2}w_{k}
```

<span dir="rtl">تتباعد السلسلة</span> {wk} <span dir="rtl">عندما</span>
$`\gamma > \frac{5}{6 - 4\epsilon}\text{ و }w_{0} \neq 0`$

<span dir="rtl">طريقة أخرى لمحاولة منع عدم الاستقرار هي استخدام طرق خاصة
لتقريب الدوال **(**</span>**Function
<span dir="rtl"></span>Approximation<span dir="rtl">)</span>**.
<span dir="rtl">على وجه الخصوص، يتم ضمان الاستقرار</span> **(Stability)
<span dir="rtl"></span>**<span dir="rtl">في طرق تقريب الدوال التي لا
تستنتج من الأهداف المرصودة. تشمل هذه الطرق، التي تُعرف بالمعدّلات</span>
**(Averagers)**<span dir="rtl">، طرق الجار الأقرب</span> **(Nearest
Neighbor Methods)** <span dir="rtl">والانحدار المحلي المرجح
**(**</span>**Locally <span dir="rtl"></span>Weighted
Regression<span dir="rtl">)</span>**<span dir="rtl">، ولكنها لا تشمل
الطرق الشائعة مثل الترميز بالبلاط</span> **(Tile Coding)
<span dir="rtl"></span>**<span dir="rtl">والشبكات العصبية
الاصطناعية</span> **(Artificial Neural Networks -
ANNs)**<span dir="rtl">.</span>

**<span dir="rtl">تمرين 11.3 (برمجة):</span>** <span dir="rtl">قم بتطبيق
"سارسا شبه التدرج ذات الخطوة الواحدة  
</span>**(One-step Semi-gradient Q-learning)
<span dir="rtl"></span>**<span dir="rtl">على **المثال المضاد لبيرد  
(**</span>**Baird’s
<span dir="rtl"></span>Counterexample<span dir="rtl">)
</span>**<span dir="rtl">وأثبت تجريبيًا أن الأوزان تتباعد</span>.

**<u>11.3 <span dir="rtl">الثالوث المميت</span> (The Deadly Triad)</u>**

<span dir="rtl">نقاشنا حتى الآن يمكن تلخيصه بالقول إن خطر عدم الاستقرار
والتباعد ينشأ عندما نجمع بين العناصر الثلاثة التالية، والتي تشكل ما
نسميه **الثالوث المميت** </span>**(The Deadly
Triad)**<span dir="rtl">:</span>

1.  **<span dir="rtl">تقريب الدوال</span> (Function
    Approximation)<span dir="rtl">:</span>** <span dir="rtl">وهي طريقة
    قوية وقابلة للتوسع للتعميم من **فضاء الحالات**</span> **(State
    Space)** <span dir="rtl">الذي يكون أكبر بكثير من الموارد الحاسوبية
    والذاكرة (مثل **تقريب الدوال الخطية**</span> **(Linear Function
    Approximation)** <span dir="rtl">أو **الشبكات العصبية الاصطناعية**
    </span>**(Artificial Neural Networks -
    ANNs)**<span dir="rtl">.</span>

2.  **<span dir="rtl">الاستدلال التمهيدي</span>
    (Bootstrapping)<span dir="rtl">:</span>** <span dir="rtl">حيث تكون
    أهداف التحديث مشتملة على تقديرات موجودة بالفعل (كما في **البرمجة
    الديناميكية**</span> **(Dynamic Programming - DP)**
    <span dir="rtl">أو طرق</span> **TD**) <span dir="rtl">بدلاً من
    الاعتماد بشكل حصري على **المكافآت الفعلية**</span> **(Actual
    Rewards)** <span dir="rtl">والعوائد الكاملة (كما في طرق **مونت
    كارلو** </span>**(Monte Carlo - MC)**<span dir="rtl">)</span>.

3.  **<span dir="rtl">التدريب خارج السياسة</span> (Off-policy
    Training)<span dir="rtl">:</span>** <span dir="rtl">حيث يتم التدريب
    على توزيع **الانتقالات** </span>**(Transitions)**
    <span dir="rtl">الذي يختلف عن التوزيع الناتج عن **سياسة الهدف**
    </span>**(Target Policy)**<span dir="rtl">. المسح عبر **فضاء
    الحالات**</span> **(State Space)** <span dir="rtl">وتحديث جميع
    **الحالات**</span> **(States)** <span dir="rtl">بشكل موحد، كما في
    **البرمجة الديناميكية** </span>**(DP)**<span dir="rtl">، لا يحترم
    **سياسة الهدف**</span> **(Target Policy)** <span dir="rtl">ويعتبر
    مثالاً على **التدريب خارج السياسة** </span>**(Off-policy
    Training)**<span dir="rtl">.</span>

<span dir="rtl">بشكل خاص، لاحظ أن الخطر ليس بسبب **التحكم**</span>
**(Control)** <span dir="rtl">أو **التكرار العام للسياسات**
</span>**(Generalized Policy Iteration)**<span dir="rtl">.</span>
<span dir="rtl">هذه الحالات أكثر تعقيدًا للتحليل، لكن عدم الاستقرار ينشأ
في حالة **التنبؤ**</span> **(Prediction)** <span dir="rtl">الأبسط كلما
اشتملت على جميع عناصر **الثالوث المميت**</span>
**<span dir="rtl">(</span>Deadly
<span dir="rtl"></span>Triad<span dir="rtl">)</span>**.
<span dir="rtl">الخطر أيضًا ليس بسبب **التعليم**</span> **(Learning)**
<span dir="rtl">أو عدم اليقين حول **البيئة**
</span>**(Environment)**<span dir="rtl">، لأنه يحدث بنفس القوة في طرق
**التخطيط** </span>**(Planning)**<span dir="rtl">، مثل **البرمجة
الديناميكية** </span>**(Dynamic Programming - DP)**<span dir="rtl">، حيث
تكون **البيئة**</span> **(Environment)** <span dir="rtl">معروفة
تمامًا</span>.

<span dir="rtl">إذا كانت هناك أي عناصر من **الثالوث المميت**</span>
**(Deadly Triad)** <span dir="rtl">موجودة، ولكن ليس كلها، يمكن تجنب عدم
الاستقرار. لذلك، من الطبيعي أن نمر على الثلاثة لنرى إذا كان هناك أي عنصر
يمكن الاستغناء عنه</span>.

<span dir="rtl">من بين الثلاثة، لا يمكن الاستغناء عن **تقريب
الدوال**</span> **(Function Approximation)** <span dir="rtl">بشكل واضح.
نحتاج إلى طرق تتوسع إلى مشكلات كبيرة وتتمتع بقوة تعبيرية كبيرة. نحتاج
على الأقل إلى **تقريب الدوال الخطية**</span> **(Linear Function
Approximation)** <span dir="rtl">مع العديد من **الميزات**</span>
**(Features)** <span dir="rtl">و**المعلمات**
</span>**(Parameters)**<span dir="rtl">.</span> **<span dir="rtl">تجميع
الحالات</span> (State Aggregation)** <span dir="rtl">أو الطرق غير
المعلمية التي تنمو في التعقيد مع **البيانات**</span> **(Data)**
<span dir="rtl">تكون ضعيفة جدًا أو مكلفة جدًا. طرق المربعات الصغرى
مثل</span> **LSTD** <span dir="rtl">لها تعقيد رباعي وبالتالي تكون مكلفة
جدًا للمشاكل الكبيرة</span>.

<span dir="rtl">يمكن الاستغناء عن **الاستدلال التمهيدي**
</span>**(Bootstrapping)**<span dir="rtl">، ولكن على حساب الكفاءة
الحاسوبية والبيانات. ربما تكون الخسائر في **الكفاءة الحاسوبية**</span>
**(Computational Efficiency)** <span dir="rtl">هي الأهم. طرق **مونت
كارلو**</span> **(Monte Carlo - MC)** <span dir="rtl">التي لا تعتمد على
**الاستدلال التمهيدي** </span>**(Bootstrapping)** <span dir="rtl">تتطلب
ذاكرة لحفظ كل ما يحدث بين كل **تنبؤ**</span> **(Prediction)**
<span dir="rtl">والحصول على **العائد النهائي** </span>**(Final
Return)**<span dir="rtl">، ويتم تنفيذ كل عملياتها الحسابية بمجرد الحصول
على **العائد النهائي** </span>**(Final Return)**<span dir="rtl">.</span>
<span dir="rtl">تكلفة هذه المسائل الحسابية قد لا تكون ظاهرة على أجهزة
الكمبيوتر التسلسلية مثل **الحواسيب من نوع فون نيومان** </span>**(Von
Neumann Computers)**<span dir="rtl">، ولكنها ستكون واضحة على الأجهزة
المخصصة. مع **الاستدلال التمهيدي**</span> **(Bootstrapping)**
<span dir="rtl">و**آثار الأهلية**</span>
**<span dir="rtl">(</span>Eligibility
<span dir="rtl"></span>Traces<span dir="rtl">)
</span>**<span dir="rtl">(الفصل 12)، يمكن التعامل مع **البيانات**</span>
**(Data)** <span dir="rtl">عندما وأينما تم توليدها، ولن يكون هناك حاجة
لاستخدامها مرة أخرى. التوفير في التواصل والذاكرة الذي يمكن تحقيقه من
خلال **الاستدلال التمهيدي**</span> **(Bootstrapping)**
<span dir="rtl">كبير</span>.

<span dir="rtl">الخسائر في **كفاءة البيانات**</span> **(Data
Efficiency)** <span dir="rtl">عند التخلي عن **الاستدلال التمهيدي**
</span>**(Bootstrapping)** <span dir="rtl">أيضًا تكون كبيرة. لقد رأينا
هذا مرارًا، كما في الفصلين 7 (الشكل 7.2) و9 (الشكل 9.2)، حيث كان هناك
درجة من **الاستدلال التمهيدي**</span> **(Bootstrapping)**
<span dir="rtl">أداءً أفضل بكثير من طرق **مونت كارلو**</span> **(Monte
Carlo Methods)** <span dir="rtl">على مهمة **التنبؤ بالحركة العشوائية**
</span>**(Random-walk Prediction Task)**<span dir="rtl">، وفي الفصل 10
حيث شوهد نفس الشيء في مهمة **التحكم في السيارة الجبلية**</span>
**(Mountain-Car Control Task)** <span dir="rtl">(الشكل 10.4). العديد من
المشاكل الأخرى تظهر تعلمًا أسرع بكثير مع **الاستدلال التمهيدي**</span>
**(Bootstrapping)** <span dir="rtl">(انظر على سبيل المثال الشكل 12.14).
غالبًا ما يؤدي **الاستدلال التمهيدي**</span> **(Bootstrapping)**
<span dir="rtl">إلى تعلم أسرع لأنه يسمح للتعليم بالاستفادة من **خاصية
الحالة** </span>**(State Property)**<span dir="rtl">، وهي القدرة على
التعرف على **الحالة** </span>**(State)** <span dir="rtl">عند العودة
إليها. من ناحية أخرى، يمكن أن يضعف **الاستدلال التمهيدي**
</span>**(Bootstrapping)** <span dir="rtl">التعليم في المشاكل التي تكون
فيها **تمثيلات الحالة (**</span>**State
<span dir="rtl"></span>Representations<span dir="rtl">)
</span>**<span dir="rtl">ضعيفة وتسبب **تعميمًا**</span>
**(Generalization)** <span dir="rtl">سيئًا (على سبيل المثال، يبدو أن هذا
هو الحال في لعبة **تيتريس** </span>**(Tetris)**<span dir="rtl">،
انظر</span>Şimşek, Algorta, and Kothiyal, 2016<span dir="rtl">).
**التمثيل السيئ للحالة**</span> **(Poor State Representation)**
<span dir="rtl">يمكن أن يؤدي أيضًا إلى **التحيز**
</span>**(Bias)**<span dir="rtl">؛ وهذا هو السبب في التقييد الأسوأ على
جودة التقريب النهائية لطرق **الاستدلال التمهيدي**
</span>**(Bootstrapping Methods)** <span dir="rtl">(المعادلة 9.14).
بالنظر إلى الموازنة، يجب اعتبار القدرة على **الاستدلال التمهيدي**</span>
**(Bootstrapping)** <span dir="rtl">ذات قيمة كبيرة. قد نختار في بعض
الأحيان عدم استخدامها عن طريق اختيار **تحديثات ذات خطوات متعددة
طويلة**</span> **(Long n-step Updates)** (<span dir="rtl">أو بارامتر
**الاستدلال التمهيدي**</span> **(Bootstrapping Parameter)**
<span dir="rtl">كبيرة،</span> 1λ≈1<span dir="rtl">؛ انظر الفصل 12) ولكن
غالبًا ما يزيد **الاستدلال التمهيدي**</span> **(Bootstrapping)**
<span dir="rtl">بشكل كبير من الكفاءة. إنها قدرة نود جدًا الاحتفاظ بها في
**مجموعة الأدوات**</span> **(Toolkit)** <span dir="rtl">لدينا</span>.

<span dir="rtl">أخيرًا، هناك **التعليم خارج السياسة**
</span>**(Off-policy Learning)**<span dir="rtl">؛ هل يمكننا الاستغناء
عنه؟ طرق **داخل السياسة**</span> **(On-policy Methods)**
<span dir="rtl">غالبًا ما تكون كافية. بالنسبة للتعليم المعزز **بدون
نموذج** </span>**(Model-free Reinforcement Learning)**<span dir="rtl">،
يمكن ببساطة استخدام **سارسا**</span> **(Sarsa)** <span dir="rtl">بدلاً
من</span> **Q-learning**<span dir="rtl">. تتيح طرق **خارج
السياسة**</span> **(Off-policy Methods)** <span dir="rtl">الحرية في
**السلوك** </span>**(Behavior)** <span dir="rtl">بعيدًا عن **سياسة
الهدف** </span>**(Target Policy)**<span dir="rtl">.</span>
<span dir="rtl">قد يعتبر هذا ميزة مريحة ولكنها ليست ضرورية. ومع ذلك، فإن
**التعليم خارج السياسة**</span> **(Off-policy Learning)**
<span dir="rtl">ضروري لحالات الاستخدام الأخرى المتوقعة، وهي الحالات التي
لم نذكرها بعد في هذا الكتاب ولكنها قد تكون مهمة للهدف الأكبر المتمثل في
إنشاء **وكيل ذكي قوي** </span>**(Powerful Intelligent
Agent)**<span dir="rtl">.</span> <span dir="rtl">في هذه الحالات، يتعلم
**الوكيل**</span> **(Agent)** <span dir="rtl">ليس فقط **دالة قيمة
واحدة**</span> **(Single Value Function)** <span dir="rtl">و**سياسة
واحدة** </span>**(Single Policy)**<span dir="rtl">، ولكن أعداد كبيرة
منها بالتوازي. هناك أدلة نفسية واسعة النطاق على أن الناس والحيوانات
يتعلمون **التنبؤ**</span> **(Prediction)** <span dir="rtl">بالعديد من
**الأحداث الحسية المختلفة**</span> **<span dir="rtl">(</span>Different
<span dir="rtl"></span>Sensory
Events<span dir="rtl">)</span>**<span dir="rtl">، وليس فقط **المكافآت**
</span>**(Rewards)**<span dir="rtl">.</span> <span dir="rtl">يمكن أن
نتفاجأ **بالأحداث غير العادية** </span>**(Unusual
Events)**<span dir="rtl">، ونصحح تنبؤاتنا بشأنها، حتى لو كانت ذات طابع
محايد (ليست جيدة ولا سيئة). من المفترض أن هذا النوع من **التنبؤ**</span>
**(Prediction)** <span dir="rtl">يشكل أساس **النماذج التنبؤية
للعالم**</span> **(Predictive Models of the World)** <span dir="rtl">مثل
تلك المستخدمة في **التخطيط**
</span>**(Planning)**<span dir="rtl">.</span> <span dir="rtl">نحن نتنبأ
بما سنراه بعد تحركات العين، وكم من الوقت سيستغرق الوصول إلى المنزل،
واحتمالية تسجيل الهدف في كرة السلة، والرضا الذي سنحصل عليه من القيام
بمشروع جديد. في جميع هذه الحالات، تعتمد **الأحداث**</span> **(Events)**
<span dir="rtl">التي نود **التنبؤ**</span> **(Predict)**
<span dir="rtl">بها على تصرفنا بطريقة معينة. لتعلمها جميعًا، بالتوازي،
يتطلب التعليم من **تيار واحد من التجارب** </span>**(One Stream of
Experience)**<span dir="rtl">.</span> <span dir="rtl">هناك العديد من
**سياسات الهدف** </span>**(Target Policies)**<span dir="rtl">، وبالتالي
لا يمكن **لسياسة السلوك الواحدة  
(**</span>**One <span dir="rtl"></span>Behavior Policy<span dir="rtl">)
</span>**<span dir="rtl">أن تساوي جميعها. ومع ذلك، فإن **التعليم
بالتوازي  
(**</span>**Parallel Learning<span dir="rtl">)</span>**
<span dir="rtl">ممكن من الناحية المفاهيمية لأن **سياسة السلوك**</span>
**(Behavior Policy)** <span dir="rtl">قد تتداخل جزئيًا مع العديد من
**سياسات الهدف** </span>**(Target Policies)**<span dir="rtl">.</span>
<span dir="rtl">للاستفادة الكاملة من هذا، يتطلب الأمر **التعليم خارج
السياسة** </span>**(Off-policy Learning)**<span dir="rtl">.</span>

**<u>11.4 <span dir="rtl">هندسة دالة القيمة الخطية</span> (Linear
Value-function Geometry)</u>**

<span dir="rtl">لفهم تحدي الاستقرار في التعليم خارج السياسة</span>
(Off-policy Learning) <span dir="rtl">بشكل أفضل، من المفيد التفكير في
**تقريب دالة القيمة**</span> **(Value Function Approximation)**
<span dir="rtl">بطريقة أكثر تجريدًا وبعيدًا عن كيفية تنفيذ التعليم. يمكننا
أن نتخيل فضاء جميع دوال القيمة الممكنة  
**(**</span>**State-value
<span dir="rtl"></span>Functions<span dir="rtl">)</span>** —
<span dir="rtl">جميع الدوال من **الحالات**</span> **(States)**
<span dir="rtl">إلى **الأرقام الحقيقية** </span>**(Real Numbers)**
v:S→Rv: S<span dir="rtl">. معظم هذه الدوال لا تتوافق مع أي سياسة. الأهم
من ذلك بالنسبة لأغراضنا هو أن معظمها لا يمكن تمثيله بواسطة **مُقرب
الدالة  
(**</span>**Function
Approximator<span dir="rtl">)</span>**<span dir="rtl">، الذي يكون مصممًا
ليحتوي على معلمات أقل بكثير من عدد **الحالات**
</span>**(States)**<span dir="rtl">.</span>

<span dir="rtl">بالنظر إلى تعداد **فضاء الحالات** </span>**(State
Space)** S={s1,s2,…,s∣S∣},<span dir="rtl">، فإن أي **دالة  
قيمة**</span> **(Value Function)** v <span dir="rtl">تتوافق مع متجه يسرد
قيمة كل حالة بالترتيب</span> \[v(s1),v(s2),…,v(s∣S∣)\]⊤ ​)
<span dir="rtl">يحتوي هذا التمثيل المتجهي لدالة القيمة على نفس عدد
المكونات كعدد **الحالات** </span>**(States)**<span dir="rtl">.</span>
<span dir="rtl">في معظم الحالات التي نريد فيها استخدام **تقريب
الدوال**</span> **<span dir="rtl">(</span>Function
<span dir="rtl"></span>Approximation<span dir="rtl">)</span>**<span dir="rtl">،
سيكون هذا العدد من المكونات كبيرًا جدًا لتمثيل المتجه بشكل صريح. ومع ذلك،
فإن فكرة هذا المتجه مفيدة من الناحية المفاهيمية. فيما يلي، نتعامل مع
**دالة القيمة**</span> **<span dir="rtl">(</span>Value
<span dir="rtl"></span>Function<span dir="rtl">)
</span>**<span dir="rtl">وتمثيلها المتجهي بالتبادل</span>.

<span dir="rtl">لتطوير الفهم، فكر في حالة تحتوي على ثلاث حالات</span>
S={s1,s2,s3} <span dir="rtl">ومعلمتين</span> w=(w1,w2) ⊤
<span dir="rtl">يمكننا بعد ذلك أن نرى جميع **دوال
القيمة/المتجهات**</span> **(Value Functions/Vectors)**
<span dir="rtl">كنقاط في فضاء ثلاثي الأبعاد. توفر المعلمات نظام إحداثيات
بديلًا فوق فضاء فرعي ثنائي الأبعاد. أي متجه وزن</span> w=(w1,w2)⊤
<span dir="rtl">هو نقطة في الفضاء الفرعي ثنائي الأبعاد وبالتالي يمثل
أيضًا دالة قيمة كاملة</span> vw <span dir="rtl">تعطي قيمًا لكل من الحالات
الثلاث. مع **تقريب دالة القيمة العامة (**</span>**General Function
<span dir="rtl"></span>Approximation<span dir="rtl">)
</span>**<span dir="rtl">قد تكون العلاقة بين الفضاء الكامل والفضاء
الفرعي للدوال القابلة للتمثيل معقدة، ولكن في حالة **تقريب دالة القيمة
الخطية**</span> **(Linear Value-function Approximation)**
<span dir="rtl">يكون الفضاء الفرعي مستوى بسيطًا، كما هو موضح في الشكل
11.3</span>.

<span dir="rtl">الآن فكر في **سياسة ثابتة**</span> **(Fixed Policy)**
<span dir="rtl">واحدة</span> π<span dir="rtl">.</span>
<span dir="rtl">نفترض أن **دالة القيمة الحقيقية  
(**</span>**True <span dir="rtl"></span>Value Function<span dir="rtl">)
</span>**<span dir="rtl">لهذه السياسة</span> vπ <span dir="rtl">معقدة
للغاية بحيث لا يمكن تمثيلها بدقة كمقاربة. وبالتالي، فإن</span> vπ
<span dir="rtl">ليست في الفضاء الفرعي؛ في الشكل يتم تصويرها على أنها فوق
الفضاء الفرعي المستوي للدوال القابلة للتمثيل. إذا لم يكن من الممكن
تمثيل</span> vπ <span dir="rtl">بدقة، فما **دالة القيمة  
(**</span>**Value Function<span dir="rtl">)</span>**
<span dir="rtl">القابلة للتمثيل الأقرب إليها؟ يتضح أن هذا سؤال دقيق وله
عدة إجابات. للبدء، نحتاج إلى مقياس للمسافة بين دالتي قيمة. بالنظر إلى
دالتي قيمة</span> v1​
<span dir="rtl">و</span>v<span dir="rtl">2</span>​<span dir="rtl">،
يمكننا التحدث عن **الفرق المتجهي** </span>**(Vector Difference)**
<span dir="rtl">بينهما،</span> v=v1−v2​<span dir="rtl">.</span>
<span dir="rtl">إذا كان</span> v <span dir="rtl">صغيرًا، فإن الدالتين
قريبتان من بعضهما البعض. ولكن كيف نقيس حجم هذا المتجه الفرق؟ **المعيار
الإقليدي التقليدي** </span>**(Conventional Euclidean Norm)**
<span dir="rtl">غير مناسب لأنه، كما نوقش في القسم 9.2، بعض
**الحالات**</span> **(States)** <span dir="rtl">أكثر أهمية من غيرها
لأنها تحدث بشكل متكرر أو لأننا نهتم بها أكثر</span>  
<span dir="rtl">(القسم 9.11). كما في القسم 9.2، دعنا نستخدم
التوزيع،</span> μ:S→\[0,1\] <span dir="rtl">لتحديد مدى أهمية تقييم
**الحالات المختلفة**</span> **(Different States)** <span dir="rtl">بدقة
(وغالبًا ما يُعتبر هذا هو **توزيع داخل السياسة  **
</span>**On-policy Distribution)**<span dir="rtl">)</span>.
<span dir="rtl">يمكننا بعد ذلك تعريف المسافة بين **دوال القيمة  
(**</span>**Value <span dir="rtl"></span>Functions<span dir="rtl">)
</span>**<span dir="rtl">باستخدام المعيار</span>
(Norm)<span dir="rtl">.</span>

``` math
|\text{|}v{|\text{|}}_{\mu}^{2} = \sum_{s \in S}^{}{\mu(s)v(s)^{2}}
```

<span dir="rtl">لاحظ أن **تقدير القيمة**</span> **(Value Estimation)
(VE)** <span dir="rtl">من **الفصل 9.2**</span> **(Section 9.2)**
<span dir="rtl">يمكن كتابته ببساطة باستخدام هذا المعيار كالتالي</span>:

``` math
\overline{VE}(w) = \text{|}{\overline{v}}_{w} - v_{\pi}\text{|}_{\mu}^{2}
```

<span dir="rtl">بالنسبة لأي **دالة قيمة** </span>**(Value Function)
v**<span dir="rtl">، فإن عملية إيجاد **دالة القيمة** </span>**(Value
Function) v** <span dir="rtl">الأقرب في **الفضاء الجزئي لدوال القيمة
القابلة للتمثيل (**</span>**Subspace of Representable
<span dir="rtl"></span>Value Functions<span dir="rtl">)
</span>**<span dir="rtl">هي عملية **إسقاط** </span>**(Projection
Operation)**<span dir="rtl">.</span>

<img src="./media/image131.png"
style="width:6.26806in;height:2.83542in" />

**<span dir="rtl">الشكل 11.3</span>:** <span dir="rtl">الهندسة الخاصة
بتقريب **دالة القيمة الخطية**</span> **<span dir="rtl">(</span>Linear
Value-Function
<span dir="rtl"></span>Approximation<span dir="rtl">)</span>**.
<span dir="rtl">يظهر الشكل ثلاثي الأبعاد فضاء جميع **دوال القيمة**
</span>**(Value Functions)** <span dir="rtl">عبر ثلاث حالات، بينما يظهر
كطائرة الفضاء الجزئي لجميع **دوال القيمة** </span>**(Value Functions)**
<span dir="rtl">القابلة للتمثيل بواسطة **مقرب دالة خطي**</span>
**(Linear Function Approximator)** <span dir="rtl">مع **البارامتر  
(**</span>**w = (w1, w2)\>**<span dir="rtl">.</span>
**<span dir="rtl">دالة القيمة الحقيقية</span> (True Value Function)
v\_{\pi}** <span dir="rtl">موجودة في الفضاء الأكبر ويمكن إسقاطها إلى
الأسفل (إلى الفضاء الجزئي باستخدام **عامل الإسقاط** </span>**(Projection
Operator) \Pi**) <span dir="rtl">للحصول على أفضل تقريب لها من حيث **خطأ
القيمة**</span> **<span dir="rtl">(</span>Value
<span dir="rtl"></span>Error<span dir="rtl">)</span>
(VE)**<span dir="rtl">.</span> <span dir="rtl">أفضل المقربات من حيث
**خطأ بيلمان** </span>**(Bellman Error) (BE)**<span dir="rtl">، **خطأ
بيلمان المسقط** </span>**(Projected Bellman Error)
(PBE)**<span dir="rtl">، و**خطأ الاختلاف الزمني**</span>
**<span dir="rtl">(</span>Temporal <span dir="rtl"></span>Difference
Error<span dir="rtl">)</span> (TDE)** <span dir="rtl">كلها قد تكون
مختلفة وتظهر في الجزء السفلي الأيمن</span>.
<span dir="rtl"></span>(**VE<span dir="rtl">،</span> BE<span dir="rtl">،
و</span>PBE** <span dir="rtl">كلها تعامل على أنها **المتجهات**</span>
**(Vectors)** <span dir="rtl">المقابلة في هذا الشكل). يأخذ **عامل
بيلمان** </span>**(Bellman Operator)** **<span dir="rtl">دالة
قيمة</span> (Value Function)** <span dir="rtl">في الطائرة إلى واحدة
خارجها، والتي يمكن بعد ذلك إسقاطها مرة أخرى. إذا قمت بتطبيق **عامل
بيلمان**</span> **(Bellman Operator)** <span dir="rtl">بشكل متكرر خارج
الفضاء (كما هو موضح باللون الرمادي في الأعلى)، فسوف تصل إلى **دالة
القيمة الحقيقية** </span>**(True Value Function)**<span dir="rtl">، كما
في البرمجة الديناميكية التقليدية. إذا بدلاً من ذلك قمت بالإسقاط مرة أخرى
إلى الفضاء الجزئي في كل خطوة، كما في الخطوة السفلية الموضحة باللون
الرمادي، فإن النقطة الثابتة ستكون نقطة **المتجه الصفري** </span>**(Zero
Vector) PBE**<span dir="rtl">.</span>

<span dir="rtl">نعرّف **عامل الإسقاط**</span> **(Projection Operator) Π**
<span dir="rtl">الذي يأخذ **دالة قيمة**</span> **(Value Function)**
<span dir="rtl">عشوائية إلى **الدالة القابلة للتمثيل**</span>
**(Representable Function)** <span dir="rtl">الأقرب وفقًا
لمعيارنا</span>:

``` math
\Pi v \doteq v_{w}\quad\text{where}\quad w = \arg{\min_{w \in R^{d}}\text{|}}v - v_{w}\text{|}_{\mu}^{2}
```

**<span dir="rtl">دالة القيمة القابلة للتمثيل</span> (Representable
Value Function)** <span dir="rtl">الأقرب إلى **دالة القيمة
الحقيقية**</span> **(True Value Function) vπ​** <span dir="rtl">هي
بالتالي إسقاطها،</span> Πvπ​<span dir="rtl">، كما هو مقترح في **الشكل
11.3**</span>. <span dir="rtl">هذا هو الحل الذي يتم العثور عليه بشكل غير
مباشر باستخدام طرق **مونت كارلو**</span> **<span dir="rtl">(</span>Monte
<span dir="rtl"></span>Carlo
Methods<span dir="rtl">)</span>**<span dir="rtl">، وإن كان ذلك غالبًا
ببطء شديد. يتم مناقشة عملية الإسقاط بشكل أكثر تفصيلًا في الصندوق الموجود
في الصفحة التالية</span>.

**<span dir="rtl">طرق الاختلاف الزمني</span> (TD Methods)**
<span dir="rtl">تجد حلولًا مختلفة. لفهم منطقها، تذكر أن **معادلة
بيلمان**</span> **(Bellman Equation)** <span dir="rtl">لـ **دالة
القيمة**</span> **(Value Function) vπ​** <span dir="rtl">هي</span>:

``` math
v_{\pi}(s) = \sum_{a}^{}{\pi\left( a \middle| s \right)\sum_{s',r}^{}{p\left( s',r \middle| s,a \right)\left\lbrack r + \gamma v_{\pi}\left( s' \right) \right\rbrack}},\quad\text{for all }s \in S.
```

<span dir="rtl">مصفوفة الإسقاط</span> (Projection Matrix)

<span dir="rtl">بالنسبة لمقرب دالة خطية</span> (Linear Function
Approximator)<span dir="rtl">، فإن عملية الإسقاط تكون خطية، مما يعني أنه
يمكن تمثيلها كمصفوفة ذات أبعاد</span> ∣S∣×∣S∣

``` math
\Pi \doteq X\left( X^{\top}DX \right)^{- 1}X^{\top}D,
```

<span dir="rtl">حيث، كما في **الفصل 9.4** </span>**(Section
9.4)**<span dir="rtl">، تشير</span> D <span dir="rtl">إلى **المصفوفة
القطرية  
(**</span>**Diagonal <span dir="rtl"></span>Matrix<span dir="rtl">)
</span>**<span dir="rtl">ذات الأبعاد</span> ∣S∣×∣S∣
<span dir="rtl">والتي تحتوي على</span> μ(s) <span dir="rtl">على القطر،
وتشير</span> X <span dir="rtl">إلى **المصفوفة ذات الأبعاد**
</span>**∣S∣×d <span dir="rtl"></span>**<span dir="rtl">والتي تحتوي
صفوفها على **متجهات الميزات** </span>**(Feature Vectors)**
$`\mathbf{X}\left( \mathbf{s} \right)^{\mathbf{\top}}`$<span dir="rtl">،
واحد لكل **حالة** </span>**(State) s**<span dir="rtl">.</span>
<span dir="rtl">إذا لم يكن للمصفوفة المعكوسة في المعادلة (11.14) وجود،
فيتم استبدالها بالمعكوس الزائف</span> (Pseudoinverse)<span dir="rtl">.
باستخدام هذه المصفوفات، يمكن كتابة المعيار المربع لمتجه كالتالي</span>:

``` math
\text{|}v\text{|}_{\mu}^{2} = v^{\top}Dv,
```

<span dir="rtl">ويمكن كتابة **دالة القيمة الخطية التقريبية**</span>
**(Approximate Linear Value Function)**
<span dir="rtl">كالتالي</span>:<span dir="rtl">  
</span>

``` math
v_{w} = Xw.
```

**<span dir="rtl">دالة القيمة الحقيقية</span> (True Value Function) vπ​**
<span dir="rtl">هي **دالة القيمة الوحيدة (**</span>**Value
<span dir="rtl"></span>Function<span dir="rtl">)
</span>**<span dir="rtl">التي تحل المعادلة (11.13) بشكل دقيق. إذا تم
استبدال **دالة القيمة التقريبية** </span>**(Approximate Value Function)
<span dir="rtl"></span>vw <span dir="rtl"></span>**<span dir="rtl">بـ
**دالة القيمة الحقيقية** </span>**vπ​**<span dir="rtl">، يمكن استخدام
الفرق بين الجانبين الأيمن والأيسر من المعادلة المعدلة كمقياس لمدى
ابتعاد</span> vw <span dir="rtl">عن</span> vπ<span dir="rtl">. نسمي هذا
الفرق **خطأ بيلمان**</span> **(Bellman Error)** <span dir="rtl">عند
الحالة</span> s<span dir="rtl">:</span>

``` math
\delta_{w}(s) \doteq \left( \sum_{a}^{}{\pi\left( a \middle| s \right)\sum_{s',r}^{}{p\left( s',r \middle| s,a \right)\left\lbrack r + \gamma v_{w}\left( s' \right) \right\rbrack}} \right) - v_{w}(s)
```

``` math
= E_{\pi}\left\lbrack R_{t + 1} + \gamma v_{w}\left( S_{t + 1} \right) - v_{w}\left( S_{t} \right)\mid S_{t} = s,A_{t} \sim \pi \right\rbrack,
```

<span dir="rtl">وهذا يوضح بوضوح العلاقة بين **خطأ بيلمان**</span>
**(Bellman Error)** <span dir="rtl">وخطأ **الاختلاف الزمني  
(**</span>**(TD <span dir="rtl"></span>Error)** <span dir="rtl">المعادلة
11.3)</span> **<span dir="rtl">خطأ بيلمان</span> (Bellman Error)**
<span dir="rtl">هو **التوقع** </span>**(Expectation)**
<span dir="rtl">لخطأ الاختلاف الزمني</span>.

**<span dir="rtl">متجه جميع أخطاء بيلمان</span> (Vector of all Bellman
Errors)**<span dir="rtl">، عند جميع الحالات، والذي يُشار إليه بـ</span>
$`\delta(w) \in R^{|S|}`$<span dir="rtl">، يسمى **متجه خطأ
بيلمان**</span> **(Bellman Error Vector)** <span dir="rtl">(موضحًا
بـ</span> BE <span dir="rtl">في **الشكل 11.3**)</span>.
<span dir="rtl">الحجم الإجمالي لهذا المتجه، وفقًا للمقياس، هو مقياس
إجمالي للخطأ في **دالة القيمة** </span>**(Value
Function)**<span dir="rtl">، ويُسمى **متوسط خطأ بيلمان التربيعي**</span>
**<span dir="rtl">(</span>Mean Squared Bellman
<span dir="rtl"></span>Error<span dir="rtl">)</span>**:

``` math
\overline{\text{BE}}(w) = \text{|}\overline{\delta}(w)\text{|}_{\overline{\mu}}^{2}
```

<span dir="rtl">لا يمكن بشكل عام تقليل **خطأ بيلمان**</span> **(BE)**
<span dir="rtl">إلى الصفر (عند هذه النقطة يكون</span>
vw=vπ<span dir="rtl">، ولكن بالنسبة لتقريب الدالة الخطية</span> (Linear
Function Approximation)<span dir="rtl">، هناك قيمة فريدة لـ</span> w
<span dir="rtl">التي يتم عندها تقليل **خطأ بيلمان**</span> **(BE)**
<span dir="rtl">إلى الحد الأدنى. هذه النقطة في الفضاء الجزئي للدوال
القابلة للتمثيل (التي تم تصنيفها كـ</span> BE <span dir="rtl">في **الشكل
11.3**</span>) <span dir="rtl">تختلف بشكل عام عن تلك التي تقلل **خطأ
القيمة**</span> **(VE)** (<span dir="rtl">الموضح كـ</span>
Πvπ<span dir="rtl">.</span> <span dir="rtl">يتم مناقشة الطرق التي تهدف
إلى تقليل **خطأ بيلمان**</span> **(BE)** <span dir="rtl">في القسمين
التاليين. يتم عرض **متجه خطأ بيلمان**</span> **(Bellman Error Vector)**
<span dir="rtl">في **الشكل 11.3** كنتيجة لتطبيق **عامل بيلمان**</span>
**<span dir="rtl">(</span>Bellman
<span dir="rtl"></span>Operator<span dir="rtl">)  
</span>**$`\mathbf{B}_{\mathbf{\pi}}\mathbf{:}\mathbf{R}^{\left| \mathbf{S} \right|}\mathbf{\rightarrow}\mathbf{R}^{\left| \mathbf{S} \right|}`$
<span dir="rtl">على **دالة القيمة التقريبية**</span> **(Approximate
Value Function)**. **<span dir="rtl">عامل بيلمان</span> (Bellman
Operator)** <span dir="rtl">يُعرف بواسطة</span>...

``` math
\left( B_{\pi}v \right)(s) \doteq \sum_{a}^{}{\pi\left( a \middle| s \right)\sum_{s',r}^{}{p\left( s',r \middle| s,a \right)\left\lbrack r + \gamma v\left( s' \right) \right\rbrack}},
```

<span dir="rtl">لكل حالة</span> $`s \in S`$ <span dir="rtl"></span>
<span dir="rtl">وكل دالة قيمة</span> v:S→R<span dir="rtl">.</span>
<span dir="rtl">يمكن كتابة **متجه خطأ بيلمان  
(**</span>**Bellman Error <span dir="rtl"></span>Vector<span dir="rtl">)
</span>**<span dir="rtl">لـ</span> v
<span dir="rtl">كالتالي</span>:<span dir="rtl">  
</span>$`\overline{\delta}(w) = B_{\pi}v_{w} - v_{w}.`$
<span dir="rtl"></span>

<span dir="rtl">إذا تم تطبيق **عامل بيلمان**</span> **(Bellman
Operator)** <span dir="rtl">على **دالة قيمة**</span> **(Value
Function)** <span dir="rtl">في الفضاء الجزئي القابل للتمثيل، فإنه سيؤدي
بشكل عام إلى إنتاج **دالة قيمة جديدة (**</span>**New Value
<span dir="rtl"></span>Function<span dir="rtl">)
</span>**<span dir="rtl">خارج هذا الفضاء الجزئي، كما هو مقترح في الشكل.
في **البرمجة الديناميكية** </span>**(Dynamic Programming)**
<span dir="rtl">(بدون تقريب الدالة)، يتم تطبيق هذا العامل بشكل متكرر على
النقاط خارج الفضاء القابل للتمثيل، كما هو موضح بالأسهم الرمادية في أعلى
**الشكل 11.3**</span>. <span dir="rtl">في النهاية، يتقارب هذا الإجراء
إلى **دالة القيمة الحقيقية** </span>**vπ​**<span dir="rtl">، النقطة
الثابتة الوحيدة لعامل بيلمان، وهي **دالة القيمة الوحيدة**</span>
**(Value Function)** <span dir="rtl">التي</span>...

``` math
v_{\pi} = B_{\pi}v_{\pi},
```

<span dir="rtl">وهذا مجرد طريقة أخرى لكتابة **معادلة بيلمان**</span>
**(Bellman Equation)** <span dir="rtl">لـ</span> π
<span dir="rtl">(المعادلة 11.13)</span>.

<span dir="rtl">ومع ذلك، مع **تقريب الدالة** </span>**(Function
Approximation)**<span dir="rtl">، لا يمكن تمثيل **دوال القيمة الوسيطة**
</span>**(Intermediate Value Functions)** <span dir="rtl">التي تقع خارج
الفضاء الجزئي. لا يمكن اتباع الأسهم الرمادية في الجزء العلوي من **الشكل
11.3** لأنه بعد التحديث الأول (الخط الداكن) يجب إعادة إسقاط **دالة
القيمة**</span> **(Value Function)** <span dir="rtl">إلى شيء قابل
للتمثيل. ثم تبدأ التكرار التالي داخل الفضاء الجزئي؛ حيث يتم أخذ **دالة
القيمة**</span> **(Value Function)** <span dir="rtl">خارج الفضاء الجزئي
مرة أخرى بواسطة **عامل بيلمان** </span>**(Bellman Operator)**
<span dir="rtl">ثم يتم إعادة إسقاطها بواسطة **عامل الإسقاط**
</span>**(Projection Operator)**<span dir="rtl">، كما هو مقترح بواسطة
السهم والخط الرمادي السفلي</span>.

<span dir="rtl">اتباع هذه الأسهم يشكل عملية شبيهة بـ **البرمجة
الديناميكية**</span> **(DP-like process)** <span dir="rtl">مع التقريب.
في هذه الحالة، نحن مهتمون بإسقاط **متجه خطأ بيلمان**</span> **(Bellman
Error Vector)** <span dir="rtl">مرة أخرى إلى الفضاء القابل للتمثيل. هذا
هو **متجه خطأ بيلمان المسقط**</span> **<span dir="rtl">(</span>Projected
Bellman Error <span dir="rtl"></span>Vector<span dir="rtl">)
</span>**$`\mathbf{\Pi}\overline{\mathbf{\delta}}\left( \mathbf{v}_{\mathbf{w}} \right)`$
<span dir="rtl">الموضح في **الشكل 11.3** كـ</span>
PBE<span dir="rtl">.</span> <span dir="rtl">حجم هذا المتجه، وفقًا
للمقياس، هو مقياس آخر للخطأ في **دالة القيمة التقريبية**
</span>**(Approximate Value Function)**<span dir="rtl">. بالنسبة لأي
**دالة قيمة تقريبية** </span>**(Approximate Value Function)
v**<span dir="rtl">، نعرف **متوسط خطأ بيلمان المسقط التربيعي**
</span>**(Mean Square Projected Bellman Error)**<span dir="rtl">، ويرمز
له بـ</span> PBE<span dir="rtl">، كالتالي</span>:

``` math
\text{PBE}(w) = \text{|}\Pi\overline{\delta}(w)\text{|}_{\mu}^{2}
```

<span dir="rtl">مع **تقريب الدالة الخطية** </span>**(Linear Function
Approximation)**<span dir="rtl">، هناك دائمًا **دالة قيمة تقريبية**
</span>**(Approximate Value Function)** <span dir="rtl">(داخل الفضاء
الجزئي) ذات</span> **PBE <span dir="rtl"></span>**<span dir="rtl">صفر؛
هذه هي النقطة الثابتة لـ **الاختلاف الزمني** </span>**wTD (TD Fixed
Point) ​**<span dir="rtl">، التي تم تقديمها في **الفصل 9.4**</span>.
<span dir="rtl">كما رأينا، هذه النقطة ليست دائمًا مستقرة تحت طرق
**الاختلاف الزمني شبه التدرجي (**</span>**Semi-gradient TD
<span dir="rtl"></span>Methods<span dir="rtl">)
</span>**<span dir="rtl">والتدريب **خارج السياسة** </span>**(Off-policy
Training)**<span dir="rtl">.</span> <span dir="rtl">كما هو موضح في
الشكل، هذه **دالة القيمة**</span> **(Value Function)**
<span dir="rtl">تختلف بشكل عام عن تلك التي تقلل **خطأ القيمة**</span>
**(VE)** <span dir="rtl">أو **خطأ بيلمان**
</span>**(BE)**<span dir="rtl">.</span> <span dir="rtl">يتم مناقشة الطرق
التي تضمن الوصول إلى هذه النقطة في الأقسام 11.7 و11.8</span>.

**<u>11.5 <span dir="rtl">النزول المتدرج في خطأ بيلمان</span> (Gradient
Descent in the Bellman Error)</u>**

**<span dir="rtl">معتمدين</span> (Armed)** <span dir="rtl">على فهم أفضل
لتقريب **دالة القيمة** </span>**(Value Function Approximation)**
<span dir="rtl">وأهدافها المختلفة، نعود الآن إلى تحدي الاستقرار في
التعليم **خارج السياسة  
(**</span>**Off-policy
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**.
<span dir="rtl">نرغب في تطبيق نهج **النزول المتدرج العشوائي
(**</span>**Stochastic <span dir="rtl"></span>Gradient
Descent<span dir="rtl">)</span> (SGD)**<span dir="rtl">، حيث يتم إجراء
التحديثات التي تكون في التوقع مساوية للميل السالب لـ **دالة الهدف**
</span>**(Objective Function)**<span dir="rtl">.</span>
<span dir="rtl">هذه الطرق دائمًا ما تتجه نحو الأسفل (في التوقع) في **دالة
الهدف** </span>**(Objective Function)**<span dir="rtl">، ولهذا السبب فهي
عادةً مستقرة وتتميز بخصائص تقارب ممتازة. من بين الخوارزميات التي تم
دراستها حتى الآن في هذا الكتاب، فقط طرق **مونت كارلو** </span>**(Monte
Carlo Methods)** <span dir="rtl">هي طرق</span> **SGD**
<span dir="rtl">حقيقية. هذه الطرق تتقارب بشكل قوي في كل من التدريب
**داخل السياسة**</span> **(On-policy Training)** <span dir="rtl">و**خارج
السياسة** </span>**(Off-policy Training)** <span dir="rtl">وكذلك بالنسبة
لمقربات الدوال العامة **اللاخطية القابلة للتفاضل**</span>
**<span dir="rtl">(</span>Nonlinear Differentiable
<span dir="rtl"></span>Function
Approximators<span dir="rtl">)</span>**<span dir="rtl">، على الرغم من
أنها غالبًا ما تكون أبطأ من الطرق **شبه التدرجية مع استخدام التمهيد**
</span>**(Semi-gradient Methods with Bootstrapping)**<span dir="rtl">،
والتي ليست طرق</span> **SGD**<span dir="rtl">. قد تتباعد الطرق شبه
التدرجية تحت التدريب **خارج السياسة** </span>**(Off-policy
Training)**<span dir="rtl">، كما رأينا سابقًا في هذا الفصل، وفي حالات
محددة من **تقريب الدوال غير الخطية**</span>
**<span dir="rtl">(</span>Nonlinear <span dir="rtl"></span>Function
Approximation<span dir="rtl">)</span>**
<span dir="rtl"></span>(Tsitsiklis and Van Roy, 1997)<span dir="rtl">.
مع طريقة</span> **SGD** <span dir="rtl">الحقيقية، لن يكون مثل هذا
التباعد ممكنًا</span>.

<span dir="rtl">جاذبية</span> **SGD** <span dir="rtl">قوية جدًا لدرجة أنه
تم بذل جهد كبير للعثور على طريقة عملية لاستخدامها في **التعليم المعزز**
</span>**(Reinforcement Learning)<span dir="rtl">.</span>**
<span dir="rtl">نقطة البداية لجميع هذه الجهود هي اختيار **دالة خطأ**
</span>**(Error Function)** <span dir="rtl">أو **دالة هدف**</span>
**(Objective Function)** <span dir="rtl">لتحسينها</span>.

<span dir="rtl">لنبدأ بالتفكير ليس في **خطأ بيلمان** </span>**(Bellman
Error)**<span dir="rtl">، بل في شيء أكثر مباشرة وسذاجة. يتم تحفيز
التعليم بالاختلاف الزمني</span> (Temporal Difference Learning)
<span dir="rtl">بواسطة</span> **TD <span dir="rtl">خطأ</span> (TD
Error)**<span dir="rtl">.</span> <span dir="rtl">لماذا لا نتخذ تقليل
التربيع المتوقع لـ **خطأ**</span> **TD** <span dir="rtl">كهدف؟ في حالة
تقريب الدالة العامة</span> <span dir="rtl">  
(</span>General
<span dir="rtl"></span>Function-Approximation<span dir="rtl">)، يكون
**خطأ**</span> **TD** <span dir="rtl">ذو الخطوة الواحدة مع الخصم</span>
(Discounting) <span dir="rtl">كالتالي</span>:

``` math
\delta_{t} = R_{t + 1} + \gamma\widehat{v}\left( S_{t + 1},w_{t} \right) - \widehat{v}\left( S_{t},w_{t} \right).
```

<span dir="rtl">دالة الهدف المحتملة هي ما يمكن تسميته **متوسط
خطأ**</span> **TD <span dir="rtl">التربيعي</span>
<span dir="rtl">(</span>Mean Squared TD
<span dir="rtl"></span>Error<span dir="rtl">)</span>**:
<span dir="rtl">  
</span>

``` math
{\text{TDE}(w) = \sum_{s \in S}^{}{\mu(s)E\left\lbrack \delta_{t}^{2}\mid S_{t} = s,A_{t} \sim \pi \right\rbrack}
}
{= \sum_{s \in S}^{}{\mu(s)E\left\lbrack \rho_{t}\delta_{t}^{2}\mid S_{t} = s,A_{t} \sim \beta \right\rbrack}
}
{= E_{\beta}\left\lbrack \rho_{t}\delta_{t}^{2} \right\rbrack.}
```

<span dir="rtl">(إذا كانت</span> μ <span dir="rtl">هي التوزيع الذي يتم
مواجهته تحت السياسة</span> b<span dir="rtl">)</span>

<span dir="rtl">المعادلة الأخيرة تأخذ الشكل المطلوب لـ **النزول المتدرج
العشوائي** </span>**(SGD)**<span dir="rtl">؛ حيث تعطي **دالة الهدف**
</span>**(Objective Function)** <span dir="rtl">على شكل توقع يمكن أخذه
من التجربة (تذكر أن التجربة ناتجة عن **السياسة السلوكية**
</span>**(Behavior Policy) b**<span dir="rtl">)</span>.
<span dir="rtl">وبالتالي، باتباع النهج القياسي لـ</span>
**SGD**<span dir="rtl">، يمكن اشتقاق التحديث لكل خطوة بناءً على عينة من
هذه القيمة المتوقعة</span>:

``` math
{w_{t + 1} = w_{t} - \frac{1}{2}\alpha\nabla\left( \rho_{t}\delta_{t}^{2} \right)
}
{= w_{t} - \alpha\rho_{t}\delta_{t}\nabla_{t}
}
{= w_{t} + \alpha\rho_{t}\delta_{t}\left( \gamma\nabla\widehat{v}\left( S_{t + 1},w_{t} \right) - \nabla\widehat{v}\left( S_{t},w_{t} \right) \right),}
```

<span dir="rtl">ستلاحظ أن هذه المعادلة مشابهة لخوارزمية **شبه التدرج**
</span>**TD (Semi-gradient TD Algorithm) <span dir="rtl">(المعادلة 11.2)
</span>**<span dir="rtl">باستثناء المصطلح النهائي الإضافي. هذا المصطلح
يكمل **التدرج** </span>**(Gradient)** <span dir="rtl">ويجعلها
خوارزمية</span> **SGD <span dir="rtl">حقيقية</span> (True SGD
Algorithm)** <span dir="rtl">مع ضمانات تقارب ممتازة. دعنا نسمي هذه
الخوارزمية **الخوارزمية الساذجة للتدرج المتبقي (**</span>**Naive
Residual-Gradient
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**
<span dir="rtl">(وفقًا لـ</span> Baird, 1995<span dir="rtl">)</span>.
<span dir="rtl">على الرغم من أن **الخوارزمية الساذجة للتدرج
المتبقي**</span> **<span dir="rtl">(</span>Naive Residual-Gradient
Algorithm<span dir="rtl">)</span>** <span dir="rtl">تتقارب بشكل قوي، إلا
أنها لا تتقارب بالضرورة إلى مكان مرغوب فيه</span>.<span dir="rtl">  
</span>

**<span dir="rtl">المثال 11.2: مثال الانقسام</span> (A-split
Example)**<span dir="rtl">، يظهر بساطة **الخوارزمية البدائية للتدرج
المتبقي** </span>**(Naive Residual-Gradient
Algorithm)**<span dir="rtl">.</span>

<span dir="rtl">لننظر في **عملية ماركوف التقريرية**</span> **(MRP)
<span dir="rtl">ذات الثلاث حالات المبيّنة على اليمين</span>**.
<span dir="rtl">تبدأ الحلقات في الحالة</span> A <span dir="rtl">ثم
"تنقسم" بشكل عشوائي، نصف الوقت تنتقل إلى</span> B <span dir="rtl">(ثم
تستمر دائمًا حتى تنتهي بمكافأة 1) ونصف الوقت تنتقل إلى الحالة</span> C
<span dir="rtl">(ثم تنتهي دائمًا بمكافأة صفر). تكون المكافأة للانتقال
الأول من</span> A <span dir="rtl">دائمًا صفرًا بغض النظر عن الطريق الذي
تسلكه الحلقة. بما أن هذه مشكلة حلقية، يمكننا اعتبار</span> γ
<span dir="rtl">تساوي 1. نفترض أيضًا التدريب **داخل السياسة**
</span>**(On-policy Training)**<span dir="rtl">، لذا فإن</span> ρt
<span dir="rtl"></span>​ <span dir="rtl">دائمًا تساوي 1، ونفترض تقريب دالة
الجدول</span> (Tabular Function Approximation)<span dir="rtl">، بحيث
تكون الخوارزميات التعليمية حرة في إعطاء قيم عشوائية ومستقلة لكل من
الحالات الثلاث. لذلك، يجب أن تكون هذه مشكلة سهلة</span>.

<span dir="rtl">ما القيم التي يجب أن تكون؟ من</span> A<span dir="rtl">،
نصف الوقت تكون العائدات 1، ونصف الوقت تكون العائدات 0؛ لذا يجب أن تكون
قيمة</span> A <span dir="rtl">هي</span> $`\frac{1}{2}`$
<span dir="rtl"></span>​<span dir="rtl">.</span>
<span dir="rtl">من</span> B <span dir="rtl">العائد دائمًا 1، لذلك يجب أن
تكون قيمتها 1، وبالمثل من</span> C <span dir="rtl">العائد دائمًا 0، لذلك
يجب أن تكون قيمتها 0. هذه هي القيم الحقيقية، وبما أن هذه مشكلة جدولة،
فإن جميع الطرق المقدمة سابقًا تتقارب إليها بدقة</span>.

<span dir="rtl">ومع ذلك، تجد **الخوارزمية البدائية للتدرج المتبقي**
</span>**(Naive Residual-Gradient Algorithm)** <span dir="rtl">قيمًا
مختلفة لـ</span> B <span dir="rtl">و</span> C<span dir="rtl">.</span>
<span dir="rtl">تتقارب الخوارزمية بحيث تكون قيمة</span> B
<span dir="rtl">هي</span> $`\frac{3}{4}`$ <span dir="rtl">وقيمة</span> C
<span dir="rtl">هي</span> $`\frac{1}{4}`$​ <span dir="rtl">(</span>A
<span dir="rtl">تتقارب بشكل صحيح إلى</span>
$`\frac{1}{2}`$​<span dir="rtl">).</span> <span dir="rtl">هذه هي في
الواقع القيم التي تقلل **خطأ** </span>**TD
(TDE)**<span dir="rtl">.</span>

<span dir="rtl">لنحسب **خطأ**</span> **TD (TDE)** <span dir="rtl">لهذه
القيم. الانتقال الأول لكل حلقة إما يكون من</span> $`\frac{1}{2}`$​
<span dir="rtl">في</span> A <span dir="rtl">إلى</span> $`\frac{3}{4}`$​
<span dir="rtl">في</span> B<span dir="rtl">، وهو تغيير بمقدار</span>
$`\frac{1}{4}`$​<span dir="rtl">، أو من</span> $`\frac{1}{2}`$​
<span dir="rtl">في</span> A <span dir="rtl">إلى</span> $`\frac{1}{4}`$
<span dir="rtl">في</span> C<span dir="rtl">، وهو تغيير بمقدار</span>
−$`\frac{1}{4}`$​<span dir="rtl">.</span> <span dir="rtl">نظرًا لأن
المكافأة على هذه الانتقالات هي صفر، و</span>γ=1<span dir="rtl">، فإن هذه
التغييرات هي **أخطاء** </span>**TD (TD Errors)**<span dir="rtl">،
وبالتالي فإن **خطأ** </span>**TD <span dir="rtl">التربيعي</span>
(Squared TD Error)** <span dir="rtl">هو دائمًا</span> 116
<span dir="rtl"></span>​ <span dir="rtl">في الانتقال الأول</span>.

<span dir="rtl">الانتقال الثاني مشابه؛ إما يكون من</span>
$`\frac{3}{4}`$ <span dir="rtl"></span>​ <span dir="rtl">في</span> B
<span dir="rtl">إلى مكافأة 1 (وحالة نهائية بقيمة 0)، أو من</span>
$`\frac{1}{4}`$ <span dir="rtl"></span>​ <span dir="rtl">في</span> C
<span dir="rtl">إلى مكافأة 0 (مرة أخرى مع حالة نهائية بقيمة 0). لذا فإن
**خطأ**</span> **TD (TD Error)** <span dir="rtl">دائمًا</span>
±$`\frac{1}{4}\ \ `$​<span dir="rtl">، وبالتالي فإن **خطأ**</span> **TD
<span dir="rtl">التربيعي</span> (Squared TD Error)**
<span dir="rtl">هو</span> $`\frac{1}{16}`$ <span dir="rtl"></span>​
<span dir="rtl">في الخطوة الثانية. لذلك، بالنسبة لهذه المجموعة من القيم،
يكون **خطأ**</span> **TD (TDE)** <span dir="rtl">في كلتا الخطوتين
هو</span> $`\frac{1}{16}`$.

<span dir="rtl">الآن دعونا نحسب **خطأ**</span> **TD (TDE)**
<span dir="rtl">للقيم الحقيقية</span> (B <span dir="rtl">عند 1،</span> C
<span dir="rtl">عند 0، و</span>A <span dir="rtl">عند</span>
$`\frac{1}{2}`$ <span dir="rtl">في هذه الحالة، يكون الانتقال الأول إما
من</span> $`\frac{1}{2}`$​ <span dir="rtl">إلى 1، في</span>
B<span dir="rtl">، أو من</span> $`\frac{1}{2}`$​ <span dir="rtl">إلى 0،
في</span> C<span dir="rtl">؛ في كلتا الحالتين يكون **الخطأ
المطلق**</span> **(Absolute Error)** <span dir="rtl">هو</span>
$`\frac{1}{2}`$ <span dir="rtl">والخطأ **التربيعي**</span> **(Squared
Error)** <span dir="rtl">هو</span> ​. $`\frac{1}{4}`$
<span dir="rtl">الانتقال الثاني لا يحتوي على خطأ لأن القيمة الابتدائية،
سواء كانت 1 أو 0 اعتمادًا على ما إذا كان الانتقال من</span> B
<span dir="rtl">أو</span> C<span dir="rtl">، تتطابق دائمًا تمامًا مع
المكافأة المباشرة والعائد. لذا فإن **خطأ**</span> **TD
<span dir="rtl">التربيعي</span> <span dir="rtl">(</span>Squared TD
<span dir="rtl"></span>Error<span dir="rtl">)
</span>**<span dir="rtl">هو</span> $`\frac{1}{4}`$ <span dir="rtl"> في
الانتقال الأول وصفر في الانتقال الثاني، مما يعطي متوسط مكافأة عبر
الانتقالين يبلغ</span> $`\frac{1}{8}`$ <span dir="rtl">.</span>
<span dir="rtl">بما أن</span> $`\frac{1}{8}`$ <span dir="rtl"></span>​
<span dir="rtl">أكبر من</span> $`\frac{1}{16}`$
<span dir="rtl"></span>​<span dir="rtl">، فإن هذا الحل أسوأ وفقًا
لـ</span> **TDE<span dir="rtl">.</span>** <span dir="rtl">في هذه المشكلة
البسيطة، لا تحتوي القيم الحقيقية على أصغر</span>
**TDE<span dir="rtl">.</span>**

<span dir="rtl">يتم استخدام تمثيل جدولي في **مثال الانقسام**
</span>**(A-split Example)**<span dir="rtl">، لذلك يمكن تمثيل القيم
الحقيقية للحالات بدقة، ومع ذلك تجد **الخوارزمية البدائية للتدرج المتبقي
(**</span>**Naive Residual-Gradient
<span dir="rtl"></span>Algorithm<span dir="rtl">)
</span>**<span dir="rtl">قيمًا مختلفة، وهذه القيم لديها</span> **TD
<span dir="rtl">خطأ</span> (TDE)** <span dir="rtl">أقل من القيم
الحقيقية. **تقليل**</span> **TDE** <span dir="rtl">هو نهج بسيط؛ من خلال
معاقبة جميع أخطاء</span> TD<span dir="rtl">، يحقق شيئًا أشبه بالتنعيم
الزمني أكثر من التنبؤ الدقيق</span>.

<span dir="rtl">يبدو أن فكرة أفضل ستكون **تقليل خطأ بيلمان**
</span>**(Minimizing the Bellman Error)**<span dir="rtl">.</span>
<span dir="rtl">إذا تم تعلم القيم الدقيقة، فإن **خطأ بيلمان**</span>
**(Bellman Error)** <span dir="rtl">سيكون صفرًا في كل مكان. لذلك، يجب ألا
تواجه خوارزمية **تقليل خطأ بيلمان**</span> **(Bellman-error-minimizing
algorithm)** <span dir="rtl">أي مشكلة في **مثال الانقسام**
</span>**(A-split Example)**<span dir="rtl">.</span> <span dir="rtl">لا
يمكننا أن نتوقع تحقيق **خطأ بيلمان صفري**</span>
**<span dir="rtl">(</span>Zero Bellman
<span dir="rtl"></span>Error<span dir="rtl">)
</span>**<span dir="rtl">بشكل عام، حيث يتطلب ذلك إيجاد **دالة القيمة
الحقيقية** </span>**(True Value Function)**<span dir="rtl">، التي نفترض
أنها خارج فضاء دوال القيمة القابلة للتمثيل. لكن الاقتراب من هذا المثالي
هو هدف يبدو طبيعيًا. كما رأينا، فإن **خطأ بيلمان**</span> **(Bellman
Error)** <span dir="rtl">مرتبط أيضًا ارتباطًا وثيقًا بـ</span> **TD
<span dir="rtl">خطأ</span> (TD Error)**<span dir="rtl">. **خطأ
بيلمان**</span> **(Bellman Error)** <span dir="rtl">لحالة معينة هو
**خطأ**</span> **TD <span dir="rtl">المتوقع</span> (Expected TD Error)**
<span dir="rtl">في تلك الحالة. لذا دعونا نكرر الاشتقاق أعلاه مع
**خطأ**</span> **TD <span dir="rtl">المتوقع</span> (Expected TD Error)**
<span dir="rtl">(جميع التوقعات هنا مشروطة ضمنيًا بـ</span>
St​<span dir="rtl">:</span>

``` math
{w_{t + 1} = w_{t} - \frac{1}{2}\alpha\nabla\left( E_{\pi}\left\lbrack \delta_{t} \right\rbrack^{2} \right)
}
{= w_{t} - \frac{1}{2}\alpha\nabla\left( E_{\beta}\left\lbrack \rho_{t}\delta_{t} \right\rbrack^{2} \right)
}
{= w_{t} - \alpha E_{\beta}\left\lbrack \rho_{t}\delta_{t} \right\rbrack\nabla E_{\beta}\left\lbrack \rho_{t}\delta_{t} \right\rbrack
}
{= w_{t} - \alpha E_{\beta}\left\lbrack \rho_{t}\left( R_{t + 1} + \gamma\widehat{v}\left( S_{t + 1},w \right) - \widehat{v}\left( S_{t},w \right) \right) \right\rbrack E_{\beta}\left\lbrack \rho_{t}\nabla_{t} \right\rbrack
}
{= w_{t} + \alpha\left\lbrack E_{\beta}\left\lbrack \rho_{t}\left( R_{t + 1} + \gamma\widehat{v}\left( S_{t + 1},w \right) \right) \right\rbrack - \widehat{v}\left( S_{t},w \right) \right\rbrack\left\lbrack \nabla\widehat{v}\left( S_{t},w \right) - \gamma E_{\beta}\left\lbrack \rho_{t}\nabla\widehat{v}\left( S_{t + 1},w \right) \right\rbrack \right\rbrack.}
```

<span dir="rtl">يُشار إلى هذا التحديث وطرق أخذ العينات المختلفة له باسم
**خوارزمية التدرج المتبقي**</span>
**<span dir="rtl">(</span>Residual-Gradient
Algorithm<span dir="rtl">)</span>**. <span dir="rtl">إذا استخدمت القيم
العينية في جميع التوقعات، فإن المعادلة أعلاه ستتقلص تقريبًا إلى</span>
(11.23)<span dir="rtl">، **الخوارزمية البدائية للتدرج المتبقي
(**</span>**Naive Residual-Gradient
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**.
<span dir="rtl">ولكن هذا نهج بسيط</span> (Naive)<span dir="rtl">، لأن
المعادلة أعلاه تتضمن الحالة التالية</span> $`S_{t + 1}`$
<span dir="rtl">التي تظهر في توقعين يتم ضربهما معًا. للحصول على عينة غير
منحازة من ناتج الضرب، يلزم الحصول على عينتين مستقلتين من الحالة التالية،
ولكن خلال التفاعل الطبيعي مع البيئة الخارجية يتم الحصول على واحدة فقط.
يمكن أخذ عينة لأحد التوقعين، ولكن ليس لكليهما</span>.

<span dir="rtl">هناك طريقتان لجعل **خوارزمية التدرج المتبقي**</span>
**(Residual-Gradient Algorithm)** <span dir="rtl">تعمل. الأولى هي في
حالة **البيئات الحتمية** </span>**(Deterministic
Environments)**<span dir="rtl">.</span> <span dir="rtl">إذا كان الانتقال
إلى الحالة التالية حتميًا، فإن العينتين ستكونان بالضرورة متطابقتين،
وستكون الخوارزمية البدائية صالحة. الطريقة الأخرى هي الحصول على عينتين
مستقلتين من الحالة التالية</span> $`S_{t + 1}`$ <span dir="rtl"></span>​
<span dir="rtl">من</span> St <span dir="rtl"></span>​<span dir="rtl">،
واحدة للتوقع الأول وأخرى للتوقع الثاني. في التفاعل الفعلي مع البيئة، قد
لا يكون ذلك ممكنًا، ولكن عند التفاعل مع **بيئة محاكاة**
</span>**(Simulated Environment)**<span dir="rtl">، يكون ذلك ممكنًا. يمكن
ببساطة العودة إلى الحالة السابقة والحصول على حالة تالية بديلة قبل
المتابعة من الحالة التالية الأولى. في أي من هاتين الحالتين، فإن
**خوارزمية التدرج المتبقي**</span> **(Residual-Gradient Algorithm)**
<span dir="rtl">مضمونة للتقارب إلى الحد الأدنى من **خطأ بيلمان**</span>
**(BE)** <span dir="rtl">تحت الظروف المعتادة على **معامل حجم
الخطوة**</span> **<span dir="rtl">(</span>Step-size
<span dir="rtl"></span>Parameter<span dir="rtl">)</span>**.
<span dir="rtl">باعتبارها طريقة</span> **SGD** <span dir="rtl">حقيقية،
فإن هذا التقارب يكون قويًا، ويشمل تقريب الدوال الخطية وغير الخطية على حد
سواء. في الحالة الخطية، يكون التقارب دائمًا إلى</span> w
<span dir="rtl">الفريد الذي يقلل من **خطأ بيلمان**
</span>**(BE)**<span dir="rtl">.</span>

<span dir="rtl">ومع ذلك، لا تزال هناك ثلاث طرق على الأقل تجعل تقارب
**خوارزمية التدرج المتبقي**</span>
**<span dir="rtl">(</span>Residual-Gradient Algorithm<span dir="rtl">)
</span>**<span dir="rtl">غير مرضٍ. الأولى هي أن هذه الطريقة بطيئة من
الناحية التجريبية، أبطأ بكثير من **الطرق شبه التدرجية**
</span>**(Semi-gradient Methods)<span dir="rtl">.</span>**
<span dir="rtl">في الواقع، اقترح مؤيدو هذه الطريقة زيادة سرعتها من خلال
دمجها مع **الطرق شبه التدرجية الأسرع**</span>
**<span dir="rtl">(</span>Faster Semi-gradient Methods<span dir="rtl">)
</span>**<span dir="rtl">في البداية، ثم التحول تدريجيًا إلى **التدرج
المتبقي** </span>**(Residual Gradient)** <span dir="rtl">لضمان
التقارب</span> (Baird and Moore, 1999)<span dir="rtl">.</span>
<span dir="rtl">الطريقة الثانية التي تجعل **خوارزمية التدرج المتبقي**
</span>**(Residual-Gradient Algorithm)** <span dir="rtl">غير مرضية هي
أنها لا تزال تتقارب إلى القيم غير الصحيحة. تحصل على القيم الصحيحة في
جميع الحالات الجدولية، مثل **مثال الانقسام**</span>
**<span dir="rtl">(</span>A-split
<span dir="rtl"></span>Example<span dir="rtl">)</span>**<span dir="rtl">،
حيث يمكن إيجاد حل دقيق لـ **معادلة بيلمان**</span> ... **(Bellman
Equation)**

**<span dir="rtl">المثال 11.3: مثال ما قبل الانقسام</span> (A-presplit
Example)**<span dir="rtl">، **مثال مضاد**</span> **(Counterexample)**
<span dir="rtl">لـ **خطأ بيلمان**
</span>**(BE)**<span dir="rtl">.</span>

<img src="./media/image132.png"
style="width:1.77986in;height:1.53889in" /><span dir="rtl">لننظر في
**عملية ماركوف التقريرية**</span> **(MRP) <span dir="rtl">ذات الثلاث
حالات الحلقية</span>** <span dir="rtl">الموضحة على اليمين: تبدأ الحلقات
إما في</span> A1 <span dir="rtl">أو</span> A2<span dir="rtl">، باحتمال
متساوٍ. تبدو هاتان الحالتان متطابقتين تمامًا بالنسبة لمقرب الدالة، مثل
حالة واحدة</span> A <span dir="rtl">تمثلها ميزة تختلف تمامًا ولا ترتبط
بتمثيل ميزات الحالتين الأخريين،</span> B
<span dir="rtl">و</span>C<span dir="rtl">، اللتين تختلفان أيضًا عن بعضهما
البعض. بشكل محدد، يحتوي بارامتر مقرب الدالة على ثلاث مكونات، أحدها يعطي
قيمة الحالة</span> B<span dir="rtl">، وآخر يعطي قيمة الحالة</span>
C<span dir="rtl">، والثالث يعطي قيمة كل من الحالتين</span> A1
<span dir="rtl">و</span>A2<span dir="rtl">.</span> <span dir="rtl">بصرف
النظر عن اختيار الحالة الأولية، فإن النظام حتمي. إذا بدأ في</span>
A1<span dir="rtl">، فإنه ينتقل إلى</span> B <span dir="rtl">بمكافأة 0 ثم
ينتهي بمكافأة 1. إذا بدأ في</span> A2<span dir="rtl">، فإنه ينتقل
إلى</span> C<span dir="rtl">، ثم ينتهي، مع مكافأتين تساويان
الصفر</span>.

<span dir="rtl">بالنسبة لخوارزمية التعليم، عند رؤية الميزات فقط، يبدو
النظام مطابقًا لمثال الانقسام</span> <span dir="rtl">(</span>A-split
<span dir="rtl"></span>Example<span dir="rtl">)</span>.
<span dir="rtl">يبدو أن النظام يبدأ دائمًا في</span> A<span dir="rtl">،
يتبعه إما</span> B <span dir="rtl">أو</span> C <span dir="rtl">باحتمال
متساوٍ، ثم ينتهي بقيمة 1 أو 0 حسب الحالة السابقة. كما في مثال
الانقسام</span> (A-split Example)<span dir="rtl">، القيم الحقيقية
لـ</span> B <span dir="rtl">و</span>C <span dir="rtl">هي 1 و0، وأفضل
قيمة مشتركة لـ</span> A1 <span dir="rtl">و</span>A2
<span dir="rtl">هي</span> $`\frac{1}{2}`$​<span dir="rtl">،
بالتناظر</span>.

<span dir="rtl">لأن هذه المشكلة تبدو من الخارج متطابقة مع مثال
الانقسام</span> (A-split Example)<span dir="rtl">، فإننا نعلم بالفعل ما
هي القيم التي ستصل إليها الخوارزميات</span>.
**<span dir="rtl">الخوارزمية</span> TD <span dir="rtl">شبه
التدرجية</span> (Semi-gradient TD)** <span dir="rtl">تتقارب إلى القيم
المثالية المذكورة، بينما **الخوارزمية البدائية للتدرج المتبقي**</span>
**<span dir="rtl">(</span>Naive Residual-Gradient
Algorithm<span dir="rtl">)</span>** <span dir="rtl">تتقارب إلى
القيم</span> $`\frac{3}{4}`$ <span dir="rtl">و</span> $`\frac{1}{4}`$
<span dir="rtl">للحالتين</span> B <span dir="rtl">و</span>C
<span dir="rtl">على التوالي. جميع الانتقالات بين الحالات حتمية، لذا
ستتقارب **خوارزمية التدرج المتبقي غير البدائية (**</span>**Non-naive
Residual-Gradient Algorithm<span dir="rtl">)
</span>**<span dir="rtl">أيضًا إلى هذه القيم (لأنها نفس الخوارزمية في هذه
الحالة). يتبع ذلك أن هذا الحل "البدائي" يجب أن يكون أيضًا الحل الذي يقلل
**خطأ بيلمان** </span>**(BE)**<span dir="rtl">، وهو كذلك. في مشكلة
حتمية، تكون أخطاء بيلمان وأخطاء</span> TD <span dir="rtl">كلها متساوية،
لذلك يكون **خطأ بيلمان**</span> **(BE)** <span dir="rtl">دائمًا هو نفسه
**خطأ** </span>**TD (TDE)**<span dir="rtl">.</span>
**<span dir="rtl">تحسين خطأ بيلمان</span> (BE)** <span dir="rtl">في هذا
المثال يؤدي إلى نفس نمط الفشل الذي يحدث مع **الخوارزمية البدائية للتدرج
المتبقي**</span> **(Naive Residual-Gradient Algorithm)**
<span dir="rtl">في مثال الانقسام</span> (A-split
Example)<span dir="rtl">.</span>

<span dir="rtl">من الممكن الوصول إلى معادلة دقيقة في الحالات الجدولية.
ولكن إذا فحصنا أمثلة تتضمن **تقريب دالة حقيقي** </span>**(Genuine
Function Approximation)**<span dir="rtl">، فإن **خوارزمية التدرج
المتبقي**</span> **<span dir="rtl">(</span>Residual-Gradient
Algorithm<span dir="rtl">)</span>**<span dir="rtl">، وفعليًا هدف **خطأ
بيلمان** </span>**(BE Objective)**<span dir="rtl">، يبدو أنهما يجدان
**دوال قيمة**</span> **(Value Functions)** <span dir="rtl">غير صحيحة.
أحد أكثر الأمثلة دلالة على ذلك هو التعديل على **مثال الانقسام**</span>
**(A-split Example)** <span dir="rtl">والمعروف باسم **مثال ما قبل
الانقسام** </span>**(A-presplit Example)**<span dir="rtl">، والذي ظهر في
الصفحة السابقة، حيث تجد **خوارزمية التدرج المتبقي
(**</span>**Residual-Gradient
<span dir="rtl"></span>Algorithm<span dir="rtl">)
</span>**<span dir="rtl">نفس الحل السيئ مثل نسختها البدائية. هذا المثال
يظهر بشكل بديهي أن **تقليل خطأ بيلمان**</span> **(Minimizing the BE)**
(<span dir="rtl">وهو ما تقوم به **خوارزمية التدرج المتبقي**</span>
**<span dir="rtl">(</span>Residual-Gradient Algorithm<span dir="rtl">)
</span>**<span dir="rtl">بشكل مؤكد قد لا يكون هدفًا مرغوبًا</span>.

<span dir="rtl">الطريقة الثالثة التي تجعل تقارب **خوارزمية التدرج
المتبقي** </span>**(Residual-Gradient Algorithm)** <span dir="rtl">غير
مرضٍ يتم شرحها في القسم التالي. مثل الطريقة الثانية، الطريقة الثالثة هي
أيضًا مشكلة في **هدف خطأ بيلمان**</span> **(BE Objective)**
<span dir="rtl">نفسه وليس مع أي خوارزمية معينة لتحقيقه</span>.

**<u>11.6 <span dir="rtl">خطأ بيلمان</span> (The Bellman Error)
<span dir="rtl">ليس قابلاً للتعليم</span> (Not Learnable)</u>**

<span dir="rtl">مفهوم **القابلية للتعليم**</span> **(Learnability)**
<span dir="rtl">الذي نقدمه في هذا القسم يختلف عن ذلك المستخدم عادةً في
**تعلم الآلة** </span>**(Machine Learning)**<span dir="rtl">.</span>
<span dir="rtl">هناك، يقال إن فرضية ما "قابلة للتعليم" إذا كانت قابلة
للتعليم بكفاءة، مما يعني أنه يمكن تعلمها ضمن عدد من الأمثلة ذي **تعقيد
حدودي**</span> **(Polynomial)** <span dir="rtl">بدلاً من أن يكون **أسيًا**
</span>**(Exponential)**<span dir="rtl">.</span> <span dir="rtl">هنا
نستخدم المصطلح بطريقة أكثر أساسية، بمعنى أنها قابلة للتعليم على الإطلاق،
مع أي مقدار من الخبرة. يتضح أن العديد من الكميات التي تبدو ذات أهمية في
**التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">لا يمكن تعلمها حتى من كمية لا نهائية من البيانات
التجريبية. هذه الكميات محددة بشكل جيد ويمكن حسابها إذا كان هناك معرفة
بالبنية الداخلية للبيئة، ولكن لا يمكن حسابها أو تقديرها من التسلسل
الملاحظ **لمتجهات الميزات** </span>**(Feature
Vectors)**<span dir="rtl">، **الإجراءات**
</span>**(Actions)**<span dir="rtl">، و**المكافآت**
</span>**(Rewards)**<span dir="rtl">.</span> <span dir="rtl">نقول إنها
**ليست قابلة للتعليم (**</span>**Not
<span dir="rtl"></span>Learnable<span dir="rtl">)</span>**.
<span dir="rtl">سيتبين أن **هدف خطأ بيلمان**</span> **(Bellman Error
Objective) (BE)** <span dir="rtl">الذي تم تقديمه في القسمين السابقين
**ليس قابلاً للتعليم**</span> **(Not Learnable)** <span dir="rtl">بهذا
المعنى. أن يكون **هدف خطأ بيلمان**</span> **(Bellman Error Objective)**
<span dir="rtl">غير قابل للتعليم من البيانات الملاحظة هو على الأرجح أقوى
سبب لعدم السعي لتحقيقه</span>.

<span dir="rtl">لتوضيح مفهوم **القابلية للتعليم**
</span>**(Learnability)**<span dir="rtl">، دعنا نبدأ ببعض الأمثلة
البسيطة. فكر في عمليتي **ماركوف مكافأة**</span> **(Markov Reward
Processes) (MRPs)** <span dir="rtl">التاليتين المرسومتين أدناه</span>:

<img src="./media/image133.png"
style="width:6.26806in;height:0.82153in" />

<span dir="rtl">حيثما يغادر حافتان **حالة**
</span>**(State)**<span dir="rtl">، يُفترض أن كلا الانتقالين يحدثان
باحتمال متساوٍ، وتشير الأرقام إلى المكافأة المستلمة. تظهر جميع
**الحالات**</span> **(States)** <span dir="rtl">بنفس الشكل؛ كلها تنتج
**متجه ميزة أحادي المكون**</span> **(Single-Component Feature Vector) x
= 1** <span dir="rtl">ولها **قيمة تقريبية** </span>**(Approximated
Value) w**<span dir="rtl">.</span> <span dir="rtl">وبالتالي، فإن الجزء
الوحيد المتغير من مسار البيانات هو تسلسل المكافآت</span>.
**<span dir="rtl">عملية ماركوف مكافأة</span> (MRP)** <span dir="rtl">على
اليسار تبقى في نفس الحالة وتنبعث منها سلسلة لا نهائية من 0 و2 بشكل
عشوائي، كل منهما باحتمال 0.5</span>. **<span dir="rtl">عملية ماركوف
مكافأة</span> (MRP)** <span dir="rtl">على اليمين، في كل خطوة، إما تبقى
في حالتها الحالية أو تنتقل إلى الحالة الأخرى، باحتمال متساوٍ.</span>
**<span dir="rtl">المكافأة</span> (Reward)** <span dir="rtl">في هذه
**عملية ماركوف مكافأة**</span> **(MRP)** <span dir="rtl">حتمية، دائمًا 0
من إحدى الحالات ودائمًا 2 من الأخرى، ولكن نظرًا لأن كل حالة لها نفس
الاحتمال في كل خطوة، فإن البيانات القابلة للملاحظة تكون مرة أخرى سلسلة
لا نهائية من 0 و2 بشكل عشوائي، وهو نفس ما تنتجه **عملية ماركوف مكافأة**
</span>**(MRP)** <span dir="rtl">على اليسار. (يمكننا افتراض أن **عملية
ماركوف مكافأة**</span> **(MRP)** <span dir="rtl">على اليمين تبدأ في إحدى
الحالتين بشكل عشوائي باحتمال متساوٍ.) وبالتالي، حتى لو كان لدينا كمية لا
نهائية من البيانات، فلن يكون من الممكن تحديد أي من هاتين العمليتين كانت
تولد البيانات. على وجه الخصوص، لا يمكننا معرفة ما إذا كانت **عملية
ماركوف مكافأة**</span> **(MRP)** <span dir="rtl">تحتوي على حالة واحدة أو
اثنتين، أو إذا كانت عشوائية أو حتمية. هذه الأشياء **ليست قابلة للتعليم**
</span>**(Not Learnable)**<span dir="rtl">.</span>

<span dir="rtl">هذه الزوج من **عمليات ماركوف مكافأة**</span> **(MRPs)**
<span dir="rtl">يوضح أيضًا أن **هدف خطأ القيمة  
(**</span>**VE <span dir="rtl"></span>Objective<span dir="rtl">)</span>
(9.1)** **<span dir="rtl">ليس قابلاً للتعليم</span> (Not
Learnable)**<span dir="rtl">.</span> <span dir="rtl">إذا كانت</span>
γ=0<span dir="rtl">، فإن القيم الحقيقية للحالات الثلاث (في كلتا
العمليتين)، من اليسار إلى اليمين، هي 1، 0، و2. افترض أن</span>
w=1<span dir="rtl">.</span> <span dir="rtl">عندئذ يكون **خطأ
القيمة**</span> **(VE)** <span dir="rtl">صفرًا في **عملية ماركوف
مكافأة**</span> **(MRP)** <span dir="rtl">على اليسار و1 في **عملية
ماركوف مكافأة**</span> **(MRP)** <span dir="rtl">على اليمين. نظرًا لأن
**خطأ القيمة**</span> **(VE)** <span dir="rtl">يختلف في كلتا المشكلتين،
إلا أن البيانات المولدة لها نفس التوزيع، فإن **خطأ القيمة**</span>
**(VE)** <span dir="rtl">لا يمكن تعلمه</span>. **<span dir="rtl">خطأ
القيمة</span> (VE)** <span dir="rtl">ليس دالة فريدة لتوزيع البيانات.
وإذا لم يكن قابلاً للتعليم، فكيف يمكن لـ **خطأ القيمة**</span> **(VE)**
<span dir="rtl">أن يكون مفيدًا كهدف للتعليم؟</span>

<span dir="rtl">إذا لم يكن من الممكن تعلم **دالة الهدف**
</span>**(Objective)**<span dir="rtl">، فهذا بالفعل يثير التساؤلات حول
فائدتها. في حالة **خطأ القيمة** </span>**(VE)**<span dir="rtl">، هناك
طريقة للتغلب على هذه المشكلة. لاحظ أن الحل نفسه،</span>
w=1<span dir="rtl">، هو الأمثل لكلتا **عمليتي ماركوف المكافأة**</span>
**(MRPs)** <span dir="rtl">أعلاه (بافتراض أن</span> μ <span dir="rtl">هي
نفسها للحالتين اللتين لا يمكن تمييزهما في **عملية ماركوف
المكافأة**</span> **(MRP)** <span dir="rtl">على اليمين). هل هذا مجرد
صدفة، أم يمكن أن يكون صحيحًا عمومًا أن جميع **عمليات ماركوف
القرار**</span> **(MDPs)** <span dir="rtl">التي لها نفس توزيع البيانات
لها أيضًا نفس متجه البارامتر الأمثل؟ إذا كان هذا صحيحًا—وسنوضح ذلك بعد
قليل—فإن **خطأ القيمة**</span> **(VE)** <span dir="rtl">يظل دالة هدف
قابلة للاستخدام.</span> **<span dir="rtl">خطأ القيمة</span> (VE)**
<span dir="rtl">ليس **قابلاً للتعليم** </span>**(Not
Learnable)**<span dir="rtl">، لكن البارامتر الذي يحسنها هو</span>!

<span dir="rtl">لفهم ذلك، من المفيد إدخال **دالة هدف طبيعية
أخرى**</span> **<span dir="rtl">(</span>Another Natural Objective
<span dir="rtl"></span>Function<span dir="rtl">)</span>**<span dir="rtl">،
وهذه المرة واحدة واضحة القابلية للتعليم. أحد الأخطاء التي تكون دائمًا
قابلة للملاحظة هو الفرق بين تقدير القيمة في كل مرة والعائد من تلك
اللحظة</span>. **<span dir="rtl">خطأ العائد التربيعي المتوسط</span>
<span dir="rtl">(</span>Mean <span dir="rtl"></span>Square Return
Error<span dir="rtl">)</span>**<span dir="rtl">، والذي يرمز له بـ</span>
**RE**<span dir="rtl">، هو **التوقع**</span> **(Expectation)**
<span dir="rtl">تحت</span> μ <span dir="rtl">لمربع هذا الخطأ. في حالة
**داخل السياسة** </span>**(On-policy)**<span dir="rtl">، يمكن كتابة
**خطأ العائد التربيعي المتوسط** </span>**(RE)**
<span dir="rtl">كالتالي</span>:

``` math
\text{RE}(w) = E\left\lbrack \left( \gamma G_{t} - \widehat{v}\left( S_{t},w \right) \right)^{2} \right\rbrack
```

``` math
= \text{VE}(w) + E\left\lbrack \left( \gamma G_{t} - v_{\pi}\left( S_{t} \right) \right)^{2} \right\rbrack.
```

<span dir="rtl">وهكذا، فإن الهدفين متطابقان باستثناء مصطلح التباين الذي
لا يعتمد على **متجه البارامتر** </span>**(Parameter
Vector)**<span dir="rtl">.</span> <span dir="rtl">لذلك يجب أن يكون
للهدفين نفس **قيمة البارامتر الأمثل**
</span>**w∗<span dir="rtl">.</span>** <span dir="rtl">تم تلخيص العلاقات
العامة في الجانب الأيسر من **الشكل 11.4**</span>.

**<span dir="rtl">تمرين 11.4</span>:** <span dir="rtl">أثبت
المعادلة</span> (11.24)<span dir="rtl">.</span>
**<span dir="rtl">تلميح</span>:** <span dir="rtl">اكتب **خطأ العائد
التربيعي**</span> **(RE)** <span dir="rtl">كـ **توقع**
</span>**(Expectation)** <span dir="rtl">على الحالات المحتملة</span> s
<span dir="rtl">للتوقع الخاص بالخطأ التربيعي بشرط أن</span>
St=s<span dir="rtl">.</span> <span dir="rtl">ثم أضف واطرح **القيمة
الحقيقية للحالة** </span>**s <span dir="rtl"></span>**<span dir="rtl">من
الخطأ (قبل التربيع)، مع جمع **القيمة الحقيقية المضافة** مع **القيمة
المقدرة** و**القيمة الحقيقية المخصومة** مع العائد. ثم، إذا قمت بتوسيع
التربيع، سينتهي بك الأمر بأن المصطلح الأكثر تعقيدًا سيكون صفرًا، تاركًا لك
المعادلة</span> (11.24)<span dir="rtl">.</span>

<span dir="rtl">الآن دعنا نعود إلى **خطأ بيلمان**
</span>**(BE)**<span dir="rtl">.</span> **<span dir="rtl">خطأ
بيلمان</span> (BE)** <span dir="rtl">يشبه **خطأ القيمة**</span> **(VE)**
<span dir="rtl">في أنه يمكن حسابه من معرفة **عملية ماركوف
القرار**</span> **(MDP)** <span dir="rtl">ولكنه **ليس قابلاً
للتعليم**</span> **(Not Learnable)** <span dir="rtl">من البيانات. لكنه
لا يشبه **خطأ القيمة**</span> **(VE)** <span dir="rtl">في أن الحل الأدنى
له **ليس قابلاً للتعليم (**</span>**Not
<span dir="rtl"></span>Learnable<span dir="rtl">)</span>**.
<span dir="rtl">يوضح الصندوق في الصفحة التالية **مثالًا مضادًا**</span>
**(Counterexample)**—<span dir="rtl">حيث توجد عمليتان **ماركوف
مكافأة**</span> **(MRPs)** <span dir="rtl">تولدان نفس توزيع البيانات
ولكن **متجه البارامتر الأمثل** </span>**(Optimal Parameter Vector)**
<span dir="rtl">لهما مختلف، مما يثبت أن **متجه البارامتر الأمثل**</span>
**<span dir="rtl">(</span>Optimal <span dir="rtl"></span>Parameter
Vector<span dir="rtl">)</span>** <span dir="rtl">ليس دالة من البيانات
وبالتالي لا يمكن تعلمه منها. الأهداف الأخرى التي نظرنا فيها،</span>
**PBE** **<span dir="rtl">و</span>TDE**<span dir="rtl">، يمكن تحديدها من
البيانات (وهي قابلة للتعليم) وتحديد الحلول المثلى التي تختلف بشكل عام عن
بعضها البعض وعن الحل الأدنى لـ</span> **BE**<span dir="rtl">.</span>
<span dir="rtl">تم تلخيص الحالة العامة في الجانب الأيمن من **الشكل
11.4**</span>.

<span dir="rtl">وبالتالي، فإن **خطأ بيلمان**</span> **(BE)**
**<span dir="rtl">ليس قابلاً للتعليم</span> (Not
Learnable)**<span dir="rtl">؛ لا يمكن تقديره من **متجهات
الميزات**</span> **(Feature Vectors)** <span dir="rtl">والبيانات
الملاحظة الأخرى. هذا يحد من **خطأ بيلمان**</span> **(BE)**
<span dir="rtl">إلى الإعدادات المعتمدة على النماذج. لا يمكن أن توجد
خوارزمية تقلل **خطأ بيلمان**</span> **(BE)** <span dir="rtl">بدون الوصول
إلى **حالات عملية ماركوف القرار الأساسية**</span> **(MDP States)**
<span dir="rtl">إلى جانب **متجهات الميزات**</span>
**<span dir="rtl">(</span>Feature
<span dir="rtl"></span>Vectors<span dir="rtl">)</span>**.
**<span dir="rtl">خوارزمية التدرج المتبقي</span> (Residual-Gradient
Algorithm)** <span dir="rtl">قادرة فقط على تقليل **خطأ بيلمان**</span>
**(BE)** <span dir="rtl">لأنها تُسمح بأخذ عينات مزدوجة من نفس الحالة—وليس
من حالة لها نفس **متجه الميزات** </span>**(Feature
Vector)**<span dir="rtl">، ولكن من حالة مضمونة أن تكون نفس الحالة
الأساسية. يمكننا أن نرى الآن أنه لا يوجد طريقة لتجنب ذلك. يتطلب **تقليل
خطأ بيلمان**</span> **(BE)** <span dir="rtl">بعض الوصول إلى **عملية
ماركوف القرار**</span> **(MDP)** <span dir="rtl">الأساسية. هذه هي إحدى
القيود المهمة لـ **خطأ بيلمان**</span> **(BE)** <span dir="rtl">بخلاف
تلك التي تم تحديدها في **مثال ما قبل الانقسام**</span> **(A-presplit
Example)** <span dir="rtl">على الصفحة 273. كل هذا يوجه المزيد من
الانتباه نحو **خطأ بيلمان المسقط**
</span>**(PBE)**<span dir="rtl">.</span>

<span dir="rtl">المثال 11.4: مثال مضاد لقابلية تعلم خطأ بيلمان</span>
<span dir="rtl">(</span>Counterexample to the Learnability of
<span dir="rtl"></span>the Bellman Error<span dir="rtl">)</span>

<span dir="rtl">لإظهار النطاق الكامل للإمكانيات، نحتاج إلى زوج من
**عمليات ماركوف المكافأة**</span> **<span dir="rtl">(</span>Markov
<span dir="rtl"></span>Reward Processes) (MRPs<span dir="rtl">)</span>**
<span dir="rtl">أكثر تعقيدًا قليلاً مما تمت دراسته سابقًا. لننظر في
**عمليتي ماركوف المكافأة**</span> **(MRPs)**
<span dir="rtl">التاليتين</span>:

<img src="./media/image134.png"
style="width:6.26806in;height:1.04792in" />

<span dir="rtl">حيثما تغادر **حالتان**
</span>**(States)**<span dir="rtl">، يُفترض أن كلا الانتقالين يحدثان
باحتمال متساوٍ، وتشير الأرقام إلى **المكافأة المستلمة** </span>**(Reward
Received)**<span dir="rtl">.</span> <span dir="rtl">يحتوي **عملية ماركوف
المكافأة**</span> **(MRP)** <span dir="rtl">على اليسار على حالتين تتم
تمثيلهما بشكل مختلف.</span> **<span dir="rtl">عملية ماركوف
المكافأة</span> (MRP)** <span dir="rtl">على اليمين تحتوي على ثلاث حالات،
حالتين منهما،</span> B <span dir="rtl">و</span>B'<span dir="rtl">،
تظهران بنفس الشكل ويجب إعطاؤهما نفس القيمة التقريبية. على وجه التحديد،
يحتوي</span> w <span dir="rtl">على مكونين، وتُعطى قيمة الحالة</span> A
<span dir="rtl">بواسطة المكون الأول وتُعطى قيمة</span> B
<span dir="rtl">و</span>B' <span dir="rtl">بواسطة المكون الثاني. تم
تصميم **عملية ماركوف المكافأة**</span> **(MRP)** <span dir="rtl">الثانية
بحيث يتم قضاء وقت متساوٍ في الحالات الثلاث، لذلك يمكننا اعتبار</span>
$`\mu(s) = \frac{1}{3}`$ <span dir="rtl">لجميع الحالات</span>
s<span dir="rtl">.</span>

<span dir="rtl">لاحظ أن توزيع البيانات القابلة للملاحظة متطابق في
**عمليتي ماركوف المكافأة** </span>**(MRPs)**<span dir="rtl">.</span>
<span dir="rtl">في كلتا الحالتين، سيرى **الوكيل**</span> **(Agent)**
<span dir="rtl">تتابعًا واحدًا للحالة</span> A <span dir="rtl">يتبعه 0، ثم
عدد معين من</span> B <span dir="rtl">الواضحة، كل منها يتبعه -1 باستثناء
الأخير الذي يتبعه 1، ثم نبدأ مرة أخرى مع</span> A <span dir="rtl">واحدة
و0، وهكذا. كل التفاصيل الإحصائية متشابهة أيضًا؛ في كلتا **عمليتي ماركوف
المكافأة** </span>**(MRPs)**<span dir="rtl">، يكون احتمال سلسلة
من</span> k <span dir="rtl">من</span> B <span dir="rtl">هو</span>
2−k<span dir="rtl">.</span>

<span dir="rtl">الآن افترض أن</span> w=0<span dir="rtl">.</span>
<span dir="rtl">في **عملية ماركوف المكافأة**</span> **(MRP)**
<span dir="rtl">الأولى، هذا هو الحل الدقيق، و**خطأ بيلمان**</span>
**(BE)** <span dir="rtl">هو صفر. في **عملية ماركوف المكافأة**</span>
**(MRP)** <span dir="rtl">الثانية، هذا الحل ينتج خطأً تربيعيًا في كل
من</span> B <span dir="rtl">و</span>B' <span dir="rtl">بقيمة 1، بحيث
يكون **خطأ
بيلمان**</span>$`\text{BE}\mathbf{=}\mathbf{\mu}\left( \mathbf{B} \right)\mathbf{\cdot}\mathbf{1 +}\mathbf{\mu}\left( \mathbf{B}^{\mathbf{'}} \right)\mathbf{\cdot}\mathbf{1 =}\frac{\mathbf{2}}{\mathbf{3}}`$<span dir="rtl">.
هاتان **عمليتا ماركوف المكافأة** </span>**(MRPs)**<span dir="rtl">،
اللتان تولدان نفس توزيع البيانات، لديهما **خطأ بيلمان** </span>**(BEs)**
<span dir="rtl">مختلف؛ **خطأ بيلمان**</span> **(BE)**
**<span dir="rtl">ليس قابلاً للتعليم</span> (Not
Learnable)<span dir="rtl">.</span>**

<span dir="rtl">علاوة على ذلك (وعلى عكس المثال السابق لـ **خطأ
القيمة**</span> **(VE)**)<span dir="rtl">، فإن القيمة المثلى لـ</span> w
<span dir="rtl">مختلفة بالنسبة للعمليتين. في **عملية ماركوف
المكافأة**</span> **(MRP)** <span dir="rtl">الأولى،</span> w=0
<span dir="rtl">يقلل **خطأ بيلمان**</span> **(BE)** <span dir="rtl">لأي
قيمة لـ</span> γ<span dir="rtl">.</span> <span dir="rtl">في **عملية
ماركوف المكافأة**</span> **(MRP)** <span dir="rtl">الثانية، القيمة
المثلى لـ</span> w <span dir="rtl">هي دالة معقدة لـ</span>
γ<span dir="rtl">، ولكن في الحد، مع اقتراب</span> γ <span dir="rtl">من
1، تصبح القيمة المثلى هي</span> (−1/2,0) ⊤<span dir="rtl">.</span>
<span dir="rtl">لذلك لا يمكن تقدير الحل الذي يقلل **خطأ بيلمان**</span>
**(BE)** <span dir="rtl">من البيانات وحدها؛ المعرفة بـ **عملية ماركوف
المكافأة**</span> **(MRP)** <span dir="rtl">بما يتجاوز ما يتم الكشف عنه
في البيانات مطلوبة. بهذا المعنى، من المستحيل مبدئيًا استخدام **خطأ
بيلمان** </span>**(BE)** <span dir="rtl">كهدف للتعليم</span>.

<span dir="rtl">قد يكون من المفاجئ أن تكون القيمة المثلى لـ</span> A
<span dir="rtl">في **عملية ماركوف المكافأة**</span> **(MRP)**
<span dir="rtl">الثانية، التي تقلل **خطأ بيلمان**
</span>**(BE)**<span dir="rtl">، بعيدة جدًا عن الصفر. تذكر أن</span> A
<span dir="rtl">لديها وزن مخصص، وبالتالي فإن قيمتها غير مقيدة بـ **تقريب
الدالة** </span>**(Function Approximation)**<span dir="rtl">.</span>
<span dir="rtl">يتم تتبع</span> A <span dir="rtl">بمكافأة 0 وانتقال إلى
حالة ذات قيمة تقارب 0، مما يشير إلى أن</span> vw(A) <span dir="rtl">يجب
أن تكون 0؛ فلماذا تكون قيمتها المثلى سلبية بشكل كبير بدلاً من أن تكون 0؟
الجواب هو أن جعل</span> vw(A) <span dir="rtl">سالبًا يقلل الخطأ عند
الوصول إلى</span> A <span dir="rtl">من</span> B<span dir="rtl">.
المكافأة على هذا الانتقال الحتمي هي 1، مما يعني أن</span> B
<span dir="rtl">يجب أن تكون لها قيمة تزيد بمقدار 1 عن</span>
A<span dir="rtl">. لأن قيمة</span> B <span dir="rtl">تقارب الصفر، تُدفع
قيمة</span> A <span dir="rtl">نحو -1. القيمة المثلى لـ</span> vπ≈−1/2v
<span dir="rtl">لـ</span> A <span dir="rtl">هي تسوية بين تقليل الأخطاء
عند مغادرة</span> A <span dir="rtl">والوصول إليها</span>.

<img src="./media/image135.png"
style="width:6.26806in;height:2.65347in" />

**<span dir="rtl">الشكل 11.4: العلاقات السببية بين توزيع البيانات،
عمليات ماركوف القرار</span> (MDPs)<span dir="rtl">، والأهداف
المختلفة</span>.**

**<span dir="rtl">اليسار، أهداف مونت كارلو</span> (Monte Carlo
Objectives)<span dir="rtl">:</span>** <span dir="rtl">يمكن
لعمليتي</span> MDP <span dir="rtl">مختلفتين أن تنتجا نفس توزيع البيانات
ومع ذلك تنتجان **قيم خطأ القيمة**</span> **(VEs)**
<span dir="rtl">مختلفة، مما يثبت أن هدف **خطأ القيمة**</span> **(VE
Objective)** <span dir="rtl">لا يمكن تحديده من البيانات وهو **ليس قابلاً
للتعليم** </span>**(Not Learnable)**<span dir="rtl">. ومع ذلك، يجب أن
يكون لكل هذه **خطأ القيمة**</span> **(VEs)** <span dir="rtl">نفس **متجه
البارامتر الأمثل** </span>**w∗**! <span dir="rtl"></span>
<span dir="rtl">علاوة على ذلك، يمكن تحديد هذا</span> **w∗
<span dir="rtl"></span>**<span dir="rtl">نفسه من خلال هدف آخر، وهو **خطأ
العائد التربيعي** </span>**(RE)**<span dir="rtl">، والذي يتم تحديده بشكل
فريد من توزيع البيانات. لذلك، فإن</span> **w∗
<span dir="rtl"></span>**<span dir="rtl">و</span>RE
<span dir="rtl">قابلة للتعليم حتى وإن كانت **خطأ القيمة**</span>
**(VEs)** <span dir="rtl">غير قابلة للتعليم</span>.

**<span dir="rtl">اليمين، أهداف التمهيد</span> (Bootstrapping
Objectives)<span dir="rtl">:</span>** <span dir="rtl">يمكن
لعمليتي</span> MDP <span dir="rtl">مختلفتين أن تنتجا نفس توزيع البيانات
ومع ذلك تنتجان **قيم خطأ بيلمان**</span> **(BEs)**
<span dir="rtl">مختلفة ولديهما **متجهات بارامتر** </span>**(Minimizing
Parameter Vectors)** <span dir="rtl">مختلفة؛ هذه **غير قابلة
للتعليم**</span> **(Not Learnable)** <span dir="rtl">من توزيع البيانات.
أهداف</span> **PBE <span dir="rtl"></span>**<span dir="rtl">و</span>TDE
<span dir="rtl">والقيم الدنيا الخاصة بها (المختلفة) يمكن تحديدها مباشرة
من البيانات وبالتالي فهي **قابلة للتعليم**
</span>**(Learnable)**<span dir="rtl">.</span>

**<u>11.7 <span dir="rtl">طرق التدرج</span> (Gradient-TD Methods)
TD-</u>**

<span dir="rtl">نحن الآن ندرس طرق **النزول المتدرج العشوائي**
</span>**(Stochastic Gradient Descent - SGD)** <span dir="rtl">لتقليل
**خطأ بيلمان المسقط** </span>**(PBE)**<span dir="rtl">.</span>
<span dir="rtl">كطرق</span> **SGD** <span dir="rtl">حقيقية، تتمتع هذه
**طرق التدرج**</span>**-TD (Gradient-TD Methods)**
<span dir="rtl">بخصائص تقارب قوية حتى تحت التدريب **خارج السياسة  
(**</span>**Off-policy Training<span dir="rtl">)</span>**
<span dir="rtl">وتقريب الدالة غير الخطية. تذكر أنه في الحالة الخطية يوجد
دائمًا حل دقيق، وهو **نقطة التقارب الثابتة لـ**</span> **TD
w​**<span dir="rtl">، حيث يكون **خطأ بيلمان المسقط**</span> **(PBE)**
<span dir="rtl">صفراً. يمكن العثور على هذا الحل باستخدام طرق **المربعات
الصغرى**</span> **(Least-Squares Methods)** <span dir="rtl">(القسم 9.8)،
ولكن فقط باستخدام طرق ذات تعقيد من الدرجة الثانية</span> **O(d^2)
<span dir="rtl"></span>**<span dir="rtl">من حيث عدد البارامترات. نحن
نسعى بدلاً من ذلك إلى طريقة</span> **SGD**<span dir="rtl">، التي ينبغي أن
تكون ذات تعقيد</span> **O(d)** <span dir="rtl">وتتمتع بخصائص تقارب قوية.
تقترب</span> **-TD <span dir="rtl">طرق التدرج</span> (Gradient-TD
Methods)** <span dir="rtl">من تحقيق هذه الأهداف، على حساب مضاعفة تقريبية
لتعقيد الحساب</span>.

<span dir="rtl">لاشتقاق طريقة</span> **SGD** <span dir="rtl">لـ **خطأ
بيلمان المسقط**</span> **(PBE)** <span dir="rtl">(بافتراض **تقريب الدالة
الخطية  
(**</span>**Linear <span dir="rtl"></span>Function
Approximation<span dir="rtl">)</span>**<span dir="rtl">)</span>
<span dir="rtl">نبدأ بتوسيع وإعادة كتابة الهدف (11.22) في **صورة
مصفوفية** </span>**(Matrix Form)**<span dir="rtl">.</span>

``` math
{\text{PBE}(w) = \text{|}\Pi\overline{\delta}(w)\text{|}_{\mu}^{2}
}
{= \left( \Pi\overline{\delta}(w) \right)^{\top}D\Pi\overline{\delta}(w)
}
{= {\overline{\delta}}^{\top}(w)\Pi^{\top}D\Pi\overline{\delta}(w)
}
{= {\overline{\delta}}^{\top}(w)DX\left( X^{\top}DX \right)^{- 1}X^{\top}D\overline{\delta}(w)
}
```
<span dir="rtl">والهوية</span>
<span dir="rtl">(</span>identity<span dir="rtl">)</span>

``` math
{\Pi^{\top}D\Pi = DX\left( X^{\top}DX \right)^{- 1}X^{\top}D
}
{= \left( X^{\top}D\overline{\delta}(w) \right)^{\top}\left( X^{\top}DX \right)^{- 1}\left( X^{\top}D\overline{\delta}(w) \right).}
```

<span dir="rtl">  
**التدرج بالنسبة إلى** </span>**w <span dir="rtl">هو</span>**

``` math
\nabla\text{PBE}(w) = 2\nabla\left\lbrack X^{\top}D\overline{\delta}(w) \right\rbrack^{\top}\left( X^{\top}DX \right)^{- 1}\left( X^{\top}D\overline{\delta}(w) \right).
```

<span dir="rtl">  
لتحويل هذا إلى طريقة **النزول المتدرج العشوائي**
</span>**(SGD)**<span dir="rtl">، علينا أن نأخذ عينة في كل خطوة زمنية
تحتوي على هذه الكمية باعتبارها القيمة المتوقعة. لنفترض أن</span> μ
<span dir="rtl">هو توزيع الحالات التي يتم زيارتها تحت **السياسة
السلوكية** </span>**(Behavior Policy)**<span dir="rtl">.</span>
<span dir="rtl">يمكن عندئذٍ كتابة العوامل الثلاثة المذكورة أعلاه من حيث
التوقعات تحت هذا التوزيع. على سبيل المثال، يمكن كتابة العامل الأخير كما
يلي</span>:<span dir="rtl">  
</span>

``` math
X^{\top}D\overline{\delta}(w) = \sum_{s}^{}{\mu(s)x(s)\overline{\delta}\left( w(s) \right)} = E\left\lbrack \rho_{t}x_{t}\delta_{t} \right\rbrack
```

<span dir="rtl">وهو ببساطة **التوقع**</span> **(Expectation)**
<span dir="rtl">لتحديث **شبه التدرج**</span> **TD(0) (Semi-gradient
TD(0))** <span dir="rtl">(المعادلة 11.2). العامل الأول هو **المصفوفة
المنقولة**</span> **(Transpose)** <span dir="rtl">للتدرج لهذا
التحديث</span>:<span dir="rtl">  
</span>

``` math
{\nabla E\left\lbrack \rho_{t}x_{t}^{\top} \right\rbrack = E\left\lbrack \rho_{t}\nabla\delta_{t}^{\top}x_{t}^{\top} \right\rbrack
}
{= E\left\lbrack \rho_{t}\nabla\left( R_{t + 1} + \gamma w^{\top}x_{t + 1} - w^{\top}x_{t} \right)^{\top}x_{t}^{\top} \right\rbrack
}
{= E\left\lbrack \rho_{t}\left( \gamma x_{t + 1} - x_{t} \right)x_{t}^{\top} \right\rbrack.}
```

<span dir="rtl">وأخيرًا، العامل الأوسط هو معكوس **مصفوفة الجداء الخارجي
المتوقع**</span> **<span dir="rtl">(</span>Expected Outer-Product
Matrix<span dir="rtl">)</span>** **<span dir="rtl">لمتجهات
الميزات</span> (Feature Vectors)**<span dir="rtl">:</span>

``` math
X^{\top}DX = \sum_{s}^{}{\mu(s)x_{s}x_{s}^{\top}} = E\left\lbrack x_{t}x_{t}^{\top} \right\rbrack.
```

<span dir="rtl">بإحلال هذه التوقعات محل العوامل الثلاثة في تعبيرنا عن
**تدرج**</span> **(Gradient)** **<span dir="rtl">خطأ بيلمان
المسقط</span> (PBE)**<span dir="rtl">، نحصل
على</span>:<span dir="rtl">  
</span>

``` math
\nabla\text{PBE}(w) = 2E\left\lbrack \rho_{t}\left( \gamma x_{t + 1} - x_{t} \right)x_{t}^{\top} \right\rbrack E\left\lbrack x_{t}x_{t}^{\top} \right\rbrack^{- 1}E\left\lbrack \rho_{t}x_{t}\delta_{t} \right\rbrack.
```

<span dir="rtl">قد لا يكون من الواضح أننا أحرزنا تقدمًا بكتابة التدرج
بهذه الصيغة. فهو عبارة عن حاصل ضرب ثلاث تعبيرات، والعاملان الأول والأخير
ليسا مستقلين. كلاهما يعتمد على **متجه الميزات التالي** </span>**xt+1
<span dir="rtl"></span>​**<span dir="rtl">؛ لا يمكننا ببساطة أخذ عينات من
كلا التوقعين ثم ضرب العينات. هذا سيعطينا تقديرًا منحازًا للتدرج تمامًا كما
في **الخوارزمية البدائية للتدرج المتبقي** </span>**(Naive
Residual-Gradient Algorithm)**<span dir="rtl">.</span>

<span dir="rtl">فكرة أخرى ستكون تقدير التوقعات الثلاثة بشكل منفصل ثم
دمجها لإنتاج تقدير غير منحاز للتدرج. هذه الفكرة ستكون فعالة، ولكنها
ستتطلب الكثير من الموارد الحسابية، خاصة لتخزين التوقعين الأولين، اللذين
يمثلان **مصفوفات من الدرجة** </span>**d×d<span dir="rtl">،</span>**
<span dir="rtl">ولحساب معكوس الثاني. يمكن تحسين هذه الفكرة. إذا تم تقدير
وتخزين اثنين من التوقعات الثلاثة، فيمكن بعد ذلك أخذ عينة من التوقع
الثالث واستخدامها بالتزامن مع الكميات المخزنة. على سبيل المثال، يمكنك
تخزين تقديرات الكميتين الثانية والثالثة (باستخدام تقنيات تحديث المعكوس
المتزايد في **القسم 9.8**)، ثم أخذ عينة من التعبير الأول. لسوء الحظ،
ستظل الخوارزمية الكلية معقدة من الدرجة الثانية **(من الدرجة**
</span>$`\mathbf{O}\left( \mathbf{d}^{\mathbf{2}} \right)`$**<span dir="rtl">)</span>**

<span dir="rtl">فكرة تخزين بعض التقديرات بشكل منفصل ثم دمجها مع العينات
هي فكرة جيدة وتستخدم أيضًا في</span> **TD <span dir="rtl">طرق
التدرج</span> (Gradient-TD Methods)**<span dir="rtl">.</span>
**<span dir="rtl">طرق التدرج</span> TD
<span dir="rtl"></span>(Gradient-TD Methods)** <span dir="rtl">تقدّر
وتخزن حاصل ضرب العاملين الثاني والثالث في **المعادلة**
</span>**(11.27)**<span dir="rtl">.</span> <span dir="rtl">هذان العاملان
هما **مصفوفة من الدرجة** </span>**d×d
<span dir="rtl"></span>**<span dir="rtl">ومتجه **من الدرجة**
</span>**d**<span dir="rtl">، لذا فإن حاصل ضربهما هو فقط **متجه من
الدرجة** </span>**d**<span dir="rtl">، مثل</span> w
<span dir="rtl">نفسه. نحن نسمي هذا المتجه الثاني المتعلم بـ</span>
v<span dir="rtl">:</span>

``` math
v \approx E\left\lbrack x_{t}x_{t}^{\top} \right\rbrack^{- 1}E\left\lbrack \rho_{t}x_{t}\delta_{t} \right\rbrack.
```

<span dir="rtl">هذه الصيغة مألوفة لطلاب **التعليم الخاضع للإشراف الخطي**
</span>**(Linear Supervised Learning)**<span dir="rtl">. إنها الحل
لمشكلة **المربعات الصغرى الخطية**</span> **(Linear Least-Squares
Problem)** <span dir="rtl">التي تحاول تقريب</span> ρtδt
<span dir="rtl"></span>​ <span dir="rtl">من **الميزات**
</span>**(Features)**<span dir="rtl">.</span> <span dir="rtl">طريقة
**النزول المتدرج العشوائي**</span> **(SGD)** <span dir="rtl">القياسية
لإيجاد **المتجه** </span>**v
<span dir="rtl"></span>**<span dir="rtl">بشكل تدريجي الذي يقلل من
**الخطأ التربيعي المتوقع**</span> (v⊤xt−ρtδt)2 <span dir="rtl">تُعرف باسم
**قاعدة المتوسط التربيعي الأدنى**</span> **(Least Mean Square - LMS)**
(<span dir="rtl">المعززة هنا بنسب **أخذ العينات بالأهمية**
</span>**(Importance Sampling Ratio)**<span dir="rtl">:</span>

``` math
v_{t + 1} \doteq v_{t} + \beta\rho_{t}\left( \delta_{t} - v_{t}^{\top}x_{t} \right)x_{t},
```

<span dir="rtl">  
حيث</span> β\>0 <span dir="rtl">هو **معامل حجم خطوة**</span>
**(Step-size Parameter)** <span dir="rtl">آخر. يمكننا استخدام هذه
الطريقة لتحقيق المعادلة (11.28) بشكل فعال باستخدام **تعقيد**
</span>**O(d) <span dir="rtl"></span>**<span dir="rtl">في التخزين
والحساب لكل خطوة</span>

<span dir="rtl">بالنظر إلى تقدير مخزن</span> vt​ <span dir="rtl">يقارب
المعادلة (11.28)، يمكننا تحديث **متجه البارامتر الرئيسي**
</span>**wt**<span dir="rtl">باستخدام طرق **النزول المتدرج
العشوائي**</span> **(SGD)** <span dir="rtl">استنادًا إلى المعادلة
(11.27). أبسط قاعدة لذلك هي</span>:

``` math
{w_{t + 1} = w_{t} - \frac{1}{2}\alpha\nabla\text{PBE}\left( w_{t} \right)
}{= w_{t} - \frac{1}{2}\alpha 2E\left\lbrack \rho_{t}\left( \gamma x_{t + 1} - x_{t} \right)x_{t}^{\top} \right\rbrack E\left\lbrack x_{t}x_{t}^{\top} \right\rbrack^{- 1}E\left\lbrack \rho_{t}x_{t}\delta_{t} \right\rbrack \approx w_{t} + \alpha E\left\lbrack \rho_{t}\left( x_{t} - \gamma x_{t + 1} \right)x_{t}^{\top} \right\rbrack E\left\lbrack x_{t}x_{t}^{\top} \right\rbrack^{- 1}E\left\lbrack \rho_{t}x_{t}\delta_{t} \right\rbrack \approx w_{t} + \alpha E\left\lbrack \rho_{t}\left( x_{t} - \gamma x_{t + 1} \right)x_{t}^{\top} \right\rbrack v_{t} \approx w_{t} + \alpha\rho_{t}\left( x_{t} - \gamma x_{t + 1} \right)x_{t}^{\top}v_{t}.}
```

<span dir="rtl">تُسمى هذه الخوارزمية</span>
**GTD2**<span dir="rtl">.</span> <span dir="rtl">لاحظ أنه إذا تم حساب
**الجداء الداخلي**</span> **(x_t^)** <span dir="rtl">أولاً، فإن
الخوارزمية بالكامل ستكون ذات تعقيد</span> O(d)<span dir="rtl">.</span>

<span dir="rtl">يمكن اشتقاق خوارزمية أفضل قليلاً عن طريق إجراء بعض
الخطوات التحليلية الإضافية قبل الاستبدال بـ</span> vt​<span dir="rtl">.
بالاستمرار من المعادلة</span> (11.29):

``` math
{w_{t + 1} = w_{t} + \alpha E\left\lbrack \rho_{t}\left( x_{t} - \theta x_{t + 1} \right)x_{t}^{\top} \right\rbrack E\left\lbrack x_{t}x_{t}^{\top} \right\rbrack^{- 1}E\left\lbrack \rho_{t}x_{t} \right\rbrack
}{= w_{t} + \alpha\left( E\left\lbrack \rho_{t}x_{t}x_{t}^{\top} \right\rbrack - \theta E\left\lbrack \rho_{t}x_{t}x_{t + 1}^{\top} \right\rbrack \right)E\left\lbrack x_{t}x_{t}^{\top} \right\rbrack^{- 1}E\left\lbrack \rho_{t}x_{t} \right\rbrack
}{= w_{t} + \alpha\left( E\left\lbrack x_{t}x_{t}^{\top} \right\rbrack - \theta E\left\lbrack \rho_{t}x_{t}x_{t + 1}^{\top} \right\rbrack \right)E\left\lbrack x_{t}x_{t}^{\top} \right\rbrack^{- 1}E\left\lbrack \rho_{t}x_{t} \right\rbrack
}{= \ w\_ t\  + \ \alpha ├(\ E\lbrack x\_ t\ \rho\_ t(t\rbrack\  - \ \theta E\ ├\lbrack\ \rho\_ t\ x\_ t\ x\_\{ t + 1\}\hat{}\top ┤\rbrack\ ┤)\ E\ ├\lbrack\ x\_ t\ x\_ t\hat{}\top ┤\rbrack\hat{}\{ - 1\}\ E\lbrack\rho\_ t\ x\_ t\rbrack\ 
}{\approx w\_ t\  + \ \alpha ├(\ E\lbrack x\_ t\ \rho\_ t(t\rbrack\  - \ \theta E\ ├\lbrack\ \rho\_ t\ x\_ t\ x\_\{ t + 1\}\hat{}\top ┤\rbrack\ v\_ t\ ┤)\ 
}{\approx w_{t} + \alpha\rho_{t}\left( x_{t} - \theta x_{t + 1}x_{t}^{\top}v_{t} \right)}
```

<span dir="rtl">والتي هي مرة أخرى من رتبة</span> O(d)
<span dir="rtl">إذا تم حساب الناتج النهائي</span> (xt⊤vt)
<span dir="rtl">أولاً. تُعرف **الخوارزمية** </span>**(Algorithm)**
<span dir="rtl">إما باسم</span> TD(0) <span dir="rtl">مع التصحيح
التدرجي</span> (TDC) <span dir="rtl">أو، بشكل آخر، باسم</span>
GTD(0)<span dir="rtl">. يوضح الشكل 11.5 عينة والسلوك المتوقع
لخوارزمية</span> TDC <span dir="rtl">على **المثال المضاد لبيرد**</span>
**<span dir="rtl">(</span>Byrd
<span dir="rtl"></span>Counterexample<span dir="rtl">)</span>**.
<span dir="rtl">كما هو مُخطط، ينخفض **الخطأ في تقدير السياسة**</span>
**(PBE)** <span dir="rtl">إلى الصفر، ولكن لاحظ أن المكونات الفردية لمتجه
**البارامتر**</span> **(Parameter Vector)** <span dir="rtl">لا تقترب من
الصفر</span>.<span dir="rtl">  
</span>

<img src="./media/image136.png"
style="width:6.26806in;height:3.93819in" />

<span dir="rtl">الشكل 11.5: سلوك **الخوارزمية**</span> **(Algorithm)**
TDC <span dir="rtl">على **المثال المضاد لبيرد  **
</span> **<span dir="rtl">(</span>Byrd
<span dir="rtl"></span>Counterexample<span dir="rtl">)</span>**.
<span dir="rtl">على اليسار يظهر تشغيل واحد نموذجي، وعلى اليمين يظهر
السلوك المتوقع لهذه **الخوارزمية**</span> **(Algorithm)**
<span dir="rtl">إذا تم إجراء التحديثات بشكل متزامن</span>
<span dir="rtl">(مشابه للمعادلة (11.9)، باستثناء متجهي البارامتر
في</span> TDC<span dir="rtl">)</span>. <span dir="rtl">كانت أحجام
الخطوات هي</span> α=0.005
<span dir="rtl">و</span>θ=0.05<span dir="rtl">.</span>

<span dir="rtl">في الواقع، هذه القيم لا تزال بعيدة عن الحل
الأمثل،</span> v^(s)=0 <span dir="rtl">لكل</span> s<span dir="rtl">، حيث
يجب أن يكون</span> w <span dir="rtl">متناسبًا مع</span>
(1,1,1,1,1,1,4,−2)⊤ <span dir="rtl"></span> <span dir="rtl">بعد 1000
تكرار، لا نزال بعيدين عن الحل الأمثل، كما يتضح من **خطأ القيمة**</span>
**(VE)** <span dir="rtl">الذي يبقى عند حوالي 2. النظام في الواقع يقترب
من الحل الأمثل، لكن التقدم بطيء للغاية لأن **الخطأ في تقدير
السياسة**</span> **(PBE)** <span dir="rtl">بالفعل قريب جدًا من
الصفر</span>.

<span dir="rtl">كل من</span> GTD2 <span dir="rtl">و</span>TDC
<span dir="rtl">يتضمنان عمليتي تعلم، واحدة رئيسية لـ</span> w
<span dir="rtl">وأخرى ثانوية لـ</span> v<span dir="rtl">.</span>
<span dir="rtl">تعتمد منطقية عملية التعليم الرئيسية على انتهاء عملية
التعليم الثانوية، على الأقل بشكل تقريبي، بينما تتقدم عملية التعليم
الثانوية دون أن تتأثر بالأولى. نطلق على هذا النوع من الاعتماد غير
المتماثل "الكَسْكَد</span>"
<span dir="rtl"></span>(**Cascade**)<span dir="rtl">.</span>
<span dir="rtl">في عمليات الكَسْكَد، نفترض غالبًا أن عملية التعليم الثانوية
تتم بسرعة أكبر وبالتالي تكون دائمًا عند قيمتها النهائية المتقاربة، جاهزة
ودقيقة لمساعدة عملية التعليم الرئيسية. غالبًا ما تعتمد إثباتات التقارب
لهذه الأساليب على هذا الافتراض بشكل صريح. تُسمى هذه الإثباتات إثباتات
الزمن ذي المسارين</span> (Two-Time-Scale Proofs)<span dir="rtl">.</span>
<span dir="rtl">الزمن السريع هو زمن عملية التعليم الثانوية، والزمن
البطيء هو زمن عملية التعليم الرئيسية. إذا كان</span> α
<span dir="rtl">هو حجم الخطوة لعملية التعليم الرئيسية، و</span>λ
<span dir="rtl">هو حجم الخطوة لعملية التعليم الثانوية، فإن هذه الإثباتات
عادةً ما تتطلب أن تكون في النهاية</span> λ→0
<span dir="rtl">و</span>α→0<span dir="rtl">.</span>

<span dir="rtl">تعتبر أساليب</span> Gradient-TD <span dir="rtl">حاليًا
أكثر الأساليب المستقرة المفهومة والمستخدمة على نطاق واسع **خارج
السياسة** </span>**(Off-Policy)**<span dir="rtl">.</span>
<span dir="rtl">توجد امتدادات لقيم الأفعال والتحكم</span>
<span dir="rtl">(مثل</span> GQ<span dir="rtl">،</span> Maei
<span dir="rtl">وآخرون، 2010)، وتتبع الأهلية</span> (GTD(λ)
<span dir="rtl">و</span>GQ(λ)<span dir="rtl">،
(</span>Maei<span dir="rtl">، 2011؛</span> Maei
<span dir="rtl">و</span>Sutton<span dir="rtl">، 2010)، وتقريب الدوال غير
الخطية</span> <span dir="rtl">(</span>Maei <span dir="rtl">وآخرون،
2009)</span>. <span dir="rtl">تم اقتراح خوارزميات هجينة تتوسط بين</span>
TD <span dir="rtl">شبه التدرجي و</span>Gradient-TD
<span dir="rtl">(</span>Hackman<span dir="rtl">، 2012؛</span> White
<span dir="rtl">و</span>White<span dir="rtl">، 2016)</span>.
<span dir="rtl">تتصرف **الخوارزميات الهجينة**</span> **(Hybrid-TD
Algorithms)** <span dir="rtl">مثل</span> Gradient-TD <span dir="rtl">في
الحالات التي تكون فيها **السياسات المستهدفة**</span> **(Target
Policies)** <span dir="rtl">وسياسات السلوك مختلفة بشكل كبير، وتتصرّف مثل
**الخوارزميات شبه التدرجية**</span> **(Semi-Gradient Algorithms)**
<span dir="rtl">في الحالات التي تكون فيها **السياسات المستهدفة**</span>
**(Target Policies)** <span dir="rtl">وسياسات السلوك متطابقة. وأخيرًا، تم
دمج فكرة</span> Gradient-TD <span dir="rtl">مع أفكار الأساليب القريبة
والتحكم المتغيرات لإنتاج أساليب أكثر كفاءة</span>
<span dir="rtl">(</span>Mahadevan <span dir="rtl">وآخرون، 2014؛</span>
Du <span dir="rtl">وآخرون، 2017)</span>.

**<u>11.8 <span dir="rtl">طرق</span> TD <span dir="rtl">التأكيديّة</span>
(Emphatic-TD Methods)</u>**

<span dir="rtl">ننتقل الآن إلى الاستراتيجية الرئيسية الثانية التي تم
استكشافها بشكل واسع للحصول على طريقة تعليم **خارج السياسة**</span>
**(Off-Policy)** <span dir="rtl">تكون رخيصة وفعالة مع تقريب الدوال. تذكر
أن أساليب</span> TD <span dir="rtl">شبه التدرجية الخطية تكون فعالة
ومستقرة عندما يتم تدريبها تحت التوزيع الخاص **داخل السياسة  
(**</span>**On-Policy<span dir="rtl">)</span>**<span dir="rtl">، وكما
أوضحنا في القسم 9.4، فإن هذا يرتبط بوجود مصفوفة</span> A
<span dir="rtl">ذات القيم الموجبة المحددة (المعادلة 9.11) وبالتوافق بين
توزيع الحالات **داخل السياسة**</span>
**<span dir="rtl">(</span>On-Policy State
<span dir="rtl"></span>Distribution<span dir="rtl">)</span>**
<span dir="rtl"></span>μπ <span dir="rtl">واحتمالات الانتقال بين
الحالات</span> p(s′∣s,a) <span dir="rtl">تحت **السياسة المستهدفة**
</span>**(Target Policy)**<span dir="rtl">.</span> <span dir="rtl">في
التعليم **خارج السياسة** </span>**(Off-Policy
Learning)**<span dir="rtl">، نقوم بإعادة وزن انتقالات الحالات باستخدام
**أخذ العينات بالأهمية**</span> **(Importance Sampling)**
<span dir="rtl">بحيث تصبح مناسبة للتعليم عن **السياسة المستهدفة**
</span>**(Target Policy)**<span dir="rtl">، لكن توزيع الحالات لا يزال
يتبع **سياسة السلوك** </span>**(Behavior Policy)**<span dir="rtl">. هناك
عدم تطابق. فكرة طبيعية هي أن نقوم بإعادة وزن الحالات بطريقة ما، مع
التركيز على بعضها وتقليل التركيز على الأخرى، بحيث نعيد توزيع التحديثات
إلى التوزيع **داخل السياسة** </span>**(On-Policy
Distribution)**<span dir="rtl">.</span> <span dir="rtl">عندئذٍ سيكون هناك
تطابق، وستتبع الاستقرار والتقارب من النتائج الموجودة بالفعل. هذه هي فكرة
طرق</span> **TD <span dir="rtl">التأكيديّة</span> (Emphatic-TD
Methods)**<span dir="rtl">، التي تم تقديمها لأول مرة لتدريب **داخل
السياسة**</span> **(On-Policy Training)** <span dir="rtl">في القسم
9.11</span>.

<span dir="rtl">في الواقع، فكرة "توزيع داخل السياسة</span> (On-Policy
Distribution)" <span dir="rtl">ليست دقيقة تمامًا، حيث يوجد العديد من
توزيعات **داخل السياسة** </span>**(On-Policy
Distributions)**<span dir="rtl">، وأي واحد منها يكفي لضمان الاستقرار.
فكر في مشكلة حلقية غير مخصومة. الطريقة التي تنتهي بها الحلقات تحدد
بالكامل باحتمالات الانتقال، لكن قد تكون هناك عدة طرق مختلفة لبدء
الحلقات. أياً كانت الطريقة التي تبدأ بها الحلقات، إذا كانت جميع انتقالات
الحالات ناتجة عن **السياسة المستهدفة** </span>**(Target
Policy)**<span dir="rtl">، فإن التوزيع الناتج للحالات يكون توزيع **داخل
السياسة** </span>**(On-Policy Distribution)**<span dir="rtl">.</span>
<span dir="rtl">قد تبدأ قريبًا من الحالة النهائية وتزور فقط عددًا قليلاً من
الحالات باحتمال كبير قبل إنهاء الحلقة. أو قد تبدأ بعيدًا وتمر بالعديد من
الحالات قبل إنهاء الحلقة. كلاهما توزيعات **داخل السياسة**</span>
**<span dir="rtl">(</span>On-Policy
<span dir="rtl"></span>Distributions<span dir="rtl">)</span>**<span dir="rtl">،
والتدريب على كلاهما باستخدام طريقة</span> TD <span dir="rtl">شبه
التدرجية الخطية سيكون مضمونًا أن يكون مستقرًا. كيفما يبدأ العملية، سينتج
توزيع **داخل السياسة** </span>**(On-Policy Distribution)**
<span dir="rtl">طالما تم تحديث جميع الحالات التي تم مواجهتها حتى
الانتهاء</span>.

<span dir="rtl">إذا كان هناك خصم، فيمكن اعتباره كإنهاء جزئي أو احتمالي
لأغراض هذه العملية. إذا كان</span> γ=0.9<span dir="rtl">، فيمكننا أن
نعتبر أنه باحتمال 0.1 تنتهي العملية في كل خطوة زمنية ثم تبدأ من جديد في
الحالة التي تم الانتقال إليها. المشكلة المخصومة هي تلك التي تنتهي وتبدأ
من جديد باستمرار باحتمال</span> 1−γ <span dir="rtl">في كل خطوة. هذا
الطريقة في التفكير حول الخصم هي مثال على مفهوم أكثر عمومية للإنهاء
الزائف</span> (Pseudo Termination) <span dir="rtl">الإنهاء الذي لا يؤثر
على تسلسل انتقالات الحالات، ولكنه يؤثر على عملية التعليم والكميات التي
يتم تعلمها. هذا النوع من الإنهاء الزائف مهم للتعليم **خارج السياسة  
(**</span>**Off-Policy Learning<span dir="rtl">)</span>**
<span dir="rtl">لأن إعادة البدء اختيارية—تذكر أنه يمكننا البدء بأي طريقة
نريدها والإنهاء يخفف الحاجة إلى تضمين الحالات التي تم مواجهتها داخل
توزيع **داخل السياسة  **
</span> **<span dir="rtl">(</span>On-Policy
Distribution<span dir="rtl">)</span>**. <span dir="rtl">بمعنى آخر، إذا
لم نعتبر الحالات الجديدة كبدايات جديدة، فإن الخصم سريعًا ما يعطينا توزيع
**داخل السياسة**</span> **(On-Policy Distribution)**
<span dir="rtl">محدود</span>.

**<span dir="rtl">الخوارزمية</span> (Algorithm)** TD
<span dir="rtl">التأكيدية ذات الخطوة الواحدة لتعلم قيم الحالات الحلقية
تُعرّف كالتالي</span>:

``` math
{\delta_{t} = R_{t + 1} + \gamma\widehat{v}\left( S_{t + 1},w_{t} \right) - \widehat{v}\left( S_{t},w_{t} \right),
}{\quad
}{w_{t + 1} = w_{t} + \alpha M_{t}\rho_{t}\delta_{t}\nabla_{w}\widehat{v}\left( S_{t},w_{t} \right),\quad
}
{M_{t} = \gamma\rho_{t - 1}M_{t - 1} + I_{t}}
```

<span dir="rtl">  
حيث أن</span> It​<span dir="rtl">، المهتمة، تكون اختيارية و</span>Mt
​<span dir="rtl">، التأكيد، يتم تهيئته إلى</span>
Mt−1=0​<span dir="rtl">.</span> <span dir="rtl">كيف تؤدي هذه
**الخوارزمية**</span> **(Algorithm)** <span dir="rtl">على **المثال
المضاد لبيرد** </span>**(Byrd Counterexample)**<span dir="rtl">؟ يوضح
الشكل 11.6 مسار المكونات المتوقعة لمتجه **البارامتر**</span>
**(Parameter Vector)** (<span dir="rtl">للحالة التي فيها</span> It=1
<span dir="rtl">لكل</span> t<span dir="rtl">)</span>.
<span dir="rtl">هناك بعض التذبذبات، لكن في النهاية يتقارب كل شيء ويصبح
**خطأ القيمة**</span> **(VE)** <span dir="rtl">صفرًا. تم الحصول على هذه
المسارات من خلال حساب التوقع لمتجه **البارامتر**</span> **(Parameter
Vector)** <span dir="rtl">بشكل تكراري دون أي تباين ناتج عن أخذ العينات
للانتقالات والمكافآت. لم نظهر نتائج تطبيق **الخوارزمية**
</span>**(Algorithm)** <span dir="rtl"></span>TD
<span dir="rtl">التأكيدية بشكل مباشر لأن التباين في **المثال المضاد
لبيرد (**</span>**Byrd
<span dir="rtl"></span>Counterexample<span dir="rtl">)</span>**
<span dir="rtl">مرتفع جدًا لدرجة أنه يكاد يكون من المستحيل الحصول على
نتائج متسقة في التجارب الحاسوبية.</span>
**<span dir="rtl">الخوارزمية</span> (Algorithm)** <span dir="rtl">تتقارب
نظريًا إلى الحل الأمثل في هذه المشكلة، ولكن في الممارسة لا تفعل ذلك.
ننتقل إلى موضوع تقليل التباين في كل هذه **الخوارزميات**
</span>**(Algorithms)** <span dir="rtl">في القسم التالي</span>.

<img src="./media/image137.png"
style="width:6.28376in;height:2.75055in" />

<span dir="rtl">الشكل 11.6: سلوك **الخوارزمية**</span> **(Algorithm)**
TD <span dir="rtl">التأكيدية ذات الخطوة الواحدة في التوقع على **المثال
المضاد لبيرد** </span>**(Byrd Counterexample)**<span dir="rtl">.</span>
<span dir="rtl">كان حجم الخطوة</span> α=0.03<span dir="rtl">.</span>

**<u>11.9 <span dir="rtl">تقليل التباين</span> (Reducing Variance)</u>**

<span dir="rtl">التعليم **خارج السياسة**</span> **(Off-Policy
Learning)** <span dir="rtl">بطبيعته يكون ذو تباين أكبر من التعليم **داخل
السياسة** </span>**(On-Policy Learning)**<span dir="rtl">.</span>
<span dir="rtl">هذا ليس مفاجئًا؛ إذا تلقيت بيانات أقل ارتباطًا **بسياسة**
</span>**(Policy)** <span dir="rtl">معينة، فيجب أن تتوقع أن تتعلم أقل عن
**قيم السياسة** </span>**(Policy’s Values)**<span dir="rtl">.</span>
<span dir="rtl">في الحالات القصوى، قد لا تتمكن من تعلم أي شيء. لا يمكنك
أن تتوقع تعلم كيفية القيادة من خلال طهي العشاء، على سبيل المثال. فقط إذا
كانت **السياسة المستهدفة**</span> **(Target Policy)**
<span dir="rtl">و**سياسة السلوك** </span>**(Behavior Policy)**
<span dir="rtl">مترابطتين، وإذا زارتا **حالات مشابهة**</span> **(Similar
States)** <span dir="rtl">واتخذتا **إجراءات مشابهة** </span>**(Similar
Actions)**<span dir="rtl">، ينبغي أن تتمكن من تحقيق تقدم ملحوظ في
التدريب **خارج السياسة** </span>**(Off-Policy
Training)**<span dir="rtl">.</span>

<span dir="rtl">من ناحية أخرى، لأي **سياسة**</span> **(Policy)**
<span dir="rtl">هناك العديد من **السياسات المجاورة**</span>
**<span dir="rtl">(</span>Neighboring
<span dir="rtl"></span>Policies<span dir="rtl">)</span>**<span dir="rtl">،
وهي سياسات مشابهة بدرجة كبيرة من حيث **الحالات التي تزورها**
</span>**(States Visited)** <span dir="rtl">و**الإجراءات التي تتخذها**
</span>**(Actions Taken)**<span dir="rtl">، ومع ذلك ليست متطابقة. السبب
وراء وجود التعليم **خارج السياسة**</span> **(Off-Policy Learning)**
<span dir="rtl">هو تمكين التعميم لهذه المجموعة الكبيرة من **السياسات ذات
العلاقة ولكن غير المتطابقة** </span>**(Related-but-Not-Identical
Policies)**<span dir="rtl">.</span> <span dir="rtl">المشكلة المتبقية هي
كيفية الاستفادة القصوى من التجربة. الآن بعد أن أصبح لدينا بعض
**الأساليب**</span> **(Methods)** <span dir="rtl">التي تكون مستقرة في
**القيمة المتوقعة**</span> **(Expected Value)** (<span dir="rtl">إذا تم
ضبط **أحجام الخطوات**</span> **<span dir="rtl">(</span>Step
<span dir="rtl"></span>Sizes<span dir="rtl">)</span>**
<span dir="rtl">بشكل صحيح، فإن الانتباه يتجه بشكل طبيعي إلى تقليل
**التباين**</span> **(Variance)** <span dir="rtl">في **التقديرات**
</span>**(Estimates)**<span dir="rtl">.</span> <span dir="rtl">هناك
العديد من الأفكار المحتملة، ويمكننا فقط التطرق إلى بعض منها في هذا النص
التمهيدي</span>.

<span dir="rtl">لماذا يكون التحكم في **التباين**</span> **(Variance)**
<span dir="rtl">أمرًا حاسمًا بشكل خاص في **الأساليب خارج السياسة**
</span>**(Off-Policy Methods)** <span dir="rtl">التي تعتمد على **أخذ
العينات بالأهمية** </span>**(Importance Sampling)**<span dir="rtl">؟ كما
رأينا، غالبًا ما يتضمن **أخذ العينات بالأهمية**</span> **(Importance
Sampling)** <span dir="rtl">مضاعفات من **نسب السياسات** </span>**(Policy
Ratios)**<span dir="rtl">.</span> <span dir="rtl">تكون النسب دائمًا واحدة
في **التوقع**</span> **(Expectation)** <span dir="rtl">(المعادلة 5.13)،
لكن قيمها الفعلية قد تكون مرتفعة جدًا أو منخفضة تصل إلى الصفر. تكون
**النسب المتعاقبة** </span>**(Successive Ratios)** <span dir="rtl">غير
مترابطة، لذلك تكون مضاعفاتها أيضًا واحدة دائمًا في **القيمة المتوقعة**
</span>**(Expected Value)**<span dir="rtl">، ولكن يمكن أن تكون ذات
**تباين كبير جدًا** </span>**(Very High
Variance)**<span dir="rtl">.</span> <span dir="rtl">تذكر أن هذه النسب
تضاعف **حجم الخطوة**</span> **(Step Size)** <span dir="rtl">في **أساليب
التدرج التنازلي العشوائي**</span> **<span dir="rtl">(</span>SGD
<span dir="rtl"></span>Methods<span dir="rtl">)</span>**<span dir="rtl">،
لذا فإن **التباين العالي**</span> **(High Variance)**
<span dir="rtl">يعني اتخاذ **خطوات**</span> **(Steps)**
<span dir="rtl">تختلف بشكل كبير في أحجامها. هذا يمثل مشكلة بالنسبة
**لأساليب التدرج التنازلي العشوائي** </span>**(SGD Methods)**
<span dir="rtl">بسبب **الخطوات الكبيرة العرضية جدًا** </span>**(Very
Large Occasional Steps)**<span dir="rtl">.</span> <span dir="rtl">يجب
ألا تكون هذه **الخطوات**</span> **(Steps)** <span dir="rtl">كبيرة جدًا
بحيث تأخذ **البارامتر**</span> **(Parameter)** <span dir="rtl">إلى جزء
من **الفضاء** </span>**(Space)** <span dir="rtl">ذي **تدرج**</span>
**(Gradient)** <span dir="rtl">مختلف جدًا. تعتمد **أساليب التدرج التنازلي
العشوائي** </span>**(SGD Methods)** <span dir="rtl">على
**المتوسط**</span> **(Averaging)** <span dir="rtl">عبر **خطوات
متعددة**</span> **(Multiple Steps)** <span dir="rtl">للحصول على فكرة
جيدة عن **التدرج** </span>**(Gradient)**<span dir="rtl">، وإذا قامت
بخطوات كبيرة من **عينات فردية**</span> **(Single Samples)**
<span dir="rtl">فإنها تصبح غير موثوقة</span>.

<span dir="rtl">إذا تم ضبط **بارامتر حجم الخطوة**</span> **(Step-Size
Parameter)** <span dir="rtl">ليكون صغيرًا بما يكفي لمنع ذلك، فإن **الخطوة
المتوقعة**</span> **(Expected Step)** <span dir="rtl">يمكن أن تكون صغيرة
جدًا، مما يؤدي إلى تعلم بطيء جدًا. يمكن أن تساعد أفكار مثل
**الزخم**</span> **(Momentum)**
<span dir="rtl">(</span>Derthick<span dir="rtl">، 1984)، أو **متوسط
بولياك-روبّيرت** </span>**(Polyak-Ruppert Averaging)**
<span dir="rtl">(</span>Polyak<span dir="rtl">، 1990؛</span>
Ruppert<span dir="rtl">،1988؛</span> Polyak
<span dir="rtl">و</span>Juditsky<span dir="rtl">، 1992)، أو التوسعات
الأخرى لهذه الأفكار بشكل كبير. طرق ضبط أحجام الخطوات بشكل تكيفي لمكونات
مختلفة من **متجه البارامتر**</span> **(Parameter Vector)**
<span dir="rtl">تكون ذات صلة أيضًا</span> (<span dir="rtl">(مثل،</span>
Jacobs<span dir="rtl">، 1988؛</span> Sutton<span dir="rtl">،
1992</span>b<span dir="rtl">،</span> c<span dir="rtl">)، وكذلك التحديثات
"المدركة لأوزان الأهمية</span> <span dir="rtl">(</span>Importance Weight
<span dir="rtl"></span>Aware<span dir="rtl">)</span>"
<span dir="rtl">التي قدمها</span> (2010)
Langford<span dir="rtl">.</span>

<span dir="rtl">في الفصل 5، رأينا كيف أن **أخذ العينات بالأهمية
الموزونة**</span> **<span dir="rtl">(</span>Weighted Importance
<span dir="rtl"></span>Sampling<span dir="rtl">)
</span>**<span dir="rtl">يكون أفضل بشكل كبير، مع تحديثات ذات تباين أقل،
من **أخذ العينات بالأهمية العادية** </span>**(Ordinary Importance
Sampling)**<span dir="rtl">. ومع ذلك، فإن تكيف **أخذ العينات بالأهمية
الموزونة** </span>**(Weighted Importance Sampling)** <span dir="rtl">مع
**تقريب الدوال**</span> **(Function Approximation)** <span dir="rtl">يعد
تحديًا وربما يمكن القيام به تقريبًا فقط مع تعقيد من رتبة</span> O(d)
<span dir="rtl">(</span>Mahmood
<span dir="rtl">و</span>Sutton<span dir="rtl">، 2015)</span>.

**<span dir="rtl">خوارزمية النسخ الاحتياطي الشجري</span> (Tree Backup
Algorithm)** <span dir="rtl">(القسم 7.5) تُظهر أنه من الممكن القيام ببعض
التعليم **خارج السياسة**</span> **(Off-Policy Learning)**
<span dir="rtl">دون استخدام **أخذ العينات بالأهمية**
</span>**(Importance Sampling)**<span dir="rtl">.</span>
<span dir="rtl">وقد تم تمديد هذه الفكرة إلى حالة **خارج السياسة  
(**</span>**Off-Policy<span dir="rtl">)</span>** <span dir="rtl">لإنتاج
طرق مستقرة وأكثر كفاءة بواسطة</span> Munos
<span dir="rtl">و</span>Stepleton <span dir="rtl">و</span>Harutyunyan
<span dir="rtl">و</span>Bellemare (2016) <span dir="rtl">وبواسطة</span>
Mahmood <span dir="rtl">و</span>Yu <span dir="rtl">و</span>Sutton
(2017)<span dir="rtl">.</span>

<span dir="rtl">استراتيجية أخرى مكملة هي السماح بتحديد **السياسة
المستهدفة**</span> **(Target Policy)** <span dir="rtl">جزئيًا بواسطة
**سياسة السلوك** </span>**(Behavior Policy)**<span dir="rtl">، بحيث لا
يمكن أن تكون مختلفة عنها بما يكفي لخلق نسب **أخذ العينات
بالأهمية**</span> **(Importance Sampling Ratios)**
<span dir="rtl">كبيرة. على سبيل المثال، يمكن تعريف **السياسة
المستهدفة**</span> **(Target Policy)** <span dir="rtl">بالإشارة إلى
**سياسة السلوك** </span>**(Behavior Policy)**<span dir="rtl">، كما هو
الحال في "المتعرفات</span> (Recognizers)" <span dir="rtl">التي
اقترحها</span> Precup <span dir="rtl">وآخرون</span> (2006).

**<u>11.10 <span dir="rtl">الملخص</span> (Summary)</u>**

<span dir="rtl">التعليم **خارج السياسة**</span> **(Off-Policy
Learning)** <span dir="rtl">يُعد تحديًا مغريًا، يختبر براعتنا في تصميم
**خوارزميات التعليم**</span> **(Learning Algorithms)**
<span dir="rtl">المستقرة والفعالة. يجعل التعليم الجدولي  
</span> **Q-learning**<span dir="rtl">التعليم **خارج السياسة**</span>
**(Off-Policy Learning)** <span dir="rtl">يبدو سهلاً، وله تعميمات طبيعية
لـ **سارسا المتوقع**</span> **(Expected Sarsa)**
<span dir="rtl">و**خوارزمية النسخ الاحتياطي الشجري**</span>
**(<span dir="rtl">(</span>Tree Backup
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**.
<span dir="rtl">ولكن كما رأينا في هذا الفصل، فإن تمديد هذه الأفكار إلى
**تقريب الدوال**</span> **<span dir="rtl">(</span>Function
<span dir="rtl"></span>Approximation<span dir="rtl">)</span>**
<span dir="rtl">المهم، حتى **تقريب الدوال الخطي** </span>**(Linear
Function Approximation)**<span dir="rtl">، يتضمن تحديات جديدة ويجبرنا
على تعميق فهمنا **لخوارزميات التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement <span dir="rtl"></span>Learning
Algorithms<span dir="rtl">)</span>**.

<span dir="rtl">لماذا نسعى لتحقيق ذلك؟ أحد الأسباب للبحث عن **خوارزميات
خارج السياسة**</span> **<span dir="rtl">(</span>Off-Policy
<span dir="rtl"></span>Algorithms<span dir="rtl">)</span>**
<span dir="rtl">هو توفير المرونة في التعامل مع التوازن بين **الاستكشاف**
</span>**(Exploration)** <span dir="rtl">و**الاستغلال**
</span>**(Exploitation)**<span dir="rtl">.</span> <span dir="rtl">سبب
آخر هو تحرير السلوك من التعليم، وتجنب هيمنة **السياسة المستهدفة**
</span>**(Target Policy)**<span dir="rtl">.</span> <span dir="rtl">يبدو
أن **التعليم**</span> **TD** <span dir="rtl">يحمل إمكانية التعليم حول
أشياء متعددة بالتوازي، واستخدام تدفق واحد من التجربة لحل العديد من
المهام في الوقت نفسه. يمكننا بالتأكيد القيام بذلك في حالات خاصة، ولكن
ليس في كل حالة نودها أو بكفاءة كما نرغب</span>.

<span dir="rtl">في هذا الفصل، قسمنا تحدي التعليم **خارج السياسة**</span>
**(Off-Policy Learning)** <span dir="rtl">إلى جزئين. الجزء الأول، تصحيح
أهداف التعليم لسياسة السلوك، يتم التعامل معه بشكل مباشر باستخدام
التقنيات التي تم تطويرها سابقًا للحالة الجدولية، وإن كان ذلك بتكلفة زيادة
**التباين**</span> **(Variance)** <span dir="rtl">في التحديثات وبالتالي
إبطاء التعليم. سيظل **التباين العالي**</span> **(High Variance)**
<span dir="rtl">على الأرجح تحديًا دائمًا للتعليم **خارج السياسة**
</span>**(Off-Policy Learning)**<span dir="rtl">.</span>

<span dir="rtl">الجزء الثاني من تحدي التعليم **خارج السياسة**</span>
**(Off-Policy Learning)** <span dir="rtl">يظهر في عدم استقرار
**أساليب**</span> **TD <span dir="rtl">شبه التدرجية</span>
(Semi-Gradient TD Methods)** <span dir="rtl">التي تتضمن
**البوتسترابينغ** </span>**(Bootstrapping)**<span dir="rtl">.</span>
<span dir="rtl">نسعى للحصول على **تقريب دوال قوي**</span>
**<span dir="rtl">(</span>Powerful Function
<span dir="rtl"></span>Approximation<span dir="rtl">)</span>**<span dir="rtl">،
التعليم **خارج السياسة** </span>**(Off-Policy
Learning)**<span dir="rtl">، وكفاءة ومرونة **أساليب** </span>**TD
<span dir="rtl">بوتسترابينغ</span> (TD Bootstrapping
Methods)**<span dir="rtl">، لكن من الصعب الجمع بين هذه الجوانب الثلاثة
الخطيرة في **خوارزمية**</span> **(Algorithm)** <span dir="rtl">واحدة دون
إدخال إمكانية عدم الاستقرار. كانت هناك عدة محاولات. الأكثر شعبية كانت
السعي لأداء **الانحدار العشوائي الحقيقي (**</span>**True Stochastic
<span dir="rtl"></span>Gradient Descent – SGD<span dir="rtl">)
</span>**<span dir="rtl">في **خطأ بيلمان**</span> **(Bellman Error -
BE)** <span dir="rtl">(المعروف أيضًا باسم **الفرق بيلمان**
</span>**(Bellman Residual)**<span dir="rtl">)</span>.
<span dir="rtl">ومع ذلك، فإن تحليلنا يستنتج أن هذا ليس هدفًا جذابًا في
كثير من الحالات، وأنه على أي حال من المستحيل تحقيقه باستخدام **خوارزمية
تعلم  
(**</span>**Learning
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**
<span dir="rtl">لا يمكن تعلم تدرج **خطأ بيلمان**</span> **(BE)**
<span dir="rtl">من تجربة تكشف فقط **متجهات الميزات**</span> **(Feature
Vectors)** <span dir="rtl">وليس **الحالات الأساسية**
</span>**(Underlying States)**<span dir="rtl">.</span>
<span dir="rtl">نهج آخر، **أساليب**
</span>**Gradient-TD**<span dir="rtl">، يؤدي **الانحدار
العشوائي**</span> **(SGD)** <span dir="rtl">في **خطأ بيلمان
المُسقَط**</span> **<span dir="rtl">(</span>Projected
<span dir="rtl"></span>Bellman Error – PBE<span dir="rtl">)</span>**.
<span dir="rtl">يمكن تعلم تدرج **خطأ بيلمان المُسقَط**</span> **(PBE)**
<span dir="rtl">بتعقيد من رتبة</span> O(d)<span dir="rtl">، ولكن بتكلفة
متجه بارامتر ثاني مع حجم خطوة ثاني.</span> **<span dir="rtl">عائلة
الأساليب</span> (Family of Methods)** <span dir="rtl">الأحدث،
**أساليب**</span> **TD <span dir="rtl">التأكيدية</span> (Emphatic-TD
Methods)**<span dir="rtl">، تُحسّن فكرة قديمة لإعادة وزن التحديثات، مع
التركيز على بعض وتقليل التركيز على البعض الآخر. بهذه الطريقة، تستعيد
الخصائص الخاصة التي تجعل التعليم **داخل السياسة**</span> **(On-Policy
Learning)** <span dir="rtl">مستقرًا باستخدام **أساليب شبه التدرج**</span>
**(Semi-Gradient Methods)** <span dir="rtl">البسيطة من الناحية
الحسابية</span>.

<span dir="rtl">المجال الكامل للتعليم **خارج السياسة**</span>
**(Off-Policy Learning)** <span dir="rtl">جديد نسبيًا وغير مستقر. أي
**أساليب**</span> **(Methods)** <span dir="rtl">هي الأفضل أو حتى الكافية
لم يتضح بعد. هل تعقيدات **الأساليب الجديدة**</span> **(New Methods)**
<span dir="rtl">التي تم تقديمها في نهاية هذا الفصل ضرورية حقًا؟ أي منها
يمكن دمجه بشكل فعال مع **أساليب تقليل التباين** </span>**(Variance
Reduction Methods)**<span dir="rtl">؟ تظل إمكانيات التعليم **خارج
السياسة**</span> **(Off-Policy Learning)** <span dir="rtl">مغرية، لكن
الطريقة المثلى لتحقيقها لا تزال لغزًا</span>.

<span dir="rtl">الفصل الثاني عشر:</span>

<span dir="rtl">تتبعات الأهلية</span> (Eligibility Traces)

<span dir="rtl">تُعتبر **تتبعات الأهلية**</span> **(Eligibility Traces)**
<span dir="rtl">واحدة من الآليات الأساسية في **التعليم المعزز**</span>
**(Reinforcement Learning)**<span dir="rtl">.</span> <span dir="rtl">على
سبيل المثال، في **خوارزمية**</span> **TD(λ)** <span dir="rtl">الشهيرة،
يشير</span> λ <span dir="rtl">إلى استخدام **تتبع الأهلية**
</span>**(Eligibility Trace)**<span dir="rtl">.</span>
<span dir="rtl">يمكن دمج أي طريقة من **طرق الفرق الزمنية**
</span>**(Temporal-Difference - TD)**<span dir="rtl">، مثل</span>
**Q-learning** <span dir="rtl">أو **سارسا**
</span>**(Sarsa)**<span dir="rtl">، مع **تتبعات الأهلية**
</span>**(Eligibility Traces)** <span dir="rtl">للحصول على طريقة أكثر
عمومية قد تتعلم بشكل أكثر كفاءة</span>. <span dir="rtl"></span>

<span dir="rtl">توحد **تتبعات الأهلية**</span> **(Eligibility Traces)**
<span dir="rtl">وتعمم **طرق الفرق الزمنية**</span> **(TD Methods)**
<span dir="rtl">و**طرق مونت كارلو** </span>**(Monte Carlo
Methods)**<span dir="rtl">.</span> <span dir="rtl">عندما يتم تعزيز **طرق
الفرق الزمنية** </span>**(TD Methods)** <span dir="rtl">باستخدام
**تتبعات الأهلية** </span>**(Eligibility Traces)**<span dir="rtl">،
فإنها تنتج عائلة من **الطرق**</span> **(Methods)** <span dir="rtl">التي
تمتد عبر طيف يحتوي على **طرق مونت كارلو**</span> **(Monte Carlo
Methods)** <span dir="rtl">في أحد الأطراف</span> (λ=1)
<span dir="rtl">و**طرق الفرق الزمنية ذات الخطوة الواحدة**</span>
**(One-Step TD Methods)** <span dir="rtl">في الطرف الآخر</span>
(λ=0)<span dir="rtl">. تقع بينهما **طرق متوسطة**</span> **(Intermediate
Methods)** <span dir="rtl">غالبًا ما تكون أفضل من أي من الطريقتين
الطرفيتين. كما توفر **تتبعات الأهلية**</span> **(Eligibility Traces)**
<span dir="rtl">طريقة لتنفيذ **طرق مونت كارلو**</span> **(Monte Carlo
Methods)** <span dir="rtl">بشكل مباشر وعلى مشاكل مستمرة دون
حلقات</span>.

<span dir="rtl">بالطبع، لقد رأينا بالفعل طريقة واحدة لتوحيد **طرق الفرق
الزمنية**</span> **(TD Methods)** <span dir="rtl">و**طرق مونت كارلو**
</span>**(Monte Carlo Methods)**<span dir="rtl">:</span>
**<span dir="rtl">طرق</span> TD <span dir="rtl">ذات الخطوات
المتعددة</span> (n-Step TD Methods)** <span dir="rtl">التي تم تناولها في
الفصل 7. ما تقدمه **تتبعات الأهلية**</span> **(Eligibility Traces)**
<span dir="rtl">بالإضافة إلى ذلك هو آلية خوارزمية أنيقة تتمتع بمزايا
حسابية كبيرة. الآلية هي **متجه ذاكرة قصيرة المدى (**</span>**Short-Term
<span dir="rtl"></span>Memory Vecto<span dir="rtl">).</span>**
<span dir="rtl">وهو **تتبع الأهلية** </span>**(Eligibility Trace)**
$`\mathbf{z}_{\mathbf{t}}\mathbf{\in}\mathbf{R}^{\mathbf{d}}`$<span dir="rtl">،
الذي يتوازى مع **متجه الوزن طويل المدى**</span> **(Long-Term Weight
Vector)**
$`\mathbf{w}_{\mathbf{t}}\mathbf{\in}\mathbf{R}^{\mathbf{d}}`$.
<span dir="rtl">الفكرة الأساسية هي أنه عندما يشارك مكون من **متجه
الوزن**</span> **(Weight Vector)** <span dir="rtl">في إنتاج قيمة مقدرة،
يتم زيادة المكون المقابل من **تتبع الأهلية**</span> **(Eligibility
Trace)** zt <span dir="rtl">ثم يبدأ في التلاشي. سيحدث التعليم في ذلك
المكون من **متجه الوزن**</span> **(Weight Vector)** <span dir="rtl">إذا
حدث **خطأ فرق الزمن**</span> **(TD Error)** <span dir="rtl">غير صفري قبل
أن يعود التتبع إلى الصفر. يقوم **معامل تلاشي التتبع**</span>
**(Trace-Decay Parameter)** λ <span dir="rtl">بتحديد معدل
التلاشي</span>.

<span dir="rtl">الميزة الحسابية الأساسية لـ **تتبعات الأهلية**</span>
**(Eligibility Traces)** <span dir="rtl">على **طرق**</span> **TD
<span dir="rtl">ذات الخطوات المتعددة</span> (n-Step TD Methods)**
<span dir="rtl">هي أنه يلزم فقط **متجه تتبع واحد** </span>**(Single
Trace Vector)** <span dir="rtl">بدلاً من تخزين آخر **متجهات الميزات**
</span>**(Feature Vectors)**<span dir="rtl">.</span>
<span dir="rtl">يحدث التعليم أيضًا بشكل مستمر ومتساوي بمرور الوقت بدلاً من
التأخير ثم اللحاق في نهاية الحلقة. بالإضافة إلى ذلك، يمكن أن يحدث
التعليم ويؤثر على السلوك فورًا بعد مواجهة **الحالة**</span> **(State)**
<span dir="rtl">بدلاً من تأخير</span> n <span dir="rtl">خطوات</span>.

<span dir="rtl">توضح **تتبعات الأهلية**</span> **(Eligibility Traces)**
<span dir="rtl">أن **الخوارزمية التعليمية** </span>**(Learning
Algorithm)** <span dir="rtl">يمكن أحيانًا تنفيذها بطريقة مختلفة للحصول
على مزايا حسابية. يتم صياغة وفهم العديد من **الخوارزميات**</span>
**(Algorithms)** <span dir="rtl">بشكل طبيعي على أنها تحديث لقيمة
**حالة**</span> **(State)** <span dir="rtl">استنادًا إلى الأحداث التي تلي
تلك **الحالة**</span> **(State)** <span dir="rtl">على مدى عدة خطوات
زمنية مستقبلية. على سبيل المثال، **طرق مونت كارلو**</span> **(Monte
Carlo Methods)** <span dir="rtl">(الفصل 5) تقوم بتحديث **حالة**</span>
**(State)** <span dir="rtl">استنادًا إلى جميع المكافآت المستقبلية،
و**طرق**</span> **TD <span dir="rtl">ذات الخطوات المتعددة  
</span> (n-Step TD Methods)** <span dir="rtl">(الفصل 7) تقوم بالتحديث
استنادًا إلى</span> n <span dir="rtl">مكافآت التالية و**الحالة
بعد**</span> **n <span dir="rtl">خطوات</span> (State n Steps)**
<span dir="rtl">في المستقبل. مثل هذه الصياغات، التي تعتمد على النظر إلى
الأمام من **الحالة المحدثة** </span>**(Updated State)**<span dir="rtl">،
تسمى **وجهات نظر أمامية** </span>**(Forward
Views)**<span dir="rtl">.</span> <span dir="rtl">تكون **وجهات النظر
الأمامية**</span> **(Forward Views)** <span dir="rtl">دائمًا معقدة بعض
الشيء في التنفيذ لأن التحديث يعتمد على أشياء لاحقة ليست متاحة في الوقت
الحالي. ومع ذلك، كما نوضح في هذا الفصل، من الممكن غالبًا تحقيق تحديثات
مشابهة تقريبًا—وأحيانًا تحديثات مطابقة تمامًا—بواسطة **خوارزمية**</span>
**(Algorithm)** <span dir="rtl">تستخدم **خطأ فرق الزمن الحالي**
</span>**(Current TD Error)**<span dir="rtl">، وتنظر إلى الوراء إلى
**الحالات التي تمت زيارتها مؤخرًا**</span> **(Recently Visited States)**
<span dir="rtl">باستخدام **تتبع الأهلية** </span>**(Eligibility
Trace)**<span dir="rtl">.</span> <span dir="rtl">تُسمى هذه الطرق البديلة
للنظر إلى وتنفيذ **الخوارزميات التعليمية**</span> **(Learning
Algorithms)** <span dir="rtl">بـ **وجهات النظر الخلفية**
</span>**(Backward Views)**<span dir="rtl">.</span>
**<span dir="rtl">وجهات النظر الخلفية</span> (Backward
Views)**<span dir="rtl">، والتحولات بين **وجهات النظر الأمامية**</span>
**(Forward Views)** <span dir="rtl">و**وجهات النظر الخلفية**
</span>**(Backward Views)**<span dir="rtl">، والتكافؤات بينها، تعود إلى
تقديم **التعليم بالفرق الزمنية** </span>**(Temporal-Difference
Learning)** <span dir="rtl">ولكنها أصبحت أكثر قوة وتطورًا منذ عام 2014.
هنا نقدم الأساسيات من **وجهة النظر الحديثة**</span> **(Modern
View)**<span dir="rtl">.</span>

<span dir="rtl">كما هو معتاد، نقوم أولاً بتطوير الأفكار بالكامل لقيم
**الحالات**</span> **(State Values)** <span dir="rtl">و**التنبؤ**
</span>**(Prediction)**<span dir="rtl">، ثم نمدها إلى **قيم
الإجراءات**</span> **(Action Values)** <span dir="rtl">و**التحكم**
</span>**(Control)**<span dir="rtl">.</span> <span dir="rtl">نقوم
بتطويرها أولاً لحالة **داخل السياسة**</span> **(On-Policy)**
<span dir="rtl">ثم نمدها إلى التعليم **خارج السياسة  
(**</span>**Off-Policy Learning<span dir="rtl">)</span>**.
<span dir="rtl">يولي عرضنا اهتمامًا خاصًا لحالة **تقريب الدوال الخطي  
(**</span>**Linear Function
Approximation<span dir="rtl">)</span>**<span dir="rtl">، حيث تكون
النتائج مع **تتبعات الأهلية  
(**</span>**Eligibility Traces<span dir="rtl">)
</span>**<span dir="rtl">أقوى. تنطبق جميع هذه النتائج أيضًا على الحالات
الجدولية وتجزئة **الحالات** </span>**(State Aggregation)**
<span dir="rtl">لأن هذه الحالات هي حالات خاصة من **تقريب الدوال الخطي  
(**</span>**Linear Function Approximation<span dir="rtl">)</span>**.

**<u>12.1 <span dir="rtl">العائد</span> (The λ-return)</u>**

<span dir="rtl">في الفصل 7، قمنا بتعريف **العائد** </span>**n
<span dir="rtl">ذو الخطوات</span> (n-Step Return)** <span dir="rtl">على
أنه مجموع أول</span> n <span dir="rtl">من المكافآت بالإضافة إلى القيمة
المقدرة **للحالة**</span> **(State)** <span dir="rtl">التي تم الوصول
إليها بعد</span> n <span dir="rtl">خطوة، مع خصم كل منها بشكل مناسب
(المعادلة 7.1). الشكل العام لتلك المعادلة، لأي **مقرب دالة**
</span>**(Parameterized Function Approximator)**<span dir="rtl">، هو
كالتالي</span>:

``` math
G_{t:t + n} = R_{t + 1} + \gamma R_{t + 2} + \cdots + \gamma^{n - 1}R_{t + n} + \gamma^{n}\widehat{v}\left( S_{t + n},w_{t + n - 1} \right),\quad 0 \leq t \leq T - n
```

<span dir="rtl">  
حيث أن</span> v^(s,w) <span dir="rtl">هي **القيمة التقريبية
للحالة**</span> **(Approximate Value of State)** s <span dir="rtl">معطاة
**متجه الوزن**</span> **(Weight Vector)** w <span dir="rtl">(الفصل 9)،
و</span>T <span dir="rtl">هو وقت انتهاء **الحلقة  
(**</span>**Episode
<span dir="rtl"></span>Termination<span dir="rtl">)</span>**<span dir="rtl">،
إن وجد. لاحظنا في الفصل 7 أن كل **عائد ذو**</span> **n
<span dir="rtl">خطوة  
(</span>n-Step
<span dir="rtl"></span>Return<span dir="rtl">)</span>**<span dir="rtl">،
لأي</span> n≥1<span dir="rtl">، هو هدف تحديث صالح لعملية **التعليم
الجدولي  
(**</span>**Tabular <span dir="rtl"></span>Learning
Update<span dir="rtl">)</span>**<span dir="rtl">، تمامًا كما هو الحال
بالنسبة لعملية تحديث **التعليم باستخدام الانحدار العشوائي**</span>
**(Approximate SGD Learning Update)** <span dir="rtl">مثل (المعادلة
9.7)</span>.

<span dir="rtl">الآن نلاحظ أن التحديث الصالح يمكن أن يتم ليس فقط تجاه أي
**عائد ذو**</span> **n <span dir="rtl">خطوة  
(</span>n-Step Return<span dir="rtl">)</span>**<span dir="rtl">، ولكن
أيضًا تجاه أي متوسط لعوائد ذات خطوات</span> n <span dir="rtl">مختلفة. على
سبيل المثال، يمكن أن يتم التحديث تجاه هدف يمثل نصف **عائد ذو
خطوتين**</span> **(Two-Step Return)** <span dir="rtl">ونصف **عائد ذو
أربع خطوات** </span>**(Four-Step Return)**<span dir="rtl">:</span>
​Gt:t+2​+21​Gt:t+4​<span dir="rtl">.</span> <span dir="rtl">يمكن متوسط أي
مجموعة من **العوائد** </span>**n <span dir="rtl">ذات الخطوات</span>
(n-Step Returns)** <span dir="rtl">بهذه الطريقة، حتى مجموعة غير محدودة،
طالما كانت الأوزان على العوائد المكونة موجبة ومجموعها يساوي 1. يتميز
**العائد المركب  
(**</span>**Composite Return<span dir="rtl">)</span>**
<span dir="rtl">بخاصية تقليل الخطأ مشابهة لتلك الخاصة **بالعوائد ذات
الخطوات** </span>**n <span dir="rtl">الفردية</span> (Individual n-Step
Returns)** <span dir="rtl">(المعادلة 7.3)، وبالتالي يمكن استخدامه لبناء
تحديثات بخصائص تقارب مضمونة. ينتج عن المتوسط نطاق جديد واسع من
**الخوارزميات** </span>**(Algorithms)**<span dir="rtl">.</span>
<span dir="rtl">على سبيل المثال، يمكن للمرء أن يقوم بمتوسط **العوائد ذات
الخطوة الواحدة**</span> **<span dir="rtl">(</span>One-Step
Returns<span dir="rtl">) </span>**<span dir="rtl">و**العوائد ذات الخطوات
اللانهائية**</span> **(Infinite-Step Returns)** <span dir="rtl">للحصول
على طريقة أخرى للربط بين **طرق**</span> **TD** <span dir="rtl">و**طرق
مونت كارلو** </span>**(Monte Carlo Methods)**<span dir="rtl">.</span>
<span dir="rtl">من حيث المبدأ، يمكن حتى أن يتم **متوسط التحديثات
المستندة إلى التجربة**</span> **(Experience-Based Updates)**
<span dir="rtl">مع **التحديثات المستندة إلى البرمجة الديناميكية**</span>
**(DP Updates)** <span dir="rtl">للحصول على مزيج بسيط من **الطرق
المستندة إلى التجربة**</span> **(Experience-Based Methods)**
<span dir="rtl">و**الطرق المستندة إلى النموذج  
(**</span>**Model-Based Methods<span dir="rtl">)</span>**
<span dir="rtl">(راجع الفصل 8)</span>.

<span dir="rtl">التحديث الذي يقوم بمتوسط **التحديثات المكونة
الأبسط**</span> **(Simpler Component Updates)** <span dir="rtl">يُسمى
**التحديث المركب** </span>**(Compound Update)**<span dir="rtl">.</span>
<span dir="rtl">يتكون **مخطط النسخ الاحتياطي  
(**</span>**Backup Diagram<span dir="rtl">) </span>**<span dir="rtl">لـ
**التحديث المركب**</span> **(Compound Update)** <span dir="rtl">من
**مخططات النسخ الاحتياطي**</span> **(Backup Diagrams)**
<span dir="rtl">لكل من **التحديثات المكونة**</span> **(Component
Updates)** <span dir="rtl">مع خط أفقي فوقها وكسر الوزن تحتها</span>.

<img src="./media/image138.png"
style="width:0.51736in;height:1.95139in" /><span dir="rtl">على سبيل
المثال، **التحديث المركب**</span> **(Compound Update)**
<span dir="rtl">للحالة المذكورة في بداية هذا القسم، والذي يخلط نصف
**عائد ذو خطوتين**</span> **(Two-Step Return)** <span dir="rtl">ونصف
**عائد ذو أربع خطوات** </span>**(Four-Step Return)**<span dir="rtl">،
يحتوي على المخطط الموضح إلى اليمين. يمكن إجراء **التحديث المركب**</span>
**(Compound Update)** <span dir="rtl">فقط عندما يكتمل أطول **التحديثات
المكونة**</span> **(Component Updates)**<span dir="rtl">. على سبيل
المثال، يمكن إجراء التحديث الموضح إلى اليمين فقط في الوقت</span> t+4
<span dir="rtl">للتقدير الذي تم تكوينه في الوقت</span>
t<span dir="rtl">.</span> <span dir="rtl">بشكل عام، يرغب المرء في تحديد
طول أطول **تحديث مكون**</span> **(Component Update)**
<span dir="rtl">بسبب التأخير المقابل في التحديثات</span>.

<span dir="rtl">يمكن فهم **خوارزمية**</span> **TD(λ)**
<span dir="rtl">كإحدى الطرق المحددة لمتوسط **التحديثات ذات الخطوات**
</span>$`\mathbf{n}`$ **<span dir="rtl">  
(</span>n-Step Updates<span dir="rtl">)</span>**. <span dir="rtl">يحتوي
هذا المتوسط على جميع **التحديثات ذات الخطوات** </span>$`\mathbf{n}`$
**<span dir="rtl">  
(</span>n-Step
<span dir="rtl"></span>Updates<span dir="rtl">)</span>**<span dir="rtl">،
حيث يتم وزن كل منها بشكل يتناسب مع</span> $`\lambda n - 1`$
<span dir="rtl">(حيث</span>
$`\lambda \in \ \lbrack 0,1\rbrack`$<span dir="rtl">)، ويتم تطبيعه
بعامل</span> $`1 - \lambda`$ <span dir="rtl">لضمان أن الأوزان مجموعها
يساوي 1 (الشكل 12.1). التحديث الناتج يكون نحو **عائد**
</span>**(Return)**<span dir="rtl">، يُسمى</span> **λ
<span dir="rtl">العائد</span> (λ-Return)**<span dir="rtl">، ويُعرّف في
شكله المعتمد على **الحالة  
(**</span>**State-Based Form<span dir="rtl">)
</span>**<span dir="rtl">بواسطة</span>:

``` math
G_{t}^{\lambda} = (1 - \lambda)\sum_{n = 1}^{\infty}{\lambda^{n - 1}G_{t:t + n}}
```

<span dir="rtl">يوضح الشكل 12.2 بشكل إضافي عملية وزن تسلسل **العوائد**
</span>$`\mathbf{n}`$ **<span dir="rtl">ذات الخطوات</span> (n-Step
Returns)** <span dir="rtl">في</span> $`\mathbf{\lambda}`$
**<span dir="rtl">العائد</span> (λ-Return)**<span dir="rtl">.</span>
<span dir="rtl">يتم إعطاء **العائد ذو الخطوة الواحدة**</span>
**(One-Step Return)** <span dir="rtl">أكبر وزن،</span>
$`1 - \lambda`$<span dir="rtl">؛ يتم إعطاء **العائد ذو الخطوتين**</span>
**(Two-Step Return)** <span dir="rtl">الوزن الأكبر التالي،</span>
($`1 - \lambda`$)<span dir="rtl">؛ يتم إعطاء **العائد ذو الثلاث
خطوات**</span> **(Three-Step Return)** <span dir="rtl">الوزن</span>
($`1 - \lambda`$) $`\lambda 2`$<span dir="rtl">؛ وهكذا. يتلاشى الوزن
بمقدار</span> $`\lambda`$ <span dir="rtl">مع كل خطوة إضافية. بعد الوصول
إلى **الحالة النهائية  
(**</span>**Terminal State<span dir="rtl">)</span>**<span dir="rtl">،
تصبح جميع **العوائد ذات** </span>$`\mathbf{n}`$
**<span dir="rtl">الخطوات</span> (n-Step Returns)**
<span dir="rtl">اللاحقة مساوية **للعائد التقليدي**
</span>**(Conventional Return)**<span dir="rtl">،</span> Gt
<span dir="rtl"></span>

<img src="./media/image139.png"
style="width:4.65405in;height:3.70659in" />

<span dir="rtl">الشكل 12.1: مخطط النسخ الاحتياطي **لخوارزمية**
</span>**TD(λ)**<span dir="rtl">.</span> <span dir="rtl">إذا كان</span>
λ=0<span dir="rtl">، فإن التحديث الإجمالي يتقلص إلى مكونه الأول، وهو
تحديث</span> **TD <span dir="rtl">ذو الخطوة الواحدة</span> (One-Step TD
Update)**<span dir="rtl">، في حين إذا كان</span> λ=1<span dir="rtl">،
فإن التحديث الإجمالي يتقلص إلى مكونه الأخير، وهو تحديث **مونت كارلو**
</span>**(Monte Carlo Update)**<span dir="rtl">.</span>

<img src="./media/image140.png"
style="width:6.26806in;height:2.49931in" />

<span dir="rtl">الشكل 12.2: الأوزان المخصصة في **العائد**</span>
**(λ-Return)** <span dir="rtl">لكل من **العوائد ذات الخطوات**</span>
**<span dir="rtl">(</span>n-Step
<span dir="rtl"></span>Returns<span dir="rtl">)</span>**.

<span dir="rtl">إذا أردنا، يمكننا فصل هذه الحدود اللاحقة للانتهاء عن
المجموع الرئيسي، مما ينتج</span>:

``` math
G_{t}^{\lambda} = (1 - \lambda)\sum_{n = 1}^{T - t - 1}{\lambda^{n - 1}G_{t:t + n}} + \lambda^{T - t - 1}G_{t}
```

<span dir="rtl">كما هو موضح في الأشكال. تجعل هذه المعادلة الأمور أكثر
وضوحًا عندما يكون</span> λ=1<span dir="rtl">. في هذه الحالة، يصبح المجموع
الرئيسي صفراً، ويتقلص الحد المتبقي إلى **العائد التقليدي  
(**</span>**Conventional Return<span dir="rtl">)</span>**.
<span dir="rtl">وبالتالي، بالنسبة لـ</span> λ=1<span dir="rtl">، يكون
التحديث وفقًا لـ **العائد** </span>**λ<span dir="rtl">  
(</span>λ-Return<span dir="rtl">) </span>**<span dir="rtl">هو **خوارزمية
مونت كارلو** </span>**(Monte Carlo Algorithm)**<span dir="rtl">.</span>
<span dir="rtl">من ناحية أخرى، إذا كان</span> λ=0<span dir="rtl">، فإن
**العائد**</span> **λ (λ-Return)** <span dir="rtl">يتقلص إلى</span>
$`Gt:t + 1`$​<span dir="rtl">، وهو **العائد ذو الخطوة الواحدة  
(**</span>**One-Step Return<span dir="rtl">)</span>**.
<span dir="rtl">وبالتالي، بالنسبة لـ</span>
$`\lambda = 0`$<span dir="rtl">، يكون التحديث وفقًا لـ</span> **λ
<span dir="rtl">العائد</span> (λ-Return)** <span dir="rtl">هو
**طريقة**</span> **TD <span dir="rtl">ذات الخطوة الواحدة</span>
(One-Step TD Method)**<span dir="rtl">.</span>

<span dir="rtl">**تمرين 12.1** تمامًا كما يمكن كتابة **العائد**</span>
**(Return)** <span dir="rtl">بشكل متكرر من حيث **المكافأة
الأولى**</span> **<span dir="rtl">  
(</span>First Reward<span dir="rtl">) </span>**<span dir="rtl">وذاته بعد
خطوة واحدة (المعادلة 3.9)، يمكن أيضًا كتابة</span> **λ
<span dir="rtl">العائد</span> (λ-Return)** <span dir="rtl">اشتق العلاقة
التكرارية المماثلة من المعادلتين (12.2) و</span>(12.1)
⇤<span dir="rtl">.</span>

<span dir="rtl">**تمرين 12.2** يصف</span> **λ
<span dir="rtl">البارامتر</span> (Parameter λ)** <span dir="rtl">سرعة
تلاشي الوزن الأسي في الشكل 12.2، وبالتالي مدى امتداد</span> **λ
<span dir="rtl">خوارزمية العائد</span> (λ-Return Algorithm)**
<span dir="rtl">في المستقبل لتحديد التحديث. ولكن في بعض الأحيان، يكون
العامل الزمني مثل</span> $`\lambda`$ <span dir="rtl">طريقة غير ملائمة
لتوصيف سرعة التلاشي. لأغراض معينة، من الأفضل تحديد **ثابت الزمن**
</span>**(Time Constant)**<span dir="rtl">، أو **نصف العمر  
(**</span>**Half-Life<span dir="rtl">)</span>**. <span dir="rtl">ما هي
المعادلة التي تربط بين</span> $`\lambda`$ <span dir="rtl">ونصف **العمر**
</span>**(Half-Life)**<span dir="rtl">،</span>
$`\tau\lambda`$​<span dir="rtl">، وهو الوقت الذي يتلاشى فيه تسلسل الأوزان
إلى نصف قيمته الأولية؟</span> $`⇤`$

<span dir="rtl">نحن الآن مستعدون لتعريف **خوارزمية التعليم**</span>
**(Learning Algorithm)** <span dir="rtl">الأولى لدينا بناءً على</span>
**λ <span dir="rtl">العائد</span> (λ-Return)**<span dir="rtl">:</span>
**<span dir="rtl">خوارزمية العائد</span> λ <span dir="rtl">غير
المتصلة</span> (Offline λ-Return Algorithm)**<span dir="rtl">.  
كـ **خوارزمية غير متصلة** </span>**(Offline
Algorithm)**<span dir="rtl">، لا تقوم بإجراء أي تغييرات على **متجه
الوزن** </span>**(Weight Vector)** <span dir="rtl">أثناء الحلقة. ثم، في
نهاية الحلقة، يتم إجراء سلسلة كاملة من التحديثات غير المتصلة وفقًا
**لقاعدة شبه التدرج المعتادة** </span>**(Semi-Gradient
Rule)**<span dir="rtl">، باستخدام **العائد** </span>**λ
<span dir="rtl">  
(</span>λ-Return<span dir="rtl">)</span>** <span dir="rtl">كهدف</span>:

``` math
w_{t + 1} = w_{t} + \alpha\left\lbrack G_{t}^{\lambda} - \widehat{v}\left( S_{t},w_{t} \right) \right\rbrack\nabla\widehat{v}\left( S_{t},w_{t} \right),\quad t = 0,\ldots,T - 1
```

<span dir="rtl">يوفر **العائد**</span> **λ (λ-Return)**
<span dir="rtl">لنا طريقة بديلة للتحرك بسلاسة بين **طرق مونت
كارلو**</span> **<span dir="rtl">(</span>Monte Carlo
<span dir="rtl"></span>Methods<span dir="rtl">)
</span>**<span dir="rtl">و**طرق**</span> **TD <span dir="rtl">ذات الخطوة
الواحدة</span> (One-Step TD Methods)**<span dir="rtl">، والتي يمكن
مقارنتها بطريقة **البوتسترابينغ** </span>$`\mathbf{n}`$
**<span dir="rtl">ذات الخطوات</span> (n-Step Bootstrapping)**
<span dir="rtl">التي تم تطويرها في الفصل 7. هناك قمنا بتقييم الفعالية
على مهمة السير العشوائي ذات الـ 19 حالة (المثال 7.1، الصفحة 144). يوضح
الشكل 12.3 أداء **خوارزمية العائد**</span> $`\mathbf{\lambda}`$
**<span dir="rtl">غير المتصلة</span> (Offline λ-Return Algorithm)**
<span dir="rtl">على هذه المهمة إلى جانب **طرق**</span> **n-Step**
<span dir="rtl">(المكررة من الشكل 7.2). كان التجربة تمامًا كما وصفت سابقًا
باستثناء أنه بالنسبة **لخوارزمية** </span>**λ
<span dir="rtl">العائد</span> (λ-Return Algorithm)**
<span dir="rtl">قمنا بتغيير</span> λ <span dir="rtl">بدلاً من</span>
$`n`$<span dir="rtl">. المقياس المستخدم للأداء هو **الجذر التربيعي لخطأ
المتوسط المربع**</span> **<span dir="rtl">(</span>Root-Mean-Squared
<span dir="rtl"></span>Error<span dir="rtl">)
</span>**<span dir="rtl">بين القيم الصحيحة والمقدرة لكل حالة مقاسة في
نهاية الحلقة، ومتوسطها على أول 10 حلقات والـ 19 حالة. لاحظ أن الأداء
العام **لخوارزميات العائد**</span> **λ <span dir="rtl">غير
المتصلة</span> <span dir="rtl">(</span>Offline λ-Return
<span dir="rtl"></span>Algorithms<span dir="rtl">)
</span>**<span dir="rtl">قابل للمقارنة مع **خوارزميات**
</span>**n-Step**<span dir="rtl">.</span> <span dir="rtl">في كلتا
الحالتين، نحصل على أفضل أداء مع قيمة متوسطة **لمعامل البوتسترابينغ**
</span>**(Bootstrapping Parameter)**<span dir="rtl">،</span> $`n`$
<span dir="rtl">في **طرق**</span> **n-Step** <span dir="rtl">و</span>λ
<span dir="rtl">في **خوارزمية العائد**</span> **λ <span dir="rtl">غير
المتصلة</span> (Offline λ-Return Algorithm)**<span dir="rtl">.</span>

<img src="./media/image141.png"
style="width:6.26806in;height:2.63264in" />

<span dir="rtl">الشكل 12.3: نتائج السير العشوائي ذات الـ 19 حالة (المثال
7.1): أداء **خوارزمية العائد**</span> **λ <span dir="rtl">غير
المتصلة</span> (Offline λ-Return Algorithm)** <span dir="rtl">إلى جانب
**طرق**</span> **TD <span dir="rtl">ذات الخطوات</span>** $`\mathbf{n}`$
**<span dir="rtl">(</span>n-Step <span dir="rtl"></span>TD
Methods<span dir="rtl">)</span>**. <span dir="rtl">في كلا الحالتين، قدمت
القيم المتوسطة **لمعامل البوتسترابينغ**</span>
**<span dir="rtl">(</span>Bootstrapping
<span dir="rtl"></span>Parameter<span dir="rtl">)</span>**
<span dir="rtl"></span>$`\lambda`$ <span dir="rtl">أو</span> $`n`$
<span dir="rtl">أفضل أداء. النتائج مع **خوارزمية العائد**</span> **λ
<span dir="rtl">غير المتصلة  
(</span>Offline λ-Return Algorithm<span dir="rtl">)</span>**
<span dir="rtl">كانت أفضل قليلاً عند أفضل قيم</span> **α
<span dir="rtl">و</span>λ**<span dir="rtl">، وعند القيم العالية
لـ</span> **α**<span dir="rtl">.</span>

<span dir="rtl">النهج الذي اتبعناه حتى الآن هو ما نسميه **المنظور
النظري** </span>**(Theoretical View)**<span dir="rtl">، أو **المنظور
الأمامي**</span> **(Forward View)** **<span dir="rtl">لخوارزمية
التعليم</span> (Learning Algorithm)**<span dir="rtl">.</span>
<span dir="rtl">لكل **حالة تمت زيارتها** </span>**(State
Visited)**<span dir="rtl">، ننظر إلى الأمام في الزمن إلى جميع المكافآت
المستقبلية ونقرر كيفية دمجها بأفضل طريقة. يمكننا أن نتخيل أنفسنا نركب
**تيار الحالات** </span>**(Stream of States)**<span dir="rtl">، ننظر إلى
الأمام من كل **حالة**</span> **(State)** <span dir="rtl">لتحديد تحديثها،
كما هو موضح في الشكل 12.4. بعد النظر إلى الأمام من وتحديث **حالة واحدة**
</span>**(One State)**<span dir="rtl">، ننتقل إلى الحالة التالية ولا
نضطر إلى العمل مع الحالة السابقة مرة أخرى. أما **الحالات المستقبلية**
</span>**(Future States)**<span dir="rtl">، من ناحية أخرى، فيتم مشاهدتها
ومعالجتها بشكل متكرر، مرة واحدة من كل نقطة مراقبة تسبقها</span>.

<img src="./media/image142.png"
style="width:6.26806in;height:1.98611in" />

<span dir="rtl">الشكل 12.4:</span> **<span dir="rtl">المنظور
الأمامي</span> (Forward View)**<span dir="rtl">.</span>
<span dir="rtl">نقرر كيفية تحديث كل **حالة**</span> **(State)**
<span dir="rtl">من خلال النظر إلى الأمام نحو **المكافآت**</span>
**(Rewards)** <span dir="rtl">و**الحالات المستقبلية** </span>**(Future
States)**<span dir="rtl">.</span>

12.2 <span dir="rtl">خوارزمية</span> TD(λ)

<span dir="rtl">تُعتبر **خوارزمية**</span> **TD(λ)**
<span dir="rtl">واحدة من أقدم وأوسع **الخوارزميات**</span>
**(Algorithms)** <span dir="rtl">استخدامًا في **التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">.</span>
<span dir="rtl">كانت أول **خوارزمية**</span> **(Algorithm)**
<span dir="rtl">تُظهر وجود علاقة رسمية بين **المنظور النظري**</span>
**(Theoretical Forward View)** <span dir="rtl">الأكثر نظرية و**المنظور
الخلفي** </span>**(Backward View)** <span dir="rtl">الأكثر توافقًا مع
الحسابات باستخدام **تتبعات الأهلية** </span>**(Eligibility
Traces)**<span dir="rtl">. هنا سنظهر بشكل تجريبي أنها تقارب **خوارزمية
العائد**</span> **λ <span dir="rtl">غير المتصلة (</span>Offline λ-Return
<span dir="rtl"></span>Algorithm<span dir="rtl">)
</span>**<span dir="rtl">التي قدمناها في القسم السابق</span>.

<span dir="rtl">تحسن **خوارزمية**</span> **TD(λ)** <span dir="rtl">على
**خوارزمية العائد**</span> **λ <span dir="rtl">غير المتصلة</span>
<span dir="rtl">(</span>Offline λ-Return
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**
<span dir="rtl">بثلاث طرق. أولاً، تقوم بتحديث **متجه الوزن**</span>
**(Weight Vector)** <span dir="rtl">في كل خطوة من **الحلقة**</span>
**(Episode)** <span dir="rtl">بدلاً من التحديث فقط في النهاية، وبالتالي
قد تكون تقديراتها أفضل بشكل أسرع. ثانيًا، تكون حساباتها موزعة بالتساوي
على طول الوقت بدلاً من أن تتم جميعها في نهاية **الحلقة**
</span>**(Episode)**<span dir="rtl">. ثالثًا، يمكن تطبيقها على **المشاكل
المستمرة**</span> **(Continuing Problems)** <span dir="rtl">وليس فقط على
**المشاكل الحلقية** </span>**(Episodic
Problems)**<span dir="rtl">.</span> <span dir="rtl">في هذا القسم نقدم
النسخة شبه التدرجية **لخوارزمية** </span>**TD(λ)** <span dir="rtl">مع
**تقريب الدوال** </span>**(Function
Approximation)**<span dir="rtl">.</span>

<span dir="rtl">مع **تقريب الدوال** </span>**(Function
Approximation)**<span dir="rtl">، يكون **متجه تتبع الأهلية**</span>
**<span dir="rtl">(</span>Eligibility Trace
<span dir="rtl"></span>Vector<span dir="rtl">)</span>**
<span dir="rtl">هو متجه</span> $`zt \in Rd`$ <span dir="rtl">يحتوي على
نفس عدد مكونات **متجه الوزن** </span>**(Weight Vector)**
<span dir="rtl"></span>$`wt`$​ <span dir="rtl">بينما يعتبر **متجه
الوزن**</span> **(Weight Vector)** <span dir="rtl">ذاكرة طويلة المدى،
تتراكم على مدى عمر النظام، فإن **متتبع الأهلية**</span> **(Eligibility
Trace)** <span dir="rtl">هو ذاكرة قصيرة المدى، تدوم عادةً لفترة أقصر من
طول **الحلقة** </span>**(Episode)**<span dir="rtl">. **تتبعات
الأهلية**</span> **(Eligibility Traces)** <span dir="rtl">تساعد في عملية
التعليم؛ تأثيرها الوحيد هو أنها تؤثر على **متجه الوزن** </span>**(Weight
Vector)**<span dir="rtl">، ثم يقوم **متجه الوزن**</span> **(Weight
Vector)** <span dir="rtl">بتحديد **القيمة المقدرة** </span>**(Estimated
Value)**<span dir="rtl">.</span>

<span dir="rtl">في **خوارزمية** </span>**TD(λ)**<span dir="rtl">، يتم
تهيئة **متجه تتبع الأهلية**</span> **(Eligibility Trace Vector)**
<span dir="rtl">إلى الصفر في بداية **الحلقة**
</span>**(Episode)**<span dir="rtl">، ويتم زيادته في كل خطوة زمنية
بواسطة **تدرج القيمة  
(**</span>**Value Gradient<span dir="rtl">)</span>**<span dir="rtl">، ثم
يبدأ في التلاشي بواسطة</span> λ<span dir="rtl">:</span>

``` math
{z_{- 1} = 0,\quad
}
{z_{t} = \gamma\lambda z_{t - 1} + \nabla\widehat{v}\left( S_{t},w_{t} \right),\quad 0 \leq t \leq T}
```

<span dir="rtl">حيث أن</span> γ <span dir="rtl">هو **معدل الخصم**</span>
**(Discount Rate)** <span dir="rtl">و</span>λ <span dir="rtl">هو
**البارامتر**</span> **(Parameter)** <span dir="rtl">الذي تم تقديمه في
القسم السابق، والذي سنطلق عليه من الآن فصاعدًا **بارامتر تلاشي
التتبع**</span> **<span dir="rtl">(</span>Trace-Decay
<span dir="rtl"></span>Parameter<span dir="rtl">)</span>**.
<span dir="rtl">يحتفظ **تتبع الأهلية**</span> **(Eligibility Trace)**
<span dir="rtl">بتتبع مكونات **متجه الوزن  
(**</span>**Weight Vector<span dir="rtl">)</span>** <span dir="rtl">التي
ساهمت، إيجابيًا أو سلبيًا، في التقييمات الأخيرة **للحالات  
(**</span>**State Valuations<span dir="rtl">)</span>**<span dir="rtl">،
حيث يتم تعريف "الأخيرة" من حيث</span>
$`\gamma\lambda`$<span dir="rtl">.</span> <span dir="rtl">(تذكر أنه في
**تقريب الدوال الخطي** </span>**(Linear Function
Approximation)**<span dir="rtl">، يكون</span> $`\nabla v\hat{}(St,wt)`$
<span dir="rtl">هو نفسه **متجه الميزات  
(**</span>**Feature
Vector<span dir="rtl">)</span>**<span dir="rtl">،</span>
$`xt`$​<span dir="rtl">، وفي هذه الحالة يكون **متجه تتبع الأهلية**</span>
**<span dir="rtl">(</span>Eligibility Trace
<span dir="rtl"></span>Vector<span dir="rtl">)
</span>**<span dir="rtl">مجرد مجموع من **متجهات المدخلات**</span>
**(Input Vectors)** <span dir="rtl">المتلاشية من الماضي). يُقال إن التتبع
يشير إلى أهلية كل مكون من **متجه الوزن**</span> **(Weight Vector)**
<span dir="rtl">للخضوع لتغييرات التعليم في حالة حدوث **حدث تعزيز**
</span>**(Reinforcing Event)**<span dir="rtl">.</span>
**<span dir="rtl">الأحداث المعززة</span> (Reinforcing Events)**
<span dir="rtl">التي نهتم بها هي **أخطاء**</span> **TD
<span dir="rtl">ذات الخطوة الواحدة</span> (One-Step TD Errors)**
<span dir="rtl">اللحظية.</span> **TD <span dir="rtl">خطأ</span> (TD
Error)** <span dir="rtl">للتنبؤ بقيمة الحالة هو</span>

``` math
\delta_{t} = R_{t + 1} + \gamma\widehat{v}\left( S_{t + 1},w_{t} \right) - \widehat{v}\left( S_{t},w_{t} \right)
```

<span dir="rtl">في **خوارزمية** </span>**TD(λ)**<span dir="rtl">، يتم
تحديث **متجه الوزن**</span> **(Weight Vector)** <span dir="rtl">في كل
خطوة بما يتناسب مع</span> **TD <span dir="rtl">الخطأ القياسي</span>
(Scalar TD Error)** <span dir="rtl">و**متجه تتبع الأهلية**
</span>**(Vector Eligibility Trace)**<span dir="rtl">:</span>

``` math
w_{t + 1} = w_{t} + \alpha\delta_{t}z_{t}
```

**<span dir="rtl">خوارزمية</span> TD(λ) <span dir="rtl">شبه
التدرجية</span> (Semi-Gradient TD(λ))** <span dir="rtl">لتقدير</span>

<img src="./media/image126.png"
style="width:6.26806in;height:3.60347in" />

<img src="./media/image143.png"
style="width:6.26806in;height:2.49931in" />

<span dir="rtl">الشكل 12.5:</span> **<span dir="rtl">المنظور الخلفي أو
الآلي</span> (Backward or Mechanistic View)**
<span dir="rtl">لخوارزمية</span> TD(λ)<span dir="rtl">.</span>
<span dir="rtl">يعتمد كل تحديث على **خطأ**</span> **TD
<span dir="rtl">الحالي</span> (Current TD Error)**
<span dir="rtl">المدمج مع **تتبعات الأهلية الحالية**</span> **(Current
Eligibility Traces)** <span dir="rtl">للأحداث الماضية</span>.

<span dir="rtl">تتوجه **خوارزمية**</span> **TD(λ)** <span dir="rtl">إلى
الخلف في الزمن. في كل لحظة ننظر إلى **خطأ**</span> **TD
<span dir="rtl">الحالي  
(</span>Current TD Error<span dir="rtl">)
</span>**<span dir="rtl">ونُسنده إلى كل **حالة سابقة**</span> **(Prior
State)** <span dir="rtl">وفقًا لمقدار مساهمة تلك **الحالة**</span>
**(State)** <span dir="rtl">في **تتبع الأهلية الحالي**</span> **(Current
Eligibility Trace)** <span dir="rtl">في ذلك الوقت. يمكننا أن نتخيل
أنفسنا نركب على **تيار الحالات** </span>**(Stream of
States)**<span dir="rtl">، نحسب</span> **TD <span dir="rtl">أخطاء</span>
(TD Errors)**<span dir="rtl">، ونرسلها إلى **الحالات السابقة**</span>
**(Previously Visited States)** <span dir="rtl">كما هو مقترح في الشكل
12.5. حيثما يتلاقى</span> **TD <span dir="rtl">خطأ</span> (TD Error)**
<span dir="rtl">مع **التتبعات** </span>**(Traces)**<span dir="rtl">،
نحصل على التحديث المعطى بالمعادلة (12.7)، حيث تتغير قيم تلك **الحالات
السابقة**</span> **(Past States)** <span dir="rtl">عندما تحدث مرة أخرى
في المستقبل</span>.

<span dir="rtl">لفهم أفضل **للمنظر الخلفي**</span> **(Backward View)**
<span dir="rtl">لـ **خوارزمية** </span>**TD(λ)**<span dir="rtl">، فكر
فيما يحدث عند قيم مختلفة لـ</span> λ<span dir="rtl">.</span>
<span dir="rtl">إذا كان</span> $`\lambda = 0`$<span dir="rtl">، فإن
المعادلة (12.5) تشير إلى أن **التتبع**</span> **(Trace)**
<span dir="rtl">عند</span> $`t`$ <span dir="rtl">هو بالضبط **تدرج
القيمة**</span> **(Value Gradient)** <span dir="rtl">المقابل لـ</span>
St​<span dir="rtl">.</span> <span dir="rtl">وبالتالي، يتقلص تحديث
**خوارزمية**</span> **TD(λ)** <span dir="rtl">(المعادلة 12.7) إلى
تحديث</span> **TD <span dir="rtl">شبه التدرجي ذو الخطوة الواحدة</span>
<span dir="rtl">(</span>One-Step Semi-Gradient TD
<span dir="rtl"></span>Update<span dir="rtl">)
</span>**<span dir="rtl">الذي تم تناوله في الفصل 9</span>
<span dir="rtl">(وفي الحالة الجدولية، إلى قاعدة</span> TD
<span dir="rtl">البسيطة  
(المعادلة 6.2)).</span> <span dir="rtl">هذا هو السبب في أن تلك
الخوارزمية كانت تسمى</span> **TD(0)**<span dir="rtl">.</span>
<span dir="rtl">من حيث الشكل 12.5،</span> **TD(0)** <span dir="rtl">هي
الحالة التي تتغير فيها **الحالة الوحيدة السابقة (**</span>**Only the One
State Preceding <span dir="rtl"></span>the Current
One<span dir="rtl">)</span>** <span dir="rtl">فقط بواسطة</span> **TD
<span dir="rtl">خطأ</span> (TD Error)**<span dir="rtl">.</span>
<span dir="rtl">بالنسبة للقيم الأكبر لـ</span>
$`\lambda`$<span dir="rtl">، ولكن لا تزال</span>
$`\lambda < 1`$<span dir="rtl">، تتغير **المزيد من الحالات السابقة**
</span>**(More of the Preceding States)**<span dir="rtl">، ولكن كل
**حالة بعيدة زمنيًا**</span> **(Temporally Distant State)**
<span dir="rtl">تتغير بشكل أقل لأن **تتبع الأهلية المقابل**
</span>**(Corresponding Eligibility Trace)** <span dir="rtl">يكون أصغر،
كما هو موضح في الشكل. نقول إن **الحالات السابقة**</span> **(Earlier
States)** <span dir="rtl">تحصل على رصيد أقل</span> **TD
<span dir="rtl">لخطأ</span> (TD Error)**<span dir="rtl">.</span>

<span dir="rtl">إذا كان</span> $`\lambda = 1`$<span dir="rtl">، فإن
الرصيد الممنوح **للحالات السابقة**</span> **(Earlier States)**
<span dir="rtl">يتناقص فقط بمقدار</span> $`\gamma`$ <span dir="rtl">لكل
خطوة. يتضح أن هذا هو الشيء الصحيح لتحقيق سلوك **مونت كارلو**
</span>**(Monte Carlo Behavior)**<span dir="rtl">. على سبيل المثال، تذكر
أن</span> **TD <span dir="rtl">خطأ</span> (TD
Error)**<span dir="rtl">،</span> $`\delta t`$​<span dir="rtl">، يتضمن
مصطلحًا غير مخصوم</span> $`R_{t + 1}`$ <span dir="rtl">عند تمريره إلى
الخلف</span> $`k`$ <span dir="rtl">خطوات، يحتاج إلى أن يُخصم، مثل أي
مكافأة في **العائد** </span>**(Return)**<span dir="rtl">، بمقدار</span>
$`\gamma k`$<span dir="rtl">، وهو بالضبط ما يحققه **تتبع الأهلية
المتناقص (**</span>**Falling Eligibility
<span dir="rtl"></span>Trace<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">إذا كان</span> $`\lambda = 1`$
<span dir="rtl">و</span>$`\gamma = 1`$<span dir="rtl">، فإن **تتبعات
الأهلية**</span> **(Eligibility Traces)** <span dir="rtl">لا تتلاشى على
الإطلاق مع الوقت. في هذه الحالة، تتصرف الطريقة مثل **طريقة مونت
كارلو**</span> **<span dir="rtl">(</span>Monte Carlo
<span dir="rtl"></span>Method<span dir="rtl">)
</span>**<span dir="rtl">لمهمة غير مخصومة وحلقية. إذا كان</span>
$`\lambda = 1`$<span dir="rtl">، فإن **الخوارزمية**</span>
**(Algorithm)** <span dir="rtl">تُعرف أيضًا باسم</span>
**TD(1)**<span dir="rtl">.</span>

<span dir="rtl">تُعد</span> **TD(1)** <span dir="rtl">طريقة لتنفيذ
**خوارزميات مونت كارلو**</span> **(Monte Carlo Algorithms)**
<span dir="rtl">تكون أكثر عمومية من تلك التي تم تقديمها سابقًا وتزيد بشكل
كبير من نطاق تطبيقها. في حين كانت **طرق مونت كارلو السابقة**</span>
**(Earlier Monte Carlo Methods)** <span dir="rtl">محدودة بالمهام
الحلقية، يمكن تطبيق</span> **TD(1)** <span dir="rtl">على المهام المستمرة
المخصومة أيضًا. علاوة على ذلك، يمكن تنفيذ</span> **TD(1)**
<span dir="rtl">بشكل تدريجي وعلى الإنترنت. أحد عيوب **طرق مونت
كارلو**</span> **(Monte Carlo Methods)** <span dir="rtl">هو أنها لا
تتعلم شيئًا من **الحلقة**</span> **(Episode)** <span dir="rtl">حتى تنتهي.
على سبيل المثال، إذا قامت **طريقة مونت كارلو للتحكم  
(**</span>**Monte Carlo Control Method<span dir="rtl">)</span>**
<span dir="rtl">باتخاذ إجراء يؤدي إلى مكافأة سيئة للغاية ولكنه لا ينهي
**الحلقة** </span>**(Episode)**<span dir="rtl">، فإن ميل الوكيل إلى
تكرار هذا الإجراء سيظل دون تغيير خلال **الحلقة**
</span>**(Episode)**<span dir="rtl">. من ناحية أخرى، تتعلم</span>
**TD(1) <span dir="rtl">عبر الإنترنت</span> (Online TD(1))**
<span dir="rtl">بطريقة</span> **TD <span dir="rtl">ذات  
</span>**$`\mathbf{n}`$ **<span dir="rtl">الخطوات</span> (n-Step TD)**
<span dir="rtl">من **الحلقة الجارية غير المكتملة**</span>
**<span dir="rtl">(</span>Incomplete Ongoing
<span dir="rtl"></span>Episode<span dir="rtl">)</span>**<span dir="rtl">،
حيث تكون</span> n <span dir="rtl"></span> <span dir="rtl">خطوات تمتد إلى
الخطوة الحالية. إذا حدث شيء جيد أو سيء بشكل غير عادي خلال **الحلقة**
</span>**(Episode)**<span dir="rtl">، يمكن **طرق التحكم المعتمدة
على**</span> **TD(1) <span dir="rtl">(</span>Control Methods
<span dir="rtl"></span>Based on TD(1)<span dir="rtl">)</span>**
<span dir="rtl">أن تتعلم على الفور وتغير سلوكها في نفس **الحلقة**
</span>**(Episode)**<span dir="rtl">.</span>

<span dir="rtl">من المفيد العودة إلى مثال السير العشوائي ذو الـ 19 حالة
(المثال 7.1) لنرى مدى نجاح</span> **TD(λ)** <span dir="rtl">في تقريب
**خوارزمية العائد**</span> **λ <span dir="rtl">غير المتصلة</span>
(Offline λ-Return Algorithm)**<span dir="rtl">.</span>
<span dir="rtl">تُظهر النتائج لكلا **الخوارزميتين**</span>
**(Algorithms)** <span dir="rtl">في الشكل 12.6. لكل قيمة</span>
$`\lambda`$<span dir="rtl">، إذا تم اختيار</span> $`\alpha`$
<span dir="rtl">بشكل مثالي لها  
(أو أصغر)، فإن **الخوارزميتين**</span> **(Both Algorithms)**
<span dir="rtl">تؤديان بشكل متطابق تقريبًا. ومع ذلك، إذا تم اختيار</span>
$`\alpha`$ <span dir="rtl"></span> <span dir="rtl">أكبر من المثالي،
فإن</span> $`\mathbf{\lambda}`$ **<span dir="rtl">خوارزمية العائد</span>
(λ-Return Algorithm)** <span dir="rtl">تكون أسوأ قليلاً فقط في حين
أن</span> **TD(λ)** <span dir="rtl">يكون أسوأ بكثير وقد يكون غير مستقر.
هذا ليس كارثيًا بالنسبة  
**لـ**</span> **TD(λ)** <span dir="rtl">في هذه المشكلة، لأن هذه القيم
العالية من **البارامتر**</span> **(Parameter)** <span dir="rtl">ليست ما
يرغب المرء في استخدامه على أي حال، ولكن بالنسبة لمشاكل أخرى يمكن أن يكون
ذلك ضعفًا كبيرًا</span>.

<img src="./media/image144.png"
style="width:6.26806in;height:2.625in" />

<span dir="rtl">الشكل 12.6: نتائج السير العشوائي ذات الـ 19 حالة (المثال
7.1): أداء **خوارزمية**</span> **TD(λ)** <span dir="rtl">إلى جانب
**خوارزمية العائد**</span> $`\mathbf{\lambda}`$ **<span dir="rtl">غير
المتصلة</span> (Offline λ-Return Algorithm)**<span dir="rtl">.</span>
<span dir="rtl">أدت **الخوارزميتان  
(**</span>**The Two Algorithms<span dir="rtl">)</span>**
<span dir="rtl">بشكل متطابق تقريبًا عند قيم</span> $`\alpha`$
<span dir="rtl">المنخفضة (أقل من المثالية)، ولكن</span> **TD(λ)**
<span dir="rtl">كان أداؤها أسوأ عند قيم</span> $`\alpha`$
<span dir="rtl">المرتفعة</span>.

<span dir="rtl">تم إثبات أن **خوارزمية**</span> **TD(λ)
<span dir="rtl">الخطية</span> (Linear TD(λ))** <span dir="rtl">تتقارب في
حالة **داخل السياسة  
(**</span>**On-Policy<span dir="rtl">) </span>**<span dir="rtl">إذا تم
تقليل **بارامتر حجم الخطوة**</span> **(Step-Size Parameter)**
<span dir="rtl">بمرور الوقت وفقًا للشروط المعتادة (المعادلة 2.7). تمامًا
كما نوقش في القسم 9.4، فإن التقارب ليس إلى **متجه الوزن الأدنى خطأً**
</span>**(Minimum-Error Weight Vector)**<span dir="rtl">، بل إلى **متجه
وزن** </span>**(Weight Vector)** <span dir="rtl">قريب يعتمد على</span>
$`\lambda`$<span dir="rtl">.</span> <span dir="rtl">يمكن الآن تعميم الحد
الأدنى لجودة الحل المقدم في ذلك القسم (المعادلة 9.14) ليشمل أي</span>
$`\lambda`$<span dir="rtl">.</span> <span dir="rtl">بالنسبة لحالة
**المشاكل المستمرة المخصومة** </span>**(Continuing Discounted
Case)**<span dir="rtl">،</span>

``` math
\overline{\text{VE}}\left( w_{1} \right) \leq \frac{1 - \gamma\lambda}{1 - \gamma}\min_{w}\overline{\text{VE}}(w)
```

<span dir="rtl">بمعنى أن **الخطأ المستمر**</span> **(Asymptotic Error)**
<span dir="rtl">لا يزيد عن</span> $`1 - \gamma\lambda`$
<span dir="rtl">من **أصغر خطأ ممكن** </span>**(Smallest Possible
Error)**<span dir="rtl">. ومع اقتراب</span> $`\lambda`$
<span dir="rtl">من 1، يقترب الحد الأدنى من **الخطأ الأدنى**
</span>**(Minimum Error)** <span dir="rtl">(ويكون الأكثر تباعدًا
عند</span> $`\lambda = 0`$<span dir="rtl">)</span>. <span dir="rtl">ومع
ذلك، في الممارسة العملية، غالبًا ما يكون</span> $`\lambda = 1`$
<span dir="rtl">هو أسوأ اختيار، كما سيتضح لاحقًا في الشكل 12.14</span>.

<span dir="rtl">**تمرين 12.3** بعض البصيرة حول كيفية تمكن
**خوارزمية**</span> **TD(λ)** <span dir="rtl">من تقريب **خوارزمية
العائد**</span> **λ <span dir="rtl">غير المتصلة</span> (Offline λ-Return
Algorithm)** <span dir="rtl">يمكن اكتسابها برؤية أن حد الخطأ للأخيرة
(بين الأقواس في المعادلة 12.4) يمكن كتابته كمجموع **أخطاء**</span> **TD
(TD Errors)** <span dir="rtl">(المعادلة 12.6) لوزن ثابت</span>
$`w`$<span dir="rtl">.</span> <span dir="rtl">أظهر ذلك، باتباع نمط
المعادلة (6.6)، واستخدام العلاقة التكرارية</span> **λ
<span dir="rtl">للعائد</span> (λ-Return)** <span dir="rtl">التي حصلت
عليها في **تمرين 12.1**</span>. ⇤

<span dir="rtl">**تمرين 12.4** استخدم نتيجتك من التمرين السابق لتظهر أنه
إذا تم حساب تحديثات الأوزان عبر **الحلقة** </span>**(Episode)**
<span dir="rtl">في كل خطوة ولكن لم تُستخدم فعليًا لتغيير الأوزان
(ظل</span> $`w`$ <span dir="rtl">ثابتًا)، فإن مجموع تحديثات الأوزان
**لـ**</span> **TD(λ)** <span dir="rtl">سيكون هو نفسه مجموع تحديثات
**خوارزمية العائد**</span> $`\mathbf{\lambda}`$ **<span dir="rtl">غير
المتصلة  
(</span>Offline λ-Return
Algorithm<span dir="rtl">)</span>**<span dir="rtl">.</span>

**<u>12.3 <span dir="rtl">طرق العائد</span> λ <span dir="rtl">المقصوص ذو
الخطوات</span> (n-Step Truncated λ-Return Methods)</u>**

<span dir="rtl">تُعتبر **خوارزمية العائد**</span> $`\mathbf{\lambda}`$
**<span dir="rtl">غير المتصلة</span> (Offline λ-Return Algorithm)**
<span dir="rtl">مثالاً مهمًا، ولكنه ذو فائدة محدودة لأنه يستخدم</span>
$`\mathbf{\lambda}`$ **<span dir="rtl">العائد</span> (λ-Return)**
<span dir="rtl">(المعادلة 12.2)، الذي لا يُعرف إلا في نهاية **الحلقة**
</span>**(Episode)**<span dir="rtl">.</span> <span dir="rtl">في حالة
**المشاكل المستمرة** </span>**(Continuing Case)**<span dir="rtl">، فإن
**العائد** </span>**λ <span dir="rtl">  
(</span>λ-Return<span dir="rtl">) </span>**<span dir="rtl">من الناحية
الفنية لا يُعرف أبدًا، حيث يعتمد على **العوائد ذات الخطوات**
</span>$`\mathbf{n}`$ **<span dir="rtl">  
(</span>n-Step Returns<span dir="rtl">)</span>**
<span dir="rtl">لقيم</span> $`n`$ <span dir="rtl">كبيرة بشكل اعتباطي،
وبالتالي على مكافآت بعيدة في المستقبل بشكل اعتباطي. ومع ذلك، يصبح
الاعتماد أضعف مع المكافآت المتأخرة لفترات أطول، حيث يتناقص  
بمقدار</span> $`\gamma\lambda`$ <span dir="rtl">لكل خطوة من التأخير.
وبالتالي، سيكون من الطبيعي تقريب هذا المفهوم عبر قص السلسلة بعد عدد معين
من الخطوات. تقدم فكرتنا الحالية عن **العوائد ذات الخطوات**
</span>$`\mathbf{n}`$ **<span dir="rtl">  
</span>(n-Step Returns)** <span dir="rtl">طريقة طبيعية للقيام بذلك حيث
يتم استبدال المكافآت المفقودة بالقيم المقدرة</span>.

<span dir="rtl">بشكل عام، نُعرّف **العائد**</span> **λ
<span dir="rtl">المقصوص</span> (Truncated λ-Return)**
<span dir="rtl">للوقت</span> $`t`$<span dir="rtl">، باستخدام البيانات
المتاحة فقط حتى أفق زمني لاحق</span> $`h`$<span dir="rtl">، على
أنه</span>:

``` math
G_{t:h}^{\lambda} = (1 - \lambda)\sum_{n = 1}^{h - t - 1}{\lambda^{n - 1}G_{t:t + n}} + \lambda^{h - t - 1}G_{t:h},\quad 0 \leq t < h \leq T
```

<img src="./media/image145.png"
style="width:6.26806in;height:4.10417in" />

<span dir="rtl">الشكل 12.7:</span> **<span dir="rtl">مخطط النسخ
الاحتياطي لـ</span> TD(λ) <span dir="rtl">المقصوص</span>
<span dir="rtl">(</span>Backup Diagram for
<span dir="rtl"></span>Truncated TD(λ)<span dir="rtl">)</span>**.

<span dir="rtl">إذا قارنت هذه المعادلة مع</span> **λ
<span dir="rtl">العائد</span> (λ-Return)** <span dir="rtl">(المعادلة
12.3)، فمن الواضح أن الأفق</span> h <span dir="rtl">يلعب نفس الدور الذي
كان يلعبه سابقًا **الوقت** </span>$`\mathbf{T}`$<span dir="rtl">، وهو وقت
الإنهاء. بينما في</span> **λ <span dir="rtl">العائد</span> (λ-Return)**
<span dir="rtl">هناك وزن متبقي يُعطى</span> $`\mathbf{G\_ t}`$
**<span dir="rtl">للعائد التقليدي</span> (Conventional Return
G_t)**<span dir="rtl">، هنا يُعطى **لأطول عائد متاح ذو الخطوات**
</span>**n<span dir="rtl">،</span> G\_{t**

**} <span dir="rtl">(</span>Longest Available n-Step Return G\_{t**

**)**) <span dir="rtl">الشكل 12.2)</span>.

<span dir="rtl">يؤدي **العائد**</span> **λ
<span dir="rtl">المقصوص</span> (Truncated λ-Return)**
<span dir="rtl">فورًا إلى ظهور عائلة من **خوارزميات العائد** </span>**λ
<span dir="rtl"></span>**$`\mathbf{n}`$ **<span dir="rtl">ذو
الخطوات</span> (n-Step λ-Return Algorithms)** <span dir="rtl">المشابهة
**لطرق  **
</span> **n-Step <span dir="rtl">(</span>n-Step Methods<span dir="rtl">)
</span>**<span dir="rtl">في الفصل 7. في جميع هذه **الخوارزميات**
</span>**(Algorithms)**<span dir="rtl">، يتم تأخير التحديثات
بمقدار</span> $`n`$ <span dir="rtl">خطوات ولا تأخذ في الاعتبار سوى
أول</span> $`n`$ <span dir="rtl">مكافآت، ولكن الآن يتم تضمين جميع
**العوائد** </span>$`\mathbf{k}`$ **<span dir="rtl">ذات الخطوات</span>
(k-Step Returns)** <span dir="rtl">لكل</span> $`1 \leq k \leq n`$
<span dir="rtl">(بينما كانت **الخوارزميات السابقة ذات الخطوات**</span>
$`\mathbf{n}`$ **(Earlier n-Step Algorithms)** <span dir="rtl">تستخدم
فقط **العائد ذو الخطوات**</span> $`\mathbf{n}`$
**<span dir="rtl">(</span>n-Step
<span dir="rtl"></span>Return<span dir="rtl">)</span>**<span dir="rtl">)،
ويتم وزنها هندسيًا كما هو موضح في الشكل 12.2. في حالة **قيمة الحالة
(**</span>**State-Value)**<span dir="rtl">، تُعرف هذه العائلة من
**الخوارزميات**</span> **(Algorithms)** <span dir="rtl">باسم</span>
**TD(λ) <span dir="rtl">المقصوص</span> (Truncated
TD(λ))**<span dir="rtl">، أو</span>
$`\mathbf{TTD(\lambda)}`$<span dir="rtl">.</span> **<span dir="rtl">مخطط
النسخ الاحتياطي المركب</span> <span dir="rtl">(</span>Compound Backup
<span dir="rtl"></span>Diagram<span dir="rtl">)</span>**<span dir="rtl">،
الموضح في الشكل 12.7، مشابه لذلك الخاص **بـ**</span> **TD(λ)
<span dir="rtl">(الشكل 12.1)</span>** <span dir="rtl">باستثناء أن **أطول
تحديث مكون**</span> **(Longest Component Update)** <span dir="rtl">هو
بحد أقصى</span> $`n`$ <span dir="rtl">خطوات بدلاً من أن يستمر دائمًا حتى
نهاية **الحلقة** </span>**(Episode)**<span dir="rtl">.</span>
<span dir="rtl">يتم تعريف</span> **TTD(λ)** <span dir="rtl">بالمعادلة
التالية (راجع المعادلة 9.15)</span>:

``` math
w_{t + n} = w_{t + n - 1} + \alpha\left\lbrack G_{t:t + n}^{\lambda} - \widehat{v}\left( S_{t},w_{t + n - 1} \right) \right\rbrack\nabla\widehat{v}\left( S_{t},w_{t + n - 1} \right),\quad 0 \leq t < T
```

<span dir="rtl">يمكن تنفيذ هذه **الخوارزمية**</span> **(Algorithm)**
<span dir="rtl">بكفاءة بحيث لا تتزايد الحسابات لكل خطوة مع</span> $`n`$
<span dir="rtl">  
(على الرغم من أن الذاكرة بالطبع يجب أن تزداد). تمامًا كما هو الحال في
**طرق**</span> **TD <span dir="rtl">ذات الخطوات </span>**$`\mathbf{n}`$
**<span dir="rtl">(</span>n-Step TD
Methods<span dir="rtl">)</span>**<span dir="rtl">، لا يتم إجراء تحديثات
في أول</span> $`n - 1`$ <span dir="rtl">خطوة زمنية من كل **حلقة**
</span>**(Episode)**<span dir="rtl">، ويتم إجراء</span> $`n - 1`$
<span dir="rtl">تحديثات إضافية عند الإنهاء. يعتمد التنفيذ الفعال على
حقيقة أن **العائد**</span> $`\mathbf{\lambda}`$
**<span dir="rtl">ذو</span>** $`\mathbf{k}`$
**<span dir="rtl">الخطوات</span> (k-Step λ-Return)**
<span dir="rtl">يمكن كتابته بالضبط كما يلي</span>:

``` math
G_{t:t + k}^{\lambda} = \widehat{v}\left( S_{t},w_{t - 1} \right) + \sum_{i = t}^{t + k - 1}{(\gamma\lambda)^{i - t}\delta_{i}}
```

<span dir="rtl">عندما</span>

``` math
\delta_{t} = R_{t + 1} + \gamma\widehat{v}\left( S_{t + 1},w_{t} \right) - \widehat{v}\left( S_{t},w_{t - 1} \right)
```

<span dir="rtl">**تمرين 12.5** عدة مرات في هذا الكتاب (غالبًا في
التمارين)، أثبتنا أن **العوائد**</span> **(Returns)**
<span dir="rtl">يمكن كتابتها كمجموع</span> **TD
<span dir="rtl">أخطاء</span> (TD Errors)** <span dir="rtl">إذا تم تثبيت
**دالة القيمة** </span>**(Value Function)**<span dir="rtl">.</span>
<span dir="rtl">لماذا تعتبر المعادلة (12.10) مثالاً آخر على ذلك؟ أثبت
المعادلة</span> (12.10).

**<u>12.4 <span dir="rtl">إعادة التحديثات: خوارزمية العائد</span> λ
<span dir="rtl">عبر الإنترنت</span> (Online λ-Return Algorithm)</u>**

<span dir="rtl">اختيار</span> $`\mathbf{n}`$ **<span dir="rtl">معامل
القص</span> (Truncation Parameter n)** <span dir="rtl">في
**خوارزمية**</span> **TD(λ) <span dir="rtl">المقصوصة</span> (Truncated
TD(λ))** <span dir="rtl">ينطوي على مقايضة. يجب أن يكون</span> $`n`$
<span dir="rtl">كبيرًا بحيث تقارب الطريقة بشكل وثيق **خوارزمية
العائد**</span> $`\mathbf{\lambda}`$ **<span dir="rtl">غير
المتصلة</span> (Offline λ-Return Algorithm)**<span dir="rtl">، ولكنه يجب
أيضًا أن يكون صغيرًا بحيث يمكن إجراء التحديثات في وقت أقرب ويمكن أن تؤثر
على السلوك في وقت أقرب. هل يمكننا الحصول على الأفضل من كلا الجانبين؟
حسنًا، نعم، من حيث المبدأ يمكننا، وإن كان ذلك على حساب التعقيد
الحسابي</span>.

<span dir="rtl">الفكرة هي أنه، في كل خطوة زمنية عندما تجمع زيادة جديدة
من البيانات، تعود وتعيد جميع التحديثات منذ بداية **الحلقة الحالية**
</span>**(Current Episode)**<span dir="rtl">.</span>
<span dir="rtl">ستكون التحديثات الجديدة أفضل من تلك التي أجريتها سابقًا
لأنه يمكنها الآن أن تأخذ في الاعتبار البيانات الجديدة للخطوة الزمنية.
بمعنى أن التحديثات تكون دائمًا نحو هدف **العائد**</span>
$`\mathbf{\lambda}`$ **<span dir="rtl">المقصوص ذو الخطوات</span>**
$`\mathbf{n}`$ **<span dir="rtl">(</span>n-Step Truncated
<span dir="rtl">  
</span>λ-Return<span dir="rtl">)</span>**<span dir="rtl">، ولكنها تستخدم
دائمًا الأفق الزمني الأحدث. في كل مرور على تلك **الحلقة**
</span>**(Episode)**<span dir="rtl">، يمكنك استخدام أفق زمني أطول قليلاً
والحصول على نتائج أفضل قليلاً. تذكر أن **العائد** </span>**λ
<span dir="rtl">المقصوص</span> (Truncated λ-Return)**
<span dir="rtl">يُعرّف في المعادلة (12.9) على أنه</span>:

``` math
G_{t:h}^{\lambda} = (1 - \lambda)\sum_{n = 1}^{h - t - 1}{\lambda^{n - 1}G_{t:t + n}} + \lambda^{h - t - 1}G_{t:h}
```

<span dir="rtl">دعونا نمر بكيفية استخدام هذا الهدف بشكل مثالي إذا لم تكن
**التعقيدات الحسابية  
(**</span>**Computational
<span dir="rtl"></span>Complexity<span dir="rtl">)
</span>**<span dir="rtl">تمثل مشكلة. تبدأ **الحلقة**</span>
**(Episode)** <span dir="rtl">بتقدير في الوقت 0 باستخدام
**الأوزان**</span> **w_0 (Weights w_0)** <span dir="rtl">من نهاية
**الحلقة السابقة** </span>**(Previous Episode)**<span dir="rtl">.</span>
<span dir="rtl">يبدأ التعليم عندما يتم تمديد **الأفق الزمني
للبيانات**</span> **(Data Horizon)** <span dir="rtl">إلى الخطوة
الزمنية 1. يمكن أن يكون الهدف من التقدير في الخطوة 0، معطى البيانات حتى
الأفق 1، هو فقط **العائد ذو الخطوة الواحدة** </span>**G\_{0:1}
<span dir="rtl"></span>(One-Step Return G\_{0:1})**<span dir="rtl">،
والذي يتضمن</span> $`R1`$ <span dir="rtl"></span>​ <span dir="rtl">ويعتمد
على التقدير</span> v(S1,w0)<span dir="rtl">. لاحظ أن هذا هو بالضبط ما
يمثله</span> G0:1λ​<span dir="rtl">، حيث يتلاشى المجموع في الحد الأول من
المعادلة إلى الصفر. باستخدام هذا الهدف للتحديث، نقوم ببناء</span>
w1​<span dir="rtl">.</span> <span dir="rtl">ثم، بعد تقدم الأفق الزمني
للبيانات إلى الخطوة 2، ماذا نفعل؟ لدينا بيانات جديدة في شكل</span>
$`R2`$ <span dir="rtl">و</span>$`S2`$​<span dir="rtl">، وكذلك</span>
$`w1`$ <span dir="rtl"></span>​ <span dir="rtl">الجديد، لذا يمكننا الآن
بناء هدف تحديث أفضل</span> $`G0:2\lambda`$ <span dir="rtl">للتحديث الأول
من</span> $`S0`$ <span dir="rtl">وكذلك هدف تحديث أفضل</span> $`G1:2`$
<span dir="rtl">للتحديث الثاني من</span> $`S1`$<span dir="rtl">.
باستخدام هذه الأهداف المحسنة، نقوم بإعادة إجراء التحديثات عند</span>
$`S2`$​<span dir="rtl">، بدءًا من جديد من</span> $`w0`$​<span dir="rtl">،
لإننا</span>​ <span dir="rtl">الآن نقوم بتقديم الأفق إلى الخطوة 3 ونكرر،
بالعودة إلى الوراء لإنتاج ثلاثة أهداف جديدة، وإعادة تنفيذ جميع التحديثات
بدءًا من</span> $`w0`$ <span dir="rtl">الأصلي لإنتاج</span>
$`w3`$​<span dir="rtl">، وهكذا. في كل مرة يتم فيها تقديم الأفق، يتم إعادة
إجراء جميع التحديثات بدءًا من</span> $`w0`$ <span dir="rtl">باستخدام
**متجه الوزن**</span> **<span dir="rtl">(</span>Weight
<span dir="rtl"></span>Vector<span dir="rtl">)</span>**
<span dir="rtl">من الأفق السابق</span>.

<span dir="rtl">تشمل هذه **الخوارزمية المفاهيمية**</span> **(Conceptual
Algorithm)** <span dir="rtl">عدة تمريرات عبر **الحلقة**
</span>**(Episode)**<span dir="rtl">، واحدة عند كل أفق، حيث يتم إنشاء
تسلسل مختلف من **متجهات الوزن**</span> **<span dir="rtl">(</span>Weight
<span dir="rtl"></span>Vectors<span dir="rtl">)</span>**.
<span dir="rtl">لوصفها بوضوح، علينا التمييز بين **متجهات الوزن**</span>
**(Weight Vectors)** <span dir="rtl">المحسوبة عند الآفاق المختلفة. دعونا
نستخدم</span> $`wth`$ <span dir="rtl">للإشارة إلى **الأوزان**</span>
**(Weights)** <span dir="rtl">المستخدمة لتوليد القيمة في الوقت</span> t
<span dir="rtl">في التسلسل حتى الأفق</span>
$`h`$<span dir="rtl">.</span> **<span dir="rtl">متجه الوزن الأول</span>
w^h_0 <span dir="rtl">(</span>First Weight Vector w^h_0<span dir="rtl">)
</span>**<span dir="rtl">في كل تسلسل هو الذي يتم توريثه من **الحلقة
السابقة**</span> **(Previous Episode)** (<span dir="rtl">لذا فهي نفسها
لجميع الآفاق</span> $`h`$)<span dir="rtl">، و**متجه الوزن
الأخير**</span> **w^h_h (Last Weight Vector w^h_h)** <span dir="rtl">في
كل تسلسل يحدد **التسلسل النهائي لمتجه الوزن**</span> **(Ultimate
Weight-Vector Sequence)** <span dir="rtl">في **الخوارزمية**
</span>**(Algorithm)**<span dir="rtl">.</span> <span dir="rtl">عند الأفق
النهائي</span> $`h = T`$<span dir="rtl">، نحصل على **الأوزان
النهائية**</span> **w^T_T <span dir="rtl">(</span>Final Weights
w^T_T<span dir="rtl">) </span>**<span dir="rtl">التي سيتم تمريرها لتشكل
**الأوزان الأولية للحلقة التالية**</span>
**<span dir="rtl">(</span>Initial Weights <span dir="rtl"></span>of the
Next Episode<span dir="rtl">)</span>**. <span dir="rtl">مع هذه
الاصطلاحات، يمكن تحديد التسلسلات الثلاثة الأولى الموصوفة في الفقرة
السابقة بشكل صريح</span>:

``` math
{h = 1:w_{1}^{1} = w_{0}^{1} + \alpha\left\lbrack G_{0:1}^{\lambda} - \widehat{v}\left( S_{0},w_{0}^{1} \right) \right\rbrack\nabla\widehat{v}\left( S_{0},w_{0}^{1} \right),\quad
}
{h = 2:w_{1}^{2} = w_{0}^{2} + \alpha\left\lbrack G_{0:2}^{\lambda} - \widehat{v}\left( S_{0},w_{0}^{2} \right) \right\rbrack\nabla\widehat{v}\left( S_{0},w_{0}^{2} \right),
}{\, w_{2}^{2} = w_{1}^{2} + \alpha\left\lbrack G_{1:2}^{\lambda} - \widehat{v}\left( S_{1},w_{1}^{2} \right) \right\rbrack\nabla\widehat{v}\left( S_{1},w_{1}^{2} \right),
}
{\quad h = 3:w_{1}^{3} = w_{0}^{3} + \alpha\left\lbrack G_{0:3}^{\lambda} - \widehat{v}\left( S_{0},w_{0}^{3} \right) \right\rbrack\nabla\widehat{v}\left( S_{0},w_{0}^{3} \right),
}{\, w_{2}^{3} = w_{1}^{3} + \alpha\left\lbrack G_{1:3}^{\lambda} - \widehat{v}\left( S_{1},w_{1}^{3} \right) \right\rbrack\nabla\widehat{v}\left( S_{1},w_{1}^{3} \right),
}{\, w_{3}^{3} = w_{2}^{3} + \alpha\left\lbrack G_{2:3}^{\lambda} - \widehat{v}\left( S_{2},w_{2}^{3} \right) \right\rbrack\nabla\widehat{v}\left( S_{2},w_{2}^{3} \right)}
```

<span dir="rtl">الصيغة العامة للتحديث هي</span>

``` math
w_{t + 1}^{h} = w_{t}^{h} + \alpha\left\lbrack G_{t:h}^{\lambda} - \widehat{v}\left( S_{t},w_{t}^{h} \right) \right\rbrack\nabla\widehat{v}\left( S_{t},w_{t}^{h} \right),\quad 0 \leq t < h \leq T
```

<span dir="rtl">هذا التحديث، مع</span> $`wt = wth`$​<span dir="rtl">،
يُعرّف **خوارزمية العائد**</span> **λ <span dir="rtl">عبر الإنترنت</span>
<span dir="rtl">(</span>Online λ-Return
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**.
**<span dir="rtl">خوارزمية العائد</span> λ <span dir="rtl">عبر
الإنترنت</span> (Online λ-Return Algorithm)** <span dir="rtl">هي
خوارزمية تعمل بالكامل عبر الإنترنت، حيث تحدد **متجه الوزن الجديد**
</span>$`\mathbf{wt}`$**​ <span dir="rtl"></span>(New Weight Vector**
$`\mathbf{wt}`$ **​)** <span dir="rtl">في كل خطوة</span> t
<span dir="rtl">خلال **الحلقة** </span>**(Episode)**<span dir="rtl">،
باستخدام المعلومات المتاحة فقط في الوقت</span>
$`t`$<span dir="rtl">.</span> <span dir="rtl">العيب الرئيسي لهذه
الخوارزمية هو أنها معقدة حسابيًا، حيث تمر عبر الجزء الذي تم تجربته من
**الحلقة** </span>**(Episode)** <span dir="rtl">حتى الآن في كل خطوة.
لاحظ أنها أكثر تعقيدًا من **خوارزمية العائد**</span> **λ
<span dir="rtl">غير المتصلة  
(</span>Offline λ-Return
Algorithm<span dir="rtl">)</span>**<span dir="rtl">، التي تمر عبر جميع
الخطوات في وقت الإنهاء ولكن لا تقوم بأي تحديثات خلال **الحلقة**
</span>**(Episode)**<span dir="rtl">.</span> <span dir="rtl">في المقابل،
يمكن توقع أن تؤدي **الخوارزمية عبر الإنترنت** </span>**(Online
Algorithm)** <span dir="rtl">بشكل أفضل من **الخوارزمية غير المتصلة**
</span>**(Offline Algorithm)**<span dir="rtl">، ليس فقط خلال
**الحلقة**</span> **(Episode)** <span dir="rtl">عندما تقوم بتحديث بينما
لا تقوم **الخوارزمية غير المتصلة (**</span>**Offline
<span dir="rtl"></span>Algorithm<span dir="rtl">)
</span>**<span dir="rtl">بأي تحديث، ولكن أيضًا في نهاية **الحلقة**</span>
**(Episode)** <span dir="rtl">لأن **متجه الوزن  
المستخدم في البوتسترابينغ**</span> **(Weight Vector Used in
Bootstrapping)** <span dir="rtl">في</span> $`Gt:h\lambda`$
<span dir="rtl">قد حصل على عدد أكبر من التحديثات المفيدة. يمكن ملاحظة
هذا التأثير إذا نظرنا بعناية إلى الشكل 12.8، الذي يقارن بين
**الخوارزميتين**</span> **(The Two Algorithms)** <span dir="rtl">في
**مهمة السير العشوائي ذات الـ 19 حالة** </span>**(19-State Random Walk
Task)**<span dir="rtl">.</span>

<img src="./media/image146.png"
style="width:6.31099in;height:2.42121in" />

<span dir="rtl">الشكل 12.8: نتائج السير العشوائي ذات الـ 19 حالة (المثال
7.1): أداء **خوارزميات العائد**</span> **λ <span dir="rtl">عبر الإنترنت
وغير المتصلة</span> (Online and Offline λ-Return
Algorithms)**<span dir="rtl">.</span> **<span dir="rtl">مقياس
الأداء</span> (Performance Measure)** <span dir="rtl">هنا هو **الخطأ في
التقييم**</span> **(VE)** <span dir="rtl">في نهاية **الحلقة**</span>
**(Episode)**<span dir="rtl">، والذي من المفترض أن يكون الحالة المثلى
**للخوارزمية غير المتصلة** </span>**(Offline
Algorithm)**<span dir="rtl">.</span> <span dir="rtl">ومع ذلك، فإن
**الخوارزمية عبر الإنترنت**</span> **(Online Algorithm)**
<span dir="rtl">تؤدي بشكل أفضل بشكل طفيف.للمقارنة، خط</span>
$`\lambda = 0`$ <span dir="rtl"></span>

<span dir="rtl">هو نفسه لكلتا الطريقتين</span>.

**<u>12.5 <span dir="rtl">خوارزمية</span> TD(λ) <span dir="rtl">الحقيقية
عبر الإنترنت</span> (True Online TD(λ))</u>**

<span dir="rtl">الخوارزمية</span> -return <span dir="rtl">عبر
الإنترنت</span> " (Online " λ -return Algorithm)" <span dir="rtl">التي
تم تقديمها سابقًا هي حاليًا أفضل **خوارزمية تعليم تفاضلي زمني**</span>
**(Temporal Difference Algorithm)** <span dir="rtl">أداءً. إنها تمثل
المثالية التي تسعى **خوارزمية**</span> **TD(λ) <span dir="rtl">عبر
الإنترنت</span> (Online TD(λ) Algorithm)** <span dir="rtl">فقط إلى
تقليدها. ولكن، كما تم تقديمها، فإن **خوارزمية**</span> **"-return
<span dir="rtl">عبر الإنترنت</span> <span dir="rtl">(</span>Online
"-return <span dir="rtl"></span>Algorithm<span dir="rtl">)
</span>**<span dir="rtl">معقدة للغاية. هل هناك طريقة لعكس هذه الخوارزمية
التي تعتمد على **الرؤية الأمامية** </span>**(Forward-view)**
<span dir="rtl">لإنتاج **خوارزمية ذات رؤية خلفية فعالة**</span>
**<span dir="rtl">(</span>Efficient Backward-view
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**
<span dir="rtl">باستخدام **آثار الأهلية** </span>**(Eligibility
Traces)**<span dir="rtl">؟ يتضح أنه بالفعل هناك تنفيذ دقيق وملائم حسابيًا
لخوارزمية</span> "-return <span dir="rtl">عبر الإنترنت</span> (Online
"-return Algorithm) \*\* <span dir="rtl">في حالة **تقريب الدوال الخطية**
</span>**(Linear Function Approximation)**<span dir="rtl">.</span>
<span dir="rtl">يُعرف هذا التنفيذ بخوارزمية</span> TD(λ)
<span dir="rtl">الحقيقية عبر الإنترنت</span> (True Online TD(λ)
Algorithm) \*\* <span dir="rtl">لأنها "أصدق" لمثالية خوارزمية</span>
"-return <span dir="rtl">عبر الإنترنت</span> (Online "-return Algorithm)
\*\* <span dir="rtl">من خوارزمية</span> TD(λ) (TD(λ)
Algorithm)<span dir="rtl">.</span>

<span dir="rtl">اشتقاق **خوارزمية**</span> **TD(λ)
<span dir="rtl">الحقيقية عبر الإنترنت</span> (True Online TD(λ)
Algorithm)** <span dir="rtl">معقد بعض الشيء ليتم تقديمه هنا (انظر القسم
التالي والملحق في ورقة فان سيجن وآخرين، 2016)، ولكن استراتيجيتها بسيطة.
يمكن ترتيب تسلسل **متجهات الوزن**</span> **(Weight Vectors)**
<span dir="rtl">الناتجة عن **خوارزمية** </span>**"-return
<span dir="rtl">عبر الإنترنت</span> (Online "-return Algorithm)**
<span dir="rtl">في شكل مثلث</span>.

<img src="./media/image147.png"
style="width:4.1837in;height:2.55856in" />

<span dir="rtl">كل **صف**</span> **(Row)** <span dir="rtl">من هذا المثلث
يتم إنتاجه في كل خطوة زمنية. ويتضح أن **متجهات الوزن** </span>**(Weight
Vectors)** <span dir="rtl">على **القطر**
</span>**(Diagonal)**<span dir="rtl">، والتي يُرمز لها بـ</span>
$`wt`$​<span dir="rtl">، هي الوحيدة المطلوبة فعليًا. الأول،</span>
$`w0`$​<span dir="rtl">، هو **متجه الوزن الأولي**</span> **(Initial
Weight Vector)** <span dir="rtl">في الحلقة، والأخير،</span>
$`wT`$​<span dir="rtl">، هو **متجه الوزن النهائي**</span> **(Final Weight
Vector)**<span dir="rtl">، وكل **متجه وزن**</span> **(Weight Vector)**
<span dir="rtl">على طول الطريق،</span> $`wt`$​<span dir="rtl">، يلعب دورًا
في **التمهيد**</span> **(Bootstrapping)** <span dir="rtl">في **العوائد**
</span>**-n<span dir="rtl">خطوة</span> <span dir="rtl">(</span>n-step
<span dir="rtl"></span>Returns<span dir="rtl">)
</span>**<span dir="rtl">الخاصة بالتحديثات. في **الخوارزمية**</span>
**(Algorithm)** <span dir="rtl">النهائية، يتم إعادة تسمية **متجهات
الوزن**</span> **(Weight Vectors)** <span dir="rtl">على **القطر**</span>
**(Diagonal)** <span dir="rtl">بدون رمز علوي، بحيث تصبح</span>
$`wt = wt`$<span dir="rtl">.</span>

<span dir="rtl">تتمثل الاستراتيجية بعد ذلك في إيجاد طريقة مضغوطة وفعالة
لحساب كل</span> $`wt`$ <span dir="rtl">من **المتجه السابق**
</span>**(Previous Vector)**<span dir="rtl">.</span> <span dir="rtl">إذا
تم ذلك، وفي الحالة الخطية التي تكون فيها</span>
$`v\hat{}(s,w) = w\top x(s)`$<span dir="rtl">، فإننا نصل إلى
**الخوارزمية** </span>**TD(λ) <span dir="rtl">الحقيقية عبر
الإنترنت</span> (True Online TD(λ) Algorithm)**<span dir="rtl">.</span>

``` math
w_{t + 1} = w_{t} + \alpha\delta_{t}z_{t} + \alpha\left( w_{t}^{\top}x_{t} - w_{t - 1}^{\top}x_{t} \right)\left( z_{t} - x_{t} \right)
```

<span dir="rtl">حيث استخدمنا الاختصار</span>
$`xt = x(St)`$<span dir="rtl">، و</span> $`\delta t`$ <span dir="rtl">تم
تعريفها كما في</span> **TD(λ)** <span dir="rtl">(المعادلة 12.6)،
و</span> $`zt`$ <span dir="rtl">يتم تعريفها بواسطة</span>:

``` math
z_{t} = \gamma z_{t - 1} + \left( 1 - \alpha\gamma z_{t - 1}^{\top}x_{t} \right)x_{t}
```

<span dir="rtl">تم إثبات أن هذه **الخوارزمية**</span> **(Algorithm)**
<span dir="rtl">تنتج نفس تسلسل **متجهات الوزن** </span>**(Weight
Vectors)**<span dir="rtl">،</span> $`wt`$​<span dir="rtl">، حيث</span>
$`0 \leq t \leq T`$<span dir="rtl">، كما في **خوارزمية**</span>
**"-return <span dir="rtl">عبر الإنترنت (</span>Online "-return
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**
<span dir="rtl"></span>(van Seijen et al. 2016)<span dir="rtl">.
وبالتالي، فإن النتائج على مهمة المشي العشوائي على اليسار في الشكل 12.8
هي أيضًا نتائجها على تلك المهمة. ومع ذلك، أصبحت **الخوارزمية**
</span>**(Algorithm)** <span dir="rtl">الآن أقل تكلفة بكثير. متطلبات
الذاكرة في</span> **TD(λ) <span dir="rtl">الحقيقية عبر الإنترنت
(</span>True <span dir="rtl"></span>Online TD(λ)<span dir="rtl">)
</span>**<span dir="rtl">مطابقة لمتطلبات</span> **TD(λ)
<span dir="rtl">التقليدية</span> (Conventional TD(λ))**<span dir="rtl">،
بينما تزيد الحسابات لكل خطوة بحوالي 50% (هناك ناتج داخلي إضافي في تحديث
**أثر الأهلية (**</span>**Eligibility
<span dir="rtl"></span>Trace<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">بشكل عام، تبقى **التعقيد الحسابي لكل خطوة**</span>
**<span dir="rtl">(</span>Per-step Computational
<span dir="rtl"></span>Complexity<span dir="rtl">)
</span>**<span dir="rtl">من رتبة</span> $`O(d)`$<span dir="rtl">،
مثل</span> **TD(λ)**<span dir="rtl">.</span> <span dir="rtl">الكود
الزائف</span> (Pseudocode) <span dir="rtl">للخوارزمية الكاملة موجود في
الصندوق</span>.

<span dir="rtl">خوارزمية</span> **TD(λ) <span dir="rtl">الحقيقية عبر
الإنترنت</span> (True Online TD(λ))** <span dir="rtl">لتقدير</span>:

<img src="./media/image148.png"
style="width:6.26806in;height:4.35278in" />

**<span dir="rtl">أثر الأهلية</span> (Eligibility Trace)**
<span dir="rtl">المستخدم في</span> **TD(λ) <span dir="rtl">الحقيقية عبر
الإنترنت</span> <span dir="rtl">(</span>True Online
<span dir="rtl"></span>TD(λ))** <span dir="rtl">يُسمى **الأثر
الهولندي**</span> **(Dutch Trace)** <span dir="rtl">لتمييزه عن الأثر
المستخدم في</span> **TD(λ)<span dir="rtl">  
(المعادلة 12.5)</span>**<span dir="rtl">، والذي يُسمى **أثر التجميع**
</span>**(Accumulating Trace)**<span dir="rtl">.</span>
<span dir="rtl">العمل السابق غالبًا ما استخدم نوعًا ثالثًا من الأثر يسمى
**أثر الاستبدال** </span>**(Replacing Trace)**<span dir="rtl">، والذي تم
تعريفه فقط للحالة الجدولية أو لمتجهات الميزات الثنائية مثل تلك التي
تنتجها **الترميز بالتبليط** </span>**(Tile
Coding)**<span dir="rtl">.</span> <span dir="rtl">يتم تعريف **أثر
الاستبدال**</span> **(Replacing Trace)** <span dir="rtl">على أساس مكون
بمكون بناءً على ما إذا كان مكون من متجه الميزات يساوي 1 أو 0</span>.

<img src="./media/image149.png"
style="width:3.73366in;height:0.8084in" />

<span dir="rtl">في الوقت الحاضر، نرى أن **آثار الاستبدال**</span>
**(Replacing Traces)** <span dir="rtl">تعد تقريبًا تقريبيًا لـ **الآثار
الهولندية** </span>**(Dutch Traces)**<span dir="rtl">، التي تفوقها بشكل
كبير.</span> **<span dir="rtl">الآثار الهولندية</span> (Dutch Traces)**
<span dir="rtl">عادةً ما تؤدي بشكل أفضل من **آثار الاستبدال**</span>
**(Replacing Traces)** <span dir="rtl">ولها أساس نظري أكثر وضوحًا</span>.
**<span dir="rtl">آثار التجميع</span> (Accumulating Traces)**
<span dir="rtl">لا تزال محط اهتمام في حالات **تقريب الدوال غير الخطية**
</span>**(Nonlinear Function Approximation)** <span dir="rtl">حيث لا
تتوفر **الآثار الهولندية** </span>**(Dutch
Traces)**<span dir="rtl">.</span>

**<u>12.6 <span dir="rtl">الآثار الهولندية</span> (Dutch Traces)
<span dir="rtl">في تعلم مونت كارلور</span>(Monte Carlo Learning)</u>**

<span dir="rtl">على الرغم من أن **آثار الأهلية**</span> **(Eligibility
Traces)** <span dir="rtl">ترتبط تاريخيًا بشكل وثيق مع **التعليم التفاضلي
الزمني** </span>**(TD Learning)**<span dir="rtl">، إلا أنه في الواقع ليس
لها علاقة مباشرة به. في الحقيقة، تنشأ **آثار الأهلية**
</span>**(Eligibility Traces)** <span dir="rtl">حتى في **تعلم مونت
كارلو** </span>**(Monte Carlo Learning)**<span dir="rtl">، كما سنوضح في
هذا القسم. سنبين أن **الخوارزمية الخطية لمونت كارلو**</span> **(Linear
MC Algorithm)** <span dir="rtl">(في الفصل التاسع)، التي يتم اعتبارها
كرؤية أمامية</span> (Forward View)<span dir="rtl">، يمكن استخدامها
لاشتقاق **خوارزمية ذات رؤية خلفية**</span> **(Backward-view Algorithm)**
<span dir="rtl">مكافئة ولكنها أرخص من الناحية الحسابية باستخدام **الآثار
الهولندية** </span>**(Dutch Traces)**<span dir="rtl">.</span>
<span dir="rtl">هذه هي الحالة الوحيدة للمكافئة بين الرؤية الأمامية
والخلفية التي نوضحها بشكل صريح في هذا الكتاب. إنها تعطي لمحة عن إثبات
التكافؤ بين</span> **TD(λ) <span dir="rtl">الحقيقية عبر الإنترنت</span>
(True Online TD(λ))
<span dir="rtl"></span>**<span dir="rtl">وخوارزمية</span> **" λ -return
<span dir="rtl">عبر الإنترنت</span> <span dir="rtl">(</span>Online
"-return
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**<span dir="rtl">،
لكنها أبسط بكثير</span>.

<span dir="rtl">النسخة الخطية من **خوارزمية التنبؤ لمونت كارلو
التدرجي**</span> **<span dir="rtl">(</span>Gradient Monte Carlo
<span dir="rtl"></span>Prediction Algorithm<span dir="rtl">)
</span>**<span dir="rtl">(صفحة 202) تقوم بسلسلة من التحديثات التالية،
واحدة لكل خطوة زمنية من **الحلقة**
</span>**(Episode)**<span dir="rtl">.</span>

``` math
w_{t + 1} \doteq w_{t} + \alpha\left\lbrack G - w_{t}^{\top}x_{t} \right\rbrack x_{t},\quad 0 \leq t < T.
```

<span dir="rtl">لتبسيط المثال، نفترض هنا أن العائد</span> $`G`$
<span dir="rtl">هو مكافأة واحدة تُستلم في نهاية **الحلقة**</span>
**(Episode)** <span dir="rtl">(وهذا هو السبب في أن</span> $`G`$
<span dir="rtl">ليس لها رمز زمني) وأنه لا يوجد تخفيض في القيمة
الزمنية</span> (Discounting)<span dir="rtl">.</span> <span dir="rtl">في
هذه الحالة، يُعرف هذا التحديث أيضًا باسم قاعدة **أقل مربعات
المتوسط**</span> **<span dir="rtl">(</span>Least Mean Square,
<span dir="rtl"></span>LMS<span dir="rtl">)</span>**.
<span dir="rtl">كخوارزمية **مونت كارلو** </span>**(Monte Carlo
Algorithm)**<span dir="rtl">، تعتمد جميع التحديثات على المكافأة/العائد
النهائي، لذا لا يمكن تنفيذ أي تحديث حتى نهاية **الحلقة**
</span>**(Episode)**<span dir="rtl">.</span> <span dir="rtl">تعتبر
**خوارزمية** </span>**MC** <span dir="rtl">خوارزمية خارجية</span>
(Offline Algorithm)<span dir="rtl">، ولا نسعى إلى تحسين هذا الجانب منها.
بل نهدف فقط إلى تنفيذ هذه الخوارزمية بمزايا حسابية. سنستمر في تحديث
**متجه الوزن**</span> **(Weight Vector)** <span dir="rtl">فقط في نهاية
**الحلقة**</span> **(Episode)**<span dir="rtl">، ولكننا سنقوم ببعض
العمليات الحسابية خلال كل خطوة من **الحلقة** </span>**(Episode)**
<span dir="rtl">وتقليلها في نهايتها. سيؤدي ذلك إلى توزيع أكثر توازنًا
للحسابات—من رتبة</span> $`O(d)`$ <span dir="rtl">لكل خطوة—وسيؤدي أيضًا
إلى إزالة الحاجة إلى تخزين **متجهات الميزات**</span> **(Feature
Vectors)** <span dir="rtl">في كل خطوة لاستخدامها لاحقًا في نهاية كل
**حلقة** </span>**(Episode)**<span dir="rtl">.</span>
<span dir="rtl">بدلًا من ذلك، سنقوم بإدخال ذاكرة متجه إضافية، وهي **أثر
الأهلية** </span>**(Eligibility Trace)**<span dir="rtl">، نحتفظ فيه
بملخص لجميع **متجهات الميزات** </span>**(Feature Vectors)**
<span dir="rtl">التي تم مشاهدتها حتى الآن. سيكون هذا كافيًا لإعادة إنشاء
نفس التحديث الإجمالي بكفاءة كما هو الحال في سلسلة تحديثات **مونت
كارلو**</span> **(MC Updates)** <span dir="rtl">(المعادلة 12.13)، بحلول
نهاية **الحلقة** </span>**(Episode)**<span dir="rtl">.</span>

``` math
{w_{T} = w_{T - 1} + \alpha\left( G - w_{T - 1}^{\top}x_{T - 1} \right)x_{T - 1}
}
{= w_{T - 1} + \alpha x_{T - 1}\left( - x_{T - 1}^{\top}w_{T - 1} \right) + \alpha Gx_{T - 1}
}
{= \left( I - \alpha x_{T - 1}x_{T - 1}^{\top} \right)w_{T - 1} + \alpha Gx_{T - 1}}
```

<span dir="rtl">حيث أن</span>
$`F_{t} \doteq I - \alpha x_{t}x_{t}^{\top}`$ <span dir="rtl">هو
**مصفوفة النسيان أو التلاشي**</span>
**<span dir="rtl">(</span>Forgetting or Fading
<span dir="rtl"></span>Matrix<span dir="rtl">)</span>**.
<span dir="rtl">الآن، بالتكرار</span>...

``` math
= F_{T - 1}\left( F_{T - 2}w_{T - 2} + \alpha Gx_{T - 2} \right) + \alpha Gx_{T - 1}
```

``` math
= F_{T - 1}F_{T - 2}w_{T - 2} + \alpha G\left( F_{T - 1}x_{T - 2} + x_{T - 1} \right)
```

``` math
= F_{T - 1}F_{T - 2}\left( F_{T - 3}w_{T - 3} + \alpha Gx_{T - 3} \right) + \alpha G\left( F_{T - 1}x_{T - 2} + x_{T - 1} \right)
```

``` math
= F_{T - 1}F_{T - 2}F_{T - 3}w_{T - 3} + \alpha G\left( F_{T - 1}F_{T - 2}x_{T - 3} + F_{T - 1}x_{T - 2} + x_{T - 1} \right)
```

``` math
= \underset{a_{T - 1}}{\overset{F_{T - 1}F_{T - 2}\cdots F_{0}w_{0}}{︸}} + \alpha GF_{T - 1}\underset{z_{T - 1}}{\overset{\sum_{k = 0}^{T - 1}F_{T - 2}F_{T - 3}\cdots F_{k + 1}x_{k}}{︸}}
```

``` math
= a_{T - 1} + \alpha Gz_{T - 1}
```

<span dir="rtl">حيث أن</span> $`aT - 1`$ <span dir="rtl"></span>​
<span dir="rtl">و</span>$`zT - 1`$ <span dir="rtl"></span>​
<span dir="rtl">هما القيمتان عند الزمن</span> $`T - 1`$
<span dir="rtl">لاثنين من **متجهات الذاكرة المساعدة**
</span>**(Auxiliary Memory Vectors)** <span dir="rtl">التي يمكن تحديثها
بشكل تدريجي دون الحاجة إلى معرفة</span> $`G`$ <span dir="rtl">ومع
تعقيد</span> $`O(d)`$ <span dir="rtl">لكل خطوة زمنية.</span>
**<span dir="rtl">متجه</span>** $`\mathbf{zt}`$ <span dir="rtl">**هو**
في الواقع **أثر أهلية بأسلوب هولندي**</span>
**<span dir="rtl">(</span>Dutch-style <span dir="rtl"></span>Eligibility
Trace<span dir="rtl">)</span>**. <span dir="rtl">يتم تهيئته إلى</span>
$`z0 = x`$ <span dir="rtl"></span>​ <span dir="rtl">ومن ثم يتم تحديثه
وفقًا لـ</span>:

``` math
{z_{t} \doteq \sum_{k = 0}^{t}{F_{t}F_{t - 1}}\cdots F_{k + 1}x_{k},\quad 1 \leq t < T
}
{= \sum_{k = 0}^{t - 1}{F_{t}F_{t - 1}}\cdots F_{k + 1}x_{k} + x_{t}
}
{= F_{t}\sum_{k = 0}^{t - 1}{F_{t - 1}F_{t - 2}}\cdots F_{k + 1}x_{k} + x_{t}
}
{= F_{t}z_{t - 1} + x_{t}
}
{= \left( I - \alpha x_{t}x_{t}^{\top} \right)z_{t - 1} + x_{t}
}
{= z_{t - 1} - \alpha x_{t}x_{t}^{\top}z_{t - 1} + x_{t}
}
{= z_{t - 1} - \alpha\left( z_{t - 1}^{\top}x_{t} \right)x_{t} + x_{t}
}
{= z_{t - 1} + \left( 1 - \alpha z_{t - 1}^{\top}x_{t} \right)x_{t}}
```

... <span dir="rtl">والذي يُعتبر **الأثر الهولندي**</span> **(Dutch
Trace)** <span dir="rtl">للحالة عندما تكون</span> $`\lambda = 1`$
<span dir="rtl">(راجع المعادلة 12.11). يتم تهيئة **متجه المساعدة**
</span>**at <span dir="rtl"></span>(Auxiliary Vector at​)**
<span dir="rtl">إلى</span> $`a0 = w`$ <span dir="rtl">ثم يتم تحديثه وفقًا
لـ</span>:

``` math
a_{t} \doteq F_{t}F_{t - 1}\cdots F_{0}w_{0} = F_{t}a_{t - 1} = a_{t - 1} - \alpha x_{t}x_{t}^{\top}a_{t - 1},\quad 1 \leq t < T.
```

**<span dir="rtl">المتجهات المساعدة</span> (Auxiliary
Vectors)**<span dir="rtl">،</span> $`at`$ <span dir="rtl"></span>​
<span dir="rtl">و</span>$`zt`$​<span dir="rtl">، يتم تحديثها في كل خطوة
زمنية</span> $`t < Tt`$<span dir="rtl">، ثم عند الزمن</span> $`T`$
<span dir="rtl">عندما يتم ملاحظة</span> $`G`$<span dir="rtl">، يتم
استخدامهما في المعادلة (12.14) لحساب</span>
$`wT`$<span dir="rtl">.</span> <span dir="rtl">بهذه الطريقة، نحقق نفس
النتيجة النهائية تمامًا كما في **خوارزمية مونت كارلو/أقل مربعات المتوسط
(**</span>**MC/LMS
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**
<span dir="rtl">التي تتمتع بخصائص حسابية ضعيفة (المعادلة 12.13)، ولكن
الآن مع **خوارزمية تزايدية**</span> **(Incremental Algorithm)**
<span dir="rtl">يكون **تعقيد الزمن والذاكرة**</span>
**<span dir="rtl">(</span>Time and Memory
<span dir="rtl"></span>Complexity<span dir="rtl">)
</span>**<span dir="rtl">لكل خطوة فيها من رتبة</span>
$`O(d)`$<span dir="rtl">.</span> <span dir="rtl">هذا أمر مفاجئ ومثير
للاهتمام لأن مفهوم **أثر الأهلية**</span> **(Eligibility Trace)**
(<span dir="rtl">وبخاصة **الأثر الهولندي**</span> **(Dutch Trace)**)
<span dir="rtl">ظهر في سياق خارج **التعليم التفاضلي الزمني**</span>
**(Temporal Difference Learning)** (<span dir="rtl">على عكس ما ورد في  
(</span>van Seijen <span dir="rtl">و</span>Sutton<span dir="rtl">،
2014)</span>. <span dir="rtl">يبدو أن **آثار الأهلية**</span>
**(Eligibility Traces)** <span dir="rtl">ليست مخصصة لتعلم التفاضل الزمني
فقط؛ بل هي أكثر أساسية من ذلك. يبدو أن الحاجة إلى **آثار الأهلية**
</span>**(Eligibility Traces)** <span dir="rtl">تنشأ كلما حاول المرء
تعلم **التنبؤات طويلة المدى**</span> **<span dir="rtl">(</span>Long-term
<span dir="rtl"></span>Predictions<span dir="rtl">)
</span>**<span dir="rtl">بطريقة فعالة</span>.

**<u>12.7 <span dir="rtl"></span>(λ) <span dir="rtl">سارسا</span>
(Sarsa(λ))</u>**

<span dir="rtl">يتطلب توسيع **آثار الأهلية**</span> **(Eligibility
Traces)** <span dir="rtl">لتشمل طرق **قيم الإجراءات
(**</span>**Action-Value <span dir="rtl"></span>Methods<span dir="rtl">)
</span>**<span dir="rtl">القليل جدًا من التعديلات على الأفكار التي تم
تقديمها بالفعل في هذا الفصل. لتعلم قيم الإجراءات التقريبية</span>
$`q\hat{}(s,a,w)`$ <span dir="rtl">بدلاً من قيم الحالات التقريبية</span>
$`v\hat{}(s,w)`$<span dir="rtl">، نحتاج إلى استخدام صيغة **قيم
الإجراءات**</span> **(Action-Value Form)** <span dir="rtl">من
**العائد**</span> **n-<span dir="rtl">خطوة</span> (n-step
Return)**<span dir="rtl">، الواردة في الفصل العاشر</span>:

``` math
G_{t:t + n} \doteq R_{t + 1} + \cdots + \gamma^{n - 1}R_{t + n} + \gamma^{n}\widehat{q}\left( S_{t + n},A_{t + n},w_{t + n - 1} \right),\quad t + n < T
```

<span dir="rtl">مع</span>
$`G_{t:t + n} \doteq G_{t}\quad\text{if }t + n \geq T.`$
<span dir="rtl"></span> <span dir="rtl">إذا كان</span>
$`t + n \geq T`$<span dir="rtl">.</span> <span dir="rtl">باستخدام هذا،
يمكننا تشكيل **صيغة قيم الإجراءات**</span> **(Action-Value Form)**
<span dir="rtl">من **الإرجاع**</span> **λ <span dir="rtl">المقتطع</span>
(Truncated λ-Return)**<span dir="rtl">، التي تكون مماثلة لصيغة **قيم
الحالة**</span> **(State-Value Form)** <span dir="rtl">(المعادلة
12.9)</span>. **<span dir="rtl">صيغة قيم الإجراءات</span> (Action-Value
Form)** <span dir="rtl">لخوارزمية **الإرجاع**</span> **λ
<span dir="rtl">غير المتصل بالإنترنت (</span>Offline λ-Return
<span dir="rtl"></span>Algorithm<span dir="rtl">)
</span>**<span dir="rtl">(المعادلة 12.4) تستخدم ببساطة</span>
$`q\hat{}`$ <span dir="rtl"></span>​ <span dir="rtl">بدلاً من</span>
$`v\hat{}`$ <span dir="rtl"></span>

``` math
w_{t + 1} \doteq w_{t} + \alpha\left\lbrack G_{t}^{\lambda} - \widehat{q}\left( S_{t},A_{t},w_{t} \right) \right\rbrack\nabla\widehat{q}\left( S_{t},A_{t},w_{t} \right),\quad t = 0,\ldots,T - 1
```

<span dir="rtl">حيث</span>
$`Gt\lambda \doteq Gt:1\lambda`$​<span dir="rtl">. يتم عرض **مخطط النسخ
المركب**</span> **(Compound Backup Diagram)** <span dir="rtl">لهذه
الرؤية الأمامية في الشكل 12.9. لاحظ التشابه مع مخطط **خوارزمية**</span>
**TD(λ) <span dir="rtl">(شكل 12.1)</span>**. <span dir="rtl">التحديث
الأول ينظر خطوة كاملة إلى الأمام، إلى زوج الحالة-الإجراء التالي، بينما
التحديث الثاني ينظر إلى الأمام خطوتين، إلى زوج الحالة-الإجراء الثاني،
وهكذا. يعتمد التحديث النهائي على العائد الكامل. يتم وزن كل تحديث من
نوع</span> -n<span dir="rtl">خطوة في</span> **λ
<span dir="rtl">العائد</span> (λ-Return)** <span dir="rtl">بنفس الطريقة
كما في</span> **TD(λ)** <span dir="rtl">وخوارزمية</span> **λ
<span dir="rtl">العائد</span> (λ-Return Algorithm)**
<span dir="rtl">(المعادلة 12.3)</span>.

<span dir="rtl">طريقة **التفاضل الزمني**</span> **(Temporal
Difference)** <span dir="rtl">لقيم الإجراءات، والمعروفة باسم
**سارسا**</span>**(λ) (Sarsa(λ))**<span dir="rtl">، تقرب هذه الرؤية
الأمامية. لديها نفس قاعدة التحديث التي تم تقديمها سابقًا لـ</span>
**TD(λ)**<span dir="rtl">:</span>

``` math
w_{t + 1} \doteq w_{t} + \alpha\delta_{t}z_{t}
```

<span dir="rtl">باستثناء، بطبيعة الحال، استخدام صيغة **قيم
الإجراءات**</span> **(Action-Value Form)** <span dir="rtl">لـ **خطأ
التفاضل الزمني** </span>**(TD Error)**<span dir="rtl">:</span>

``` math
\delta_{t} \doteq R_{t + 1} + \gamma\widehat{q}\left( S_{t + 1},A_{t + 1},w_{t} \right) - \widehat{q}\left( S_{t},A_{t},w_{t} \right)
```

<span dir="rtl">  
وصيغة **قيم الإجراءات**</span> **(Action-Value Form)**
<span dir="rtl">لـ **أثر الأهلية** </span>**(Eligibility
Trace)**<span dir="rtl">:</span>

``` math
{z_{- 1} \doteq 0,\quad
}
{z_{t} \doteq \gamma\lambda z_{t - 1} + \nabla\widehat{q}\left( S_{t},A_{t},w_{t} \right),\quad 0 \leq t \leq T.}
```

<img src="./media/image150.png"
style="width:6.18387in;height:5.39213in" />

<span dir="rtl">الشكل 12.9: مخطط النسخ</span> (Backup Diagram)
<span dir="rtl">لخوارزمية</span> (λ) <span dir="rtl">سارسا</span>
(Sarsa(λ))<span dir="rtl">.</span> <span dir="rtl">قارن مع الشكل
12.1</span>.

**<span dir="rtl">الكود الزائف الكامل</span> (Pseudocode)**
<span dir="rtl">لخوارزمية</span>**(λ)** **<span dir="rtl">سارسا</span>
(Sarsa(λ))** <span dir="rtl">مع **تقريب الدوال الخطية** </span>**(Linear
Function Approximation)** <span dir="rtl">و**الميزات الثنائية**
</span>**(Binary Features)**<span dir="rtl">، ومع استخدام إما **آثار
التجميع**</span> **(Accumulating Traces)** <span dir="rtl">أو **آثار
الاستبدال** </span>**(Replacing Traces)**<span dir="rtl">، مُعطى في
الصندوق في الصفحة التالية. يبرز هذا الكود الزائف بعض التحسينات الممكنة
في الحالة الخاصة بالميزات الثنائية (حيث تكون الميزات إما نشطة (=1) أو
غير نشطة</span> (=0)<span dir="rtl">)</span>.

**<span dir="rtl">المثال 12.1: الآثار في</span> Gridworld**

<span dir="rtl">يمكن أن يؤدي استخدام **آثار الأهلية**</span>
**(Eligibility Traces)** <span dir="rtl">إلى زيادة كفاءة **خوارزميات
التحكم**</span> **(Control Algorithms)** <span dir="rtl">بشكل كبير
مقارنةً بالطرق ذات الخطوة الواحدة وحتى مقارنةً بالطرق ذات</span> $`n`$
<span dir="rtl">خطوة. يتم توضيح السبب في ذلك من خلال المثال التالي
في</span> $`Gridworld`$<span dir="rtl">.</span>

<img src="./media/image151.png"
style="width:6.26806in;height:1.45972in" />

<span dir="rtl">اللوحة الأولى تُظهر المسار الذي اتخذه **العميل**</span>
**(Agent)** <span dir="rtl">في **حلقة**</span> **(Episode)**
<span dir="rtl">واحدة. كانت **القيم التقديرية**</span> **(Estimated
Values)** <span dir="rtl">الأولية صفراً، وكانت جميع **المكافآت**</span>
**(Rewards)** <span dir="rtl">صفراً باستثناء **مكافأة إيجابية**</span>
**(Positive Reward)** <span dir="rtl">عند موقع الهدف المشار إليه
بـ</span> $`\mathbf{G}`$<span dir="rtl">.</span> <span dir="rtl">تشير
الأسهم في اللوحات الأخرى إلى أي **قيم الإجراء**</span>
**(Action-Values)** <span dir="rtl">التي ستزداد، وبكمية، عند الوصول إلى
الهدف وذلك وفقًا للخوارزميات المختلفة.</span> **<span dir="rtl">طريقة
الخطوة الواحدة</span> (One-Step Method)** <span dir="rtl">ستزيد فقط من
**قيمة الإجراء الأخير** </span>**(Last Action Value)**<span dir="rtl">،
بينما **طريقة**</span> **n-step** <span dir="rtl">ستزيد بالتساوي **قيم
الإجراءات** </span>$`\mathbf{n}`$ **<span dir="rtl">الأخيرة</span> (Last
n Actions’ Values)**<span dir="rtl">، وطريقة **الأثر الاهليجي**
</span>**(Eligibility Trace Method)** <span dir="rtl">ستقوم بتحديث جميع
**قيم الإجراءات**</span> **(Action Values)** <span dir="rtl">حتى بداية
**الحلقة** </span>**(Episode)**<span dir="rtl">، بدرجات متفاوتة تتلاشى
بمرور الزمن. عادةً ما تكون استراتيجية التلاشي هي الأفضل</span>.

**<span dir="rtl">سارسا</span> (Sarsa)** <span dir="rtl">مع **الميزات
الثنائية**</span> **(Binary Features)** <span dir="rtl">و**تقريب الدوال
الخطية**</span> **<span dir="rtl">(</span>Linear
<span dir="rtl"></span>Function Approximation<span dir="rtl">)
</span>**<span dir="rtl">لتقدير</span>...

<img src="./media/image152.png"
style="width:6.26806in;height:5.30764in" />

<span dir="rtl">تُظهر **الشكل 12.10** (على اليسار) في الصفحة التالية
نتائج استخدام **سارسا**</span> **(λ)** <span dir="rtl">في مهمة **السيارة
الجبلية**</span> **(Mountain Car)** <span dir="rtl">التي تم تقديمها في
**المثال 10.1**.</span> <span dir="rtl">كانت **تقريب الدوال**</span>
**<span dir="rtl">(</span>Function
<span dir="rtl"></span>Approximation<span dir="rtl">)</span>**<span dir="rtl">،
واختيار **الإجراءات** </span>**(Action Selection)**
<span dir="rtl">وتفاصيل **البيئة** </span>**(Environmental Details)**
<span dir="rtl">كما هي بالضبط في **الفصل 10**، وبالتالي من المناسب
مقارنة هذه النتائج عدديًا بنتائج الفصل 10 لخوارزمية **سارسا**</span>
**(n-step)** <span dir="rtl">(على الجانب الأيمن من الشكل). النتائج
السابقة كانت تعتمد على طول التحديث</span> n<span dir="rtl">، بينما هنا
في **سارسا**</span> **(λ)** <span dir="rtl">نقوم بتغيير **معامل
الأثر**</span> **<span dir="rtl">(</span>Trace
<span dir="rtl"></span>Parameter<span dir="rtl">)</span>**
<span dir="rtl"></span>λ<span dir="rtl">، الذي يلعب دورًا مشابهًا. يبدو أن
استراتيجية **التلاشي التدريجي للتمهيد**</span>
**<span dir="rtl">(</span>Fading-Trace Bootstrapping
Strategy<span dir="rtl">) </span>**<span dir="rtl">في **سارسا**</span>
**(λ)** <span dir="rtl">تؤدي إلى تعلم أكثر كفاءة في هذه المشكلة</span>.

<span dir="rtl">يوجد أيضًا إصدار **قيمة الإجراء**</span>
**(Action-Value)** <span dir="rtl">من **طريقة**</span> **TD
<span dir="rtl">المثالية عبر الإنترنت (</span>Ideal
<span dir="rtl"></span>Online TD
Method<span dir="rtl">)</span>**<span dir="rtl">، وهي **خوارزمية العائد
عبر الإنترنت** </span>**(λ-return Algorithm)** <span dir="rtl">(المذكورة
في **القسم 12.4**)</span> <span dir="rtl">وتنفيذها الفعال
كخوارزمية</span> TD **<span dir="rtl">عبر الإنترنت الحقيقية</span> (True
Online TD(λ))** <span dir="rtl">(المذكورة في **القسم 12.5**).</span>
<span dir="rtl">كل ما ورد في **القسم 12.4** يظل صحيحًا دون تغيير سوى
استخدام شكل **العائد**</span> **n-step <span dir="rtl">للإجراءات</span>
(Action-Value Form of the n-step Return)** <span dir="rtl">المذكور في
بداية **القسم الحالي**.</span> <span dir="rtl">التحليلات في **القسمين
12.5 و12.6** تنطبق أيضًا على **قيم الإجراءات** </span>**(Action
Values)**<span dir="rtl">، والتغيير الوحيد هو استخدام **متجهات ميزات
الحالة-الإجراء**</span> **<span dir="rtl">(</span>State–Action
<span dir="rtl"></span>Feature Vectors<span dir="rtl">)</span>**
$`xₜ\  = \ x\ (Sₜ,\ Aₜ)`$ <span dir="rtl">بدلاً من **متجهات ميزات
الحالة**</span> **<span dir="rtl">(</span>State Feature
<span dir="rtl"></span>Vectors<span dir="rtl">)</span>**
<span dir="rtl"></span>$`xₜ\  = \ x(Sₜ)`$<span dir="rtl">.</span>

<span dir="rtl">الكود الزائف للخوارزمية الفعالة الناتجة، المسماة **سارسا
الحقيقية عبر الإنترنت**</span> **<span dir="rtl">(</span>True Online
<span dir="rtl"></span>Sarsa(λ)<span dir="rtl">)</span>**<span dir="rtl">،
موجود في المربع على الصفحة التالية. يُقارن الشكل أدناه أداء النسخ
المختلفة من **سارسا** </span>**(λ)** <span dir="rtl">في مثال **السيارة
الجبلية** </span>**(Mountain Car)**<span dir="rtl">.</span>

<img src="./media/image153.png"
style="width:6.26806in;height:2.7875in" />

<span dir="rtl">**الشكل 12.10**:</span> <span dir="rtl">الأداء المبكر في
مهمة **السيارة الجبلية**</span> **(Mountain Car)**
<span dir="rtl">لخوارزمية **سارسا** </span>**(λ)** <span dir="rtl">مع
**استبدال الآثار**</span> **(Replacing Traces)**
<span dir="rtl">وخوارزمية **سارسا**</span> **(n-step)**
<span dir="rtl">(منسوخة من **الشكل 10.4**)</span> <span dir="rtl">كدالة
لحجم الخطوة،</span> $`\mathbf{\alpha}`$<span dir="rtl">.</span>

<img src="./media/image154.png"
style="width:6.26806in;height:2.65556in" />

<span dir="rtl">**الشكل 12.11**:</span> <span dir="rtl">مقارنة مختصرة
لخوارزميات **سارسا**</span> **(λ)** <span dir="rtl">في مهمة **السيارة
الجبلية  
(**</span>**Mountain
<span dir="rtl"></span>Car<span dir="rtl">)</span>**.
<span dir="rtl">تفوقت **خوارزمية سارسا**</span> **(λ)
<span dir="rtl">الحقيقية عبر الإنترنت</span>
<span dir="rtl">(</span>True Online
<span dir="rtl"></span>Sarsa(λ)<span dir="rtl">)
</span>**<span dir="rtl">على **سارسا**</span> **(λ)**
<span dir="rtl">التقليدية في كل من **الآثار التراكمية**
</span>**(Accumulating Traces)** <span dir="rtl">و**استبدال الآثار**
</span>**(Replacing Traces)**<span dir="rtl">.</span>
<span dir="rtl">كما تضمن الشكل إصدارًا من **سارسا**</span> **(λ)**
<span dir="rtl">باستخدام **استبدال الآثار**</span> **(Replacing
Traces)** <span dir="rtl">حيث كانت الآثار الخاصة بالحالة والإجراءات غير
المختارة تُعاد إلى الصفر في كل خطوة زمنية</span>.

**<span dir="rtl">خوارزمية سارسا</span> (λ) <span dir="rtl">الحقيقية عبر
الإنترنت</span> (True Online Sarsa(λ))**
<span dir="rtl">لتقدير</span>...

<img src="./media/image155.png"
style="width:6.3184in;height:4.47272in" />

<span dir="rtl">أخيرًا، هناك أيضًا نسخة مقطوعة من **سارسا**
</span>**(λ)**<span dir="rtl">، تسمى **سارسا الأمامي**</span> **(λ)
<span dir="rtl">(</span>Forward
<span dir="rtl"></span>Sarsa(λ)<span dir="rtl">)</span>**
<span dir="rtl"></span>(van Seijen, 2016)<span dir="rtl">، والتي تبدو
وكأنها طريقة تحكم فعالة بشكل خاص خالية من النموذج للاستخدام بالتزامن مع
**الشبكات العصبية الاصطناعية متعددة الطبقات**</span>
**<span dir="rtl">(</span>Multi-Layer <span dir="rtl"></span>Artificial
Neural Networks<span dir="rtl">)</span>**.

**<u>12.8 <span dir="rtl">المتغيرين</span> λ <span dir="rtl">و</span>
γ</u>**

<span dir="rtl">نحن نبدأ الآن في الوصول إلى نهاية تطويرنا لخوارزميات
**التفاضل الزمني**</span> **(TD) <span dir="rtl">الأساسية</span>
(Fundamental TD Learning Algorithms)**<span dir="rtl">. لتقديم
الخوارزميات النهائية في أشكالها الأكثر عمومية، من المفيد تعميم درجة
**التمهيد**</span> **(Bootstrapping)** <span dir="rtl">و**الخصم**</span>
**(Discounting)** <span dir="rtl">إلى ما هو أبعد من **المعاملات
الثابتة**</span> **(Constant Parameters)** <span dir="rtl">لتصبح **دوال
تعتمد على الحالة والإجراء** </span>**(Functions Potentially Dependent on
the State and Action)**<span dir="rtl">. بمعنى أنه سيكون لكل خطوة زمنية
قيمة مختلفة لـ</span> $`\mathbf{\lambda}`$
<span dir="rtl">و</span>$`\gamma`$<span dir="rtl">، يشار إليها بـ</span>
$`\mathbf{\lambda ₜ}`$
<span dir="rtl">و</span>$`\gamma\mathbf{ₜ}`$<span dir="rtl">.</span>
<span dir="rtl">نحن نغير الآن الترميز بحيث تكون  
</span>**λ<span dir="rtl">:</span> S × A → \[0, 1\]**
<span dir="rtl">دالة من **الحالات**</span> **(States)**
<span dir="rtl">و**الإجراءات**</span> **(Actions)** <span dir="rtl">إلى
**الفاصل الواحدي** </span>**(Unit Interval)**
<span dir="rtl">بحيث</span>
$`\mathbf{\lambda ₜ\  = \ \lambda\ (Sₜ,\ Aₜ)}`$<span dir="rtl">،
وبالمثل،</span>
$`\mathbf{\gamma:\ S\  \rightarrow \ \lbrack 0,\ 1\rbrack}`$
<span dir="rtl">هي دالة من **الحالات** </span>**(States)**
<span dir="rtl">إلى الفاصل الواحدي بحيث</span>
$`\mathbf{\gamma ₜ\  = \ \gamma(Sₜ)}`$<span dir="rtl">.</span>

<span dir="rtl">إدخال دالة</span> **γ**<span dir="rtl">، التي تسمى
**دالة الإنهاء** </span>**(Termination Function)**<span dir="rtl">، له
أهمية خاصة لأنه يغير من **العائد** </span>**(Return)**<span dir="rtl">،
وهو **المتغير العشوائي الأساسي**</span> **(Fundamental Random
Variable)** <span dir="rtl">الذي نسعى لتقدير توقعه. الآن يتم تعريف
العائد بشكل أكثر عمومية كما يلي</span>:

``` math
G_{t} \doteq R_{t + 1} + \gamma_{t + 1}G_{t + 1}
```

``` math
G_{t} = R_{t + 1} + \gamma_{t + 1}R_{t + 2} + \gamma_{t + 1}\gamma_{t + 2}R_{t + 3} + \gamma_{t + 1}\gamma_{t + 2}\gamma_{t + 3}R_{t + 4} +
```

``` math
G_{t} = \sum_{k = t}^{\infty}{\left( \prod_{i = t + 1}^{k}\gamma_{i} \right)R_{k + 1}}
```

<span dir="rtl">حيث، لضمان أن تكون المجاميع محدودة، نحتاج إلى أن
يكون</span> $`k = 0\prod k = t\infty\gamma k = 0`$ <span dir="rtl">مع
احتمال يساوي واحدًا لجميع</span> $`t`$<span dir="rtl">. إحدى الجوانب
المريحة في هذا التعريف هي أنه يتيح تقديم **الإعداد الحلقاتي**
</span>**(Episodic Setting)** <span dir="rtl">وخوارزمياته من خلال تيار
واحد من الخبرة، دون الحاجة إلى حالات نهائية خاصة، أو توزيعات بدء، أو
أوقات إنهاء محددة. تصبح الحالة التي كانت في السابق حالة نهائية **حالة**
</span>**(State)** <span dir="rtl">عندها</span> $`\gamma(s) = 0`$
<span dir="rtl">وتنتقل إلى توزيع البداية. بهذه الطريقة (وباختيار</span>
$`\gamma( \cdot )`$ <span dir="rtl">كقيمة ثابتة في جميع الحالات الأخرى)
يمكننا استعادة الإعداد الحلقاتي الكلاسيكي كحالة خاصة. يشمل **الإنهاء
المعتمد على الحالة**</span> **(State Dependent Termination)**
<span dir="rtl">حالات التنبؤ الأخرى مثل **الإنهاء الزائف**
</span>**(Pseudo Termination)**<span dir="rtl">، حيث نسعى إلى التنبؤ
بكمية معينة دون تغيير تدفق عملية ماركوف. يمكن اعتبار العوائد المخفضة
ككمية من هذا القبيل، وفي هذه الحالة، يوحد الإنهاء المعتمد على الحالة بين
الحالات الحلقاتية والمستمرة المخفضة. (لا تزال الحالة المستمرة غير
المخفضة بحاجة إلى معالجة خاصة)</span>.

**<span dir="rtl">التعميم إلى التمهيد المتغير</span> (Variable
Bootstrapping)** <span dir="rtl">ليس تغييرًا في المشكلة، مثل الخصم، بل هو
تغيير في استراتيجية الحل. يؤثر التعميم على **العوائد**</span> **λ
<span dir="rtl">للإجراءات والحالات  
(</span>λ -returns for States and
Actions<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">يمكن كتابة العائد الجديد المعتمد على الحالة بطريقة
متكررة كما يلي</span>:

``` math
G_{t}^{\lambda} \doteq R_{t + 1} + \gamma_{t + 1}\left\lbrack \left( 1 - \lambda_{t + 1} \right)\widehat{v}\left( S_{t + 1},w_{t} \right) + \lambda_{t + 1}G_{t + 1}^{\lambda} \right\rbrack
```

<span dir="rtl">هنا قمنا بإضافة الحرف</span> "s" <span dir="rtl">إلى
الرمز العلوي</span> "λ" <span dir="rtl">لتذكيرنا بأن هذا هو
**العائد**</span> **(Return)** <span dir="rtl">الذي يعتمد على **التمهيد
من قيم الحالات** </span>**(Bootstrapping from State
Values)**<span dir="rtl">، مما يميزه عن العوائد التي تعتمد على **التمهيد
من قيم الإجراءات** </span>**(Bootstrapping from Action
Values)**<span dir="rtl">، والتي سنقدمها لاحقًا مع الحرف</span> "a"
<span dir="rtl">في الرمز العلوي. تنص هذه المعادلة على أن **عائد**
</span>**λ<span dir="rtl">  
</span> (λ-Return)** <span dir="rtl">يتكون من **المكافأة الأولى**
</span>**(First Reward)**<span dir="rtl">، غير المخفضة وغير المتأثرة
بالتمهيد، بالإضافة إلى احتمال وجود **مصطلح ثانٍ**</span> **(Second
Term)** <span dir="rtl">يعتمد على مدى عدم الخصم في الحالة التالية</span>
<span dir="rtl">(أي، وفقًا لـ</span> $`\gamma ₜ₊₁`$<span dir="rtl">؛ تذكر
أن هذه القيمة تكون صفرًا إذا كانت الحالة التالية نهائية).</span>
<span dir="rtl">وبالنسبة للمدى الذي لا نقوم فيه بإنهاء الحالة التالية،
لدينا مصطلح ثانٍ ينقسم إلى حالتين اعتمادًا على **درجة التمهيد**
</span>**(Degree of Bootstrapping)** <span dir="rtl">في الحالة. عندما
نقوم بالتمهيد، يكون هذا المصطلح هو **القيمة المقدرة**</span>
**(Estimated Value)** <span dir="rtl">عند الحالة، بينما، عندما لا نقوم
بالتمهيد، يكون المصطلح هو **عائد** </span>**λ <span dir="rtl">للخطوة
الزمنية التالية</span> (λ-Return for the Next Time
Step)**<span dir="rtl">.</span> **<span dir="rtl">عائد</span> λ
<span dir="rtl">المستند إلى الإجراءات</span> (Action-Based λ-Return)**
<span dir="rtl">يأخذ شكل **سارسا**
</span>**(Sarsa)**<span dir="rtl">.</span>

``` math
G_{t}^{\lambda} \doteq R_{t + 1} + \gamma_{t + 1}\left\lbrack \left( 1 - \lambda_{t + 1} \right)\widehat{q}\left( S_{t + 1},A_{t + 1},w_{t} \right) + \lambda_{t + 1}G_{t + 1}^{\lambda} \right\rbrack
```

<span dir="rtl">أو يأخذ شكل **سارسا المتوقع** </span>**(Expected
Sarsa)**<span dir="rtl">.</span>

``` math
G_{t}^{\lambda} \doteq R_{t + 1} + \gamma_{t + 1}\left\lbrack \left( 1 - \lambda_{t + 1} \right)\overline{V_{t}}\left( S_{t + 1} \right) + \lambda_{t + 1}G_{t + 1}^{\lambda} \right\rbrack
```

<span dir="rtl">حيث يتم تعميم المعادلة (7.8) لتقريب الدوال على النحو
التالي</span>:

``` math
\overline{V_{t}}(s) \doteq \sum_{a}^{}{\pi\left( a \middle| s \right)\widehat{q}\left( s,a,w_{t} \right)}
```

<span dir="rtl">**التمرين 12.7**:</span> <span dir="rtl">قم بتعميم
المعادلات الثلاثة المتكررة أعلاه إلى نسخها المقطوعة، وذلك بتعريف  
</span>$`Gt:h\lambda`$
<span dir="rtl">و</span>$`Gat:h\lambda\ `$​<span dir="rtl">.</span>

**<u>12.9 <span dir="rtl">الآثار خارج السياسة</span> (Off-Policy Traces)
<span dir="rtl">باستخدام متغيرات التحكم</span>
<span dir="rtl">(</span>Control Variates<span dir="rtl">)</span></u>**

<span dir="rtl">الخطوة الأخيرة هي دمج **أخذ العينات على أساس الأهمية**
</span>**(Importance Sampling)**<span dir="rtl">.</span>
<span dir="rtl">على عكس حالة طرق</span> **n-step**<span dir="rtl">، في
حالة العوائد</span> **λ <span dir="rtl">غير المقطوعة</span>
(Non-Truncated λ-Returns)** <span dir="rtl">الكاملة، لا يوجد خيار عملي
حيث يتم أخذ العينات على أساس الأهمية خارج العائد المستهدف. بدلاً من ذلك،
ننتقل مباشرة إلى تعميم التمهيد الخاص بأخذ العينات على أساس الأهمية لكل
قرار مع **متغيرات التحكم**</span> **(Control Variates)**
<span dir="rtl">(المذكور في **القسم 7.4**).</span> <span dir="rtl">في
حالة **الحالة** </span>**(State)**<span dir="rtl">، فإن التعريف النهائي
لعائد</span> **λ <span dir="rtl"></span>**<span dir="rtl">يعمم المعادلة
(12.18)، بعد نموذج المعادلة (7.13)، ليصبح كما يلي</span>:

``` math
G_{t}^{\lambda} \doteq \rho_{t}\left( R_{t + 1} + \gamma_{t + 1}\left( \left( 1 - \lambda_{t + 1} \right)\widehat{v}\left( S_{t + 1},w_{t} \right) + \lambda_{t + 1}G_{t + 1}^{\lambda} \right) \right) + \left( 1 - \rho_{t} \right)\widehat{v}\left( S_{t},w_{t} \right)
```

<span dir="rtl">حيث أن</span> $`\rho t = \pi(At \mid St)\ b`$
<span dir="rtl">هو **نسبة أخذ العينات على أساس الأهمية**</span>
**<span dir="rtl">(</span>Importance Sampling
<span dir="rtl"></span>Ratio<span dir="rtl">)</span>**
<span dir="rtl">المعتادة لخطوة واحدة. مثل العوائد الأخرى التي رأيناها في
هذا الكتاب، يمكن تقريب النسخة المقطوعة من هذا العائد ببساطة من خلال
مجاميع **خطأ التفاضل الزمني المعتمد على الحالة**</span>
**<span dir="rtl">(</span>State-Based TD
Error<span dir="rtl">)</span>**.

``` math
\delta_{t}(s) \doteq R_{t + 1} + \gamma_{t + 1}\widehat{v}\left( S_{t + 1},w_{t} \right) - \widehat{v}\left( S_{t},w_{t} \right)
```

<span dir="rtl">كما يلي</span>

``` math
G_{t}^{\lambda} \approx \widehat{v}\left( S_{t},w_{t} \right) + \rho_{t}\sum_{k = t}^{\infty}{\delta_{k}(s)\prod_{i = t + 1}^{k}{\gamma_{i}\lambda_{i}\rho_{i}}}
```

**<span dir="rtl">والتقريب يصبح دقيقًا إذا لم تتغير دالة القيمة
التقريبية</span>.**

**<span dir="rtl">التمرين 12.8</span>**: <span dir="rtl">اثبت أن
المعادلة (12.24) تصبح دقيقة إذا لم تتغير دالة القيمة. لتوفير الكتابة،
اعتبر الحالة عند</span> $`t = 0`$<span dir="rtl">، واستخدم
الترميز</span> $`Vk \doteq v\hat{}(Sk,w)`$<span dir="rtl">.</span>

**<span dir="rtl">التمرين 12.9</span>**: <span dir="rtl">النسخة المقطوعة
من العائد العام خارج السياسة</span> (General Off-Policy Return)
<span dir="rtl">يُشار إليها بـ</span> $`Gt:h\lambda`$
​<span dir="rtl">.</span> <span dir="rtl">حاول تخمين المعادلة الصحيحة
بناءً على المعادلة</span> (12.24)<span dir="rtl">.</span>

**<span dir="rtl">الصيغة السابقة للعائد</span> λ
<span dir="rtl">(المعادلة 12.24)</span>** <span dir="rtl">مريحة
للاستخدام في تحديث الرؤية الأمامية</span>
<span dir="rtl">(</span>Forward-View Update<span dir="rtl">).</span>

``` math
w_{t + 1} = w_{t} + \alpha\left( G_{t}^{\lambda} - \widehat{v}\left( S_{t},w_{t} \right) \right)\nabla\widehat{v}\left( S_{t},w_{t} \right)
```

``` math
\approx w_{t} + \alpha\rho_{t}\sum_{k = t}^{\infty}{\delta_{k}(s)\left( \prod_{i = t + 1}^{k}{\gamma_{i}\lambda_{i}\rho_{i}} \right)\nabla\widehat{v}\left( S_{t},w_{t} \right)}
```

**<span dir="rtl">والذي يبدو للعين الخبيرة كتحديث</span> TD
<span dir="rtl">قائم على الأهلية</span> (Eligibility-Based TD Update)**
<span dir="rtl">فالمنتج يشبه أثر الأهلية</span> (Eligibility Trace)
<span dir="rtl">ويتم ضربه في أخطاء</span> TD<span dir="rtl">.</span>
<span dir="rtl">لكن هذا مجرد خطوة زمنية واحدة من الرؤية الأمامية</span>
(Forward View)<span dir="rtl">.</span> <span dir="rtl">العلاقة التي نبحث
عنها هي أن تحديث الرؤية الأمامية</span> (Forward-View
Update)<span dir="rtl">، إذا جمعناها على مدى الزمن، تكون تقريبًا مساوية
لتحديث الرؤية الخلفية</span> (Backward-View Update)<span dir="rtl">، إذا
جمعناها على مدى الزمن (هذه العلاقة تقريبية فقط لأنها تتجاهل التغييرات في
دالة القيمة). المجموع لتحديث الرؤية الأمامية على مدى الزمن هو</span>...

``` math
\sum_{t = 1}^{\infty}\left( w_{t + 1} - w_{t} \right) \approx \sum_{t = 1}^{\infty}{\sum_{k = t}^{\infty}{\alpha\rho_{t}\delta_{k}(s)\nabla\widehat{v}\left( S_{t},w_{t} \right)\prod_{i = t + 1}^{k}{\gamma_{i}\lambda_{i}\rho_{i}}}}
```

``` math
= \sum_{k = 1}^{\infty}{\sum_{t = 1}^{k}{\alpha\rho_{t}\nabla\widehat{v}\left( S_{t},w_{t} \right)\delta_{k}(s)\prod_{i = t + 1}^{k}{\gamma_{i}\lambda_{i}\rho_{i}}}}
```

<span dir="rtl">باستخدام قاعدة الجمع</span> (Summation
Rule)<span dir="rtl">:</span>

``` math
\sum_{t = x}^{y}{\sum_{k = t}^{y}{\sum_{k = x}^{y}\sum_{t = x}^{k}}}
```

``` math
= \sum_{k = 1}^{\infty}{\alpha\delta_{k}(s)\sum_{t = 1}^{k}{\rho_{t}\nabla\widehat{v}\left( S_{t},w_{t} \right)\prod_{i = t + 1}^{k}{\gamma_{i}\lambda_{i}\rho_{i}}}}
```

<span dir="rtl">والذي سيكون في شكل مجموع تحديث</span> TD
<span dir="rtl">بالرؤية الخلفية إذا كان بالإمكان كتابة التعبير بأكمله من
المجموع الثاني على اليسار وتحديثه بشكل تدريجي كأثر أهلية</span>
(Eligibility Trace)<span dir="rtl">، وهو ما سنبين الآن أنه يمكن القيام
به. أي أننا سنوضح أنه إذا كان هذا التعبير هو أثر الأهلية في الزمن</span>
k<span dir="rtl">، فإنه يمكننا تحديثه من قيمته في الزمن</span>
$`k\  - \ 1`$ <span dir="rtl">كما يلي</span>:

``` math
z_{k} = \sum_{t = 1}^{k}{\rho_{t}\nabla\widehat{v}\left( S_{t},w_{t} \right)\prod_{i = t + 1}^{k}{\gamma_{i}\lambda_{i}\rho_{i}}}
```

``` math
= \sum_{t = 1}^{k - 1}{\rho_{t}\nabla\widehat{v}\left( S_{t},w_{t} \right)\prod_{i = t + 1}^{k}{\gamma_{i}\lambda_{i}\rho_{i}}} + \rho_{k}\nabla\widehat{v}\left( S_{k},w_{k} \right)
```

``` math
= \gamma_{k}\lambda_{k}\rho_{k}\underset{z_{k - 1}}{\overset{\sum_{t = 1}^{k - 1}{\rho_{t}\nabla\widehat{v}\left( S_{t},w_{t} \right)\prod_{i = t + 1}^{k - 1}{\gamma_{i}\lambda_{i}\rho_{i}}}}{︸}} + \rho_{k}\nabla\widehat{v}\left( S_{k},w_{k} \right)
```

``` math
= \rho_{k}\left( \gamma_{k}\lambda_{k}z_{k - 1} + \nabla\widehat{v}\left( S_{k},w_{k} \right) \right)
```

<span dir="rtl">والتي، بتغيير الفهرس من</span> $`k`$
<span dir="rtl">إلى</span> $`t`$<span dir="rtl">، تصبح تحديث أثر التراكم
العام لقيم الحالات</span>:

``` math
z_{t} \doteq \rho_{t}\left( \gamma_{t}\lambda_{t}z_{t - 1} + \nabla\widehat{v}\left( S_{t},w_{t} \right) \right)
```

<span dir="rtl">هذا أثر الأهلية</span> (**Eligibility
Trace**)<span dir="rtl">، جنبًا إلى جنب مع قاعدة تحديث البارامتر شبه
التدرجية (</span>**Semi-Gradient Parameter-Update
Rule**<span dir="rtl">) المعتادة لخوارزمية</span> **TD(λ)**
<span dir="rtl">  
(المعادلة 12.7)، يشكل خوارزمية</span> TD(λ) <span dir="rtl">العامة التي
يمكن تطبيقها على البيانات داخل السياسة</span> **(On-Policy)**
<span dir="rtl">أو خارج السياسة</span>
**(Off-Policy)**<span dir="rtl">.</span> <span dir="rtl">في حالة داخل
السياسة، تكون الخوارزمية بالضبط</span> TD(λ) <span dir="rtl">لأن</span>
$`\rho t`$ <span dir="rtl">دائمًا تساوي 1 والمعادلة (12.25) تصبح أثر
التراكم المعتاد  
(المعادلة 12.5)</span> <span dir="rtl">(ممتدة إلى</span> λ
<span dir="rtl">و</span>γ <span dir="rtl">المتغيرين).</span>
<span dir="rtl">في حالة خارج السياسة، غالبًا ما تعمل الخوارزمية بشكل جيد،
ولكن كطريقة شبه تدرجية، فهي غير مضمونة الاستقرار. في الأقسام القليلة
التالية، سننظر في تمديدات لها تضمن الاستقرار</span>.

<span dir="rtl">يمكن اتباع سلسلة خطوات مشابهة جدًا لاستنباط آثار الأهلية
خارج السياسة</span> <span dir="rtl">(</span>**Off-Policy
<span dir="rtl"></span>Eligibility Traces**<span dir="rtl">) لطرق قيم
الإجراءات</span> (**Action-Value Methods**)
<span dir="rtl">وخوارزميات</span> Sarsa(λ) <span dir="rtl">العامة
المقابلة. يمكن البدء إما بالشكل التكراري لعائد</span> λ
<span dir="rtl">العام المستند إلى الإجراءات (المعادلتين 12.19 أو 12.20)،
ولكن الأخيرة</span> <span dir="rtl">(شكل</span> Sarsa
<span dir="rtl">المتوقع)</span> <span dir="rtl">تكون أبسط. سنمدد
(المعادلة 12.20) إلى حالة خارج السياسة بعد نموذج (المعادلة 7.14)
لإنتاج</span>...

``` math
G_{t}^{\lambda} \doteq R_{t + 1} + \gamma_{t + 1}\left\lbrack \left( 1 - \lambda_{t + 1} \right)\overline{V_{t}}\left( S_{t + 1} \right) + \lambda_{t + 1}\left( \rho_{t + 1}G_{t + 1}^{\lambda} + \overline{V_{t}}\left( S_{t + 1} \right) - \rho_{t + 1}\widehat{q}\left( S_{t + 1},A_{t + 1},w_{t} \right) \right) \right\rbrack
```

``` math
= R_{t + 1} + \gamma_{t + 1}\left( \overline{V_{t}}\left( S_{t + 1} \right) + \lambda_{t + 1}\rho_{t + 1}\left( G_{t + 1}^{\lambda} - \widehat{q}\left( S_{t + 1},A_{t + 1},w_{t} \right) \right) \right)
```

<span dir="rtl">حيث أن</span> Vˉt ($`S_{t + 1}`$)
<span dir="rtl"></span> <span dir="rtl">كما هو مُعطى في المعادلة (12.21).
مرة أخرى، يمكن كتابة</span> **λ <span dir="rtl">عائد</span>
<span dir="rtl">  
(</span>λ-Return<span dir="rtl">) </span>**<span dir="rtl">بشكل تقريبي
كمجموع</span> **TD <span dir="rtl">أخطاء</span> (TD
Errors)**<span dir="rtl">.</span>

``` math
G_{t}^{\lambda} \approx \widehat{q}\left( S_{t},A_{t},w_{t} \right) + \sum_{k = t}^{\infty}{\delta_{k}^{a}\prod_{i = t + 1}^{k}{\gamma_{i}\lambda_{i}\rho_{i}}}
```

<span dir="rtl">باستخدام صيغة التوقع لخطأ</span> TD
<span dir="rtl">المستند إلى الإجراءات</span> (Action-Based TD
Error)<span dir="rtl">:</span>

``` math
\delta_{t}^{a} = R_{t + 1} + \gamma_{t + 1}\overline{V_{t}}\left( S_{t + 1} \right) - \widehat{q}\left( S_{t},A_{t},w_{t} \right)
```

<span dir="rtl">**التمرين 12.10**:</span> <span dir="rtl">اثبت أن
المعادلة (12.27) تصبح دقيقة إذا لم تتغير دالة القيمة. لتوفير الكتابة،
اعتبر الحالة عند</span> $`t = 0`$<span dir="rtl">، واستخدم
الترميز</span> $`Qk = q\hat{}(Sk,Ak,w`$) <span dir="rtl">تلميح: ابدأ
بكتابة</span> $`\delta 0a`$
<span dir="rtl">و0</span>G0λ​<span dir="rtl">، ثم</span>
$`G0\lambda - Q0`$​<span dir="rtl">.</span>

<span dir="rtl">التمرين 12.11: النسخة المقطوعة من العائد العام خارج
السياسة</span> (off-policy) <span dir="rtl">يتم الإشارة إليها
بالرمز</span> $`Gt:h\lambda`$<span dir="rtl">.</span>
<span dir="rtl">خمن المعادلة الصحيحة لها، بناءً على المعادلة</span>
(12.27)<span dir="rtl">.</span>  
<span dir="rtl">باستخدام خطوات مشابهة تمامًا لتلك المستخدمة في حالة
الحالة</span> (state case)<span dir="rtl">، يمكن كتابة تحديث الرؤية
الأمامية بناءً على المعادلة</span> (12.27)<span dir="rtl">، وتحويل مجموع
التحديثات باستخدام قاعدة الجمع، وأخيرًا استنتاج الشكل التالي لتتبع
الأهلية لقيم الإجراءات</span> (action values)<span dir="rtl">:</span>

``` math
z_{t} = \gamma_{t}\lambda_{t}\rho_{t}z_{t - 1} + \widehat{r}q\left( S_{t},A_{t},w_{t} \right)
```

<span dir="rtl">تتبع الأهلية</span> (**Eligibility Trace**)
<span dir="rtl">هذا، مع خطأ التقدير المعتمد على التوقع</span>
<span dir="rtl">(</span>**Expectation-Based <span dir="rtl"></span>TD
Error**<span dir="rtl">)</span> (12.28) <span dir="rtl">وقاعدة تحديث
البارامتر شبه التدرج</span> <span dir="rtl">(</span>**Semi-Gradient
<span dir="rtl"></span>Parameter-Update Rule**<span dir="rtl">)
المعتادة</span> (12.7)<span dir="rtl">، يشكل خوارزمية</span> **Expected
Sarsa(λ)** <span dir="rtl">الأنيقة والفعالة التي يمكن تطبيقها إما على
البيانات داخل السياسة</span> (**On-Policy**) <span dir="rtl">أو خارج
السياسة  
</span>**(Off-Policy)**<span dir="rtl">.</span> <span dir="rtl">ربما
تكون هذه الخوارزمية هي الأفضل من نوعها في الوقت الحالي (على الرغم من أنه
بالطبع لا يوجد ضمان للاستقرار حتى يتم دمجها بطريقة ما مع إحدى الطرق
المقدمة في الأقسام التالية). في حالة داخل السياسة</span> (**On-Policy**)
<span dir="rtl">مع</span> $`\lambda`$ <span dir="rtl">و</span>$`\gamma`$
<span dir="rtl">ثابتين، وخطأ التقدير العادي للحالة-الإجراء  
(</span>**State-Action TD Error**<span dir="rtl">)</span>
(12.16)<span dir="rtl">، ستكون الخوارزمية مطابقة لخوارزمية</span>
**Sarsa(λ)** <span dir="rtl">المقدمة في الفصل 12.7. التمرين 12.12: وضح
بالتفصيل الخطوات الموضحة أعلاه لاشتقاق المعادلة</span> (12.29)
<span dir="rtl">من المعادلة</span> (12.27)<span dir="rtl">.</span>
<span dir="rtl">ابدأ بالتحديث</span> (12.15)<span dir="rtl">،
استبدل</span> $`Gt\lambda`$ <span dir="rtl">بـ من المعادلة</span>
(12.26)<span dir="rtl">، ثم اتبع خطوات مشابهة لتلك التي أدت إلى
المعادلة</span> (12.25)<span dir="rtl">.</span>
<span dir="rtl">عند</span> $`\lambda = 1`$<span dir="rtl">، تصبح هذه
الخوارزميات مرتبطة بشكل وثيق بخوارزميات *مونت كارلو شجرة البحث*</span>
**(Monte Carlo Tree Search)** <span dir="rtl">المقابلة. قد يتوقع المرء
أن تتواجد مكافئة دقيقة للمشاكل الحلقية</span> (**Episodic Problems**)
<span dir="rtl">وتحديثات اوفلاين (</span>**Offline
Updating**<span dir="rtl">)*،* لكن في الواقع، العلاقة أكثر تعقيدًا وأقل
وضوحًا من ذلك. تحت هذه الظروف الأكثر ملاءمة، لا يوجد تساوي تحديثات حلقة
بحلقة، بل تساوي في توقعاتها فقط. يجب ألا يكون هذا مفاجئًا لأن هذه الطرق
تقوم بتحديثات لا رجعة فيها عند تطور المسار، بينما طرق مونت كارلو
(</span>**Monte Carlo**<span dir="rtl">) الحقيقية لن تقوم بأي تحديث إذا
كان لأي إجراء ضمنه احتمال صفري وفقًا للسياسة المستهدفة</span> (**Target
Policy**)<span dir="rtl">. على وجه الخصوص، جميع هذه الطرق، حتى
عند</span> $`\lambda = 1`$<span dir="rtl">، لا تزال تعتمد على
التمهيد</span> (**Bootstrapping**) <span dir="rtl">بمعنى أن أهدافها
تعتمد على تقديرات القيمة الحالية – الأمر فقط هو أن هذا الاعتماد يتلاشى
في القيمة المتوقعة. ما إذا كان هذا الأمر جيدًا أو سيئًا في الممارسة هو
سؤال آخر. في الآونة الأخيرة، تم اقتراح طرق تحقق تكافؤًا دقيقًا</span>
<span dir="rtl">(</span>Sutton, Mahmood, Precup
<span dir="rtl"></span>and van Hasselt, 2014<span dir="rtl">)</span>.
<span dir="rtl">هذه الطرق تتطلب متجهًا إضافيًا من "الأوزان المؤقتة</span>
<span dir="rtl">(</span>**Provisional**
**Weights**<span dir="rtl">)</span> <span dir="rtl">التي تتابع التحديثات
التي تم إجراؤها ولكن قد تحتاج إلى التراجع (أو التركيز) اعتمادًا على
الإجراءات المتخذة لاحقًا. النسخ الحالة وحالة-الإجراء لهذه الطرق
تُسمى</span> $`PTD(\lambda)`$ <span dir="rtl">و</span>$`PQ(\lambda)`$
<span dir="rtl">على التوالي، حيث يرمز الحرف</span> "$`P`$"
<span dir="rtl">إلى "مؤقت</span> (**Provisional**)<span dir="rtl">. لم
تُثبت بعد العواقب العملية لكل هذه الطرق الجديدة خارج السياسة</span>
(Off-Policy) <span dir="rtl">بشكل كامل. بلا شك، ستنشأ مشكلات ذات تباين
عالٍ كما يحدث في جميع الطرق خارج السياسة باستخدام أخذ العينات
بالأهمية</span> <span dir="rtl">(</span>**Importance**
**Sampling**<span dir="rtl">)</span> <span dir="rtl">(الفصل 11.9). إذا
كان</span> $`\lambda < 1`$<span dir="rtl">، فإن جميع هذه الخوارزميات
خارج السياسة  
(</span>**Off**-**Policy**<span dir="rtl">) تتضمن التمهيد</span>
(**Bootstrapping**) <span dir="rtl">وينطبق الثلاثي القاتل</span>
(**Deadly** **Triad**) <span dir="rtl">(الفصل 11.3)، مما يعني أنه يمكن
ضمان استقرارها فقط في الحالة الجدولية</span> (**Tabular**
**Case**)<span dir="rtl">، في تجميع الحالات</span> (**State**
**Aggregation**)<span dir="rtl">، وفي أشكال أخرى محدودة من تقريب
الدوال</span> <span dir="rtl">(</span>**Function**
<span dir="rtl"></span>Approximation<span dir="rtl">)</span>.
<span dir="rtl">بالنسبة لأشكال تقريب الدوال الخطية</span> (Linear
Function Approximation) <span dir="rtl">والأكثر عمومية، قد يتباعد متجه
البارامترات إلى ما لا نهاية كما في الأمثلة في الفصل 11. كما ناقشنا هناك،
فإن تحدي التعليم خارج السياسة</span> (Off-Policy Learning)
<span dir="rtl">له جزآن. تتعامل تتبعات الأهلية خارج السياسة</span>
(Off-Policy Eligibility Traces) <span dir="rtl">بفعالية مع الجزء الأول
من التحدي، وهو تصحيح القيمة المتوقعة للأهداف، ولكن ليس على الإطلاق مع
الجزء الثاني من التحدي، والذي يتعلق بتوزيع التحديثات</span> (Update
Distribution)<span dir="rtl">.</span> <span dir="rtl">يتم تلخيص
الاستراتيجيات الخوارزمية لمواجهة الجزء الثاني من تحدي التعليم خارج
السياسة</span> (Off-Policy Learning) <span dir="rtl">باستخدام تتبعات
الأهلية في الفصل 12.11. التمرين 12.13: ما هي النسخ "تتبع هولندي</span>
(Dutch-Trace)" <span dir="rtl">وتتبع استبدالي</span>
<span dir="rtl">(</span>Replacing-Trace<span dir="rtl">)</span>
<span dir="rtl">من تتبعات الأهلية خارج السياسة</span> (Off-Policy
Eligibility Traces) <span dir="rtl">لطرق قيم الحالة</span> (State-Value
Methods) <span dir="rtl">وقيم الإجراء</span> (Action-Value
Methods)<span dir="rtl">؟</span>

**<u>12.10 <span dir="rtl">خوارزمية</span> Q(λ) <span dir="rtl">الخاصة
بـ</span> Watkins <span dir="rtl">إلى خوارزمية الاحتياطي الشجري</span>
<span dir="rtl">  
(</span>Tree-Backup<span dir="rtl">)</span></u>**

<span dir="rtl">على مر السنين، تم اقتراح العديد من الطرق لتمديد
خوارزمية</span> $`Q - learning`$ <span dir="rtl">إلى تتبعات
الأهلية</span> (Eligibility Traces)<span dir="rtl">. الطريقة الأصلية هي
خوارزمية</span> Watkins’s Q(λ)<span dir="rtl">، التي تقلل تتبعات الأهلية
بالطريقة المعتادة طالما تم اتخاذ إجراء جشع</span> (Greedy
Action)<span dir="rtl">، ثم تقطع التتبعات إلى الصفر بعد أول إجراء غير
جشع</span> (Non-Greedy Action)<span dir="rtl">.</span>
<span dir="rtl">يظهر مخطط النسخ الاحتياطي</span>
<span dir="rtl">(</span>Backup
<span dir="rtl"></span>Diagram<span dir="rtl">) الخاص بخوارزمية</span>
Watkins’s <span dir="rtl"></span>Q(λ) <span dir="rtl">في الشكل 12.12. في
الفصل 6، قمنا بتوحيد خوارزمية</span> Q-learning
<span dir="rtl">و</span>Expected Sarsa <span dir="rtl">*في* النسخة خارج
السياسة</span> (Off-Policy) <span dir="rtl">من الأخيرة، والتي
تتضمن</span> Q*-*learning <span dir="rtl">كحالة خاصة، وتعممها على سياسات
الهدف التعسفية</span> <span dir="rtl">(</span>Arbitrary
<span dir="rtl"></span>Target Policies<span dir="rtl">)</span>.
<span dir="rtl">في القسم السابق من هذا الفصل، أكملنا معالجتنا
*لـ*</span> Expected Sarsa <span dir="rtl">بتعميمه على تتبعات الأهلية
خارج السياسة</span> (Off-Policy Eligibility
Traces)<span dir="rtl">.</span> <span dir="rtl">ومع ذلك، في الفصل 7،
قمنا بتمييز</span> n-step Expected Sarsa <span dir="rtl">عن</span>
n-step Tree Backup<span dir="rtl">، حيث احتفظ الأخير بخاصية عدم استخدام
أخذ العينات بالأهمية</span> (Importance
Sampling)<span dir="rtl">.</span> <span dir="rtl">يبقى إذن تقديم النسخة
الخاصة بتتبعات الأهلية لخوارزمية الاحتياطي الشجري</span> (Tree
Backup)<span dir="rtl">، والتي سنطلق عليها  
</span>Tree-Backup(λ)<span dir="rtl">، أو اختصارًا</span>
TB(λ)*<span dir="rtl">.</span>* <span dir="rtl">يمكن القول إن هذه
الخوارزمية هي الوريث الحقيقي لـ *  *
</span>Q-learning <span dir="rtl">لأنها تحتفظ بجاذبية عدم استخدام أخذ
العينات بالأهمية حتى مع إمكانية تطبيقها على البيانات خارج السياسة</span>
(Off-Policy Data)<span dir="rtl">.</span>

<img src="./media/image156.png"
style="width:6.26806in;height:3.86806in" />

<span dir="rtl">الشكل 12.12: مخطط النسخ الاحتياطي</span> (Backup
Diagram) <span dir="rtl">الخاص بخوارزمية</span> Watkins’s
Q(λ)<span dir="rtl">*.* سلسلة التحديثات المكونة تنتهي إما بنهاية
الحلقة</span> (Episode) <span dir="rtl">أو مع أول إجراء غير جشع</span>
(Non-Greedy Action)<span dir="rtl">، أيهما يأتي أولاً</span>.

<span dir="rtl">مفهوم</span> TB(λ) <span dir="rtl">بسيط. كما هو موضح في
مخطط النسخ الاحتياطي</span> (Backup Diagram) <span dir="rtl">الخاص بها
في الشكل 12.13، يتم وزن التحديثات الشجرية</span> (Tree-Backup Updates)
<span dir="rtl">لكل طول  
(من القسم 7.5) بالطريقة المعتادة اعتمادًا على بارامتر التمهيد</span>
(Bootstrapping Parameter) λ<span dir="rtl">. للحصول على المعادلات
التفصيلية، مع المؤشرات الصحيحة على بارامترات التمهيد</span>
(Bootstrapping) <span dir="rtl">والتخفيض</span> (Discounting)
<span dir="rtl">العامة، من الأفضل البدء بالشكل التكراري</span>
(Recursive Form) <span dir="rtl"></span>(12.20)
<span dir="rtl">للعائد</span> (λ-Return) <span dir="rtl">باستخدام قيم
الإجراءات</span> (Action Values)<span dir="rtl">، ثم توسيع حالة
التمهيد</span> (Bootstrapping Case) <span dir="rtl">للهدف بعد
النموذج</span> (7.16)<span dir="rtl">.</span>

``` math
G_{t}^{\lambda} = R_{t + 1} + \gamma_{t + 1}\left( \lambda_{t + 1}{\overline{V}}_{t}\left( S_{t + 1} \right) + \left( 1 - \lambda_{t + 1} \right)\left\lbrack \sum_{a \neq A_{t + 1}}^{}{\pi\left( a \middle| S_{t + 1} \right)\widehat{q}\left( S_{t + 1},a,w_{t} \right)} + \pi\left( A_{t + 1} \middle| S_{t + 1} \right)G_{t + 1}^{\lambda} \right\rbrack \right)
```

``` math
G_{t}^{\lambda} = R_{t + 1} + \gamma_{t + 1}\left( {\overline{V}}_{t}\left( S_{t + 1} \right) + \lambda_{t + 1}\pi\left( A_{t + 1} \middle| S_{t + 1} \right)\left( G_{t + 1}^{\lambda} - \widehat{q}\left( S_{t + 1},A_{t + 1},w_{t} \right) \right) \right)
```

<span dir="rtl">وفقًا للنمط المعتاد، يمكن أيضًا كتابته بشكل تقريبي</span>
<span dir="rtl">(متجاهلًا التغييرات في دالة القيمة التقريبية</span>
(Approximate Value Function)<span dir="rtl">) كمجموع من</span> TD
<span dir="rtl">أخطاء</span> (TD Errors)<span dir="rtl">.</span>

``` math
G_{t}^{\lambda} \approx \widehat{q}\left( S_{t},A_{t},w_{t} \right) + \sum_{k = t + 1}^{\infty}\left( \prod_{i = t + 1}^{k}{\gamma_{i}\lambda_{i}\pi\left( A_{i} \middle| S_{i} \right)} \right)
```

<span dir="rtl">باستخدام صيغة التوقع لخطأ</span> TD
<span dir="rtl">المعتمد على الإجراء</span> (Action-Based TD Error)
(12.28)<span dir="rtl">. باتباع نفس الخطوات كما في القسم السابق، نصل إلى
تحديث خاص لتتبع الأهلية</span> <span dir="rtl">(</span>Eligibility
Trace<span dir="rtl">) يتضمن احتمالات السياسة المستهدفة</span>
(Target-Policy Probabilities) <span dir="rtl">للإجراءات المختارة</span>.

``` math
z_{t} = \gamma_{t}\lambda_{t}\pi\left( A_{t} \middle| S_{t} \right)z_{t - 1} + \widehat{r}q\left( S_{t},A_{t},w_{t} \right)
```

<img src="./media/image157.png"
style="width:6.26806in;height:5.19514in" />

<span dir="rtl">الشكل 12.13: مخطط النسخ الاحتياطي</span> (Backup
Diagram) <span dir="rtl">للنسخة</span> λ <span dir="rtl">من خوارزمية
الاحتياطي الشجري</span> (Tree Backup)<span dir="rtl">.</span>

<span dir="rtl">هذا، مع قاعدة تحديث البارامتر المعتادة</span>
(12.7)<span dir="rtl">، يُعرّف خوارزمية</span>
TB(λ)<span dir="rtl">.</span> <span dir="rtl">مثل جميع الخوارزميات شبه
التدرجية</span> (Semi-Gradient Algorithms)<span dir="rtl">، لا
تضمن</span> TB(λ) <span dir="rtl">الاستقرار عند استخدامها مع بيانات خارج
السياسة</span> (Off-Policy Data) <span dir="rtl">ومع تقريب دوال
قوي</span> <span dir="rtl">(</span>Powerful Function
<span dir="rtl"></span>Approximator<span dir="rtl">)</span>.
<span dir="rtl">للحصول على تلك الضمانات، يجب دمج</span> TB(λ)
<span dir="rtl">مع إحدى الطرق المقدمة في القسم التالي</span>.

<span dir="rtl">التمرين 12.14: كيف يمكن تمديد خوارزمية</span> Double
Expected Sarsa <span dir="rtl">إلى تتبعات الأهلية</span> (Eligibility
Traces)<span dir="rtl">؟</span>

**<u>12.11 <span dir="rtl">طرق مستقرة خارج السياسة</span> (Off-Policy)
<span dir="rtl">مع تتبعات الأهلية</span> (Traces)</u>**

<span dir="rtl">تم اقتراح عدة طرق باستخدام تتبعات الأهلية</span>
(Eligibility Traces) <span dir="rtl">التي تحقق ضمانات الاستقرار في
التدريب خارج السياسة</span> (Off-Policy Training)<span dir="rtl">، وهنا
نقدم أربعًا من أهمها باستخدام تدوين الكتاب القياسي، بما في ذلك دوال
التمهيد</span> (Bootstrapping Functions) <span dir="rtl">والتخفيض</span>
<span dir="rtl">(</span>Discounting
<span dir="rtl"></span>Functions<span dir="rtl">) العامة. جميع هذه الطرق
تعتمد إما على أفكار</span> Gradient-TD <span dir="rtl">أو</span>
Emphatic-TD <span dir="rtl">التي تم تقديمها في الفصلين 11.7 و11.8. تفترض
جميع الخوارزميات تقريب دوال خطي</span> <span dir="rtl">(</span>Linear
<span dir="rtl"></span>Function Approximation<span dir="rtl">)، على
الرغم من أن هناك امتدادات لتقريب الدوال غير الخطي</span> (Nonlinear
Function Approximation) <span dir="rtl">يمكن العثور عليها أيضًا في
الأدبيات</span>.

<span dir="rtl">خوارزمية</span> GTD(λ) <span dir="rtl">هي خوارزمية تتبع
الأهلية المشابهة لخوارزمية</span> TDC<span dir="rtl">، وهي الأفضل من بين
خوارزميات التنبؤ</span> Gradient-TD <span dir="rtl">لقيم الحالة</span>
(State-Value) <span dir="rtl">التي نوقشت في الفصل 11.7. هدفها هو تعلم
بارامتر</span> $`wt`$ <span dir="rtl"></span>​
<span dir="rtl">بحيث</span>
$`v\hat{}(s,w)\  \approx wt\top x(s) \approx v\pi(s)`$ ​<span dir="rtl">،
حتى من البيانات الناتجة عن اتباع سياسة أخرى</span>
$`b`$<span dir="rtl">.</span> <span dir="rtl">تحديثها هو</span>:

``` math
w_{t + 1} = w_{t} + \alpha\left( z_{t} - \alpha\gamma_{t + 1}\left( 1 - \lambda_{t + 1} \right)\left( z_{t}^{\top v_{t}} \right)x_{t + 1} \right)
```

<span dir="rtl">مع</span> $`st`$ <span dir="rtl">و</span> $`zt`$
<span dir="rtl">و</span> $`\rho t`$ <span dir="rtl">معرفين بالطرق
المعتادة لقيم الحالة</span> (State Values) <span dir="rtl">وفقًا
للمعادلات</span> (12.23) <span dir="rtl"></span>(11.1)
<span dir="rtl">و..</span>

``` math
v_{t + 1} = v_{t} + \beta\left( z_{t} - \beta\left( v_{t}^{\top x_{t}} \right)x_{t} \right)
```

<span dir="rtl">كما هو الحال في الفصل 11.7، فإن</span> $`v \in Rd`$
<span dir="rtl">هو متجه بنفس أبعاد</span> $`w`$<span dir="rtl">، يتم
تهيئته إلى</span> $`v0 = 0`$<span dir="rtl">، و</span>$`\beta > 0`$
<span dir="rtl">هو بارامتر خطوة ثانٍ</span> (Second Step-Size
Parameter)<span dir="rtl">.</span>

<span dir="rtl">خوارزمية</span> GQ(λ) <span dir="rtl">هي خوارزمية</span>
Gradient-TD <span dir="rtl">لقيم الإجراءات</span> (Action Values)
<span dir="rtl">مع تتبعات الأهلية</span> (Eligibility
Traces)<span dir="rtl">.</span> <span dir="rtl">هدفها هو تعلم
بارامتر</span> $`wt`$ <span dir="rtl"></span>​<span dir="rtl">بحيث  
</span>$`q\hat{}(s,a,wt) \approx wt\top x(s,a) \approx \ q_{\pi}(s,a)`$
<span dir="rtl">من البيانات خارج السياسة</span> (Off-Policy
Data)<span dir="rtl">. إذا كانت السياسة المستهدفة</span> (Target Policy)
<span dir="rtl">جشعة</span> ε-greedy<span dir="rtl">، أو متحيزة نحو
السياسة الجشعة لتقدير</span> $`q\hat{}`$​<span dir="rtl">، فإن
خوارزمية</span> $`GQ(\lambda)`$ <span dir="rtl">يمكن استخدامها كخوارزمية
تحكم. التحديث الخاص بها هو</span>:

``` math
w_{t + 1} = w_{t} + \alpha\left( z_{t} - \alpha\gamma_{t + 1}\left( 1 - \lambda_{t + 1} \right)\left( z_{t}^{\top v_{t}} \right){\overline{x}}_{t + 1} \right)
```

<span dir="rtl">حيث</span> $`x‾t`$ <span dir="rtl">هو متوسط متجه
الخصائص</span> (Feature Vector) <span dir="rtl">لـ</span> $`St`$
<span dir="rtl"></span>​ <span dir="rtl">تحت السياسة المستهدفة</span>
<span dir="rtl">(</span>Target
<span dir="rtl"></span>Policy<span dir="rtl">)</span>.

``` math
{\overline{x}}_{t} = \sum_{a}^{}{\pi\left( a \middle| S_{t} \right)x\left( S_{t},a \right)}
```

$`\delta t`$​ <span dir="rtl">هو الشكل المتوقع</span> TD
<span dir="rtl">لخطأ</span> (TD Error) <span dir="rtl">والذي يمكن كتابته
على النحو التالي</span>:

``` math
\delta_{t} = R_{t + 1} + \gamma_{t + 1}w_{t_{t + 1}^{\top\overline{x}}} - w_{t}^{\top x_{t}}
```

$`zt`$​ <span dir="rtl">يتم تعريفه بالطريقة المعتادة لقيم
الإجراءات</span> (Action Values) <span dir="rtl">كما في المعادلة</span>
(12.29)<span dir="rtl">، والباقي كما في خوارزمية</span>
GTD(λ)<span dir="rtl">، بما في ذلك التحديث لـ</span> $`vt`$
<span dir="rtl"></span>​ <span dir="rtl">في المعادلة</span>
(12.30)<span dir="rtl">.</span>

<span dir="rtl">خوارزمية</span> HTD(λ) <span dir="rtl">هي خوارزمية هجينة
لقيم الحالة</span> (State-Value) <span dir="rtl">تجمع بين جوانب
من</span> GTD(λ) <span dir="rtl">و</span>TD(λ)<span dir="rtl">.</span>
<span dir="rtl">السمة الأكثر جاذبية لها هي أنها تعميم صارم
لخوارزمية</span> TD(λ) <span dir="rtl">للتعليم خارج السياسة</span>
(Off-Policy Learning)<span dir="rtl">، مما يعني أنه إذا كانت سياسة
السلوك</span> (Behavior Policy) <span dir="rtl">هي نفسها السياسة
المستهدفة</span> (Target Policy)<span dir="rtl">، فإن</span> HTD(λ)
<span dir="rtl">تصبح نفسها</span> TD(λ)<span dir="rtl">، وهذا غير صحيح
بالنسبة لخوارزمية</span> GTD(λ)<span dir="rtl">.</span>
<span dir="rtl">هذا جذاب لأن</span> TD(λ) <span dir="rtl">غالبًا ما تكون
أسرع من</span> GTD(λ) <span dir="rtl">عندما تتقارب كلتا الخوارزميتين،
كما أن</span> TD(λ) <span dir="rtl">تتطلب ضبط حجم خطوة واحد فقط. يتم
تعريف</span> HTD(λ) <span dir="rtl">على النحو التالي</span>:

``` math
w_{t + 1} = w_{t} + \alpha\left( \delta_{t}z_{t} + \alpha\left( \left( z_{t} - \overline{z_{t}} \right)^{\top v_{t}} \right)\left( x_{t} - \gamma_{t + 1}x_{t + 1} \right) \right)
```

``` math
v_{t + 1} = v_{t} + \beta\left( \delta_{t}z_{t} - \beta\left( \overline{z_{t}^{\top v_{t}}} \right)\left( x_{t} - \gamma_{t + 1}x_{t + 1} \right) \right)
```

``` math
z_{t} = \rho_{t}\left( \gamma_{t}\lambda_{t}z_{t - 1} + x_{t} \right)
```

$`\overline{z_{t}} = \gamma_{t}\lambda_{t}\overline{z_{t - 1}} + x_{t}`$
<span dir="rtl">مع</span> $`z_{- 1}^{b} = 0`$

<span dir="rtl">حيث أن</span> $`\beta > 0`$ <span dir="rtl">هو مرة أخرى
بارامتر خطوة ثاني</span> (Second Step-Size
Parameter)<span dir="rtl">.</span> <span dir="rtl">بالإضافة إلى مجموعة
ثانية من الأوزان،</span> $`vt`$​<span dir="rtl">، تحتوي خوارزمية</span>
HTD(λ) <span dir="rtl">أيضًا على مجموعة ثانية من تتبعات الأهلية،</span>
$`ztb`$​<span dir="rtl">.</span> <span dir="rtl">هذه التتبعات هي تتبعات
أهلية تراكمية تقليدية لسياسة السلوك</span> (Behavior Policy)
<span dir="rtl">وتصبح مساوية لـ</span> $`zt`$ <span dir="rtl"></span>​
<span dir="rtl">إذا كانت جميع</span> $`\rho t`$
<span dir="rtl">تساوي</span> $`1`$<span dir="rtl">، مما يؤدي إلى أن يكون
المصطلح الأخير في تحديث</span> $`wt`$ <span dir="rtl">صفراً ويقلل التحديث
العام إلى خوارزمية</span> TD(λ)<span dir="rtl">.</span>

<span dir="rtl">خوارزمية</span> Emphatic TD(λ) <span dir="rtl">هي امتداد
لخوارزمية</span> Emphatic-TD <span dir="rtl">خطوة واحدة</span>
<span dir="rtl">(</span>One-Step <span dir="rtl"></span>Emphatic-TD
Algorithm<span dir="rtl">) (في الفصلين 9.11 و11.8) إلى تتبعات
الأهلية</span> <span dir="rtl">(</span>Eligibility
<span dir="rtl"></span>Traces<span dir="rtl">)</span>.
<span dir="rtl">تحتفظ الخوارزمية الناتجة بضمانات تقارب قوية خارج
السياسة</span> <span dir="rtl">(</span>Off-Policy
<span dir="rtl"></span>Convergence Guarantees<span dir="rtl">) بينما
تمكن من أي درجة من التمهيد</span> (Bootstrapping)<span dir="rtl">، وإن
كان ذلك على حساب ارتفاع التباين</span> (High Variance)
<span dir="rtl">والتقارب البطيء المحتمل</span>
<span dir="rtl">(</span>Potentially Slow
<span dir="rtl"></span>Convergence<span dir="rtl">)</span>.
<span dir="rtl">يتم تعريف</span> Emphatic TD(λ) <span dir="rtl">على
النحو التالي</span>:

``` math
w_{t + 1} = w_{t} + \alpha\delta_{t}z_{t}
```

``` math
\delta_{t} = R_{t + 1} + \gamma_{t + 1}w_{t}^{\top x_{t + 1}} - w_{t}^{\top x_{t}}
```

``` math
z_{t} = \rho_{t}\left( \gamma_{t}\lambda_{t}z_{t - 1} + M_{t}x_{t} \right)
```

``` math
M_{t} = \lambda_{t}I_{t} + \left( 1 - \lambda_{t} \right)F_{t}
```

``` math
F_{t} = \rho_{t - 1}\gamma_{t}F_{t - 1} + I_{t}
```

<span dir="rtl">حيث أن</span> $`Mt \geq 0`$ <span dir="rtl">هو الشكل
العام للتأكيد</span> (Emphasis)<span dir="rtl">، و</span>$`Ft \geq 0`$
<span dir="rtl">يُطلق عليه تتبع التتابع</span>
<span dir="rtl">(</span>Follow-On Trace<span dir="rtl">)،
و</span>$`It \geq 0\ `$<span dir="rtl">هو الاهتمام</span>
(Interest)<span dir="rtl">، كما هو موضح في القسم 11.8. لاحظ أن</span>
$`Mt`$​<span dir="rtl">، مثل</span> $`\delta t`$​<span dir="rtl">، ليس في
الحقيقة متغير ذاكرة إضافي. يمكن إزالته من الخوارزمية عن طريق استبدال
تعريفه في معادلة تتبع الأهلية</span> (Eligibility
Trace)<span dir="rtl">.</span> <span dir="rtl">الكود الزائف</span>
(Pseudocode) <span dir="rtl">والبرامج الخاصة بالنسخة الحقيقية عبر
الإنترنت من خوارزمية</span> Emphatic-TD(λ) <span dir="rtl">متاحة على
الإنترنت  
(</span>Sutton, 2015b<span dir="rtl">).</span>

<span dir="rtl">في حالة داخل السياسة</span> (On-Policy)
<span dir="rtl">(</span>$`\rho t = 1`$ <span dir="rtl">لكل</span>
$`t`$<span dir="rtl">)، تكون خوارزمية</span> Emphatic-TD(λ)
<span dir="rtl">مشابهة لخوارزمية</span> TD(λ) <span dir="rtl">التقليدية،
لكنها لا تزال مختلفة بشكل كبير. في الواقع، بينما يُضمن تقارب
خوارزمية</span> Emphatic-TD(λ) <span dir="rtl">لجميع الدوال المعتمدة على
الحالة</span> λ<span dir="rtl">، فإن خوارزمية</span> TD(λ)
<span dir="rtl">ليست كذلك. يتم ضمان تقارب</span> TD(λ)
<span dir="rtl">فقط لجميع القيم الثابتة لـ</span>
λ<span dir="rtl">.</span> <span dir="rtl">انظر المثال المضاد لـ</span>
Yu <span dir="rtl">(</span>Ghiassian, <span dir="rtl"></span>Rafiee, and
Sutton, 2016<span dir="rtl">)</span>.

**<u>12.12 <span dir="rtl">قضايا التنفيذ</span> (Implementation
Issues)</u>**

<span dir="rtl">قد يبدو في البداية أن الطرق الجدولية</span> (Tabular
Methods) <span dir="rtl">التي تستخدم تتبعات الأهلية</span> (Eligibility
Traces) <span dir="rtl">أكثر تعقيدًا بكثير من الطرق التي تعتمد على خطوة
واحدة</span> <span dir="rtl">(</span>One-Step
<span dir="rtl"></span>Methods<span dir="rtl">)</span>.
<span dir="rtl">قد يتطلب تنفيذ بسيط وساذج تحديث كل حالة</span> (State)
(<span dir="rtl">أو زوج حالة-إجراء</span>
<span dir="rtl">(</span>State-Action Pair<span dir="rtl">)</span>
<span dir="rtl">لكل من تقدير القيمة</span> (Value Estimate)
<span dir="rtl">وتتبع الأهلية</span> (Eligibility Trace)
<span dir="rtl">في كل خطوة زمنية. لن تكون هذه مشكلة في التنفيذات على
أجهزة الحاسوب المتوازية التي تستخدم تعليمات فردية وبيانات متعددة</span>
(Single-Instruction, Multiple-Data) <span dir="rtl">أو في تطبيقات شبكات
عصبية اصطناعية</span> (Artificial Neural Networks - ANN)
<span dir="rtl">محتملة، ولكنها تمثل مشكلة للتنفيذات على أجهزة الحاسوب
التسلسلية التقليدية</span> (Conventional Serial Computers).
<span dir="rtl">لحسن الحظ، بالنسبة للقيم النموذجية لـ</span> $`\lambda`$
<span dir="rtl">و</span>$`\gamma`$<span dir="rtl">، تكون تتبعات
الأهلية</span> (Eligibility Traces) <span dir="rtl">لمعظم الحالات</span>
(States) <span dir="rtl">في الغالب قريبة من الصفر؛ فقط تلك الحالات التي
تم زيارتها مؤخرًا سيكون لديها تتبعات</span> (Traces) <span dir="rtl">أعلى
بشكل كبير من الصفر، وفقط هذه الحالات القليلة تحتاج إلى تحديث لتقريب هذه
الخوارزميات بدقة</span>.

<span dir="rtl">في الممارسة العملية، يمكن للتنفيذات على أجهزة الحاسوب
التقليدية</span> (Conventional Computers) <span dir="rtl">أن تحتفظ بتتبع
وتحديث فقط التتبعات</span> (Traces) <span dir="rtl">التي تكون أعلى بشكل
كبير من الصفر. باستخدام هذه الحيلة، يكون العبء الحسابي</span>
(Computational Expense) <span dir="rtl">لاستخدام التتبعات</span>
(Traces) <span dir="rtl">في الطرق الجدولية</span> (Tabular Methods)
<span dir="rtl">عادةً أكبر بقليل من الطرق التي تعتمد على خطوة
واحدة</span> (One-Step Methods)<span dir="rtl">. يعتمد المقدار الدقيق
بالطبع على</span> $`\lambda`$ <span dir="rtl">و</span>$`\gamma`$
<span dir="rtl">وعلى العبء الحسابي للعمليات الأخرى. لاحظ أن الحالة
الجدولية</span> (Tabular Case) <span dir="rtl">هي في بعض النواحي الحالة
الأسوأ من حيث التعقيد الحسابي</span> (Computational Complexity)
<span dir="rtl">لتتبعات الأهلية</span> (Eligibility
Traces)<span dir="rtl">.</span> <span dir="rtl">عندما يتم استخدام تقريب
الدوال</span> (Function Approximation)<span dir="rtl">، فإن المزايا
الحسابية</span> <span dir="rtl">(</span>Computational
<span dir="rtl"></span>Advantages<span dir="rtl">) لعدم استخدام
التتبعات</span> (Traces) <span dir="rtl">تنخفض بشكل عام. على سبيل
المثال، إذا تم استخدام الشبكات العصبية الاصطناعية</span> (Artificial
Neural Networks - ANN) <span dir="rtl">والانتشار العكسي</span>
(Backpropagation)<span dir="rtl">، فإن تتبعات الأهلية</span>
(Eligibility Traces) <span dir="rtl">تسبب عادةً زيادة مضاعفة في الذاكرة
المطلوبة والحسابات لكل خطوة. يمكن أن تكون طرق العائد المقطوع</span>
λ-return <span dir="rtl">(الفصل 12.3) فعالة من الناحية الحسابية</span>
(Computationally Efficient) <span dir="rtl">على أجهزة الحاسوب
التقليدية</span> (Conventional Computers)<span dir="rtl">، على الرغم من
أنها تتطلب دائمًا بعض الذاكرة الإضافية</span> (Additional
Memory)<span dir="rtl">.</span>

**<u>12.13 <span dir="rtl">الخاتمة</span> (Conclusions)</u>**

<span dir="rtl">تتبعات الأهلية</span> (Eligibility Traces)
<span dir="rtl">بالتزامن مع أخطاء</span> TD <span dir="rtl"></span>(TD
Errors) <span dir="rtl">توفر طريقة فعالة وتدريجية للتحول والاختيار بين
طرق مونت كارلو</span> (Monte Carlo Methods) <span dir="rtl">وطرق</span>
TD<span dir="rtl">.</span> <span dir="rtl">لقد مكنت طرق الـ</span>
n-step <span dir="rtl">في الفصل السابع من تحقيق ذلك أيضًا، ولكن طرق تتبع
الأهلية</span> <span dir="rtl">(</span>Eligibility Trace
Methods<span dir="rtl">) أكثر عمومية وغالبًا ما تكون أسرع في التعليم
وتوفر مقايضات مختلفة من حيث التعقيد الحسابي</span> (Computational
Complexity)<span dir="rtl">.</span> <span dir="rtl">قدم هذا الفصل مقدمة
لفهم نظري أنيق وناشئ لتتبعات الأهلية</span> (Eligibility Traces)
<span dir="rtl">للتعليم داخل السياسة</span> (On-Policy)
<span dir="rtl">وخارج السياسة  
(</span>Off-Policy<span dir="rtl">) وللتمهيد</span> (Bootstrapping)
<span dir="rtl">والتخفيض</span> (Discounting) <span dir="rtl">المتغيرين.
أحد جوانب هذه النظرية الأنيقة هو الطرق الحقيقية عبر الإنترنت</span>
(True Online Methods)<span dir="rtl">، التي تعيد إنتاج سلوك الطرق
المثالية المكلفة تمامًا مع الحفاظ على ملاءمة الحوسبة لطرق</span> TD
<span dir="rtl">التقليدية. جانب آخر هو إمكانية اشتقاقات</span>
(Derivations) <span dir="rtl">تتحول تلقائيًا من طرق النظر إلى الأمام
البديهية</span> <span dir="rtl">(</span>Forward-View
Methods<span dir="rtl">) إلى خوارزميات أكثر كفاءة تعتمد على النظر إلى
الخلف</span> <span dir="rtl">(</span>Backward-View
<span dir="rtl"></span>Algorithms<span dir="rtl">)</span>.
<span dir="rtl">لقد وضحنا هذه الفكرة العامة في اشتقاق بدأ بخوارزمية مونت
كارلو</span> <span dir="rtl">(</span>Monte Carlo
<span dir="rtl"></span>Algorithm<span dir="rtl">) الكلاسيكية والمكلفة
وانتهى بتنفيذ تدريجي رخيص غير</span> TD <span dir="rtl">باستخدام نفس
تتبع الأهلية</span> (Eligibility Trace) <span dir="rtl">المستخدم في
طرق</span> TD <span dir="rtl">الحقيقية عبر الإنترنت</span>
<span dir="rtl">(</span>True Online TD
<span dir="rtl"></span>Methods<span dir="rtl">)</span>.

<span dir="rtl">كما ذكرنا في الفصل الخامس، قد تكون لطرق مونت
كارلو</span> (Monte Carlo Methods) <span dir="rtl">مزايا في المهام غير
الماركوفيية</span> (Non-Markov Tasks) <span dir="rtl">لأنها لا تعتمد على
التمهيد</span> (Bootstrapping)<span dir="rtl">. ولأن تتبعات
الأهلية</span> (Eligibility Traces) <span dir="rtl">تجعل طرق</span> TD
<span dir="rtl">أكثر شبهًا بطرق مونت كارلو</span>
<span dir="rtl">(</span>Monte <span dir="rtl"></span>Carlo
Methods<span dir="rtl">)، فإنها قد تقدم مزايا في هذه الحالات أيضًا. إذا
كنت ترغب في استخدام طرق</span> TD <span dir="rtl">بسبب مزاياها الأخرى،
ولكن المهمة جزئيًا غير ماركوفيانية</span> (Non-Markov)<span dir="rtl">،
فإن استخدام طريقة تتبع الأهلية</span> (Eligibility Trace Method)
<span dir="rtl">هو الحل المناسب. تعتبر تتبعات الأهلية</span>
<span dir="rtl">(</span>Eligibility
<span dir="rtl"></span>Traces<span dir="rtl">) خط الدفاع الأول ضد
المكافآت المتأخرة لفترة طويلة</span> (Long-Delayed Rewards)
<span dir="rtl">والمهام غير الماركوفيية</span> (Non-Markov
Tasks)<span dir="rtl">.</span>

<span dir="rtl">من خلال تعديل</span> $`\lambda`$<span dir="rtl">، يمكننا
وضع طرق تتبع الأهلية</span> (Eligibility Trace Methods)
<span dir="rtl">في أي مكان على امتداد بين طرق مونت كارلو</span> (Monte
Carlo Methods) <span dir="rtl">وطرق</span> TD <span dir="rtl">ذات الخطوة
الواحدة</span> (One-Step TD Methods)<span dir="rtl">.</span>
<span dir="rtl">أين ينبغي لنا أن نضعها؟ لا نملك بعد إجابة نظرية جيدة على
هذا السؤال، لكن يبدو أن إجابة تجريبية واضحة بدأت تظهر. في المهام التي
تحتوي على العديد من الخطوات لكل حلقة</span> (Episode) <span dir="rtl">أو
العديد من الخطوات ضمن نصف العمر للتخفيض  
(</span>Half-Life <span dir="rtl"></span>of
Discounting<span dir="rtl">)، يبدو أنه من الأفضل بكثير استخدام تتبعات
الأهلية</span> <span dir="rtl">(</span>Eligibility
<span dir="rtl"></span>Traces<span dir="rtl">) بدلاً من عدم استخدامها
(انظر على سبيل المثال الشكل 12.14). من ناحية أخرى، إذا كانت التتبعات
طويلة جدًا بحيث تنتج طريقة مونت كارلو</span> (Monte Carlo Method)
<span dir="rtl">نقية أو شبه نقية، فإن الأداء يتدهور بشكل حاد. يبدو أن
الخيار الأفضل هو مزيج وسيط. يجب استخدام تتبعات الأهلية</span>
(Eligibility Traces) <span dir="rtl">لتقريبنا من طرق مونت كارلو</span>
(Monte Carlo Methods)<span dir="rtl">، ولكن ليس تمامًا. في المستقبل قد
يكون من الممكن تعديل المقايضة</span> (Trade-Off) <span dir="rtl">بين
طرق</span> TD <span dir="rtl">وطرق مونت كارلو</span> (Monte Carlo
Methods) <span dir="rtl">بدقة أكبر باستخدام</span> λ
<span dir="rtl">المتغير، ولكن في الوقت الحاضر ليس من الواضح كيف يمكن
القيام بذلك بشكل موثوق ومفيد</span>.

<span dir="rtl">تتطلب الطرق التي تستخدم تتبعات الأهلية</span>
(Eligibility Traces) <span dir="rtl">مزيدًا من الحسابات مقارنة بالطرق ذات
الخطوة الواحدة</span> (One-Step Methods)<span dir="rtl">، ولكن في
المقابل تقدم تعلمًا أسرع بشكل ملحوظ، خاصةً عندما تتأخر المكافآت</span>
(Rewards) <span dir="rtl">لعدة خطوات. لذلك غالبًا ما يكون من المنطقي
استخدام تتبعات الأهلية</span> (Eligibility Traces) <span dir="rtl">عندما
تكون البيانات نادرة ولا يمكن معالجتها بشكل متكرر، كما هو الحال غالبًا في
التطبيقات عبر الإنترنت</span> (Online
Applications)<span dir="rtl">.</span> <span dir="rtl">من ناحية أخرى، في
التطبيقات غير المتصلة بالإنترنت</span> (Offline Applications)
<span dir="rtl">حيث يمكن توليد البيانات بتكلفة منخفضة، ربما من خلال
محاكاة غير مكلفة، فإنه غالبًا لا يستحق استخدام تتبعات الأهلية</span>
(Eligibility Traces)<span dir="rtl">.</span> <span dir="rtl">في هذه
الحالات، الهدف ليس استخراج المزيد من كمية محدودة من البيانات، بل ببساطة
معالجة أكبر قدر ممكن من البيانات بأسرع وقت ممكن. في هذه الحالات، لا
يستحق التسريع لكل داتوم</span> (Datum) <span dir="rtl">بسبب
التتبعات</span> (Traces) <span dir="rtl">تكلفة الحوسبة، وتفضل الطرق ذات
الخطوة الواحدة</span> (One-Step Methods)<span dir="rtl">.</span>

<img src="./media/image158.png"
style="width:6.26806in;height:4.76042in" />

<span dir="rtl">الشكل 12.14: تأثير</span> λ <span dir="rtl">على أداء
التعليم المعزز</span> (Reinforcement Learning) <span dir="rtl">في أربع
مشاكل اختبار مختلفة. في جميع الحالات، يكون الأداء عادةً أفضل (رقم أقل في
الرسم البياني) عند قيمة متوسطة لـ</span> λ<span dir="rtl">.</span>
<span dir="rtl">اللوحتان اليساريتان هما تطبيقات على مهام تحكم ذات حالة
مستمرة بسيطة باستخدام خوارزمية</span> Sarsa(λ) <span dir="rtl">وترميز
البلاط</span> (Tile Coding)<span dir="rtl">، مع إما تتبعات
الاستبدال</span> <span dir="rtl">(</span>Replacing
<span dir="rtl"></span>Traces<span dir="rtl">) أو التتبعات
التراكمية</span> (Accumulating Traces) (Sutton,
1996)<span dir="rtl">.</span> <span dir="rtl">اللوحة العلوية اليمنى هي
لتقييم السياسة</span> (Policy Evaluation) <span dir="rtl">في مهمة المشي
العشوائي</span> (Random Walk Task) <span dir="rtl">باستخدام
خوارزمية</span> TD(λ) (Singh and Sutton, 1996)<span dir="rtl">.</span>
<span dir="rtl">اللوحة السفلية اليمنى هي بيانات غير منشورة لمهمة موازنة
القطب</span> (Pole-Balancing Task) <span dir="rtl">(المثال 3.4) من دراسة
سابقة  
(</span>Sutton, 1984<span dir="rtl">)</span>.

<span dir="rtl">الفصل الثالث عشر:  
طرق تدرج السياسة</span> (Policy Gradient Methods)

<span dir="rtl">في هذا الفصل، نناقش شيئًا جديدًا. حتى الآن في هذا الكتاب،
كانت تقريبًا جميع **الأساليب** </span>**(Methods)** <span dir="rtl">تعتمد
على **قيمة الإجراء** </span>**(Action-Value Methods)**<span dir="rtl">؛
حيث تعلمت **قيم الإجراءات** </span>**(Action Values)**
<span dir="rtl">ثم اختارت **الإجراءات**</span> **(Actions)**
<span dir="rtl">بناءً على قيم الإجراءات المقدرة؛ لم تكن
**سياساتها**</span> **(Policies)** <span dir="rtl">أن توجد حتى بدون
تقديرات قيم الإجراءات. في هذا الفصل، نناقش **أساليب**
</span>**(Methods)** <span dir="rtl">تعتمد على تعلم **سياسة
بارامتر**</span> **(Parameterized Policy)** <span dir="rtl">يمكنها
اختيار الإجراءات دون الرجوع إلى **دالة القيمة** </span>**(Value
Function)**<span dir="rtl">.</span> <span dir="rtl">قد يتم استخدام دالة
القيمة لتعلم **بارامتر** </span>**(Parameter)** <span dir="rtl">السياسة،
ولكنها ليست ضرورية لاختيار الإجراءات. نستخدم الرمز</span>
$`\theta \in Rd'`$ <span dir="rtl">لتمثيل **متجه**</span> **(Vector)**
<span dir="rtl">بارامتر السياسة. لذلك نكتب</span>
$`\pi\left( a \middle| s,\theta \right) = \text{Pr\{}A_{t} = a \mid S_{t} = s,\theta_{t} = \theta\text{\}}`$,
<span dir="rtl">لتمثيل احتمال اتخاذ الإجراء</span> $`a`$
<span dir="rtl">في الزمن</span> $`t`$ <span dir="rtl">معطى أن
**البيئة**</span> **(Environment)** <span dir="rtl">في **الحالة**
</span>**(State)** <span dir="rtl"></span>$`s`$ <span dir="rtl">في
الزمن</span> $`t`$ <span dir="rtl">مع **البارامتر**
</span>**(Parameter)**
<span dir="rtl"></span>$`\theta`$<span dir="rtl">. إذا استخدم
**الأسلوب**</span> **(Method)** <span dir="rtl">أيضًا **دالة قيمة
متعلمة** </span>**(Learned Value Function)**<span dir="rtl">، فإن متجه
أوزان دالة القيمة يُرمز له بـ</span> $`w \in Rd`$
<span dir="rtl">كالمعتاد، مثل</span>
$`v\hat{}(s,w)`$<span dir="rtl">.</span>

<span dir="rtl">في هذا الفصل، نناقش **أساليب**</span> **(Methods)**
<span dir="rtl">لتعلم بارامتر السياسة بناءً على **التدرج**
</span>**(Gradient)** <span dir="rtl">لمقياس أداء عددي</span>
$`J(\theta)`$ <span dir="rtl">فيما يتعلق ببارامتر السياسة. تسعى هذه
**الأساليب**</span> **(Methods)** <span dir="rtl">إلى تعظيم الأداء، لذلك
فإن تحديثاتها تقارب الصعود التدرجي في</span>
$`J`$<span dir="rtl">.</span>

``` math
\theta_{t + 1} = \theta_{t} + \alpha\widehat{\nabla J\left( \theta_{t} \right)}
```

<span dir="rtl">حيث أن</span>
$`\nabla J\left( \theta_{t} \right) \in R^{d'}`$ <span dir="rtl"></span>
<span dir="rtl">هو تقدير عشوائي ذو توقع يقارب التدرج لمقياس الأداء
بالنسبة للحجة</span> $`\theta t`$​<span dir="rtl">جميع
**الأساليب**</span> **(Methods)** <span dir="rtl">التي تتبع هذا المخطط
العام نطلق عليها **أساليب تدرج السياسة** </span>**(Policy Gradient
Methods)**<span dir="rtl">، سواء أكانت تتعلم أيضًا **دالة قيمة
تقريبية**</span> **<span dir="rtl">(</span>Approximate
<span dir="rtl"></span>Value Function<span dir="rtl">)
</span>**<span dir="rtl">أم لا.</span> **<span dir="rtl">الأساليب</span>
(Methods)** <span dir="rtl">التي تتعلم تقريبات لكل من السياسة ودالة
القيمة تُسمى غالبًا **أساليب الممثل–الناقد** </span>**(Actor–Critic
Methods)**<span dir="rtl">، حيث يشير مصطلح "الممثل" إلى **السياسة
المتعلمة** </span>**(Learned Policy)**<span dir="rtl">، و"الناقد" يشير
إلى **دالة القيمة المتعلمة**</span> **<span dir="rtl">(</span>Learned
<span dir="rtl"></span>Value
Function<span dir="rtl">)</span>**<span dir="rtl">، والتي تكون عادةً
**دالة قيمة الحالة** </span>**(State-Value
Function)**<span dir="rtl">.</span>

<span dir="rtl">أولاً، نتناول حالة الحلقات المنفصلة</span> (Episodic
Case)<span dir="rtl">، حيث يتم تعريف الأداء على أنه قيمة حالة البدء وفقًا
للسياسة البارامتر، قبل الانتقال إلى حالة الاستمرار</span> (Continuing
Case) <span dir="rtl">حيث يتم تعريف الأداء على أنه معدل المكافأة
المتوسط، كما في **الفصل** </span>**(Section)**
10.3<span dir="rtl">.</span> <span dir="rtl">في النهاية، نتمكن من
التعبير عن **الخوارزميات**</span> **(Algorithms)** <span dir="rtl">لكلتا
الحالتين بعبارات متشابهة جدًا</span>.

**<u>13.1 <span dir="rtl">تقريب السياسة</span> (Policy Approximation)
<span dir="rtl">ومزاياها</span> (Advantages)</u>**

<span dir="rtl">في **أساليب تدرج السياسة** </span>**(Policy Gradient
Methods)**<span dir="rtl">، يمكن **تعبئة السياسة**
</span>**(Parameterized Policy)** <span dir="rtl">بأي طريقة، طالما
أن</span> $`\pi(a \mid s,\theta)`$ <span dir="rtl">قابلة للتفاضل بالنسبة
إلى **بارامتراتها** </span>**(Parameters)**<span dir="rtl">، أي طالما
أن</span> $`\nabla\pi(a \mid s,\theta)`$ <span dir="rtl">وهو **المتجه
العمودي  **
</span>**(Column Vector)** <span dir="rtl">للمشتقات الجزئية لـ</span>
$`\pi(a \mid s,\theta)`$ <span dir="rtl">بالنسبة لمكونات</span> θ
<span dir="rtl">موجود وقابل للحساب لجميع</span>
$`s \in S`$<span dir="rtl">، و</span>A(s)a∈A<span dir="rtl">،
و</span>θ∈Rd′<span dir="rtl">.</span> <span dir="rtl">في الواقع، لضمان
**الاستكشاف** </span>**(Exploration)**<span dir="rtl">، نشترط عمومًا أن
السياسة لا تصبح حتمية (أي أن</span>
$`\pi(a \mid s,\theta)\  \in \ (0,1)`$ <span dir="rtl">لجميع</span>
$`s`$ <span dir="rtl">و</span>$`a`$
<span dir="rtl">و</span>$`\theta`$<span dir="rtl">)</span>.
<span dir="rtl">في هذا القسم، نعرض النوع الأكثر شيوعًا من
**البارامترية**</span> **(Parameterization)** <span dir="rtl">لمساحات
الإجراءات المنفصلة ونشير إلى **مزاياها** </span>**(Advantages)**
<span dir="rtl">مقارنة بأساليب **قيمة الإجراء (**</span>**Action-Value
<span dir="rtl"></span>Methods<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">تقدم الأساليب المعتمدة على السياسة أيضًا طرقًا مفيدة
للتعامل مع مساحات الإجراءات المستمرة، كما سنصف لاحقًا في **الفصل**
</span>**(Section)** 13.7<span dir="rtl">.</span>

<span dir="rtl">إذا كانت **مساحة الإجراءات**</span> **(Action Space)**
<span dir="rtl">منفصلة وليست كبيرة جدًا، فإن نوعًا طبيعيًا وشائعًا من
**البارامترية**</span> **(Parameterization)** <span dir="rtl">هو تكوين
تفضيلات عددية بارامتر</span> $`h(s,a,\theta)\  \in R`$,
<span dir="rtl">لكل زوج من **الحالة والإجراء** </span>**(State-Action
Pair)**<span dir="rtl">.</span> <span dir="rtl">تُمنح الإجراءات ذات
التفضيلات الأعلى في كل حالة أعلى احتمالات للاختيار، على سبيل المثال،
وفقًا لتوزيع **الأقصى الأسي اللين**</span>
**<span dir="rtl">(</span>Exponential Soft-Max
Distribution<span dir="rtl">)</span>**.

``` math
\pi\left( a \middle| s,\theta \right) = \frac{e^{h(s,a,\theta)}}{\sum_{b}^{}e^{h(s,b,\theta)}}
```

<span dir="rtl">حيث أن</span> $`e \approx 2.71828`$ <span dir="rtl">هو
أساس **اللوغاريتم الطبيعي** </span>**(Natural
Logarithm)**<span dir="rtl">.</span> <span dir="rtl">لاحظ أن المقام هنا
هو فقط ما يتطلبه الأمر لكي تكون احتمالات الإجراءات في كل حالة مجموعها
يساوي واحدًا. نطلق على هذا النوع من **بارامترية السياسة**</span>
**(Policy Parameterization)** <span dir="rtl">الأقصى اللين  
(</span>Soft-Max<span dir="rtl">)</span> <span dir="rtl">في تفضيلات
الإجراءات</span>.

<span dir="rtl">يمكن أن يتم **تعبئة**</span> **(Parameterize)**
<span dir="rtl">تفضيلات الإجراءات نفسها بشكل اعتباطي. على سبيل المثال،
يمكن حسابها بواسطة **شبكة عصبية اصطناعية عميقة** </span>**(Deep
Artificial Neural Network)**<span dir="rtl">، حيث تكون</span> $`\theta`$
<span dir="rtl">هي **متجه**</span> **(Vector)** <span dir="rtl">جميع
أوزان الاتصال للشبكة (كما في نظام</span> **AlphaGo**
<span dir="rtl">الموصوف في **الفصل)
(**</span>16.6<span dir="rtl">)</span>. <span dir="rtl">أو يمكن أن تكون
التفضيلات ببساطة خطية في **المزايا**
</span>**(Features)**<span dir="rtl">.</span>

``` math
h(s,a,\theta) = \theta^{\top}x(s,a)
```

<span dir="rtl">باستخدام **متجهات الميزات**</span> **(Feature Vectors)**
$`x\ (s,a)\  \in Rd'`$ <span dir="rtl">التي تم إنشاؤها بواسطة أي من
**الأساليب**</span> **(Methods)** <span dir="rtl">الموصوفة في **الفصل**
</span>9<span dir="rtl">.</span>

<span dir="rtl">إحدى **مزايا**</span> **(Advantages)**
**<span dir="rtl">بارامترية السياسات</span> (Parameterized Policies)**
<span dir="rtl">وفقًا لطريقة الأقصى اللين</span> (Soft-Max)
<span dir="rtl">في تفضيلات الإجراءات هي أن السياسة التقريبية يمكن أن
تقترب من سياسة حتمية، في حين أنه مع اختيار الإجراءات بناءً على
"الجشع</span> (ϵ-Greedy)" <span dir="rtl">في قيم الإجراءات، يوجد دائمًا
احتمال</span> ϵ <span dir="rtl">لاختيار إجراء عشوائي. بالطبع، يمكن
اختيار الإجراءات وفقًا لتوزيع "الأقصى اللين</span> (Soft-Max)
<span dir="rtl">المستند إلى **قيم الإجراءات** </span>**(Action
Values)**<span dir="rtl">، ولكن هذا وحده لن يسمح للسياسة بالاقتراب من
سياسة حتمية. بدلاً من ذلك، ستتقارب تقديرات قيمة الإجراءات مع قيمها
الحقيقية المقابلة، والتي ستختلف بمقدار محدود، مما يؤدي إلى احتمالات
محددة غير صفرية أو واحد. إذا شمل توزيع الأقصى اللين</span> (Soft-Max)
**<span dir="rtl">معامل درجة الحرارة</span> (Temperature
Parameter)**<span dir="rtl">، فيمكن تقليل درجة الحرارة بمرور الوقت
للاقتراب من الحتمية، ولكن في الممارسة العملية سيكون من الصعب اختيار جدول
تخفيض درجة الحرارة، أو حتى درجة الحرارة الابتدائية، دون معرفة مسبقة أكثر
بقيم الإجراءات الحقيقية مما نرغب في الافتراض</span>.

**<span dir="rtl">تفضيلات الإجراءات</span> (Action Preferences)**
<span dir="rtl">تختلف لأنها لا تقترب من قيم محددة؛ بدلاً من ذلك، يتم
دفعها لإنتاج **السياسة العشوائية المثلى** </span>**(Optimal Stochastic
Policy)**<span dir="rtl">.</span> <span dir="rtl">إذا كانت السياسة
المثلى حتمية، فسيتم دفع تفضيلات الإجراءات المثلى إلى ما لا نهاية أعلى من
جميع الإجراءات غير المثلى (إذا سمحت البارامترية بذلك)</span>.

<span dir="rtl">الميزة الثانية من **مزايا**</span> **(Advantages)**
**<span dir="rtl">بارامترية السياسات</span> (Parameterized Policies)**
<span dir="rtl">وفقًا لطريقة الأقصى اللين</span> (Soft-Max)
<span dir="rtl">في تفضيلات الإجراءات هي أنها تُمكن من اختيار الإجراءات
باحتمالات اعتباطية. في المشكلات التي تتضمن **تقريب الدوال**
</span>**(Function Approximation)** <span dir="rtl">بشكل كبير، قد تكون
أفضل سياسة تقريبية هي سياسة عشوائية. على سبيل المثال، في ألعاب الورق
التي تحتوي على معلومات غير كاملة، يكون اللعب الأمثل غالبًا هو القيام
بشيئين مختلفين باحتمالات محددة، كما هو الحال عند الخداع في لعبة
البوكر</span>.

<span dir="rtl">أساليب **قيمة الإجراء**</span> **(Action-Value
Methods)** <span dir="rtl">لا تمتلك طريقة طبيعية لإيجاد **السياسات
المثلى العشوائية** </span>**(Stochastic Optimal
Policies)**<span dir="rtl">، في حين أن **أساليب تقريب السياسات**</span>
**<span dir="rtl">(</span>Policy <span dir="rtl"></span>Approximating
Methods<span dir="rtl">)</span>** <span dir="rtl">يمكنها ذلك، كما هو
موضح في **المثال** </span>13.1<span dir="rtl">.</span>

<span dir="rtl">المثال 13.1: الممر القصير مع الإجراءات المعكوسة</span>
<span dir="rtl">(</span>Short Corridor with Switched
<span dir="rtl"></span>Actions<span dir="rtl">)</span>

<span dir="rtl">فكر في شبكة **الممر القصير**</span> **(Short Corridor
Gridworld)** <span dir="rtl">الموضحة في الشكل المدرج في الرسم البياني
أدناه.</span> **<span dir="rtl">المكافأة</span> (Reward)**
<span dir="rtl">هي 1- لكل خطوة، كالمعتاد. في كل واحدة من الحالات الثلاث
غير النهائية، يوجد فقط إجراءان: اليمين واليسار. هذه **الإجراءات**</span>
**(Actions)** <span dir="rtl">لها نفس النتائج المعتادة في الحالة الأولى
والثالثة (اليسار لا يؤدي إلى أي حركة في الحالة الأولى)، ولكن في الحالة
الثانية يتم عكسهما، بحيث يؤدي اليمين إلى الانتقال إلى اليسار واليسار إلى
الانتقال إلى اليمين</span>.

<span dir="rtl">تعتبر هذه **المشكلة**</span> **(Problem)**
<span dir="rtl">صعبة لأن جميع الحالات تبدو متطابقة تحت **تقريب الدوال**
</span>**(Function Approximation)**<span dir="rtl">. على وجه الخصوص،
نحدد</span> $`x(s,right)\  = \ \lbrack 1,0\rbrack\ \top`$,
<span dir="rtl">و</span> $`x(s,left) = \lbrack 0,1\rbrack\top`$
<span dir="rtl">لجميع</span> $`s`$<span dir="rtl">.</span>
**<span dir="rtl">أسلوب قيمة الإجراء</span> (Action-Value Method)**
<span dir="rtl">مع اختيار الإجراءات وفقًا للجشع</span> ϵ-Greedy
<span dir="rtl">يكون مجبرًا على الاختيار بين سياستين فقط: اختيار اليمين
باحتمال مرتفع</span> 1−ϵ/2 <span dir="rtl">في جميع الخطوات أو اختيار
اليسار بنفس الاحتمال المرتفع في جميع الأوقات. إذا كان</span>
ϵ=0.1<span dir="rtl">، فإن هاتين السياستين تحققان قيمة (عند حالة البدء)
أقل من 44- و82- على التوالي، كما هو موضح في الرسم البياني</span>.

<span dir="rtl">يمكن أن يقوم **الأسلوب**</span> **(Method)**
<span dir="rtl">بتحقيق أداء أفضل بكثير إذا تمكن من تعلم احتمال محدد
لاختيار اليمين. أفضل احتمال هو حوالي 0.59، والذي يحقق قيمة حوالي
11.6-</span>.

<img src="./media/image159.png"
style="width:6.26806in;height:3.00417in" />

<span dir="rtl">ربما تكون أبسط ميزة يمكن أن تتمتع بها **بارامترية
السياسة**</span> **(Policy Parameterization)** <span dir="rtl">على
**بارامترية قيمة الإجراء**</span> **(Action-Value Parameterization)**
<span dir="rtl">هي أن **السياسة**</span> **(Policy)** <span dir="rtl">قد
تكون دالة أبسط للتقريب. تختلف المشاكل في تعقيد سياساتها ووظائف قيم
الإجراءات. بالنسبة للبعض، تكون **دالة قيمة الإجراء**</span>
**(Action-Value Function)** <span dir="rtl">أبسط وبالتالي أسهل في
التقريب. بالنسبة للآخرين، تكون **السياسة**</span> **(Policy)**
<span dir="rtl">أبسط. في الحالة الأخيرة، عادة ما يتعلم **الأسلوب المعتمد
على السياسة**</span> **(Policy-Based Method)** <span dir="rtl">بشكل أسرع
ويؤدي إلى سياسة متفوقة على المدى البعيد  
(كما في لعبة</span> Tetris<span dir="rtl">؛ انظر</span> Şimsek, Algorta,
and Kothiyal, 2016<span dir="rtl">)</span>.

<span dir="rtl">أخيرًا، نلاحظ أن اختيار **بارامترية السياسة**</span>
**(Policy Parameterization)** <span dir="rtl">هو أحيانًا وسيلة جيدة
لإدخال المعرفة المسبقة حول الشكل المطلوب للسياسة في نظام **التعليم
المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning System<span dir="rtl">)</span>**.
<span dir="rtl">غالبًا ما يكون هذا هو السبب الأكثر أهمية لاستخدام **أسلوب
التعليم المعتمد على السياسة** </span>**(Policy-Based Learning
Method)**<span dir="rtl">.</span>

<span dir="rtl">**تمرين 13.1** استخدم معرفتك بـ **شبكة الممر**</span>
**(Gridworld)** <span dir="rtl">وديناميكياتها لتحديد تعبير رمزي دقيق
لاحتمال اختيار الإجراء الأيمن في **المثال**
</span>13.1<span dir="rtl">.</span>

**<u>13.2 <span dir="rtl">نظرية تدرج السياسة</span> (The Policy Gradient
Theorem)</u>**

<span dir="rtl">بالإضافة إلى **المزايا العملية**</span> **(Practical
Advantages)** <span dir="rtl">التي تتمتع بها **بارامترية السياسة**
</span>**(Policy Parameterization)** <span dir="rtl">مقارنة باختيار
الإجراءات بناءً على الجشع</span> <span dir="rtl">(</span>ϵ-Greedy Action
<span dir="rtl"></span>Selection<span dir="rtl">)، هناك أيضًا ميزة نظرية
مهمة. مع **بارامترية السياسة المستمرة**</span>
**<span dir="rtl">(</span>Continuous <span dir="rtl"></span>Policy
Parameterization<span dir="rtl">)</span>**<span dir="rtl">، تتغير
احتمالات الإجراءات بسلاسة كدالة للبارامتر المتعلم، في حين أنه في اختيار
الإجراءات بناءً على الجشع</span> ϵ-Greedy<span dir="rtl">، قد تتغير
احتمالات الإجراءات بشكل كبير نتيجة لتغير صغير جدًا في قيم الإجراءات
المقدرة، إذا أدى هذا التغيير إلى أن يصبح إجراء مختلف هو الأعلى قيمة.
وبشكل كبير، بسبب هذه الخاصية، تتوفر ضمانات أقوى للتقارب في **أساليب تدرج
السياسة**</span> **(Policy-Gradient Methods)** <span dir="rtl">مقارنة
بأساليب **قيمة الإجراء**</span> **<span dir="rtl">(</span>Action-Value
<span dir="rtl"></span>Methods<span dir="rtl">)</span>**.
<span dir="rtl">على وجه الخصوص، فإن استمرارية اعتماد السياسة على
البارامترات هي التي تمكن أساليب تدرج السياسة من تقريب **الصعود
التدرجي**</span> **(Gradient Ascent)** <span dir="rtl">كما في
المعادلة</span> (13.1)<span dir="rtl">.</span>

<span dir="rtl">تحدد الحالات المنفصلة والمستمرة **مقياس الأداء**</span>
**(Performance Measure)** $`J(\theta)`$ <span dir="rtl">بشكل مختلف،
وبالتالي يجب التعامل مع كل منهما بشكل منفصل إلى حد ما. ومع ذلك، سنحاول
تقديم كلتا الحالتين بشكل موحد، وسنطور تدوينًا بحيث يمكن وصف النتائج
النظرية الرئيسية بمجموعة واحدة من المعادلات</span>.

<span dir="rtl">في هذا القسم، نتناول الحالة المنفصلة، والتي نعرف فيها
مقياس الأداء كقيمة حالة البداية للحلقة. يمكننا تبسيط التدوين دون فقدان
أي عمومية معنوية من خلال افتراض أن كل حلقة تبدأ في حالة معينة (غير
عشوائية)</span> $`s0`$​<span dir="rtl">.</span> <span dir="rtl">ثم، في
الحالة المنفصلة، نعرف الأداء كالتالي</span>:

``` math
J(\theta) \doteq v_{\pi_{\theta}}\left( s_{0} \right)
```

<span dir="rtl">حيث أن</span> $`v\pi\theta`$ <span dir="rtl"></span>​​
<span dir="rtl">هو **دالة القيمة الحقيقية**</span> **(True Value
Function)** <span dir="rtl">للسياسة</span>
$`\pi\theta`$​<span dir="rtl">، وهي السياسة التي يتم تحديدها
بواسطة</span> θ<span dir="rtl">.</span> <span dir="rtl">من هنا فصاعدًا في
نقاشنا، سنفترض عدم وجود خصم</span> (γ=1) <span dir="rtl">للحالة
المنفصلة، على الرغم من أننا من أجل الشمولية سنضمّن إمكانية الخصم في
**الخوارزميات المعبأة** </span>**(Boxed
Algorithms)**<span dir="rtl">.</span>

<span dir="rtl">مع **تقريب الدوال** </span>**(Function
Approximation)**<span dir="rtl">، قد يبدو من الصعب تغيير **بارامتر
السياسة** </span>**(Policy Parameter)** <span dir="rtl">بطريقة تضمن
التحسين. تكمن المشكلة في أن الأداء يعتمد على كل من **اختيارات
الإجراءات**</span> **(Action Selections)** <span dir="rtl">وتوزيع
الحالات التي تُتخذ فيها هذه الاختيارات، وكلاهما يتأثر بالبارامتر الخاص
بالسياسة. بالنظر إلى **الحالة** </span>**(State)**<span dir="rtl">، يمكن
حساب تأثير بارامتر السياسة على **الإجراءات**
</span>**(Actions)**<span dir="rtl">، وبالتالي على **المكافأة**
</span>**(Reward)**<span dir="rtl">، بطريقة مباشرة نسبيًا من خلال المعرفة
بالبارامترية. لكن تأثير السياسة على توزيع الحالات هو دالة على
**البيئة**</span> **(Environment)** <span dir="rtl">وعادةً ما يكون غير
معروف. كيف يمكننا تقدير **تدرج الأداء**</span> **(Performance
Gradient)** <span dir="rtl">بالنسبة إلى بارامتر السياسة عندما يعتمد
التدرج على التأثير غير المعروف لتغييرات السياسة على توزيع
الحالات؟</span>

<span dir="rtl">لحسن الحظ، هناك إجابة نظرية ممتازة لهذا التحدي على شكل
**نظرية تدرج السياسة  
(**</span>**Policy <span dir="rtl"></span>Gradient
Theorem<span dir="rtl">)</span>**<span dir="rtl">، والتي توفر تعبيرًا
تحليليًا لتدرج الأداء بالنسبة لبارامتر السياسة (وهو ما نحتاج إلى تقريبه
لتحقيق **الصعود التدرجي**</span> **(Gradient Ascent)**
<span dir="rtl">كما في المعادلة</span> (13.1)<span dir="rtl">)، وهذا
التعبير لا يتضمن مشتقة توزيع الحالات</span>.

<span dir="rtl">تُثبت **نظرية تدرج السياسة**</span> **(Policy Gradient
Theorem)** <span dir="rtl">للحالة المنفصلة أن</span>:

``` math
\nabla J(\theta) \propto \sum_{s}^{}{\mu(s)\sum_{a}^{}{q_{\pi}(s,a)\nabla\pi\left( a \middle| s,\theta \right)}}
```

<span dir="rtl">حيث أن **التدرجات**</span> **(Gradients)**
<span dir="rtl">هي **متجهات عمودية**</span> **(Column Vectors)**
<span dir="rtl">للمشتقات الجزئية فيما يتعلق بمكونات</span>
θ<span dir="rtl">، وتشير</span> $`\pi`$ <span dir="rtl">إلى
**السياسة**</span> **(Policy)** <span dir="rtl">المقابلة لمتجه
البارامتر</span> $`\theta`$<span dir="rtl">.</span>
<span dir="rtl">الرمز</span> $`\propto`$ <span dir="rtl">هنا يعني
"يتناسب مع". في الحالة المنفصلة، يكون **ثابت التناسب**
</span>**(Constant of Proportionality)** <span dir="rtl">هو متوسط طول
الحلقة، وفي الحالة المستمرة يكون 1، بحيث تصبح العلاقة في الواقع مساواة.
التوزيع</span> $`\mu`$ <span dir="rtl">هنا (كما في **الفصلين**</span> 9
<span dir="rtl">و10) هو **توزيع السياسة الحالية**</span>
**<span dir="rtl">(</span>On-Policy
<span dir="rtl"></span>Distribution<span dir="rtl">)
</span>**<span dir="rtl">تحت السياسة</span> $`\pi`$
<span dir="rtl"></span>(<span dir="rtl">انظر الصفحة 199). تم إثبات
**نظرية تدرج السياسة**</span> **<span dir="rtl">(</span>Policy Gradient
<span dir="rtl"></span>Theorem<span dir="rtl">)</span>**
<span dir="rtl">للحالة المنفصلة في الصندوق على الصفحة السابقة</span>.

<span dir="rtl">إثبات نظرية تدرج السياسة</span> (Proof of the Policy
Gradient Theorem) <span dir="rtl">للحالة المنفصلة</span> (Episodic Case)

<span dir="rtl">بواسطة **حساب التفاضل الأساسي**</span> **(Elementary
Calculus)** <span dir="rtl">وإعادة ترتيب المصطلحات، يمكننا إثبات **نظرية
تدرج السياسة**</span> **(Policy Gradient Theorem)** <span dir="rtl">من
المبادئ الأساسية. للحفاظ على التدوين بسيطًا، سنترك ضمنيًا في جميع الحالات
أن</span> $`\pi`$ <span dir="rtl">هي دالة تعتمد على</span>
$`\theta`$<span dir="rtl">، وأن جميع **التدرجات** </span>**(Gradients)**
<span dir="rtl">هي أيضًا ضمنيًا بالنسبة إلى</span>
$`\theta`$<span dir="rtl">.</span> <span dir="rtl">أولاً، لاحظ أن **تدرج
دالة قيمة الحالة**</span> **<span dir="rtl">(</span>Gradient
<span dir="rtl"></span>of the State-Value Function<span dir="rtl">)
</span>**<span dir="rtl">يمكن كتابته من حيث **دالة قيمة الإجراء**</span>
**<span dir="rtl">(</span>Action-Value
<span dir="rtl"></span>Function<span dir="rtl">)</span>**
<span dir="rtl">كالتالي</span>:

``` math
\nabla v_{\pi}(s) = \nabla\left\lbrack \sum_{a}^{}{\pi\left( a \middle| s \right)q_{\pi}(s,a)} \right\rbrack,\quad\text{for all }s \in S
```

``` math
= \sum_{a}^{}\left\lbrack \nabla\pi\left( a \middle| s \right)q_{\pi}(s,a) + \pi\left( a \middle| s \right)\nabla q_{\pi}(s,a) \right\rbrack
```

``` math
= \sum_{a}^{}\left\lbrack \nabla\pi\left( a \middle| s \right)q_{\pi}(s,a) + \pi\left( a \middle| s \right)\nabla\sum_{s',r}^{}{p\left( s',r \middle| s,a \right)\left( r + v_{\pi}\left( s' \right) \right)} \right\rbrack
```

``` math
= \sum_{a}^{}\left\lbrack \nabla\pi\left( a \middle| s \right)q_{\pi}(s,a) + \pi\left( a \middle| s \right)\sum_{s'}^{}{p\left( s' \middle| s,a \right)\nabla v_{\pi}\left( s' \right)} \right\rbrack
```

``` math
= \sum_{a}^{}\left\lbrack \nabla\pi\left( a \middle| s \right)q_{\pi}(s,a) + \pi\left( a \middle| s \right)\sum_{s'}^{}{p\left( s' \middle| s,a \right)\sum_{a'}^{}\left\lbrack \nabla\pi\left( a' \middle| s' \right)q_{\pi}\left( s',a' \right) + \pi\left( a' \middle| s' \right)\sum_{s^{''}}^{}{p\left( s^{''} \middle| s',a' \right)\nabla v_{\pi}\left( s^{''} \right)} \right\rbrack} \right\rbrack
```

``` math
= \sum_{x \in S}^{}{\sum_{k = 0}^{\infty}{\text{Pr}(s \rightarrow x,k,\pi)\sum_{a}^{}{\nabla\pi\left( a \middle| x \right)q_{\pi}(x,a)}}}
```

<span dir="rtl">بعد التكرار المتكرر لفك الاشتقاق، حيث أن</span>
$`Pr(s \rightarrow x,k,\pi)`$ <span dir="rtl">هو احتمال الانتقال من
**الحالة** </span>**(State)** <span dir="rtl"></span>$`s`$
<span dir="rtl">إلى **الحالة**</span> **(State)** $`x`$
<span dir="rtl">في</span> $`k`$ <span dir="rtl">خطوة تحت **السياسة**
</span>**(Policy)** $`\pi`$<span dir="rtl">.</span> <span dir="rtl">يصبح
واضحًا عندها أن</span>:

``` math
\nabla J(\theta) = \nabla v_{\pi}\left( s_{0} \right)
```

``` math
\nabla J(\theta) = \nabla v_{\pi}\left( s_{0} \right) = \sum_{s}^{}{\sum_{k = 0}^{\infty}{\text{Pr}\left( s_{0} \rightarrow s,k,\pi \right)\sum_{a}^{}{\nabla\pi\left( a \middle| s \right)q_{\pi}(s,a)}}} = \sum_{s}^{}{\eta(s)\sum_{a}^{}{\nabla\pi\left( a \middle| s \right)q_{\pi}(s,a)}}\,\left( \text{box page 199} \right) = \sum_{s'}^{}{\eta\left( s' \right)\sum_{s}^{}{P_{\eta}\left( s' \rightarrow s \right)\sum_{a}^{}{\nabla\pi\left( a \middle| s \right)q_{\pi}(s,a)}}} = \sum_{s'}^{}{\eta\left( s' \right)\sum_{s}^{}{\mu(s)\sum_{a}^{}{\nabla\pi\left( a \middle| s \right)q_{\pi}(s,a)}}}\,\left( \text{Eq. 9.3} \right) \propto \sum_{s}^{}{\mu(s)\sum_{a}^{}{\nabla\pi\left( a \middle| s \right)q_{\pi}(s,a)}}
```

**<u>13.3 <span dir="rtl">ريإنفورس: تدرج سياسة مونت كارلو</span>
<span dir="rtl">(</span>REINFORCE: Monte Carlo Policy
<span dir="rtl"></span>Gradient<span dir="rtl">)</span></u>**

<span dir="rtl">نحن الآن جاهزون لاشتقاق **خوارزمية تعلم تدرج
السياسة**</span> **<span dir="rtl">(</span>Policy-Gradient Learning
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**
<span dir="rtl">الأولى لدينا. تذكر استراتيجيتنا العامة للصعود التدرجي
العشوائي **(**</span>**Stochastic <span dir="rtl"></span>Gradient
Ascent<span dir="rtl">)</span>**
<span dir="rtl"></span>(13.1)<span dir="rtl">، والتي تتطلب طريقة للحصول
على عينات بحيث يكون توقع **تدرج العينة**</span> **(Sample Gradient)**
<span dir="rtl">متناسبًا مع **التدرج الفعلي**</span> **(Actual
Gradient)** <span dir="rtl">لمقياس الأداء كدالة للبارامتر. يجب أن تكون
**تدرجات العينات**</span> **(Sample Gradients)** <span dir="rtl">متناسبة
فقط مع التدرج لأنه يمكن امتصاص أي ثابت للتناسب في **حجم الخطوة**
</span>**(Step Size) α**<span dir="rtl">، والذي يمكن أن يكون عشوائيًا
بشكل عام. تقدم **نظرية تدرج السياسة**</span> **(Policy Gradient
Theorem)** <span dir="rtl">تعبيرًا دقيقًا متناسبًا مع التدرج؛ وكل ما هو
مطلوب هو بعض الطرق لأخذ عينات يكون توقعها مساوياً أو مقارباً لهذا
التعبير</span>.

<span dir="rtl">لاحظ أن الجانب الأيمن من **نظرية تدرج السياسة**</span>
**(Policy Gradient Theorem)** <span dir="rtl">هو مجموع على
**الحالات**</span> **(States)** <span dir="rtl">موزونة بمدى تكرار حدوث
هذه الحالات تحت **السياسة الهدف**</span>
**<span dir="rtl">(</span>Target
<span dir="rtl"></span>Policy<span dir="rtl">)</span>
π**<span dir="rtl">؛ إذا تم اتباع السياسة</span> π<span dir="rtl">، فإن
**الحالات**</span> **(States)** <span dir="rtl">سيتم مواجهتها بهذه
النسب. لذلك</span>:

``` math
\nabla J(\theta) \propto \sum_{s}^{}{\mu(s)\sum_{a}^{}{q_{\pi}(s,a)\nabla\pi\left( a \middle| s,\theta \right)}}
```

``` math
= E_{\pi}\left\lbrack \sum_{a}^{}{q_{\pi}\left( S_{t},a \right)\nabla\pi\left( a \middle| S_{t},\theta \right)} \right\rbrack
```

<span dir="rtl">يمكننا التوقف هنا وتفعيل **خوارزمية الصعود التدرجي
العشوائي**</span> **<span dir="rtl">(</span>Stochastic Gradient-Ascent
Algorithm<span dir="rtl">) </span>**<span dir="rtl">الخاصة بنا كما في
المعادلة (13.1) كالتالي</span>:

``` math
\theta_{t + 1} \doteq \theta_{t} + \alpha\sum_{a}^{}{\widehat{q}\left( S_{t},a,w \right)\nabla\pi\left( a \middle| S_{t},\theta \right)}
```

<span dir="rtl">حيث أن</span> q^ <span dir="rtl">هو تقريب متعلم
لـ</span> $`q_{\pi}`$​<span dir="rtl">.</span> <span dir="rtl">هذه
**الخوارزمية** </span>**(Algorithm)**<span dir="rtl">، التي تم تسميتها
**أسلوب كل الإجراءات**</span> **(All-Actions Method)**
<span dir="rtl">لأن تحديثها يتضمن جميع **الإجراءات**
</span>**(Actions)**<span dir="rtl">، واعدة وتستحق مزيدًا من الدراسة.
ولكن اهتمامنا الحالي هو **خوارزمية ريإنفورس الكلاسيكية**</span>
**<span dir="rtl">(</span>Classical <span dir="rtl"></span>REINFORCE
Algorithm<span dir="rtl">)</span>** (Willams, 1992) <span dir="rtl">التي
يتضمن تحديثها في الزمن</span> t <span dir="rtl">فقط الإجراء</span>
At​<span dir="rtl">، وهو الإجراء الوحيد الذي تم اتخاذه فعليًا في
الزمن</span> t<span dir="rtl">.</span>

<span dir="rtl">نواصل اشتقاق **خوارزمية ريإنفورس**</span>
**(REINFORCE)** <span dir="rtl">عن طريق إدخال</span> $`At`$
<span dir="rtl">بنفس الطريقة التي قدمنا بها</span> $`St`$
<span dir="rtl">في المعادلة (13.6) — من خلال استبدال مجموع قيم المتغير
العشوائي بتوقع تحت السياسة</span> $`\pi`$<span dir="rtl">، ثم أخذ عينة
من التوقع. المعادلة (13.6) تتضمن مجموعًا مناسبًا على الإجراءات، ولكن كل حد
فيها ليس مضروبًا في</span> $`\pi(a \mid St,\theta)`$ <span dir="rtl">كما
هو مطلوب لتوقع تحت</span> $`\pi`$<span dir="rtl">.</span>
<span dir="rtl">لذا نقدم مثل هذا الوزن، دون تغيير المساواة، عن طريق ضرب
ثم قسمة الحدود المجمعة على</span>
$`\pi(a \mid St,\theta)`$<span dir="rtl">. متابعةً من (13.6)، نحصل
على</span>:

``` math
{\nabla J(\theta) = E_{\pi}\left\lbrack \sum_{a}^{}\frac{\pi\left( a \middle| S_{t},\theta \right)q_{\pi}\left( S_{t},a \right)\nabla\pi\left( a \middle| S_{t},\theta \right)}{\pi\left( a \middle| S_{t},\theta \right)} \right\rbrack
}
{= E_{\pi}\left\lbrack \frac{q_{\pi}\left( S_{t},A_{t} \right)\nabla\pi\left( A_{t} \middle| S_{t},\theta \right)}{\pi\left( A_{t} \middle| S_{t},\theta \right)} \right\rbrack
}
{= E_{\pi}\left\lbrack G_{t}\frac{\nabla\pi\left( A_{t} \middle| S_{t},\theta \right)}{\pi\left( A_{t} \middle| S_{t},\theta \right)} \right\rbrack}
```

<span dir="rtl">حيث أن</span> $`Gt`$ <span dir="rtl">هو
**العائد**</span> **(Return)** <span dir="rtl">كالمعتاد. التعبير النهائي
داخل الأقواس هو بالضبط ما نحتاجه، كمية يمكن أخذ عينة منها في كل خطوة
زمنية ويكون توقعها مساويًا **للتدرج**
</span>**(Gradient)**<span dir="rtl">.</span> <span dir="rtl">استخدام
هذه العينة لتفعيل **خوارزمية الصعود التدرجي العشوائي**</span>
**<span dir="rtl">(</span>Stochastic Gradient Ascent
<span dir="rtl"></span>Algorithm<span dir="rtl">)
</span>**<span dir="rtl">العامة (13.1) ينتج **تحديث ريإنفورس**
</span>**(REINFORCE Update)**<span dir="rtl">:</span>

``` math
\theta_{t + 1} \doteq \theta_{t} + \alpha G_{t}\frac{\nabla\pi\left( A_{t} \middle| S_{t},\theta_{t} \right)}{\pi\left( A_{t} \middle| S_{t},\theta_{t} \right)}
```

<span dir="rtl">هذا التحديث له جاذبية منطقية. كل زيادة تكون متناسبة مع
حاصل ضرب **العائد** </span>**(Return)** $`\mathbf{Gt}`$
<span dir="rtl">و**متجه** </span>**(Vector)**<span dir="rtl">، وهو
**تدرج**</span> **(Gradient)** <span dir="rtl">احتمالية اتخاذ الإجراء
الذي تم اتخاذه فعلاً مقسومًا على احتمالية اتخاذ ذلك الإجراء. المتجه هو
الاتجاه في **فضاء البارامترات**</span> **(Parameter Space)**
<span dir="rtl">الذي يزيد من احتمالية تكرار الإجراء</span> $`At`$
<span dir="rtl">في الزيارات المستقبلية للحالة</span>
$`St`$​<span dir="rtl">.</span> <span dir="rtl">يقوم التحديث بزيادة
**متجه البارامتر**</span> **(Parameter Vector)** <span dir="rtl">في هذا
الاتجاه بشكل متناسب مع العائد، ومعكوسًا مع احتمال الإجراء. العامل الأول
منطقي لأنه يحرك البارامتر في الاتجاهات التي تفضل الإجراءات التي تحقق
أعلى عائد. العامل الثاني منطقي لأنه لولا ذلك لكانت الإجراءات التي يتم
اختيارها بشكل متكرر في وضع أفضل (حيث ستكون التحديثات غالبًا في اتجاهها)
وقد تتفوق حتى لو لم تحقق أعلى عائد</span>.

<span dir="rtl">لاحظ أن **ريإنفورس**</span> **(REINFORCE)**
<span dir="rtl">يستخدم العائد الكامل من الزمن</span> t<span dir="rtl">،
والذي يشمل جميع المكافآت المستقبلية حتى نهاية الحلقة. من هذا المنطلق،
يعد **ريإنفورس**</span> **(REINFORCE)** **<span dir="rtl">خوارزمية مونت
كارلو</span> (Monte Carlo Algorithm)** <span dir="rtl">ويتم تعريفه بشكل
جيد فقط للحالة المنفصلة مع جميع التحديثات التي تتم بعد إتمام الحلقة (مثل
**خوارزميات مونت كارلو**</span> **(Monte Carlo Algorithms)**
<span dir="rtl">في **الفصل** </span>5<span dir="rtl">)</span>.
<span dir="rtl">هذا موضح بشكل صريح في **الصندوق**</span> **(Boxed)**
<span dir="rtl">في الصفحة التالية</span>.

<span dir="rtl">لاحظ أن التحديث في السطر الأخير من **الكود
الزائف**</span> **(Pseudocode)** <span dir="rtl">يبدو مختلفًا إلى حد ما
عن قاعدة تحديث **ريإنفورس** </span>**(REINFORCE Update Rule)**
(13.8)<span dir="rtl">.</span> <span dir="rtl">أحد الفروق هو أن **الكود
الزائف**</span> **(Pseudocode)** <span dir="rtl">يستخدم التعبير
المختصر</span> $`\nabla\ln\pi(At \mid St,\theta t)`$
<span dir="rtl">لمتجه الكسر  
</span>$`\nabla\pi(At \mid St,\theta t)`$​ <span dir="rtl">في (13.8). أن
هذين التعبيرين للمتجه متكافئان يعود إلى الهوية</span>
$`\nabla lnx = x\nabla x`$<span dir="rtl">. هذا المتجه تم إعطاؤه عدة
أسماء ورموز في الأدبيات؛ سنشير إليه ببساطة باسم **متجه الأهلية**
</span>**(Eligibility Vector)**<span dir="rtl">. لاحظ أنه هو المكان
الوحيد الذي يظهر فيه **بارامتر السياسة  
(**</span>**Policy
<span dir="rtl"></span>Parameterization<span dir="rtl">)</span>**
<span dir="rtl">في **الخوارزمية**
</span>**(Algorithm)**<span dir="rtl">.</span>

<span dir="rtl">ريإنفورس: التحكم في تدرج سياسة مونت كارلو</span>
<span dir="rtl">(</span>REINFORCE: Monte-Carlo Policy-Gradient
Control<span dir="rtl">) للحالة المنفصلة</span> (Episodic)
<span dir="rtl">من أجل</span> π∗

<img src="./media/image160.png"
style="width:6.26806in;height:2.22292in" />

<span dir="rtl">الفرق الثاني بين تحديث **الكود الزائف**</span>
**(Pseudocode)** <span dir="rtl">وتحديث</span> REINFORCE
<span dir="rtl">في المعادلة (13.8) هو أن الأول يتضمن عامل</span>
$`\gamma t`$ <span dir="rtl">وذلك لأننا، كما ذُكر سابقًا، في النص نتعامل
مع الحالة غير المخصومة</span> $`\gamma = 1`$ <span dir="rtl">بينما في
**الخوارزمية**</span> **(Algorithm)** <span dir="rtl">الموجودة في
الصندوق، نقدم الخوارزمية للحالة العامة المخصومة. جميع الأفكار تنطبق على
الحالة المخصومة مع التعديلات المناسبة (بما في ذلك الصندوق في الصفحة 199)
ولكنها تتطلب تعقيدات إضافية تشتت الانتباه عن الأفكار الرئيسية</span>.

**<span dir="rtl">تمرين 13.2:</span>** <span dir="rtl">قم بتعميم الصندوق
في الصفحة 199، **نظرية تدرج السياسة**</span>
**<span dir="rtl">(</span>Policy Gradient
<span dir="rtl"></span>Theorem<span dir="rtl">)</span>**
<span dir="rtl">في المعادلة (13.5)، برهان **نظرية تدرج السياسة**
</span>**(Policy Gradient Theorem)** <span dir="rtl">(في الصفحة 325)،
والخطوات المؤدية إلى تحديث</span> **REINFORCE** <span dir="rtl">في
المعادلة (13.8)، بحيث تنتهي المعادلة (13.8) بعامل</span> $`\gamma t`$
<span dir="rtl">وبالتالي تتماشى مع **الخوارزمية**</span> **(Algorithm)**
<span dir="rtl">العامة المقدمة في **الكود الزائف**
</span>**(Pseudocode)**<span dir="rtl">.</span>

<span dir="rtl">توضح **الشكل 13.1**</span> **(Figure 13.1)**
<span dir="rtl">أداء</span> **REINFORCE** <span dir="rtl">على شبكة
**الممر القصير**</span> **<span dir="rtl">(</span>Short-Corridor
Gridworld<span dir="rtl">) </span>**<span dir="rtl">من **المثال
13.1**</span>.

<img src="./media/image161.png"
style="width:6.28874in;height:3.11881in" />

<span dir="rtl">الشكل 13.1: أداء</span> **REINFORCE**
<span dir="rtl">على شبكة **الممر القصير** </span>**(Short-Corridor
Gridworld)** <span dir="rtl">من **المثال 13.1**</span>.
<span dir="rtl">مع اختيار جيد لحجم الخطوة، تقترب المكافأة الكلية لكل
**حلقة**</span> **(Episode)** <span dir="rtl">من القيمة المثلى لحالة
البداية</span>.

<span dir="rtl">باعتباره **طريقة تدرج عشوائية** </span>**(Stochastic
Gradient Method)**<span dir="rtl">، يتمتع</span> **REINFORCE**
<span dir="rtl">بخصائص نظرية جيدة للتقارب. من خلال البناء، يكون التحديث
المتوقع على مدار **حلقة** </span>**(Episode)** <span dir="rtl">في نفس
اتجاه **تدرج الأداء** </span>**(Performance
Gradient)**<span dir="rtl">.</span> <span dir="rtl">هذا يضمن تحسنًا في
الأداء المتوقع لحجم خطوة</span> α <span dir="rtl">صغير بما فيه الكفاية،
والتقارب إلى **أقصى محلي**</span> **(Local Optimum)**
<span dir="rtl">تحت شروط **التقريب العشوائي**</span> **(Stochastic
Approximation)** <span dir="rtl">القياسية لتناقص</span>
$`\alpha`$<span dir="rtl">.</span> <span dir="rtl">ومع ذلك، باعتباره
**طريقة مونت كارلو** </span>**(Monte Carlo Method)**<span dir="rtl">، قد
يكون</span> **REINFORCE** <span dir="rtl">عالي **التباين**</span>
**(Variance)** <span dir="rtl">وبالتالي قد ينتج عنه تعلم بطيء</span>.

**<span dir="rtl">تمرين 13.3:</span>** <span dir="rtl">في القسم 13.1،
نظرنا في **بارامترية السياسة** </span>**(Policy Parameterization)**
<span dir="rtl">باستخدام</span> **Soft-max** <span dir="rtl">في
**تفضيلات الإجراءات**</span> **(Action Preferences)**
<span dir="rtl">(المعادلة 13.2) مع **تفضيلات الإجراءات الخطية**</span>
**(Linear Action Preferences)** <span dir="rtl">(المعادلة 13.3). لهذا
**البارامترية** </span>**(Parameterization)**<span dir="rtl">، أثبت أن
**متجه الأهلية**</span> **(Eligibility Vector)**
<span dir="rtl">هو</span>...

``` math
\nabla_{\theta}\log\pi\left( a \middle| s,\theta \right) = x(s,a) - \sum_{b}^{}{\pi\left( b \middle| s,\theta \right)x(s,b)}
```

<span dir="rtl">باستخدام **التعريفات**</span> **(Definitions)**
<span dir="rtl">و**حساب التفاضل الأساسي** </span>**(Elementary
Calculus)**

**<u>13.4 <span dir="rtl">ريإنفورس مع خط الأساس</span> (REINFORCE with
Baseline)</u>**

<span dir="rtl">يمكن تعميم **نظرية تدرج السياسة**</span> **(Policy
Gradient Theorem)** (13.5) <span dir="rtl">لتشمل مقارنة **قيمة
الإجراء**</span> **(Action Value)** <span dir="rtl">مع **خط
أساس**</span> **(Baseline)** <span dir="rtl">اعتباطي</span>
$`b(s)`$<span dir="rtl">:</span>

``` math
\nabla J(\theta) \propto \sum_{s}^{}{\mu(s)\sum_{a}^{}{\left( q_{\pi}(s,a) - b(s) \right)\nabla\pi\left( a \middle| s,\theta \right)}}
```

<span dir="rtl">يمكن أن يكون **خط الأساس**</span> **(Baseline)**
<span dir="rtl">أي **دالة** </span>**(Function)**<span dir="rtl">، حتى
لو كان **متغيرًا عشوائيًا** </span>**(Random Variable)**<span dir="rtl">،
طالما أنه لا يتغير مع</span> $`a`$<span dir="rtl">؛ تظل
**المعادلة**</span> **(Equation)** <span dir="rtl">صالحة لأن الكمية
المطروحة تساوي صفرًا</span>:

``` math
\sum_{a}^{}{b(s)\nabla\pi\left( a \middle| s,\theta \right)} = b(s)\nabla\sum_{a}^{}{\pi\left( a \middle| s,\theta \right)} = b(s)\nabla 1 = 0
```

<span dir="rtl">يمكن استخدام **نظرية تدرج السياسة**</span> **(Policy
Gradient Theorem)** <span dir="rtl">مع **خط الأساس**
</span>**(Baseline)** (13.10) <span dir="rtl">لاشتقاق قاعدة تحديث
باستخدام خطوات مشابهة لتلك في القسم السابق. قاعدة التحديث التي ننتهي بها
هي نسخة جديدة من **ريإنفورس**</span> **(REINFORCE)**
<span dir="rtl">تتضمن خط أساس عام</span>:

``` math
\theta_{t + 1} \doteq \theta_{t} + \alpha\left( G_{t} - b\left( S_{t} \right) \right)\frac{\nabla\pi\left( A_{t} \middle| S_{t},\theta_{t} \right)}{\pi\left( A_{t} \middle| S_{t},\theta_{t} \right)}
```

<span dir="rtl">لأن **خط الأساس**</span> **(Baseline)**
<span dir="rtl">يمكن أن يكون صفراً بشكل موحد، فإن هذا التحديث هو تعميم
صارم **لريإنفورس** </span>**(REINFORCE)**<span dir="rtl">.</span>
<span dir="rtl">بشكل عام، لا يغير **خط الأساس**</span> **(Baseline)**
<span dir="rtl">من القيمة المتوقعة للتحديث، ولكنه يمكن أن يؤثر بشكل كبير
على **التباين** </span>**(Variance)**<span dir="rtl">.</span>
<span dir="rtl">على سبيل المثال، رأينا في **الفصل**</span> **(Section)**
2.8 <span dir="rtl">أن خط الأساس المشابه يمكن أن يقلل بشكل كبير من
التباين (وبالتالي يسرع من التعليم) في **خوارزميات قطاع الطرق**
</span>**(Gradient Bandit Algorithms)**<span dir="rtl">.</span>
<span dir="rtl">في خوارزميات قطاع الطرق كان خط الأساس مجرد رقم (متوسط
المكافآت التي تم مشاهدتها حتى الآن)، ولكن بالنسبة لـ **عمليات اتخاذ
القرار**</span> **(MDPs)** <span dir="rtl">يجب أن يتغير **خط
الأساس**</span> **(Baseline)** <span dir="rtl">مع **الحالة**
</span>**(State)**<span dir="rtl">.</span>

<span dir="rtl">في بعض الحالات، تكون جميع الإجراءات ذات قيم عالية ونحتاج
إلى **خط أساس**</span> **(Baseline)** <span dir="rtl">عالٍ لتمييز
الإجراءات ذات القيم الأعلى عن تلك ذات القيم الأقل؛ في حالات أخرى، تكون
جميع الإجراءات ذات قيم منخفضة ويكون **خط أساس**</span> **(Baseline)**
<span dir="rtl">منخفض هو المناسب</span>.

<span dir="rtl">أحد الخيارات الطبيعية لخط الأساس هو تقدير **قيمة
الحالة** </span>**(State Value) <span dir="rtl"></span>**
$`v\hat{}(St,w)\ `$<span dir="rtl">حيث</span> $`w \in Rm`$
<span dir="rtl">هو **متجه الوزن**</span> **(Weight Vector)**
<span dir="rtl">المتعلم بواسطة أحد الأساليب المقدمة في الفصول
السابقة</span>.

<span dir="rtl">نظرًا لأن **ريإنفورس**</span> **(REINFORCE)**
<span dir="rtl">هو **أسلوب مونت كارلو**</span> **(Monte Carlo Method)**
<span dir="rtl">لتعلم **بارامتر السياسة** </span>**(Policy Parameter)**
$`\mathbf{\theta}`$<span dir="rtl">، يبدو من الطبيعي أيضًا استخدام
**أسلوب مونت كارلو** </span>**(Monte Carlo Method)**
<span dir="rtl">لتعلم **أوزان قيمة الحالة** </span>**(State-Value
Weights)** $`\mathbf{w}`$<span dir="rtl">.</span> <span dir="rtl">يتم
تقديم خوارزمية كاملة **بالكود الزائف**</span> **(Pseudocode)**
<span dir="rtl">لـ **ريإنفورس مع خط الأساس**</span>
**<span dir="rtl">(</span>REINFORCE <span dir="rtl"></span>with
Baseline<span dir="rtl">) </span>**<span dir="rtl">باستخدام **دالة قيمة
الحالة المتعلمة** </span>**(Learned State-Value Function)**
<span dir="rtl">كخط أساس في الصندوق أدناه</span>.

<span dir="rtl">ريإنفورس مع خط الأساس</span> (REINFORCE with Baseline)
<span dir="rtl">(الحالة المنفصلة</span> Episodic<span dir="rtl">)،
لتقدير</span>

<img src="./media/image162.png"
style="width:6.26806in;height:2.86875in" />

<span dir="rtl">تحتوي هذه **الخوارزمية**</span> **(Algorithm)**
<span dir="rtl">على حجمين للخطوة، يرمز لهما بـ</span> $`\alpha\theta`$
<span dir="rtl"></span>​ <span dir="rtl">و</span>$`\alpha w`$​
<span dir="rtl">(حيث أن</span> $`\alpha\theta`$​
<span dir="rtl">هو</span> $`\alpha`$ <span dir="rtl">في المعادلة</span>
(13.11)<span dir="rtl">)</span>. <span dir="rtl">اختيار حجم الخطوة للقيم
(هنا</span> $`\alpha w`$​<span dir="rtl">)</span> <span dir="rtl">يعد
نسبيًا سهلاً؛ في **الحالة الخطية**</span> **(Linear Case)**
<span dir="rtl">لدينا قواعد إرشادية لتحديده، مثل</span>
$`\alpha_{w} = \frac{0.1}{E\left\lbrack \text{|}\nabla\widehat{v}\left( S_{t},w \right)\text{|}_{\mu}^{2} \right\rbrack}`$
<span dir="rtl">انظر **الفصل** </span>**(**9.6)<span dir="rtl">. من
الأقل وضوحًا كيفية تحديد حجم الخطوة لبارامترات السياسة،</span>
$`\alpha\theta`$​<span dir="rtl">، والتي تعتمد قيمتها المثلى على نطاق
تغير المكافآت وعلى **بارامترية السياسة** </span>**(Policy
Parameterization)**

<img src="./media/image163.png"
style="width:6.26806in;height:2.99792in" />

<span dir="rtl">**الشكل 13.2**: إضافة **خط أساس**</span> **(Baseline)**
<span dir="rtl">إلى **ريإنفورس**</span> **(REINFORCE)**
<span dir="rtl">يمكن أن يجعله يتعلم بشكل أسرع بكثير، كما هو موضح هنا في
شبكة **الممر القصير**</span> **<span dir="rtl">(</span>Short-Corridor
<span dir="rtl"></span>Gridworld<span dir="rtl">)
</span>**<span dir="rtl">(المثال 13.1). حجم الخطوة المستخدم هنا لـ
**ريإنفورس البسيط**</span> **<span dir="rtl">(</span>Plain
<span dir="rtl"></span>REINFORCE<span dir="rtl">)
</span>**<span dir="rtl">هو الحجم الذي يؤدي فيه أفضل أداء (لأقرب قوة من
اثنين؛ انظر الشكل 13.1)</span>.

<span dir="rtl">**الشكل 13.2** يقارن بين سلوك **ريإنفورس**</span>
**(REINFORCE)** <span dir="rtl">مع وبدون **خط أساس**</span>
**(Baseline)** <span dir="rtl">في شبكة **الممر القصير**</span>
**(Short-Corridor Gridworld)** <span dir="rtl">(المثال 13.1). هنا،
**دالة قيمة الحالة التقريبية**</span> **(Approximate State-Value
Function)** <span dir="rtl">المستخدمة في خط الأساس هي</span>
$`\ v\hat{}(s,w) = w`$<span dir="rtl">أي أن</span> w <span dir="rtl">هو
مكون واحد،</span> $`w`$<span dir="rtl">.</span>

**<u>13.5 <span dir="rtl">أساليب الممثل–الناقد</span> (Actor–Critic
Methods)</u>**

<span dir="rtl">على الرغم من أن **أسلوب ريإنفورس مع خط الأساس**</span>
**<span dir="rtl">(</span>REINFORCE-with-baseline
<span dir="rtl"></span>Method<span dir="rtl">)
</span>**<span dir="rtl">يتعلم كل من **السياسة**</span> **(Policy)**
<span dir="rtl">و**دالة قيمة الحالة** </span>**(State-Value
Function)**<span dir="rtl">، إلا أننا لا نعتبره **أسلوب
الممثل–الناقد**</span> **(Actor–Critic Method)** <span dir="rtl">لأن
دالة قيمة الحالة تُستخدم فقط كخط أساس وليس كناقد. بمعنى آخر، لا تُستخدم
للتمهيد (تحديث تقدير القيمة لحالة بناءً على القيم المقدرة للحالات
اللاحقة)، ولكن فقط كخط أساس للحالة التي يتم تحديث تقديرها. هذا التمييز
مهم، لأن التمهيد هو الطريقة الوحيدة التي من خلالها نقدم
**الانحياز**</span> **(Bias)** <span dir="rtl">والاعتماد غير النهائي على
جودة تقريب الدالة. كما رأينا، فإن الانحياز الذي يُدخل من خلال التمهيد
والاعتماد على تمثيل الحالة غالبًا ما يكون مفيدًا لأنه يقلل التباين ويسرع
من عملية التعليم.</span> **<span dir="rtl">ريإنفورس مع خط الأساس</span>
<span dir="rtl">(</span>REINFORCE <span dir="rtl"></span>with
Baseline<span dir="rtl">) </span>**<span dir="rtl">غير متحيز وسيتقارب في
النهاية إلى الحد الأدنى المحلي، ولكنه مثل كل **أساليب مونت
كارلو**</span> **(Monte Carlo Methods)** <span dir="rtl">يميل إلى
التعليم ببطء (إنتاج تقديرات ذات تباين عالي) ويكون من الصعب تنفيذه عبر
الإنترنت أو في المشكلات المستمرة. كما رأينا سابقًا في هذا الكتاب، مع
**أساليب الفرق الزمنية**</span> **(Temporal-Difference Methods)**
<span dir="rtl">يمكننا التخلص من هذه الصعوبات، ومن خلال **الأساليب
متعددة الخطوات**</span> **(Multi-Step Methods)** <span dir="rtl">يمكننا
اختيار درجة التمهيد بمرونة. للحصول على هذه المزايا في حالة **أساليب تدرج
السياسة** </span>**(Policy Gradient Methods)** <span dir="rtl">نستخدم
**أساليب الممثل–الناقد**</span> **(Actor–Critic Methods)**
<span dir="rtl">مع ناقد يعتمد على التمهيد</span>.

<span dir="rtl">أولاً، ندرس **أساليب الممثل–الناقد ذات الخطوة الواحدة**
</span>**(One-Step Actor–Critic Methods)**<span dir="rtl">، وهي النظير
لأساليب</span> TD <span dir="rtl">التي تم تقديمها في **الفصل**</span>
**(Chapter)** 6 <span dir="rtl">مثل</span> TD(0)
<span dir="rtl">و</span>Sarsa(0) <span dir="rtl">و  
</span>Q-learning<span dir="rtl">.</span> <span dir="rtl">الجاذبية
الرئيسية لأساليب الخطوة الواحدة هي أنها تعمل بشكل كامل عبر الإنترنت
وتدريجية، ومع ذلك تتجنب تعقيدات **آثار الأهلية** </span>**(Eligibility
Traces)**<span dir="rtl">.</span> <span dir="rtl">إنها حالة خاصة من
أساليب آثار الأهلية، وليست عامة مثلها، ولكنها أسهل في الفهم.</span>
**<span dir="rtl">أساليب الممثل–الناقد ذات الخطوة الواحدة</span>
(One-Step Actor–Critic Methods)** <span dir="rtl">تستبدل العائد الكامل
لـ **ريإنفورس** </span>**(REINFORCE)** <span dir="rtl">(المعادلة 13.11)
بالعائد ذو الخطوة الواحدة (وتستخدم دالة قيمة الحالة المتعلمة كخط أساس)
كما يلي</span>:

``` math
\theta_{t + 1} \doteq \theta_{t} + \alpha\left( G_{t:t + 1} - \widehat{v}\left( S_{t},w \right) \right)\frac{\nabla\pi\left( A_{t} \middle| S_{t},\theta_{t} \right)}{\pi\left( A_{t} \middle| S_{t},\theta_{t} \right)}\quad\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (13.12) = \theta_{t} + \alpha\left( R_{t + 1} + \gamma\widehat{v}\left( S_{t + 1},w \right) - \widehat{v}\left( S_{t},w \right) \right)\frac{\nabla\pi\left( A_{t} \middle| S_{t},\theta_{t} \right)}{\pi\left( A_{t} \middle| S_{t},\theta_{t} \right)}\quad(13.13) = \theta_{t} + \alpha\delta_{t}\frac{\nabla\pi\left( A_{t} \middle| S_{t},\theta_{t} \right)}{\pi\left( A_{t} \middle| S_{t},\theta_{t} \right)}
```

<span dir="rtl">الطريقة الطبيعية لتعلم **دالة قيمة الحالة**</span>
**(State-Value Function)** <span dir="rtl">التي تقترن مع هذا الأسلوب
هي</span> **TD(0) <span dir="rtl">شبه التدرجي</span> (Semi-Gradient
TD(0))**<span dir="rtl">.</span> **<span dir="rtl">الكود الزائف</span>
(Pseudocode)** <span dir="rtl">للخوارزمية الكاملة موضح في الصندوق في
أعلى الصفحة التالية. لاحظ أنها الآن **خوارزمية**</span> **(Algorithm)**
<span dir="rtl">تعمل بالكامل عبر الإنترنت وتدريجيًا، حيث تتم معالجة
**الحالات** </span>**(States)**<span dir="rtl">، **الإجراءات**
</span>**(Actions)**<span dir="rtl">، و**المكافآت**</span> **(Rewards)**
<span dir="rtl">فور حدوثها، ولا يتم الرجوع إليها مرة أخرى</span>.

<span dir="rtl">الممثل–الناقد ذو الخطوة الواحدة</span> (One-step
Actor–Critic) <span dir="rtl">(الحالة المنفصلة</span>
Episodic<span dir="rtl">)، لتقدير</span>

<img src="./media/image164.png"
style="width:6.30198in;height:3.28371in" />

<span dir="rtl">تعتبر التعميمات</span> (Generalizations)
<span dir="rtl">إلى العرض الأمامي</span> (Forward View)
<span dir="rtl">لطرق</span> n_step <span dir="rtl">ومن ثم إلى</span>
-return <span dir="rtl">الخوارزمية</span> (Algorithm)
<span dir="rtl">مباشرة. حيث يتم استبدال العائد ذو الخطوة الواحدة  
(</span>One-Step Return<span dir="rtl">) في المعادلة (13.12) بـ</span>
$`Gt:t + n`$ <span dir="rtl"></span>​ <span dir="rtl">أو</span>
$`Gt\lambda`$ <span dir="rtl">على التوالي. أما العرض الخلفي</span>
(Backward View) <span dir="rtl">لخوارزمية</span> "-return
<span dir="rtl">فهو أيضًا بسيط، حيث يستخدم تتبعات الأهلية</span>
(Eligibility Traces) <span dir="rtl">المنفصلة لكل من الفاعل</span>
(Actor) <span dir="rtl">والناقد</span> (Critic)<span dir="rtl">، كل
منهما وفق الأنماط المذكورة في **الفصل 12**.</span> <span dir="rtl">الكود
الزائف</span> (Pseudocode) <span dir="rtl">للخوارزمية الكاملة معروض في
المربع أدناه</span>.

<span dir="rtl">الممثل-الناقد</span> (Actor-Critic) <span dir="rtl">مع
تتبعات الأهلية</span> (Eligibility Traces) <span dir="rtl">(حلقيًا</span>
– episodic<span dir="rtl">)، لتقدير...</span>

<img src="./media/image165.png"
style="width:6.26806in;height:4.29167in" />

**<u>13.6 <span dir="rtl">تدرج السياسة</span> (Policy Gradient)
<span dir="rtl">للمشاكل المستمرة</span> (Continuing Problems)</u>**

<span dir="rtl">كما تم مناقشته في **الفصل 10.3**، بالنسبة للمشاكل
المستمرة</span> (Continuing Problems) <span dir="rtl">التي لا تحتوي على
حدود للحلقات</span> (Episode Boundaries)<span dir="rtl">، نحتاج إلى
تعريف الأداء</span> (Performance) <span dir="rtl">من حيث معدل
المكافأة</span> (Reward) <span dir="rtl">المتوسط لكل خطوة زمنية</span>
(Time Step)<span dir="rtl">:</span>

``` math
J(\theta) \doteq r(\pi) \doteq \lim_{h \rightarrow \infty}{}\frac{1}{h}\sum_{t = 1}^{h}{E\left\lbrack R_{t}\mid S_{0},A_{0:t - 1} \sim \pi \right\rbrack}\quad(13.15)\quad
```

``` math
{= \lim_{t \rightarrow \infty}E\left\lbrack R_{t}\mid S_{0},A_{0:t - 1} \sim \pi \right\rbrack
}
{= \sum_{s}^{}{\mu(s)\sum_{a}^{}{\pi\left( a\mid s \right)\sum_{s',r}^{}{p\left( s',r\mid s,a \right)r}}},}
```

<span dir="rtl">حيث أن</span> $`\mu`$ <span dir="rtl">هو توزيع الحالة
المستقر</span> (Steady-State Distribution) <span dir="rtl">تحت
السياسة</span> (Policy) $`\pi`$<span dir="rtl">، بحيث</span>:

``` math
\mu(s) \doteq \lim_{t \rightarrow \infty}{\Pr\text{\{}}S_{t} = s \mid A_{0:t} \sim \pi\text{\}},
```

<span dir="rtl">والذي يُفترض أنه موجود ومستقل عن الحالة الابتدائية</span>
$`S0`$ <span dir="rtl"></span>​ <span dir="rtl">وفق فرضية الإرغودية  
(</span>Ergodicity–Assumption<span dir="rtl">)</span>.
<span dir="rtl">تذكر أن هذا هو التوزيع الخاص الذي، إذا اخترت الإجراءات
وفقًا للسياسة</span> π<span dir="rtl">، ستبقى في نفس التوزيع</span>:

``` math
\sum_{s}^{}{\mu(s)\sum_{a}^{}{\pi\left( a\mid s,\theta \right)p\left( s'\mid s,a \right)}} = \mu\left( s' \right),\quad\text{for all }s' \in S.
```

<span dir="rtl">الكود الزائف</span> (Pseudocode) <span dir="rtl">الكامل
لخوارزمية الممثل-الناقد</span> (Actor-Critic Algorithm)
<span dir="rtl">في حالة المشاكل المستمرة</span> (Continuing Case)
<span dir="rtl">وفق العرض الخلفي</span> (Backward View)
<span dir="rtl">معروض في المربع أدناه</span>.

<span dir="rtl">الممثل-الناقد</span> (Actor-Critic) <span dir="rtl">مع
تتبعات الأهلية</span> (Eligibility Traces) <span dir="rtl">(للمشاكل
المستمرة</span> – continuing<span dir="rtl">)، لتقدير...</span>

<img src="./media/image166.png"
style="width:6.26806in;height:4.2125in" />

<span dir="rtl">بالطبع، في حالة المشاكل المستمرة</span> (Continuing
Case)<span dir="rtl">، نُعرّف القيم</span> $`v\pi(s)`$
<span dir="rtl">بأنها</span>
$`v_{\pi}(s) \doteq E_{\pi}\left\lbrack G_{t}\mid S_{t} = s \right\rbrack\text{ و }q_{\pi}(s,a)`$

<span dir="rtl">و</span> ​$`q_{\pi}`$ (s,a) <span dir="rtl">بأنها</span>
$`q_{\pi}(s,a) \doteq E_{\pi}\left\lbrack G_{t}\mid S_{t} = s,A_{t} = a \right\rbrack`$
<span dir="rtl">بالنسبة للعائد التفاضلي</span> (Differential
Return)<span dir="rtl">:</span>

``` math
G_{t} \doteq R_{t + 1} - r(\pi) + R_{t + 2} - r(\pi) + R_{t + 3} - r(\pi) + \ \ ...
```

<span dir="rtl">مع هذه التعريفات البديلة</span> (Alternate
Definitions)<span dir="rtl">، يبقى صحيحًا مبرهنة تدرج السياسة</span>
<span dir="rtl">(</span>Policy <span dir="rtl"></span>Gradient
Theorem<span dir="rtl">)</span> <span dir="rtl">كما ورد في حالة
الحلقات</span> (Episodic Case) <span dir="rtl">في المعادلة (13.5)
بالنسبة لحالة المشاكل المستمرة</span> (Continuing
Case)<span dir="rtl">.</span> <span dir="rtl">يتم تقديم إثبات في المربع
على الصفحة التالية. كما تظل معادلات العرض الأمامي</span> (Forward View)
<span dir="rtl">والعرض الخلفي</span> (Backward View) <span dir="rtl">كما
هي</span>.

<span dir="rtl">إثبات مبرهنة تدرج السياسة</span> (Policy Gradient
Theorem) <span dir="rtl">في حالة المشاكل المستمرة</span> (Continuing
Case)

<span dir="rtl">يبدأ إثبات **مبرهنة تدرج السياسة**</span> **(Policy
Gradient Theorem)** <span dir="rtl">في حالة **المشاكل المستمرة**
</span>**(Continuing Case)** <span dir="rtl">بشكل مشابه لحالة
**الحلقات** </span>**(Episodic Case)**<span dir="rtl">.</span>
<span dir="rtl">مرة أخرى، نترك ضمنيًا في جميع الحالات أن
**السياسة**</span> **(Policy)** $`\mathbf{\pi}`$ <span dir="rtl">هي دالة
تعتمد على **البارامتر** </span>**(Parameter)
<span dir="rtl"></span>**$`\mathbf{\theta}`$ <span dir="rtl">وأن
**التدرجات**</span> **(Gradients)** <span dir="rtl">تكون بالنسبة
إلى</span> $`\theta`$<span dir="rtl">.</span> <span dir="rtl">تذكر أنه
في حالة **المشاكل المستمرة**</span> **<span dir="rtl">(</span>Continuing
<span dir="rtl"></span>Case<span dir="rtl">)</span>**<span dir="rtl">،
يكون **الدالة الهدفية**</span> **(Objective Function)**
$`\mathbf{J(\theta)}`$ <span dir="rtl">تساوي **معدل المكافأة**</span>
**<span dir="rtl">(</span>Average
<span dir="rtl"></span>Reward<span dir="rtl">)
</span>**$`\mathbf{r(\pi)}`$ <span dir="rtl">(المعادلة 13.15) وأن
**القيمة الحالة**</span> **(State-Value)** $`\mathbf{v\pi}`$
<span dir="rtl">و**القيمة الإجرائية** </span>**(Action-Value)
<span dir="rtl"></span>** $`q_{\pi}`$<span dir="rtl">يشيران إلى
القيم</span> (Values) <span dir="rtl">بالنسبة **للعائد التفاضلي
(**</span>**Differential
<span dir="rtl"></span>Return<span dir="rtl">)</span>**
<span dir="rtl">(المعادلة 13.17). يمكن كتابة **تدرج دالة القيمة الحالة
(**</span>**Gradient of <span dir="rtl"></span>the State-Value
Function<span dir="rtl">)</span>** <span dir="rtl">لأي حالة</span>
$`s \in S`$ <span dir="rtl">كما يلي</span>:

``` math
\nabla v_{\pi}(s) = \nabla\left\lbrack \sum_{a}^{}{\pi\left( a\mid s \right)q_{\pi}(s,a)} \right\rbrack,\text{ for all }s \in S\text{                                                 (Exercise 3.18)} = \sum_{a}^{}{\left\lbrack \nabla\pi\left( a\mid s \right)q_{\pi}(s,a) + \pi\left( a\mid s \right)\nabla q_{\pi}(s,a) \right\rbrack\text{ }\text{          }\text{(product rule of calculus)}} = \sum_{a}^{}\left\lbrack \nabla\pi\left( a\mid s \right)q_{\pi}(s,a) + \pi\left( a\mid s \right)\nabla\left( \sum_{s',r}^{}{p\left( s',r\mid s,a \right)} \right)\left\lbrack r - r(\theta) + v_{\pi}\left( s' \right) \right\rbrack \right\rbrack = \sum_{a}^{}\left\lbrack \nabla\pi\left( a\mid s \right)q_{\pi}(s,a) + \pi\left( a\mid s \right)\left\lbrack - \nabla r(\theta) + \sum_{s'}^{}{p\left( s'\mid s,a \right)\nabla v_{\pi}\left( s' \right)} \right\rbrack \right\rbrack.
```

<span dir="rtl">بعد إعادة ترتيب المصطلحات</span> (Re-arranging
Terms)<span dir="rtl">، نحصل على</span>:

``` math
\nabla r(\theta) = \sum_{a}^{}\left\lbrack \nabla\pi\left( a\mid s \right)q_{\pi}(s,a) + \pi\left( a\mid s \right)\sum_{s'}^{}{p\left( s'\mid s,a \right)\nabla v_{\pi}\left( s' \right)} \right\rbrack - \nabla v_{\pi}(s).
```

<span dir="rtl">لاحظ أن الطرف الأيسر يمكن كتابته كـ</span>
$`\nabla J(\theta)`$<span dir="rtl">، وأنه لا يعتمد على الحالة</span>
$`s`$<span dir="rtl">.</span> <span dir="rtl">وبالتالي، فإن الطرف الأيمن
لا يعتمد على</span> s <span dir="rtl">أيضًا، ويمكننا جمعه بأمان على جميع
الحالات</span> $`s \in S`$ <span dir="rtl">، موزونًا بـ</span>
$`\mu(s)`$<span dir="rtl">، دون تغييره (لأن</span>
∑sμ(s)=1<span dir="rtl">)</span>:

``` math
\nabla J(\theta) = \sum_{s}^{}{\mu(s)\sum_{a}^{}\left( \nabla\pi\left( a\mid s \right)q_{\pi}(s,a) + \pi\left( a\mid s \right)\sum_{s'}^{}{p\left( s'\mid s,a \right)\nabla v_{\pi}\left( s' \right)} \right)} - \nabla v_{\pi}(s) = \sum_{s}^{}{\mu(s)\sum_{a}^{}{\nabla\pi\left( a\mid s \right)q_{\pi}(s,a)}} + \sum_{s}^{}{\mu(s)\sum_{a}^{}{\pi\left( a\mid s \right)\sum_{s'}^{}{p\left( s'\mid s,a \right)\nabla v_{\pi}\left( s' \right)}}} - \sum_{s}^{}{\mu(s)\nabla v_{\pi}(s)}
```

``` math
\sum_{s}^{}{\mu(s)\sum_{a}^{}{\nabla\pi\left( a \middle| s \right)q\pi(s,a)}} + \underset{\mu\left( s' \right)\ (13.16)}{\overset{\sum_{s'}^{}{\sum_{s}^{}{\mu(s)\sum_{a}^{}{\pi\left( a \middle| s \right)p\left( s' \middle| s,a \right)}}}}{︸}}\nabla v\pi\left( s' \right) - \sum_{s}^{}{\mu(s)\nabla v\pi(s)}
```

<span dir="rtl">.</span>

<span dir="rtl">.</span>$`\sum_{s}^{}{\mu(s)\sum_{a}^{}{\nabla\pi\left( a \middle| s \right)q\pi(s,a)}} + \sum_{s'}^{}{\mu\left( s' \right)\nabla v\pi\left( s' \right)} - \sum_{s}^{}{\mu(s)\nabla v\pi(s)}`$

``` math
\sum_{s}^{}{\mu(s)\sum_{a}^{}{\nabla\pi\left( a \middle| s \right)q\pi(s,a)}}.\quad\text{Q.E.D.}
```

**<u>13.7 <span dir="rtl">التمثيل البياني للسياسة للأفعال
المستمرة</span> <span dir="rtl">(</span>Policy Parameterization for
<span dir="rtl"></span>Continuous Actions<span dir="rtl">)</span></u>**

<span dir="rtl">تقدم **الطرق المعتمدة على السياسة**</span> (Policy-based
methods) <span dir="rtl">طرقًا عملية للتعامل مع **مساحات الأفعال
الكبيرة**</span> (large action spaces)<span dir="rtl">، بما في ذلك
**المساحات المستمرة** (</span>continuous
<span dir="rtl"></span>spaces<span dir="rtl">) ذات عدد غير محدود من
الأفعال. بدلاً من حساب **الاحتمالات المتعلمة**</span> (learned
probabilities) <span dir="rtl">لكل من الأفعال العديدة، نتعلم **إحصاءات
توزيع الاحتمالات** (</span>statistics of the probability
<span dir="rtl"></span>distribution<span dir="rtl">)</span>.
<span dir="rtl">على سبيل المثال، قد تكون مجموعة الأفعال هي **الأعداد
الحقيقية**</span> (real numbers)<span dir="rtl">، مع اختيار الأفعال من
**توزيع طبيعي**</span> (normal distribution)<span dir="rtl">.</span>

<span dir="rtl">تُكتب **دالة كثافة الاحتمال**</span> (probability density
function) <span dir="rtl">للتوزيع الطبيعي تقليديًا على النحو
التالي</span>:

``` math
p(x) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left( - \frac{(x - \mu)^{2}}{2\sigma^{2}} \right)
```

<img src="./media/image167.png" style="width:2.96389in;height:2.05in" /><span dir="rtl">حيث</span>
$`\mu`$ <span dir="rtl">و</span>$`\sigma`$ <span dir="rtl">هنا هما
**المتوسط**</span> **(mean)** <span dir="rtl">و**الانحراف
المعياري**</span> **(standard deviation)** <span dir="rtl">للتوزيع
الطبيعي، و</span>$`\pi`$ <span dir="rtl">هنا هو مجرد العدد</span>
$`\mathbf{\pi \approx 3.14159}`$ <span dir="rtl">تُظهر دوال كثافة
الاحتمال</span> (**probability density functions**) <span dir="rtl">لعدة
متوسطات وانحرافات معيارية مختلفة على اليمين. القيمة</span> $`p(x)`$
<span dir="rtl">هي **كثافة الاحتمال**</span>
**<span dir="rtl">(</span>density of the
<span dir="rtl"></span>probability<span dir="rtl">)
</span>**<span dir="rtl">عند</span> $`x`$<span dir="rtl">، وليست
الاحتمالية. يمكن أن تكون أكبر من 1؛ المساحة الكلية تحت</span> $`p(x)`$
<span dir="rtl">يجب أن تكون مساوية لـ 1. بشكل عام، يمكن أخذ التكامل
تحت</span> $`p(x)`$ <span dir="rtl">لأي مدى من قيم</span> $`x`$
<span dir="rtl">للحصول على **الاحتمال**</span> **(probability)**
<span dir="rtl">وقوع</span> $`x`$ <span dir="rtl">ضمن ذلك المدى</span>.
<span dir="rtl"></span>

<span dir="rtl">لإنتاج تمثيل بياني للسياسة، يمكن تعريف السياسة كدالة
كثافة الاحتمال الطبيعي على فعل ذو قيمة حقيقية، مع متوسط وانحراف معياري
يتم تحديدهما بواسطة **مقاربين برامترين**</span>
**<span dir="rtl">(</span>parametric <span dir="rtl"></span>function
approximators<span dir="rtl">) </span>**<span dir="rtl">يعتمدون على
الحالة</span>.

``` math
\pi\left( a \middle| s,\theta \right) = \frac{1}{\sigma(s,\theta)\sqrt{2\pi}}\exp\left( - \frac{\left( a - \mu(s,\theta) \right)^{2}}{2\sigma(s,\theta)^{2}} \right)
```

<span dir="rtl">حيث</span> μ:S×Rd0→R\mu : S \times \mathbb{R}^{d_0} \to
\mathbb{R}μ:S×Rd0​→R <span dir="rtl">و</span> σ:S×Rd0→R+\sigma : S \times
\mathbb{R}^{d_0} \to \mathbb{R}^+σ:S×Rd0​→R+ <span dir="rtl">هما مقاربان
برامترين</span>

<span dir="rtl">لإكمال المثال، نحتاج فقط إلى تقديم شكل لهذين
المقاربين</span> <span dir="rtl">(</span>parameterized function
<span dir="rtl"></span>approximators<span dir="rtl">)</span>.
<span dir="rtl">لهذا، نقسم متجه برامترات السياسة</span> θ
<span dir="rtl">إلى جزئين،</span>
$`\theta = \lbrack\theta\mu,\theta\sigma\rbrack\top`$ <span dir="rtl">،
جزء واحد لاستخدامه في تقريب المتوسط</span> (mean) <span dir="rtl">وجزء
آخر لتقريب الانحراف المعياري</span> <span dir="rtl">(</span>standard
<span dir="rtl"></span>deviation<span dir="rtl">)</span>.
<span dir="rtl">يمكن تقريب المتوسط كدالة خطية</span> (linear
function)<span dir="rtl">.</span> <span dir="rtl">يجب أن يكون الانحراف
المعياري دائمًا إيجابيًا، ومن الأفضل تقريبه كدالة أسية لدالة خطية</span>
<span dir="rtl">(</span>exponential of a linear
<span dir="rtl"></span>function<span dir="rtl">)</span>.
<span dir="rtl">وبالتالي،</span>

``` math
\mu(s,\theta) = \theta_{\mu}^{\top}x_{\mu}(s)
```

<span dir="rtl">و</span>

``` math
\sigma(s,\theta) = \exp\left( \theta_{\sigma}^{\top}x_{\sigma}(s) \right)
```

<span dir="rtl">حيث أن</span> $`x\mu(s)`$ <span dir="rtl">و</span>$`x`$
<span dir="rtl">هما **متجهات ميزات الحالة**</span> **(State Feature
Vectors)** <span dir="rtl">التي ربما تم بناؤها باستخدام إحدى الطرق
الموصوفة في **الفصل التاسع.** باستخدام هذه التعريفات، يمكن تطبيق جميع
**الخوارزميات**</span> **(Algorithms)** <span dir="rtl">الموصوفة في بقية
هذا الفصل لتعلم اختيار **الإجراءات ذات القيم الحقيقية**
</span>**(Real-Valued Actions)**<span dir="rtl">.</span>

<span dir="rtl">**التمرين 13.4**:</span> <span dir="rtl">أظهر أنه
بالنسبة لبارامتر</span> **Gaussian** **<span dir="rtl">السياسة</span>
(Policy)** (13.19)<span dir="rtl">، فإن **متجه الأهلية**</span>
**(Eligibility Vector)** <span dir="rtl">له الجزأين التاليين</span>:

``` math
\nabla\ln\pi\left( a \middle| s,\theta_{\mu} \right) = \frac{\nabla\pi\left( a \middle| s,\theta_{\mu} \right)}{\pi\left( a \middle| s,\theta \right)} = \frac{1}{\sigma(s,\theta)^{2}}\left( a - \mu(s,\theta) \right)x_{\mu}(s),
```

<span dir="rtl">و</span>

``` math
\nabla\ln\pi\left( a \middle| s,\theta \right) = \frac{\nabla\pi\left( a \middle| s,\theta \right)}{\pi\left( a \middle| s,\theta \right)} = \frac{\left( a - \mu(s,\theta) \right)^{2}}{\sigma(s,\theta)^{2}} - \frac{1}{\sigma(s,\theta)^{2}}x_{\mu}(s).
```

<span dir="rtl">**التمرين 13.5:** وحدة</span> **Bernoulli-logistic**
<span dir="rtl">هي وحدة عشوائية تشبه العصبون تُستخدم في بعض **الشبكات
العصبية الاصطناعية** </span>**(Artificial Neural
Networks)<span dir="rtl">.</span>** <span dir="rtl">في **الفصل
9.6**</span> <span dir="rtl">مدخلها في الوقت</span> $`t`$
<span dir="rtl">هو **متجه الميزات** </span>**(Feature Vector)**
$`x(St)`$<span dir="rtl">؛ ناتجها،</span> $`At`$​<span dir="rtl">، هو
متغير عشوائي له قيمتان،  
0 و1، حيث</span> $`Pr\ \{ At = 1\}`$ <span dir="rtl">و</span>
$`Pr\{ At = 0\} = 1 - Pt`$​ <span dir="rtl">(توزيع</span>
**Bernoulli**<span dir="rtl">)</span>. <span dir="rtl">دع</span>
$`h(s,0,\theta)`$ <span dir="rtl">و</span>$`h(s,1,\theta)`$
<span dir="rtl">يمثلان التفضيلات في **الحالة**</span> **(State)** s
<span dir="rtl">لإجراءات الوحدة الاثنين المعطاة بارامتر **السياسة**
</span>**(Policy)** <span dir="rtl"></span>θ<span dir="rtl">. افترض أن
الفرق بين تفضيلات **الإجراءات**</span> **(Actions)** <span dir="rtl">يتم
إعطاؤه بمجموع مرجح لمدخل **متجه** </span>**(Vector)**
<span dir="rtl">الوحدة، أي افترض أن  
</span>$`\ ,h(s,1,\theta)\  - h(s,0,\theta)\  = \theta\top x(s)`$<span dir="rtl">حيث</span>
θ <span dir="rtl">هو **متجه الأوزان** </span>**(Weight Vector)**
<span dir="rtl">للوحدة</span>.

$`(a)`$ <span dir="rtl">أظهر أنه إذا تم استخدام توزيع</span>
**Exponential Soft-Max** <span dir="rtl"></span>(13.2)
<span dir="rtl">لتحويل تفضيلات الإجراءات إلى **سياسات**
</span>**(Policies)**<span dir="rtl">، فإن</span>
Pt=π(1∣St,θt)=11+exp⁡(−θt⊤x(St))P_t = \pi(1\|S_t, \theta_t) =
\frac{1}{1 + \exp(-\theta^\top_t x(S_t))}Pt​=π(1∣St​,θt​)=1+exp(−θt⊤​x(St​))1​
(<span dir="rtl">دالة</span> **Logistic**<span dir="rtl">)</span>.

$`(b)`$ <span dir="rtl">ما هو تحديث</span> **Monte-Carlo REINFORCE**
<span dir="rtl">من</span> $`\theta t`$ <span dir="rtl">إلى</span>
$`\theta t + 1`$ <span dir="rtl">عند استلام العائد</span>
$`Gt`$​<span dir="rtl">؟</span>

$`(c)`$ <span dir="rtl">عبّر عن **الأهلية**</span> **(Eligibility)**
$`\nabla\ln\pi(a \mid s,\theta)`$ <span dir="rtl">لوحدة</span>
**Bernoulli-logistic** <span dir="rtl">من حيث</span>
$`a`$<span dir="rtl">،</span> $`x(s)`$<span dir="rtl">،
و</span>$`\ \pi(a \mid s,\theta)\ `$<span dir="rtl">بحساب **التدرج**
</span>**(Gradient)**<span dir="rtl">.</span>

<span dir="rtl">تلميح: احسب بشكل منفصل لكل **إجراء**</span> **(Action)**
<span dir="rtl">مشتق اللوغاريتم أولاً بالنسبة  
لـ</span> $`Pt = \pi(a \mid s,\theta t)`$<span dir="rtl">، ثم اجمع
النتائج في تعبير واحد يعتمد على</span> $`a`$
<span dir="rtl">و</span>$`Pt`$​<span dir="rtl">، ثم استخدم قاعدة السلسلة،
مع ملاحظة أن مشتق دالة</span> **Logistic** $`f(x)`$
<span dir="rtl">هو</span> $`f(x)(1 - f(x)`$<span dir="rtl">.</span>

**<u>13.8 <span dir="rtl">الملخص</span> (Summary)</u>**

**<span dir="rtl">قبل هذا الفصل</span> (Prior to this
chapter)**<span dir="rtl">، ركز هذا الكتاب على **أساليب قيم الإجراءات**
</span>**(Action-Value Methods)**<span dir="rtl">، أي الأساليب التي
تتعلم قيم **الإجراءات**</span> **(Actions)** <span dir="rtl">ثم تستخدمها
لتحديد اختيار **الإجراءات** </span>**(Action
Selections)**<span dir="rtl">.</span> <span dir="rtl">أما في هذا الفصل،
فقد تناولنا الأساليب التي تتعلم **سياسة ذات بارامترات**</span>
**(Parameterized Policy)** <span dir="rtl">تُمكن من اتخاذ
**الإجراءات**</span> **(Actions)** <span dir="rtl">دون الرجوع إلى
تقديرات **قيم الإجراءات** </span>**(Action-Value
Estimates)**<span dir="rtl">.</span> <span dir="rtl">بشكل خاص، تناولنا
**أساليب تدرج السياسة** </span>**(Policy-Gradient
Methods)**<span dir="rtl">، أي الأساليب التي تقوم بتحديث بارامتر
**السياسة** </span>**(Policy Parameter)** <span dir="rtl">في كل خطوة
باتجاه تقدير **تدرج الأداء** </span>**(Gradient of Performance)**
<span dir="rtl">بالنسبة لبارامتر **السياسة** </span>**(Policy
Parameter)**<span dir="rtl">.</span>

<span dir="rtl">تمتلك الأساليب التي تتعلم وتخزن **بارامتر
السياسة**</span> **(Policy Parameter)** <span dir="rtl">العديد من
المزايا. فهي تستطيع تعلم احتمالات محددة لاتخاذ **الإجراءات**
</span>**(Actions)**<span dir="rtl">.</span> <span dir="rtl">كما يمكنها
تعلم مستويات مناسبة من **الاستكشاف**</span> **(Exploration)**
<span dir="rtl">واقتراب **السياسات الحتمية**</span> **(Deterministic
Policies)** <span dir="rtl">تدريجيًا. يمكنها أيضًا التعامل بشكل طبيعي مع
فضاءات **الإجراءات المستمرة**</span>
**<span dir="rtl">(</span>Continuous Action
<span dir="rtl"></span>Spaces<span dir="rtl">)</span>**.
<span dir="rtl">كل هذه الأمور تكون سهلة للأساليب القائمة على
**السياسة**</span> **<span dir="rtl">(</span>Policy-Based
<span dir="rtl"></span>Methods<span dir="rtl">)
</span>**<span dir="rtl">ولكنها صعبة أو مستحيلة بالنسبة لأساليب
**الجشع**</span> **(ε-Greedy Methods)** <span dir="rtl">ولأساليب **قيم
الإجراءات**</span> **(Action-Value Methods)** <span dir="rtl">بشكل عام.
بالإضافة إلى ذلك، في بعض المشاكل تكون **السياسة**</span> **(Policy)**
<span dir="rtl">أسهل في التمثيل ببارامترات مقارنة بدالة **القيمة**
</span>**(Value Function)**<span dir="rtl">؛ هذه المشاكل تكون أكثر
ملاءمة لأساليب **السياسات ذات البارامترات**</span>
**<span dir="rtl">(</span>Parameterized Policy
<span dir="rtl"></span>Methods<span dir="rtl">)</span>**.

<span dir="rtl">كما تمتلك أساليب **السياسة ذات البارامترات**</span>
**(Parameterized Policy Methods)** <span dir="rtl">ميزة نظرية مهمة على
أساليب **قيم الإجراءات**</span> **(Action-Value Methods)**
<span dir="rtl">في شكل **نظرية تدرج السياسة** </span>**(Policy Gradient
Theorem)**<span dir="rtl">، التي تقدم صيغة دقيقة حول كيفية تأثير
**الأداء** </span>**(Performance)** <span dir="rtl">على **بارامتر
السياسة**</span> **(Policy Parameter)** <span dir="rtl">دون الحاجة إلى
اشتقاقات توزيع **الحالة** </span>**(State
Distribution)**<span dir="rtl">.</span> <span dir="rtl">هذه
**النظرية**</span> **(Theorem)** <span dir="rtl">توفر أساسًا نظريًا لجميع
أساليب **تدرج السياسة** </span>**(Policy Gradient
Methods)**<span dir="rtl">.</span>

<span dir="rtl">ينبثق أسلوب</span> **REINFORCE** <span dir="rtl">مباشرةً
من **نظرية تدرج السياسة** </span>**(Policy Gradient
Theorem)**<span dir="rtl">.</span> <span dir="rtl">إضافة دالة **قيمة
الحالة**</span> **(State-Value Function)** <span dir="rtl">كخط أساس يقلل
من **التباين**</span> **(Variance)** <span dir="rtl">في</span>
**REINFORCE** <span dir="rtl">دون إدخال انحياز. استخدام دالة **قيمة
الحالة** </span>**(State-Value Function)**
<span dir="rtl">للإقلاع</span> (Bootstrapping) <span dir="rtl">يُدخل
**انحياز**</span> **(Bias)** <span dir="rtl">ولكنه غالبًا ما يكون مرغوبًا
لنفس السبب الذي يجعل أساليب</span> **TD <span dir="rtl">المتدرجة</span>
(Bootstrapping TD Methods)** <span dir="rtl">غالبًا ما تكون متفوقة على
أساليب</span> **Monte Carlo** <span dir="rtl">(تقليل التباين بشكل كبير).
دالة **قيمة الحالة**</span> **(State-Value Function)**
<span dir="rtl">تعطي الفضل أو تنتقد اختيارات **السياسة**
</span>**(Policy's Action Selections)**<span dir="rtl">، وبناءً على ذلك
تُسمى الأولى **الناقد**</span> **(Critic)** <span dir="rtl">والأخرى
**الممثل** </span>**(Actor)**<span dir="rtl">، وتُعرف هذه الأساليب عمومًا
باسم أساليب **الممثل-الناقد** </span>**(Actor-Critic
Methods)**<span dir="rtl">.</span>

<span dir="rtl">بشكل عام، توفر أساليب **تدرج السياسة**</span>
**(Policy-Gradient Methods)** <span dir="rtl">مجموعة مختلفة تمامًا من
نقاط القوة والضعف مقارنة بأساليب **قيم الإجراءات**
</span>**(Action-Value Methods)**<span dir="rtl">.</span>
<span dir="rtl">اليوم، تُعتبر أقل فهمًا من بعض النواحي، ولكنها موضوع مثير
للبحث المستمر</span>.

<span dir="rtl">الجزء الثالث: نظرة أعمق</span> (Part III: Looking
Deeper)

<span dir="rtl">في هذا الجزء الأخير من الكتاب، ننظر إلى ما وراء الأفكار
القياسية **للتعليم المعزز (**</span>**Reinforcement
Learning**<span dir="rtl">) التي تم تقديمها في الجزئين الأولين من الكتاب
لنستعرض بإيجاز علاقاتها مع **علم النفس** </span>**(Psychology)
<span dir="rtl"></span>**<span dir="rtl">و**علم الأعصاب**</span>
**<span dir="rtl">(</span>Neuroscience<span dir="rtl">)</span>**<span dir="rtl">،
وعينة من تطبيقات **التعليم المعزز** </span>**(Reinforcement Learning
Applications)**<span dir="rtl">، وبعض المجالات النشطة للبحث المستقبلي في
**التعليم المعزز**</span> **(Reinforcement
Learning)<span dir="rtl">.</span>**

<span dir="rtl">الفصل الرابع عشر:  
علم النفس</span> (Psychology)

<span dir="rtl">في الفصول السابقة، قمنا بتطوير أفكار
**الخوارزميات**</span> **(Algorithms)** <span dir="rtl">بناءً على
اعتبارات حسابية فقط. في هذا الفصل، ننظر إلى بعض هذه
**الخوارزميات**</span> **(Algorithms)** <span dir="rtl">من منظور آخر:
منظور **علم النفس**</span> **(Psychology)** <span dir="rtl">ودراسته
لكيفية تعلم الحيوانات. أهداف هذا الفصل هي، أولاً، مناقشة الطرق التي
تتوافق بها أفكار وخوارزميات **التعليم المعزز**</span> **(Reinforcement
Learning)** <span dir="rtl">مع ما اكتشفه علماء النفس حول **تعلم
الحيوانات** </span>**(Animal Learning)**<span dir="rtl">، وثانيًا، شرح
التأثير الذي يحدثه **التعليم المعزز**</span> **(Reinforcement
Learning)** <span dir="rtl">على دراسة **تعلم الحيوانات**
</span>**(Animal Learning)**<span dir="rtl">.</span>

<span dir="rtl">النظام الواضح الذي يوفره **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">والذي ينظم المهام،
**العوائد** </span>**(Returns)**<span dir="rtl">،
و**الخوارزميات**</span> **(Algorithms)** <span dir="rtl">يثبت أنه مفيد
بشكل كبير في تفسير البيانات التجريبية، وفي اقتراح أنواع جديدة من
التجارب، وفي الإشارة إلى العوامل التي قد تكون حاسمة في التلاعب والقياس.
فكرة تحسين **العائد**</span> **(Return)** <span dir="rtl">على المدى
الطويل، التي تعد جوهر **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">، تسهم في فهمنا لخصائص **تعلم الحيوانات  
(**</span>**Animal
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**
<span dir="rtl">وسلوكها التي كانت تبدو محيرة</span>.

<span dir="rtl">بعض التوافقات بين **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">والنظريات النفسية ليست
مفاجئة لأن تطوير **التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">استلهم من نظريات **التعليم النفسي**
</span>**(Psychological Learning Theories)**<span dir="rtl">. ومع ذلك،
كما هو موضح في هذا الكتاب، يستكشف **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">مواقف مثالية من منظور باحث
أو مهندس في **الذكاء الاصطناعي** </span>**(Artificial
Intelligence)**<span dir="rtl">، بهدف حل المشكلات الحسابية باستخدام
**الخوارزميات الفعالة** </span>**(Efficient
Algorithms)**<span dir="rtl">، بدلاً من تكرار أو تفسير كيفية تعلم
الحيوانات بالتفصيل. ونتيجة لذلك، فإن بعض التوافقات التي نصفها تربط بين
أفكار نشأت بشكل مستقل في مجالاتها الخاصة. نحن نعتقد أن هذه النقاط من
الاتصال ذات أهمية خاصة لأنها تكشف عن مبادئ حسابية مهمة للتعليم، سواء كان
التعليم في **الأنظمة الاصطناعية**</span> **(Artificial Systems)**
<span dir="rtl">أو **الأنظمة الطبيعية** </span>**(Natural
Systems)**<span dir="rtl">.</span>

<span dir="rtl">في الغالب، نصف التوافقات بين **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">ونظريات **التعليم**
</span>**(Learning Theories)** <span dir="rtl">التي تم تطويرها لشرح
كيفية تعلم الحيوانات مثل الجرذان، الحمام، والأرانب في تجارب **المختبرات
المضبوطة** </span>**(Controlled Laboratory
Experiments)**<span dir="rtl">.</span> <span dir="rtl">تم إجراء الآلاف
من هذه التجارب طوال القرن العشرين، ولا تزال العديد منها تُجرى حتى اليوم.
وعلى الرغم من أن هذه التجارب أحيانًا تُعتبر غير ذات صلة بالقضايا الأوسع في
**علم النفس** </span>**(Psychology)**<span dir="rtl">، إلا أنها تستكشف
خصائص دقيقة لـ **تعلم الحيوانات** </span>**(Animal
Learning)**<span dir="rtl">، وغالبًا ما تكون مدفوعة بأسئلة نظرية
دقيقة</span>.

<span dir="rtl">مع تحول تركيز **علم النفس**</span> **(Psychology)**
<span dir="rtl">إلى الجوانب الأكثر إدراكية للسلوك، أي إلى العمليات
الذهنية مثل التفكير والتفكير المنطقي، أصبحت تجارب **تعلم
الحيوانات**</span> **(Animal Learning)** <span dir="rtl">أقل أهمية في
**علم النفس**</span> **(Psychology)** <span dir="rtl">مما كانت عليه في
السابق. ولكن هذا النوع من التجارب أدى إلى اكتشاف مبادئ
**التعليم**</span> **(Learning Principles)** <span dir="rtl">التي تعتبر
أساسية ومنتشرة في جميع أنحاء المملكة الحيوانية، مبادئ يجب ألا تُهمل عند
تصميم أنظمة **التعليم الاصطناعي  
(**</span>**Artificial Learning Systems<span dir="rtl">)</span>**.
<span dir="rtl">بالإضافة إلى ذلك، كما سنرى، بعض جوانب المعالجة الإدراكية
تتصل بشكل طبيعي بالمنظور الحسابي الذي يوفره **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**.

<span dir="rtl">تتضمن القسم الأخير من هذا الفصل مراجع ذات صلة بالتوافقات
التي نناقشها وكذلك بالتوافقات التي نتجاهلها. نأمل أن يشجع هذا الفصل
القراء على استكشاف جميع هذه التوافقات بشكل أعمق. كما يتضمن هذا القسم
الأخير مناقشة حول كيفية ارتباط المصطلحات المستخدمة في **التعليم المعزز**
</span>**(Reinforcement Learning)** <span dir="rtl">بتلك المستخدمة في
**علم النفس** </span>**(Psychology)**<span dir="rtl">.</span>
<span dir="rtl">العديد من المصطلحات والعبارات المستخدمة في **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">مستعارة من
نظريات **تعلم الحيوانات** </span>**(Animal Learning
Theories)**<span dir="rtl">، لكن المعاني الحسابية/الهندسية لهذه
المصطلحات والعبارات لا تتطابق دائمًا مع معانيها في **علم النفس**
</span>**(Psychology)**<span dir="rtl">.</span>

**<u>14.1 <span dir="rtl">التنبؤ والتحكم</span> (Prediction and
Control)</u>**

**<span dir="rtl">الخوارزميات</span> (Algorithms)** <span dir="rtl">التي
نصفها في هذا الكتاب تنقسم إلى فئتين واسعتين:</span>
**<span dir="rtl">الخوارزميات الخاصة بالتنبؤ</span> (Prediction
Algorithms)** <span dir="rtl">و**الخوارزميات الخاصة بالتحكم**</span>
**<span dir="rtl">(</span>Control
<span dir="rtl"></span>Algorithms<span dir="rtl">)</span>**.
<span dir="rtl">تنشأ هذه الفئات بشكل طبيعي في طرق حل مشكلة **التعليم
المعزز** </span>**(Reinforcement Learning)** <span dir="rtl">التي تم
تقديمها في **الفصل الثالث**.</span> <span dir="rtl">في العديد من
النواحي، تتوافق هذه الفئات على التوالي مع فئات التعليم التي درسها علماء
النفس بشكل مكثف</span>: **<span dir="rtl">التكييف الكلاسيكي</span>
(Classical, or Pavlovian, Conditioning)** <span dir="rtl">و**التكييف
الآلي**</span> **<span dir="rtl">(</span>Instrumental, or
<span dir="rtl"></span>Operant, Conditioning<span dir="rtl">)</span>**.
<span dir="rtl">هذه التوافقات ليست مصادفة تمامًا بسبب تأثير علم النفس على
**التعليم المعزز** </span>**(Reinforcement Learning)**<span dir="rtl">،
لكنها مع ذلك لافتة للنظر لأنها تربط بين أفكار نشأت من أهداف
مختلفة</span>.

<span dir="rtl">تقوم **خوارزميات التنبؤ**</span> **(Prediction
Algorithms)** <span dir="rtl">المقدمة في هذا الكتاب بتقدير كميات تعتمد
على كيفية توقع تطور ميزات بيئة **العميل**</span> **(Agent)**
<span dir="rtl">في المستقبل. نحن نركز بشكل خاص على تقدير مقدار
**المكافأة**</span> **(Reward)** <span dir="rtl">التي يمكن أن يتوقع
**العميل**</span> **(Agent)** <span dir="rtl">الحصول عليها في المستقبل
أثناء تفاعله مع **البيئة**
</span>**(Environment)**<span dir="rtl">.</span> <span dir="rtl">في هذا
الدور، تُعد **خوارزميات التنبؤ**</span>
**<span dir="rtl">(</span>Prediction
<span dir="rtl"></span>Algorithms<span dir="rtl">)</span>**
**<span dir="rtl">خوارزميات تقييم السياسات</span> (Policy Evaluation
Algorithms)**<span dir="rtl">، وهي مكونات أساسية من
**الخوارزميات**</span> **(Algorithms)** <span dir="rtl">لتحسين
**السياسات** </span>**(Policies)**<span dir="rtl">.</span>
<span dir="rtl">ولكن **خوارزميات التنبؤ** </span>**(Prediction
Algorithms)** <span dir="rtl">لا تقتصر على التنبؤ بالمكافأة المستقبلية؛
يمكنها التنبؤ بأي ميزة من ميزات **البيئة**</span> **(Environment)**
<span dir="rtl">(انظر، على سبيل المثال،</span> Modayil, White, and
Sutton, 2014<span dir="rtl">)</span>. <span dir="rtl">يستند التوافق بين
**خوارزميات التنبؤ**</span> **(Prediction Algorithms)**
<span dir="rtl">و**التكييف الكلاسيكي** </span>**(Classical
Conditioning)** <span dir="rtl">على خاصيتهم المشتركة في التنبؤ بالمحفزات
القادمة، سواء كانت تلك المحفزات مكافأة أو عقوبة</span>.

<span dir="rtl">الوضع في تجربة **التكييف الآلي**</span> **(Instrumental,
or Operant, Conditioning)** <span dir="rtl">مختلف. هنا، يتم إعداد الجهاز
التجريبي بحيث يتم إعطاء الحيوان شيئًا يحبه (مكافأة) أو شيئًا لا يحبه
(عقوبة) بناءً على ما فعله الحيوان. يتعلم الحيوان زيادة ميوله لإنتاج
السلوك المكافأة وتقليل ميوله لإنتاج السلوك المعاقب. يُقال إن المحفز
المعزز يعتمد على سلوك الحيوان، بينما في **التكييف الكلاسيكي**
</span>**(Classical Conditioning)** <span dir="rtl">لا يعتمد (على الرغم
من صعوبة إزالة جميع تبعيات السلوك في تجربة **التكييف الكلاسيكي**
</span>**(Classical Conditioning)**<span dir="rtl">)</span>.
<span dir="rtl">تجارب **التكييف الآلي**</span>
**<span dir="rtl">(</span>Instrumental
<span dir="rtl"></span>Conditioning Experiments<span dir="rtl">)
</span>**<span dir="rtl">تشبه تلك التي ألهمت **قانون الأثر
لثورندايك**</span> **<span dir="rtl">(</span>Thorndike’s
<span dir="rtl"></span>Law of Effect<span dir="rtl">)
</span>**<span dir="rtl">الذي ناقشناه باختصار في **الفصل الأول.**</span>
**<span dir="rtl">التحكم</span> (Control)** <span dir="rtl">هو جوهر هذا
الشكل من التعليم، الذي يتوافق مع تشغيل **خوارزميات تحسين
السياسات**</span> **<span dir="rtl">(</span>Policy-Improvement
<span dir="rtl"></span>Algorithms<span dir="rtl">)
</span>**<span dir="rtl">في **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">.</span>

<span dir="rtl">التفكير في **التكييف الكلاسيكي**</span> **(Classical
Conditioning)** <span dir="rtl">من حيث **التنبؤ**
</span>**(Prediction)**<span dir="rtl">، و**التكييف الآلي**</span>
**(Instrumental Conditioning)** <span dir="rtl">من حيث **التحكم**
</span>**(Control)**<span dir="rtl">، هو نقطة انطلاق لربط رؤيتنا
الحسابية لـ **التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">بـ **تعلم الحيوانات  
(**</span>**Animal Learning<span dir="rtl">)</span>**<span dir="rtl">،
لكن في الواقع، الوضع أكثر تعقيدًا من ذلك. هناك أكثر من مجرد **التنبؤ**
</span>**(Prediction)** <span dir="rtl">في **التكييف الكلاسيكي**
</span>**(Classical Conditioning)**<span dir="rtl">؛ فهو يتضمن أيضًا
**الإجراء** </span>**(Action)**<span dir="rtl">، وبالتالي فهو نمط من
**التحكم** </span>**(Control)**<span dir="rtl">، يُطلق عليه أحيانًا
**التحكم البافلوفي** </span>**(Pavlovian
Control)**<span dir="rtl">.</span> <span dir="rtl">علاوة على ذلك، يتفاعل
**التكييف الكلاسيكي**</span> **<span dir="rtl">(</span>Classical
<span dir="rtl"></span>Conditioning<span dir="rtl">)</span>**
<span dir="rtl">و**التكييف الآلي**</span> **(Instrumental
Conditioning)** <span dir="rtl">بطرق مثيرة للاهتمام، حيث من المحتمل أن
يكون كلا النوعين من التعليم مشتركين في معظم الحالات التجريبية. على الرغم
من هذه التعقيدات، فإن محاذاة التمييز بين **التكييف
الكلاسيكي/الآلي**</span>
**<span dir="rtl">(</span>Classical/Instrumental
<span dir="rtl"></span>Distinction<span dir="rtl">)
</span>**<span dir="rtl">مع التمييز بين **التنبؤ/التحكم**</span>
**(Prediction/Control Distinction)** <span dir="rtl">هو تقريب أولي مريح
في ربط **التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">بـ **تعلم الحيوانات (**</span>**Animal
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**.

<span dir="rtl">في **علم النفس**
</span>**(Psychology)**<span dir="rtl">، يُستخدم مصطلح **التعزيز**</span>
**(Reinforcement)** <span dir="rtl">لوصف **التعليم**
</span>**(Learning)** <span dir="rtl">في كل من **التكييف
الكلاسيكي**</span> **(Classical)** <span dir="rtl">و**التكييف
الآلي**</span> **<span dir="rtl">(</span>Instrumental
<span dir="rtl"></span>Conditioning<span dir="rtl">)</span>**.
<span dir="rtl">في الأصل كان يشير فقط إلى تقوية نمط من **السلوك**
</span>**(Behavior)**<span dir="rtl">، ولكنه يُستخدم أيضًا بشكل متكرر لضعف
نمط من **السلوك** </span>**(Behavior)**<span dir="rtl">.</span>
<span dir="rtl">يُطلق على المحفز الذي يُعتبر سببًا للتغيير في
**السلوك**</span> **(Behavior)** <span dir="rtl">اسم **المعزز**
</span>**(Reinforcer)**<span dir="rtl">، سواء كان معتمدًا على **سلوك**
</span>**(Behavior)** <span dir="rtl">الحيوان السابق أم لا. في نهاية هذا
الفصل، نناقش هذه المصطلحات بمزيد من التفصيل وكيفية ارتباطها بالمصطلحات
المستخدمة في **التعليم الآلي** </span>**(Machine
Learning)**<span dir="rtl">.</span>

**<u>14.2 <span dir="rtl">التكييف الكلاسيكي</span> (Classical
Conditioning)</u>**

<span dir="rtl">ثناء دراسة نشاط **الجهاز الهضمي** </span>**(Digestive
System)**<span dir="rtl">، اكتشف عالم الفسيولوجيا الروسي الشهير **إيفان
بافلوف**</span> **(Ivan Pavlov)** <span dir="rtl">أن **الاستجابات
الفطرية**</span> **(Innate Responses)** **<span dir="rtl">للحيوان</span>
(Animal)** <span dir="rtl">تجاه **محفزات معينة**</span> **(Certain
Stimuli)** <span dir="rtl">يمكن أن تُستثار بواسطة **محفزات**
</span>**(Stimuli)** <span dir="rtl">أخرى غير مرتبطة على الإطلاق
**بالمحفزات الفطرية** </span>**(Inborn
Triggers)**<span dir="rtl">.</span> <span dir="rtl">كانت موضوعات تجاربه
**كلابًا**</span> **(Dogs)** <span dir="rtl">خضعت لجراحة بسيطة للسماح
بقياس دقيق لشدة **انعكاسها اللعابي (**</span>**Salivary
<span dir="rtl"></span>Reflex<span dir="rtl">)</span>**.
<span dir="rtl">في إحدى الحالات التي وصفها، لم يكن **الكلب**</span>
**(Dog)** <span dir="rtl">يسيل **لعابه**</span> **(Salivate)**
<span dir="rtl">في معظم الظروف، ولكن بعد حوالي 5 ثوانٍ من تقديم
**الطعام** </span>**(Food)**<span dir="rtl">، أنتج حوالي ست قطرات من
**اللعاب** </span>**(Saliva)** <span dir="rtl">على مدار الثواني القليلة
التالية. بعد عدة مرات من تقديم **محفز آخر (**</span>**Another
<span dir="rtl"></span>Stimulus<span dir="rtl">)</span>**<span dir="rtl">،
غير مرتبط **بالطعام** </span>**(Food)**<span dir="rtl">، في هذه الحالة
كان صوت **المترونوم** </span>**(Metronome)**<span dir="rtl">، قبل إدخال
**الطعام**</span> **(Food)** <span dir="rtl">بقليل، سال **لعاب
الكلب**</span> **(Dog Salivate)** <span dir="rtl">استجابةً لصوت
**المترونوم**</span> **(Metronome)** <span dir="rtl">بنفس الطريقة التي
استجاب بها **للطعام** </span>**(Food)**<span dir="rtl">.</span>
<span dir="rtl">كتب بافلوف: "لقد تم استدعاء نشاط **الغدة
اللعابية**</span> **(Salivary Gland)** <span dir="rtl">بواسطة **نبضات
الصوت**</span> **(Sound Impulses)** —<span dir="rtl">وهو **محفز**</span>
**(Stimulus)** <span dir="rtl">غريب تمامًا عن **الطعام**
</span>**(Food)**<span dir="rtl">"</span> (Pavlov, 1927, p.
22)<span dir="rtl">.</span>

<span dir="rtl">ملخصًا لأهمية هذا الاكتشاف، كتب بافلوف</span>:

"<span dir="rtl">من الواضح تمامًا أنه في الظروف الطبيعية يجب أن يستجيب
**الحيوان الطبيعي** </span>**(Normal Animal)** <span dir="rtl">ليس فقط
**للمحفزات**</span> **(Stimuli)** <span dir="rtl">التي تجلب **منفعة
فورية**</span> **(Immediate Benefit)** <span dir="rtl">أو **ضررًا**
</span>**(Harm)**<span dir="rtl">، ولكن أيضًا للوكالات الفيزيائية أو
الكيميائية الأخرى—مثل **موجات الصوت**</span>
**<span dir="rtl">(</span>Sound
<span dir="rtl"></span>Waves<span dir="rtl">)
</span>**<span dir="rtl">و**الضوء**</span> **(Light)**
<span dir="rtl">وما إلى ذلك—التي تشير في حد ذاتها فقط إلى اقتراب هذه
**المحفزات** </span>**(Stimuli)**<span dir="rtl">؛ على الرغم من أنه ليس
من مشهد وصوت **الوحش المفترس**</span> **(Beast of Prey)**
<span dir="rtl">الذي يشكل في حد ذاته ضررًا **للحيوان الأصغر**
</span>**(Smaller Animal)**<span dir="rtl">، ولكن **أسنانه
ومخالبه**</span> **<span dir="rtl">  
(</span>Teeth and
Claws<span dir="rtl">)</span>**.<span dir="rtl">"</span> (Pavlov, 1927,
p. 14)

<span dir="rtl">ربط **المحفزات الجديدة**</span> **(New Stimuli)**
**<span dir="rtl">بالانعكاسات الفطرية</span> (Innate Reflexes)**
<span dir="rtl">بهذه الطريقة يُطلق عليه الآن **التكييف الكلاسيكي**
</span>**(Classical, or Pavlovian,
Conditioning)**<span dir="rtl">.</span> <span dir="rtl">أطلق بافلوف (أو
بالأحرى، مترجموه) على **الاستجابات الفطرية**</span> **(Inborn
Responses)** (<span dir="rtl">مثل **إفراز اللعاب**</span>
**(Salivation)** <span dir="rtl">في التجربة التي تم وصفها أعلاه)
اسم</span> **"<span dir="rtl">الاستجابات غير المشروطة"</span>
(Unconditioned Responses (URs))**<span dir="rtl">، وعلى **المحفزات
الطبيعية**</span> **<span dir="rtl">(</span>Natural Triggering
<span dir="rtl"></span>Stimuli<span dir="rtl">)
</span>**<span dir="rtl">(مثل **الطعام**
</span>**(Food)**<span dir="rtl">)</span> <span dir="rtl">اسم</span>
**"<span dir="rtl">المحفزات غير المشروطة</span>"
<span dir="rtl">(</span>Unconditioned Stimuli
<span dir="rtl"></span>(USs)<span dir="rtl">)</span>**<span dir="rtl">،
وعلى **الاستجابات الجديدة**</span> **(New Responses)**
<span dir="rtl">التي تُثار بواسطة **المحفزات التنبؤية**
</span>**(Predictive Stimuli)** <span dir="rtl">(مثل، هنا أيضًا **إفراز
اللعاب** </span>**(Salivation)**<span dir="rtl">)</span>
<span dir="rtl">اسم</span> **"<span dir="rtl">الاستجابات
المشروطة:</span> (Conditioned Responses (CRs))**<span dir="rtl">.</span>

**<span dir="rtl">المحفز</span> (Stimulus)** <span dir="rtl">الذي يكون
في البداية **محايدًا** </span>**(Neutral)**<span dir="rtl">، بمعنى أنه لا
يستثير عادةً **استجابات قوية**</span> **(Strong Responses)**
<span dir="rtl">(مثل صوت **المترونوم**
</span>**(Metronome)**<span dir="rtl">)، يصبح</span>
**"<span dir="rtl">محفزًا مشروطًا"</span> (Conditioned Stimulus (CS))**
<span dir="rtl">مع تعلم **الحيوان**</span> **(Animal)**
<span dir="rtl">أنه يتنبأ **بالمحفز غير المشروط** </span>**(US)**
<span dir="rtl">وبالتالي يبدأ في إنتاج **استجابة مشروطة**</span>
**(CR)** <span dir="rtl">استجابةً **للمحفز المشروط**
</span>**(CS)**<span dir="rtl">.</span> <span dir="rtl">لا تزال هذه
المصطلحات تُستخدم في وصف تجارب **التكييف الكلاسيكي (**</span>**Classical
Conditioning <span dir="rtl"></span>Experiments<span dir="rtl">)
</span>**<span dir="rtl">(على الرغم من أن الترجمات الأفضل كانت ستكون
"مشروط" و"غير مشروط" بدلاً من مشروط وغير مشروط). يُطلق على **المحفز غير
المشروط**</span> **(US)** <span dir="rtl">اسم **المعزز**</span>
**(Reinforcer)** <span dir="rtl">لأنه يعزز إنتاج **استجابة
مشروطة**</span> **(CR)** <span dir="rtl">استجابةً **للمحفز المشروط**
</span>**(CS)**<span dir="rtl">.</span>

<img src="./media/image168.png"
style="width:3.28056in;height:2.90694in" /><span dir="rtl">يظهر ترتيب
**المحفزات**</span> **(Stimuli)** <span dir="rtl">في نوعين شائعين من
تجارب **التكييف الكلاسيكي**</span> **<span dir="rtl">(</span>Classical
<span dir="rtl"></span>Conditioning Experiments<span dir="rtl">)
</span>**<span dir="rtl">على اليمين. في **التكييف التأخيري**</span>
**<span dir="rtl">(</span>Delay
<span dir="rtl"></span>Conditioning<span dir="rtl">)</span>**<span dir="rtl">،
يمتد **المحفز المشروط**</span> **(Conditioned Stimulus (CS))**
<span dir="rtl">طوال فترة **الفاصل بين المحفزين**</span>
**<span dir="rtl">(</span>Interstimulus <span dir="rtl"></span>Interval
(ISI)<span dir="rtl">)</span>**<span dir="rtl">، وهو الفترة الزمنية بين
بداية **المحفز المشروط**</span> **(CS Onset)** <span dir="rtl">وبداية
**المحفز غير المشروط (**</span>**Unconditioned Stimulus
(US)<span dir="rtl">) </span>**<span dir="rtl">مع انتهاء **المحفز
المشروط**</span> **(CS)** <span dir="rtl">عندما ينتهي **المحفز غير
المشروط**</span> **(US)** <span dir="rtl">في نسخة شائعة تظهر هنا. في
**التكييف الأثري** </span>**(Trace Conditioning)**<span dir="rtl">، يبدأ
**المحفز غير المشروط**</span> **(US)** <span dir="rtl">بعد انتهاء
**المحفز المشروط** </span>**(CS)**<span dir="rtl">، وتسمى الفترة الزمنية
بين انتهاء **المحفز المشروط**</span> **(CS Offset)**
<span dir="rtl">وبداية **المحفز غير المشروط**</span> **(US Onset)**
<span dir="rtl">بـ **الفاصل الأثري** </span>**(Trace
Interval)**<span dir="rtl">.</span>

**<span dir="rtl">إفراز لعاب</span> (Salivation)**
**<span dir="rtl">كلاب بافلوف</span> (Pavlov's Dogs)**
<span dir="rtl">عند سماع صوت **المترونوم** </span>**(Metronome)**
<span dir="rtl">هو مجرد مثال واحد على **التكييف الكلاسيكي**
</span>**(Classical Conditioning)**<span dir="rtl">، الذي تم دراسته بشكل
مكثف عبر العديد من أنظمة **الاستجابة**</span> **(Response Systems)**
<span dir="rtl">في العديد من أنواع **الحيوانات** </span>**(Animal
Species)**<span dir="rtl">.</span> <span dir="rtl">غالبًا ما تكون
**الاستجابات غير المشروطة** </span>**(Unconditioned Responses (URs))**
<span dir="rtl">تحضيرية بطريقة ما، مثل **إفراز اللعاب**
</span>**(Salivation)** <span dir="rtl">في **كلاب بافلوف**
</span>**(Pavlov's Dog)**<span dir="rtl">، أو حماية بطريقة ما، مثل
**رمشه العين**</span> **(Eye Blink)** <span dir="rtl">استجابةً لشيء مزعج
**للعين** </span>**(Eye)**<span dir="rtl">، أو **التجمد**</span>
**(Freezing)** <span dir="rtl">استجابةً لرؤية **مفترس**
</span>**(Predator)**<span dir="rtl">.</span> <span dir="rtl">إن تجربة
العلاقة التنبؤية بين **المحفز المشروط**</span> **(CS)**
<span dir="rtl">و**المحفز غير المشروط**</span> **(US)**
<span dir="rtl">على مدى سلسلة من التجارب تجعل **الحيوان**</span>
**(Animal)** <span dir="rtl">يتعلم أن **المحفز المشروط**</span> **(CS)**
<span dir="rtl">يتنبأ **بالمحفز غير المشروط** </span>**(US)**
<span dir="rtl">بحيث يمكن **للحيوان**</span> **(Animal)**
<span dir="rtl">أن يستجيب **للمحفز المشروط**</span> **(CS)**
**<span dir="rtl">باستجابة مشروطة</span> (Conditioned Response (CR))**
<span dir="rtl">تُعد **الحيوان**</span> **(Animal)** <span dir="rtl">أو
تحميه من **المحفز غير المشروط**</span> **(US)**
<span dir="rtl">المتوقع</span>.

<span dir="rtl">بعض **الاستجابات المشروطة**</span> **(CRs)**
<span dir="rtl">تشبه **الاستجابات غير المشروطة**</span> **(URs)**
<span dir="rtl">ولكنها تبدأ في وقت أبكر وتختلف بطرق تزيد من فعاليتها. في
نوع من التجارب التي دُرست بشكل مكثف، على سبيل المثال، يُتنبأ بـ **نفخة
هواء**</span> **(Air Puff)** **<span dir="rtl">للعين</span> (Eye)**
**<span dir="rtl">الأرنب</span> (Rabbit)** <span dir="rtl">بواسطة
**نغمة** </span>**(Tone CS)**<span dir="rtl">، مما يؤدي إلى **استجابة
غير مشروطة**</span> **(UR)** <span dir="rtl">تتكون من إغلاق **الغشاء
الداخلي الوقائي**</span> **<span dir="rtl">(</span>Nictitating
<span dir="rtl"></span>Membrane<span dir="rtl">)</span>**.
<span dir="rtl">بعد تجربة واحدة أو أكثر، تبدأ **النغمة**</span> **(Tone
CS)** <span dir="rtl">في إثارة **استجابة مشروطة** </span>**(CR)**
<span dir="rtl">تتكون من إغلاق **الغشاء**</span> **(Membrane)**
<span dir="rtl">قبل حدوث **نفخة الهواء**</span> **(Air Puff)**
<span dir="rtl">وتصبح في النهاية موقوتة بحيث يحدث **الإغلاق
الكامل**</span> **(Peak Closure)** <span dir="rtl">عندما يكون من المحتمل
أن يحدث **نفخة الهواء** </span>**(Air Puff)**<span dir="rtl">.</span>
<span dir="rtl">هذه **الاستجابة المشروطة**
</span>**(CR)**<span dir="rtl">، التي تُبادر **بتوقع**
</span>**(Anticipation)** **<span dir="rtl">نفخة الهواء</span> (Air
Puff)** <span dir="rtl">وتوقيتها المناسب، تقدم حماية أفضل من مجرد بدء
**الإغلاق** </span>**(Closure)** <span dir="rtl">كرد فعل **للمحفز غير
المشروط**</span> **(US)** <span dir="rtl">المزعج</span>.

<span dir="rtl">القدرة على التصرف بتوقع للأحداث المهمة من خلال تعلم
العلاقات التنبؤية بين **المحفزات**</span> **(Stimuli)**
<span dir="rtl">مفيدة جدًا لدرجة أنها موجودة على نطاق واسع عبر **المملكة
الحيوانية** </span>**(Animal Kingdom)**<span dir="rtl">.</span>

**<u>14.2.1 <span dir="rtl">الحجب والتكييف من الدرجة العليا</span>
(Blocking and Higher-order Conditioning)</u>**

<span dir="rtl">تمت ملاحظة العديد من الخصائص المثيرة للاهتمام **للتكييف
الكلاسيكي**</span> **<span dir="rtl">(</span>Classical
<span dir="rtl"></span>Conditioning<span dir="rtl">)
</span>**<span dir="rtl">في التجارب. بالإضافة إلى الطبيعة التوقعية
**للاستجابات المشروطة** </span>**(Conditioned Responses
(CRs))**<span dir="rtl">، فإن خاصيتين تم ملاحظتهما بشكل واسع وأساسي في
تطوير نماذج **التكييف الكلاسيكي**</span> **(Classical Conditioning
Models)** <span dir="rtl">هما **الحجب** </span>**(Blocking)**
<span dir="rtl">و**التكييف من الدرجة العليا** </span>**(Higher-order
Conditioning)**<span dir="rtl">.</span> <span dir="rtl">يحدث **الحجب**
</span>**(Blocking)** <span dir="rtl">عندما يفشل **الحيوان**</span>
**(Animal)** <span dir="rtl">في تعلم **استجابة مشروطة**</span> **(CR)**
<span dir="rtl">عند تقديم **محفز مشروط**</span> **(Conditioned Stimulus
(CS))** <span dir="rtl">محتمل مع **محفز مشروط**</span> **(CS)**
<span dir="rtl">آخر كان قد استُخدم سابقًا **لتكييف**</span>
**(Conditioning)** **<span dir="rtl">الحيوان</span> (Animal)**
<span dir="rtl">لإنتاج تلك **الاستجابة المشروطة**
</span>**(CR)**<span dir="rtl">.</span>

<span dir="rtl">على سبيل المثال، في المرحلة الأولى من تجربة
**الحجب**</span> **(Blocking Experiment)** <span dir="rtl">التي تشمل
**تكييف غشاء الأرنب النكتتي** </span>**(Rabbit Nictitating Membrane
Conditioning)**<span dir="rtl">، يتم أولاً **تكييف الأرنب**</span>
**(Rabbit Conditioned)** <span dir="rtl">مع **نغمة**</span> **(Tone
CS)** <span dir="rtl">و**نفخة هواء** </span>**(Air Puff US)**
<span dir="rtl">لإنتاج **استجابة مشروطة**</span> **(CR)**
<span dir="rtl">تتمثل في إغلاق **غشائه النكتتي**</span> **(Nictitating
Membrane)** <span dir="rtl">توقعًا **لنفخة الهواء** </span>**(Air
Puff)**<span dir="rtl">.</span> <span dir="rtl">تتكون المرحلة الثانية من
التجربة من تجارب إضافية يتم فيها إضافة **محفز ثانٍ** </span>**(Second
Stimulus)**<span dir="rtl">، مثل **ضوء**
</span>**(Light)**<span dir="rtl">، إلى **النغمة**</span> **(Tone)**
<span dir="rtl">لتشكيل **محفز مشروط مركب**</span> **(Compound Tone/Light
CS)** <span dir="rtl">متبوعًا بنفس **نفخة الهواء** </span>**(Air Puff
US)**<span dir="rtl">.</span> <span dir="rtl">في المرحلة الثالثة من
التجربة، يتم تقديم **المحفز الثاني**</span> **(Second Stimulus)**
<span dir="rtl">فقط، أي **الضوء** </span>**(Light)**<span dir="rtl">،
**للأرنب**</span> **(Rabbit)** <span dir="rtl">لمعرفة ما إذا كان
**الأرنب**</span> **(Rabbit)** <span dir="rtl">قد تعلم الاستجابة له
**باستجابة مشروطة** </span>**(CR)**<span dir="rtl">. يتبين أن
**الأرنب**</span> **(Rabbit)** <span dir="rtl">ينتج عددًا قليلاً جدًا، أو
لا ينتج أي **استجابات مشروطة** </span>**(CRs)** <span dir="rtl">استجابةً
**للضوء** </span>**(Light)**<span dir="rtl">:</span> <span dir="rtl">لقد
تم حجب **التعليم**</span> **(Learning)** **<span dir="rtl">لضوء</span>
(Light)** <span dir="rtl">بواسطة **التعليم السابق** </span>**(Previous
Learning)** **<span dir="rtl">للنغمة</span>
(Tone)**<span dir="rtl">.</span> <span dir="rtl">نتائج **الحجب**</span>
**(Blocking Results)** <span dir="rtl">مثل هذه تحدت الفكرة القائلة بأن
**التكييف**</span> **(Conditioning)** <span dir="rtl">يعتمد فقط على
**التجاور الزمني البسيط**</span> **<span dir="rtl">(</span>Simple
<span dir="rtl"></span>Temporal
Contiguity<span dir="rtl">)</span>**<span dir="rtl">، أي أن شرطًا ضروريًا
وكافيًا **للتكييف**</span> **(Conditioning)** <span dir="rtl">هو أن يتبع
**المحفز غير المشروط**</span> **(US)** **<span dir="rtl">المحفز
المشروط</span> (CS)** <span dir="rtl">بشكل متكرر وبفارق زمني ضئيل. في
القسم التالي، نصف **نموذج ريسكولا-فاغنر**</span> **(Rescorla–Wagner
Model)** <span dir="rtl">الذي قدم تفسيرًا مؤثرًا **للحجب**
</span>**(Blocking)**<span dir="rtl">.</span>

<span dir="rtl">يحدث **التكييف من الدرجة العليا**</span> **(Higher-order
Conditioning)** <span dir="rtl">عندما يعمل **محفز مشروط**</span>
**(CS)** <span dir="rtl">تم **تكييفه مسبقًا**</span> **(Previously
Conditioned)** <span dir="rtl">كـ **محفز غير مشروط**</span> **(US)**
<span dir="rtl">في **تكييف**</span> **(Conditioning)**
**<span dir="rtl">محفز</span> (Stimulus)** <span dir="rtl">آخر كان في
البداية **محايدًا** </span>**(Neutral)**<span dir="rtl">.</span>
<span dir="rtl">وصف **بافلوف**</span> **(Pavlov)** <span dir="rtl">تجربة
قام فيها مساعده أولاً **بتكييف كلب**</span> **(Conditioned a Dog)**
**<span dir="rtl">ليسيل لعابه</span> (Salivate)** <span dir="rtl">عند
سماع صوت **مترونوم**</span> **(Metronome)** <span dir="rtl">الذي كان
يتنبأ **بمحفز غير مشروط**</span> **(Food US)**<span dir="rtl">، كما تم
وصفه أعلاه. بعد هذه المرحلة من **التكييف**
</span>**(Conditioning)**<span dir="rtl">، أُجريت عدد من التجارب حيث تم
وضع **مربع أسود** </span>**(Black Square)**<span dir="rtl">، الذي كان
**الكلب**</span> **(Dog)** <span dir="rtl">في البداية **غير
مبالٍ**</span> **(Indifferent)** <span dir="rtl">به، في مجال رؤية
**الكلب**</span> **(Dog)** <span dir="rtl">متبوعًا بصوت **المترونوم**
</span>**(Metronome)**—<span dir="rtl">ولم يتبع ذلك **الطعام**
</span>**(Food)**<span dir="rtl">.</span> <span dir="rtl">في عشر تجارب
فقط، بدأ **الكلب**</span> **(Dog)** **<span dir="rtl">يسيل لعابه</span>
(Salivate)** <span dir="rtl">بمجرد رؤية **المربع الأسود**
</span>**(Black Square)**<span dir="rtl">، على الرغم من أن رؤيته لم تكن
متبوعة **بالطعام** </span>**(Food)**<span dir="rtl">.</span>

<span dir="rtl">عمل صوت **المترونوم**</span> **(Metronome)**
<span dir="rtl">نفسه **كمحفز غير مشروط**</span> **(US)**
<span dir="rtl">في **تكييف** </span>**(Conditioning)**
**<span dir="rtl">استجابة مشروطة</span> (CR)** **<span dir="rtl">للمربع
الأسود</span> (Black Square CS)**<span dir="rtl">.</span>
<span dir="rtl">كان هذا **تكييفًا من الدرجة الثانية**
</span>**(Second-order Conditioning)**<span dir="rtl">.</span>
<span dir="rtl">إذا كان **المربع الأسود**</span>
**<span dir="rtl">(</span>Black
<span dir="rtl"></span>Square<span dir="rtl">)
</span>**<span dir="rtl">قد استُخدم **كمحفز غير مشروط**</span> **(US)**
<span dir="rtl">لإقامة **استجابات مشروطة**</span> **(CRs)**
**<span dir="rtl">لمحفز مشروط</span> (CS)** <span dir="rtl">آخر كان في
البداية **محايدًا** </span>**(Neutral)**<span dir="rtl">، لكان هذا
**تكييفًا من الدرجة الثالثة**</span>
**<span dir="rtl">(</span>Third-order
<span dir="rtl"></span>Conditioning<span dir="rtl">)</span>**<span dir="rtl">،
وهكذا. من الصعب إثبات **التكييف من الدرجة العليا**</span>
**<span dir="rtl">(</span>Higher-order
<span dir="rtl"></span>Conditioning<span dir="rtl">)</span>**<span dir="rtl">،
خاصةً فوق **الدرجة الثانية** </span>**(Second Order)**<span dir="rtl">،
جزئيًا لأن **المعزز من الدرجة العليا**</span> **(Higher-order
Reinforcer)** <span dir="rtl">يفقد قيمته المعززة بسبب عدم تكرار **المحفز
غير المشروط الأصلي**</span> **(Original US)** <span dir="rtl">أثناء
تجارب **التكييف من الدرجة العليا**</span>
**<span dir="rtl">(</span>Higher-order
<span dir="rtl"></span>Conditioning Trials<span dir="rtl">).
</span>**<span dir="rtl">ولكن في ظل الظروف المناسبة، مثل مزج تجارب
**الدرجة الأولى  
(**</span>**First-order Trials<span dir="rtl">)
</span>**<span dir="rtl">مع تجارب **الدرجة العليا**</span>
**(Higher-order Trials)** <span dir="rtl">أو عن طريق توفير **محفز منشط
عام** </span>**(General Energizing Stimulus)**<span dir="rtl">، يمكن
إثبات **التكييف من الدرجة العليا** </span>**(Higher-order
Conditioning)** <span dir="rtl">بما يتجاوز **الدرجة الثانية**
</span>**(Second Order)**<span dir="rtl">.</span>

<span dir="rtl">كما يحدث **التكييف الآلي من الدرجة العليا**
</span>**(Higher-order Instrumental Conditioning)**
<span dir="rtl">أيضًا. في هذه الحالة، يصبح **المحفز**</span>
**(Stimulus)** <span dir="rtl">الذي يتنبأ باستمرار **بالتعزيز
الأولي**</span> **<span dir="rtl">(</span>Primary
<span dir="rtl"></span>Reinforcement<span dir="rtl">) معززًا</span>
(Reinforcer)** <span dir="rtl">بحد ذاته، حيث يكون **التعزيز
أوليًا**</span> **<span dir="rtl">(</span>Primary
<span dir="rtl"></span>Reinforcement<span dir="rtl">)
</span>**<span dir="rtl">إذا كانت **جودته المكافئة أو العقابية**</span>
**<span dir="rtl">(</span>Rewarding or Penalizing
<span dir="rtl"></span>Quality<span dir="rtl">)</span>**
<span dir="rtl">قد تم بناؤها في **الحيوان**</span> **(Animal)**
<span dir="rtl">بواسطة **التطور**
</span>**(Evolution)**<span dir="rtl">.</span> <span dir="rtl">يصبح
**المحفز التنبؤي**</span> **(Predicting Stimulus)**
**<span dir="rtl">معززًا ثانويًا</span> (Secondary
Reinforcer)**<span dir="rtl">، أو بشكل أعم، **معززًا من الدرجة
العليا**</span> **(Higher-order Reinforcer)** <span dir="rtl">أو **معززًا
مشروطًا**</span> **<span dir="rtl">(</span>Conditioned
<span dir="rtl"></span>Reinforcer<span dir="rtl">)</span>**
—<span dir="rtl">يُعد المصطلح الأخير أفضل عندما يكون **المحفز المعزز
المتوقع**</span> **<span dir="rtl">(</span>Predicted
<span dir="rtl"></span>Reinforcing Stimulus<span dir="rtl">)
</span>**<span dir="rtl">هو نفسه **معززًا ثانويًا**</span> **(Secondary)**
<span dir="rtl">أو حتى **من الدرجة العليا**
</span>**(Higher-order)**<span dir="rtl">. يوفر **المعزز
المشروط**</span> **(Conditioned Reinforcer)** **<span dir="rtl">تعزيزًا
مشروطًا</span> (Conditioned Reinforcement)**<span dir="rtl">: **مكافأة
مشروطة**</span> **(Conditioned Reward)** <span dir="rtl">أو **عقوبة
مشروطة** </span>**(Conditioned Penalty)**<span dir="rtl">.</span>
<span dir="rtl">يعمل **التعزيز المشروط**</span>
**<span dir="rtl">(</span>Conditioned
<span dir="rtl"></span>Reinforcement<span dir="rtl">)
</span>**<span dir="rtl">مثل **التعزيز الأولي**</span> **(Primary
Reinforcement)** <span dir="rtl">في زيادة ميل **الحيوان**
</span>**(Animal)** <span dir="rtl">لإنتاج **سلوك**</span>
**(Behavior)** <span dir="rtl">يؤدي إلى **مكافأة مشروطة**
</span>**(Conditioned Reward)**<span dir="rtl">، وتقليل ميل
**الحيوان**</span> **(Animal)** <span dir="rtl">لإنتاج **سلوك**</span>
**(Behavior)** <span dir="rtl">يؤدي إلى **عقوبة مشروطة**
</span>**(Conditioned Penalty)**<span dir="rtl">.</span>

<span dir="rtl">يُعد **التعزيز المشروط**</span> **(Conditioned
Reinforcement)** <span dir="rtl">ظاهرة رئيسية تشرح، على سبيل المثال،
لماذا نعمل من أجل **المعزز المشروط**</span> **(Conditioned Reinforcer)**
**<span dir="rtl">المال</span> (Money)**<span dir="rtl">، الذي تستمد
قيمته فقط مما يتنبأ به وجوده. في **أساليب الممثل-الناقد**</span>
**(Actor–Critic Methods)** <span dir="rtl">الموصوفة في **الفصل
13.5**</span> **(Section 13.5)** (<span dir="rtl">والمناقشة في سياق
**علم الأعصاب**</span> **(Neuroscience)** <span dir="rtl">في **الفصلين
15.7 و15.8**، يستخدم **الناقد**</span> **(Critic)**
**<span dir="rtl">طريقة</span> TD <span dir="rtl"></span>(TD Method)**
<span dir="rtl">لتقييم **سياسة** </span>**(Policy)**
**<span dir="rtl">الممثل</span> (Actor)**<span dir="rtl">، وتوفر
**تقديرات القيمة**</span> **<span dir="rtl">(</span>Value
<span dir="rtl"></span>Estimates<span dir="rtl">) تعزيزًا مشروطًا</span>
(Conditioned Reinforcement)** **<span dir="rtl">للممثل</span>
(Actor)**<span dir="rtl">، مما يسمح **للممثل**</span> **(Actor)**
<span dir="rtl">بتحسين **سياساته**
</span>**(Policies)**<span dir="rtl">. يساعد هذا النظير **للتكييف الآلي
من الدرجة العليا (**</span>**Higher-order
<span dir="rtl"></span>Instrumental
Conditioning<span dir="rtl">)</span>** <span dir="rtl">في معالجة **مشكلة
تخصيص الفضل (**</span>**Credit-assignment
<span dir="rtl"></span>Problem<span dir="rtl">)</span>**
<span dir="rtl">المذكورة في **الفصل 1.7**</span> <span dir="rtl">لأن
**الناقد** </span>**(Critic)** <span dir="rtl">يقدم **تعزيزًا
لحظيًا**</span> **<span dir="rtl">(</span>Moment-by-moment
Reinforcement<span dir="rtl">) للممثل</span> (Actor)**
<span dir="rtl">عندما يكون **إشارة المكافأة الأولية**</span>
**<span dir="rtl">(</span>Primary <span dir="rtl"></span>Reward
Signal<span dir="rtl">) </span>**<span dir="rtl">مؤجلة. سنناقش هذا بمزيد
من التفصيل أدناه في **الفصل 14.4.**</span>

**<u>14.2.2 <span dir="rtl">نموذج ريسكولا-فاغنر</span> (The
Rescorla–Wagner Model)</u>**

<span dir="rtl">قام **ريسقولا وفاغنر**</span> **(Rescorla and Wagner)**
<span dir="rtl">بإنشاء **نموذجهم**</span> **(Model)**
<span dir="rtl">بشكل أساسي لتفسير **الحجب**
</span>**(Blocking)**<span dir="rtl">.</span> <span dir="rtl">الفكرة
الأساسية في **نموذج ريسقولا-فاغنر** </span>**(Rescorla–Wagner Model)**
<span dir="rtl">هي أن **الحيوان**</span> **(Animal)**
<span dir="rtl">يتعلم فقط عندما تنتهك الأحداث توقعاته، بمعنى آخر، فقط
عندما يتفاجأ **الحيوان**</span> **(Animal)** <span dir="rtl">(دون
الإشارة بالضرورة إلى أي توقع أو عاطفة واعية). سنقدم أولاً **نموذج ريسقولا
وفاغنر**</span> **(Rescorla and Wagner Model)** <span dir="rtl">باستخدام
**المصطلحات**</span> **(Terminology)** <span dir="rtl">و**الرموز**
</span>**(Notation)** <span dir="rtl">الخاصة بهم قبل الانتقال إلى
**المصطلحات**</span> **(Terminology)** <span dir="rtl">و**الرموز**
</span>**(Notation)** <span dir="rtl">التي نستخدمها لوصف</span> **TD
<span dir="rtl">نموذج</span> (TD Model)**<span dir="rtl">.</span>

<span dir="rtl">إليك كيفية وصف **ريسقولا وفاغنر**</span> **(Rescorla and
Wagner)** **<span dir="rtl">لنموذجهم</span>
(Model)**<span dir="rtl">.</span> <span dir="rtl">يقوم
**النموذج**</span> **(Model)** <span dir="rtl">بتعديل "قوة
الترابط</span> (Associative Strength)" <span dir="rtl">لكل **محفز مكون**
</span>**(Component Stimulus)** <span dir="rtl">من **محفز مركب**
</span>**(Compound CS)**<span dir="rtl">، وهو رقم يمثل مدى قوة أو
موثوقية توقع هذا **المحفز المكون**</span> **(Component Stimulus)**
<span dir="rtl">لحدوث **محفز غير مشروط** </span>**(Unconditioned
Stimulus (US))**<span dir="rtl">.</span> <span dir="rtl">عندما يتم تقديم
**محفز مركب**</span> **(Compound CS)** <span dir="rtl">مكون من عدة
**محفزات مكونات**</span> **(Component Stimuli)** <span dir="rtl">في
تجربة **التكييف الكلاسيكي  
(**</span>**Classical Conditioning
Trial<span dir="rtl">)</span>**<span dir="rtl">، تتغير **قوة
الترابط**</span> **(Associative Strength)** <span dir="rtl">لكل **محفز
مكون**</span> **(Component Stimulus)** <span dir="rtl">بطريقة تعتمد على
**قوة الترابط الإجمالية**</span> **<span dir="rtl">(</span>Aggregate
<span dir="rtl"></span>Associative Strength<span dir="rtl">)
</span>**<span dir="rtl">المرتبطة بكل **المحفز المركب**
</span>**(Stimulus Compound)**<span dir="rtl">، وليس فقط على **قوة
الترابط**</span> **(Associative Strength)** <span dir="rtl">لكل
**مكون**</span> **(Component)** <span dir="rtl">نفسه</span>.

<span dir="rtl">قام **ريسقولا وفاغنر**</span> **(Rescorla and Wagner)**
<span dir="rtl">بدراسة **محفز مركب**</span> **(Compound CS)**
<span dir="rtl">مكون من **المحفزين**</span> **A
<span dir="rtl">و</span>X (Stimuli A and X)**<span dir="rtl">، حيث قد
يكون **الحيوان**</span> **(Animal)** <span dir="rtl">قد مر بالفعل بتجربة
**المحفز** </span>**A (Stimulus A)**<span dir="rtl">، وقد يكون
**المحفز**</span> **X (Stimulus X)** <span dir="rtl">جديدًا **للحيوان**
</span>**(Animal)**<span dir="rtl">.</span> <span dir="rtl">دع</span>
VA​<span dir="rtl">، و</span>VX​<span dir="rtl">، و</span>VAX
<span dir="rtl"></span>​ <span dir="rtl">تمثل على التوالي **قوى
الترابط**</span> **(Associative Strengths)**
<span dir="rtl">للمحفزين</span> A
<span dir="rtl">و</span>X<span dir="rtl">، والمحفز المركب</span>
AX<span dir="rtl">.</span> <span dir="rtl">لنفترض أنه في تجربة ما، تم
تقديم **المحفز المركب** </span>**AX <span dir="rtl">(</span>Compound CS
AX<span dir="rtl">)</span>** <span dir="rtl">متبوعًا **بمحفز غير مشروط**
</span>**(US)**<span dir="rtl">، والذي نسميه</span> **Y
<span dir="rtl">المحفز</span> (Stimulus Y)**<span dir="rtl">.</span>
<span dir="rtl">إذن، تتغير **قوى الترابط**</span> **(Associative
Strengths)** <span dir="rtl">لمكونات **المحفز**</span> **(Stimulus
Components)** <span dir="rtl">وفقًا لهذه التعبيرات</span>:

``` math
\nabla V_{A} = \alpha_{A} \cdot \beta_{Y}\left( R_{Y} - V_{AX} \right)
```

``` math
\nabla V_{X} = \alpha_{X} \cdot \beta_{Y}\left( R_{Y} - V_{AX} \right),
```

<span dir="rtl">حيث أن</span> \alpha\_{A&Y} <span dir="rtl">و</span>
\alpha\_{X&Y} <span dir="rtl">هما **بارامترات حجم الخطوة**</span>
**<span dir="rtl">(</span>Step-size
<span dir="rtl"></span>Parameters)**<span dir="rtl">، التي تعتمد على
هويات **مكونات المحفز المشروط**</span> **(CS Components)**
<span dir="rtl">و**المحفز غير المشروط** </span>**(US)**<span dir="rtl">،
و</span>$`RY`$ <span dir="rtl">هو المستوى النهائي **لقوة
الترابط**</span> **(Associative Strength)** <span dir="rtl">التي يمكن أن
يدعمها **المحفز غير المشروط** </span>**(US Y)**<span dir="rtl">.</span>
<span dir="rtl">(استخدم **ريسقولا وفاغنر**</span>
**<span dir="rtl">(</span>Rescorla and
<span dir="rtl"></span>Wagner<span dir="rtl">)
</span>**<span dir="rtl">الرمز " بدلاً من</span> $`R`$<span dir="rtl">،
لكننا نستخدم</span> $`R`$ <span dir="rtl">لتجنب الالتباس مع استخدامنا
للرمز " ولأننا نفكر عادة في هذا على أنه مقدار **إشارة المكافأة**
</span>**(Reward Signal)**<span dir="rtl">، مع التنبيه إلى أن **المحفز
غير المشروط**</span> **(US)** <span dir="rtl">في **التكييف
الكلاسيكي**</span> **(Classical Conditioning)** <span dir="rtl">ليس
بالضرورة مكافئًا أو عقابيًا)</span>.

<span dir="rtl">افتراض أساسي في **النموذج**</span> **(Model)**
<span dir="rtl">هو أن **قوة الترابط الإجمالية**</span>
**<span dir="rtl">(</span>Aggregate Associative
<span dir="rtl"></span>Strength<span dir="rtl">)</span>**
<span dir="rtl"></span>$`VA`$ <span dir="rtl">تساوي</span>
$`VA + VX`$​<span dir="rtl">.</span> <span dir="rtl">تصبح **قوى
الترابط**</span> **(Associative Strengths)** <span dir="rtl">كما تغيرت
بهذه النسب هي **قوى الترابط**</span> **(Associative Strengths)**
<span dir="rtl">في بداية التجربة التالية</span>.

<span dir="rtl">لكي يكون النموذج كاملاً، يحتاج إلى آلية **لتوليد
الاستجابة**</span> **<span dir="rtl">(</span>Response-generation
<span dir="rtl"></span>Mechanism<span dir="rtl">)</span>**<span dir="rtl">،
وهي طريقة لرسم القيم من</span> $`Vs`$ <span dir="rtl"></span>​
<span dir="rtl">إلى **الاستجابات المشروطة**
</span>**(CRs)**<span dir="rtl">.</span> <span dir="rtl">لأن هذه الخريطة
تعتمد على تفاصيل الحالة التجريبية، لم يحدد **ريسقولا وفاغنر**
</span>**(Rescorla and Wagner)** <span dir="rtl">خريطة محددة ولكنهم
افترضوا ببساطة أن القيم الأكبر من</span> $`Vs`$​ <span dir="rtl">ستنتج
**استجابات مشروطة**</span> **(CRs)** <span dir="rtl">أقوى أو أكثر
احتمالًا، وأن القيم السالبة من</span> Vs <span dir="rtl">تعني أنه لن يكون
هناك **استجابات مشروطة** </span>**(CRs)**<span dir="rtl">.</span>

<span dir="rtl">يفسر **نموذج ريسقولا-فاغنر**</span> **(Rescorla–Wagner
Model)** <span dir="rtl">اكتساب **الاستجابات المشروطة** </span>**(CRs)**
<span dir="rtl">بطريقة تشرح **الحجب**
</span>**(Blocking)**<span dir="rtl">.</span> <span dir="rtl">طالما أن
**قوة الترابط الإجمالية**</span> **<span dir="rtl">(</span>Aggregate
<span dir="rtl"></span>Associative
Strength<span dir="rtl">)</span>**<span dir="rtl">،</span>
$`VAXV`$​<span dir="rtl">، **للمحفز المركب**</span> **(Stimulus
Compound)** <span dir="rtl">أقل من المستوى النهائي **لقوة الترابط**
</span>**(Associative Strength)**<span dir="rtl">،</span>
$`RY`$​<span dir="rtl">، الذي يمكن أن يدعمه **المحفز غير المشروط**
</span>**(US Y)**<span dir="rtl">، فإن **خطأ التنبؤ**
</span>**(Prediction Error)** $`RY - VAX`$​ <span dir="rtl">يكون موجبًا.
هذا يعني أنه على مدى التجارب المتتالية، تزداد **قوى الترابط**</span>
**(Associative Strengths)** $`VAV\_ AVA`$​
<span dir="rtl">و</span>$`VXV`$ **<span dir="rtl">لمكونات المحفز</span>
(Stimulus Components)** <span dir="rtl">حتى تساوي **قوة الترابط
الإجمالية**</span> **<span dir="rtl">(</span>Aggregate
<span dir="rtl"></span>Associative Strength<span dir="rtl">)</span>**
$`VAXV`$ **<span dir="rtl">قوة الترابط النهائية</span>
(**$`\mathbf{RY}`$**)**<span dir="rtl">، عند هذه النقطة تتوقف **قوى
الترابط** </span>**(Associative Strengths)** <span dir="rtl">عن التغير
(ما لم يتغير **المحفز غير المشروط**
</span>**(US)**<span dir="rtl">)</span>. <span dir="rtl">عندما يتم إضافة
**مكون جديد**</span> **(New Component)** <span dir="rtl">إلى **محفز
مركب**</span> **(Compound CS)** <span dir="rtl">الذي تم **تكييف
الحيوان**</span> **(Animal Conditioned)** <span dir="rtl">عليه بالفعل،
فإن **التكييف الإضافي**</span> **<span dir="rtl">(</span>Further
<span dir="rtl"></span>Conditioning<span dir="rtl">)
</span>**<span dir="rtl">مع **المركب المحسن**</span> **(Augmented
Compound)** <span dir="rtl">ينتج عنه زيادة قليلة أو معدومة **في قوة
الترابط**</span> **(Associative Strength)** **<span dir="rtl">لمكون
المحفز المضاف</span> <span dir="rtl">(</span>Added CS
<span dir="rtl"></span>Component<span dir="rtl">)
</span>**<span dir="rtl">لأن **الخطأ**</span> **(Error)**
<span dir="rtl">قد تم تقليله بالفعل إلى الصفر، أو إلى قيمة منخفضة. يتم
بالفعل التنبؤ بحدوث **المحفز غير المشروط**</span> **(US)**
<span dir="rtl">بشكل شبه كامل، لذلك يتم تقديم خطأ قليل جدًا أو معدوم - أو
مفاجأة - من قبل **المحفز الجديد** </span>**(New CS
Component)**<span dir="rtl">.</span> <span dir="rtl">يحجب **التعليم
السابق**</span> **<span dir="rtl">(</span>Prior
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**
<span dir="rtl">التعليم **للمكون الجديد** </span>**(New
Component)**<span dir="rtl">.</span>

<span dir="rtl">للانتقال من **نموذج ريسقولا وفاغنر**</span> **(Rescorla
and Wagner Model)** <span dir="rtl">إلى **نموذج**</span> **TD
<span dir="rtl">للتكييف الكلاسيكي</span> (TD Model of Classical
Conditioning)** <span dir="rtl">(الذي نسميه فقط **نموذج** </span>**TD
(TD Model)**<span dir="rtl">)، نعيد أولاً صياغة **نموذجهم**</span>
**(Model)** <span dir="rtl">من حيث **المفاهيم**</span> **(Concepts)**
<span dir="rtl">التي نستخدمها في جميع أنحاء هذا الكتاب. بشكل محدد، نطابق
**الترميز**</span> **(Notation)** <span dir="rtl">الذي نستخدمه **للتعليم
مع التقريب الخطي**</span> **(Learning with Linear Function
Approximation)** <span dir="rtl">(الفصل 9.4)، ونفكر في عملية
**التكييف**</span> **(Conditioning Process)** <span dir="rtl">كواحدة من
تعلم التنبؤ</span> **"<span dir="rtl">بمقدار المحفز غير المشروط</span>
(Magnitude of the US)"** <span dir="rtl">في تجربة بناءً على **المحفز
المركب** </span>**(Compound CS)** <span dir="rtl">المقدم في تلك التجربة،
حيث يكون **مقدار المحفز غير المشروط**</span> **(US Y)**
<span dir="rtl">هو</span> $`RY`$ <span dir="rtl"></span>​
**<span dir="rtl">لنموذج ريسقولا-فاغنر</span> (Rescorla–Wagner Model)**
<span dir="rtl">كما هو موضح أعلاه. نقدم أيضًا **الحالات**
</span>**(States)**<span dir="rtl">.</span> <span dir="rtl">نظرًا لأن
**نموذج ريسقولا-فاغنر**</span> **(Rescorla–Wagner Model)**
<span dir="rtl">هو **نموذج على مستوى التجربة** </span>**(Trial-level
Model)**<span dir="rtl">، مما يعني أنه يتعامل مع كيفية تغير **قوى
الترابط**</span> **<span dir="rtl">(</span>Associative
<span dir="rtl"></span>Strengths<span dir="rtl">)
</span>**<span dir="rtl">من تجربة إلى أخرى دون النظر إلى أي تفاصيل حول
ما يحدث داخل وبين التجارب، فنحن لا نحتاج إلى النظر في كيفية تغير
**الحالات**</span> **(States)** <span dir="rtl">أثناء تجربة حتى نقدم
**نموذج**</span> **TD <span dir="rtl">الكامل</span> (Full TD Model)**
<span dir="rtl">في القسم التالي. بدلاً من ذلك، هنا نفكر ببساطة في
**الحالة**</span> **(State)** <span dir="rtl">كطريقة لوصف تجربة من حيث
مجموعة **مكونات المحفز المشروط**</span> **(Component CSs)**
<span dir="rtl">التي تكون موجودة في التجربة</span>.

<span dir="rtl">إذن، افترض أن نوع التجربة، أو الحالة</span>
(state)<span dir="rtl">،</span> s <span dir="rtl">يتم وصفه بواسطة متجه
قيم حقيقية من الخصائص</span>
$`x\ (s) = (x1(s)،x2(s),\ldots,xd(s))`$T<span dir="rtl">، حيث</span>
$`xi(s) = 1`$ <span dir="rtl">إذا كان</span> $`CSi`$​<span dir="rtl">،
وهو المكون</span> $`i`$ <span dir="rtl">من</span> CS
<span dir="rtl">المركب</span> (compound CS)<span dir="rtl">، موجودًا في
التجربة و0 خلاف ذلك. ثم، إذا كان المتجه ذو الأبعاد</span> $`d`$
<span dir="rtl">للقوى الترابطية</span> (associative strengths)
<span dir="rtl">هو</span> $`w`$<span dir="rtl">، فإن القوة الترابطية
الإجمالية لنوع التجربة</span> $`s`$ <span dir="rtl">هي</span>:

``` math
\widehat{v}(s,w) = w^{\top}x(s)
```

<span dir="rtl">يتوافق هذا مع تقدير القيمة في **التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">، ونفكر فيه كتنبؤ
المحفز غير المشروط</span> **(Unconditioned Stimulus
prediction)**<span dir="rtl">.</span> <span dir="rtl">الآن، دعنا
نستخدم</span> t <span dir="rtl">مؤقتًا للإشارة إلى رقم التجربة الكاملة
وليس إلى معناها المعتاد كخطوة زمنية (سنعود إلى المعنى المعتاد لـ</span>
$`t`$ <span dir="rtl">عندما نوسع هذا ليشمل نموذج **التفاضل
الزمني**</span> **(TD model)** <span dir="rtl">أدناه)، ونفترض أن</span>
$`\mathbf{St}`$ <span dir="rtl">هي الحالة المقابلة للتجربة</span>
t<span dir="rtl">. يؤدي تكييف التجربة</span> $`t`$ <span dir="rtl">إلى
تحديث **متجه القوة الترابطية**</span>
**<span dir="rtl">(</span>Associative <span dir="rtl"></span>Strength
Vector<span dir="rtl">)</span>** $`\mathbf{wt\ }`$
<span dir="rtl">إلى</span> $`\mathbf{wt + 1\ }`$**​** <span dir="rtl">على
النحو التالي</span>:

``` math
w_{t + 1} = w_{t} + \alpha\delta_{t}x\left( S_{t} \right)
```

<span dir="rtl">حيث أن</span> $`\alpha`$ <span dir="rtl">هو **بارامتر
حجم الخطوة** </span>**(Step-size parameter)**<span dir="rtl">، ولأننا
هنا نصف نموذج  
**ريسكورلا-فاجنر** </span>**(Rescorla-Wagner model)**<span dir="rtl">،
فإن</span> $`\delta t`$ <span dir="rtl">هو **خطأ التنبؤ**</span>
**<span dir="rtl">(</span>Prediction
<span dir="rtl"></span>Error<span dir="rtl">)</span>**.

``` math
\delta_{t} = R_{t} - \widehat{v}\left( S_{t},w_{t} \right)
```

<span dir="rtl">يتوافق</span> $`Rt`$ <span dir="rtl">مع الهدف من
**التنبؤ**</span> **(Prediction)** <span dir="rtl">في **التجربة**
</span>**(Trial)** $`t`$<span dir="rtl">، أي أنه يمثل مقدار **المحفز غير
المشروط** </span>**(Unconditioned Stimulus (US))**<span dir="rtl">، أو
بمعنى آخر، القوة الترابطية التي يمكن أن يدعمها **المحفز غير
المشروط**</span> **(Unconditioned Stimulus (US))** <span dir="rtl">في
التجربة. لاحظ أنه بسبب العامل</span> $`x(St)`$ <span dir="rtl">في
المعادلة (14.2)، يتم تعديل فقط القوى الترابطية لمكونات **المحفز الشرطي**
</span>**(Conditioned Stimulus (CS))** <span dir="rtl">الموجودة في
التجربة نتيجة لهذه التجربة. يمكن اعتبار  
**خطأ التنبؤ**</span> **(Prediction Error)** <span dir="rtl">كمقياس
للمفاجأة، والقوة الترابطية الإجمالية كتوقع **الحيوان**
</span>**(Animal)** <span dir="rtl">الذي يُخيب عندما لا يتطابق مع مقدار
الهدف **المحفز غير المشروط**</span>
**<span dir="rtl">(</span>Unconditioned <span dir="rtl"></span>Stimulus
(US)<span dir="rtl">)</span>**.

<span dir="rtl">من منظور **تعلم الآلة** </span>**(Machine
Learning)**<span dir="rtl">، يعد نموذج **ريسكورلا-فاجنر**</span>
**<span dir="rtl">(</span>Rescorla-Wagner
model<span dir="rtl">)</span>** <span dir="rtl">قاعدة تعليمية تحت
الإشراف لتصحيح الخطأ. فهو في الأساس نفس قاعدة التعليم **أقل متوسط مربع**
</span>**(Least Mean Square (LMS))**<span dir="rtl">، أو قاعدة</span>
**Widrow-Hoff** <span dir="rtl">للتعليم</span> (Widrow and Hoff, 1960)
<span dir="rtl">التي تجد الأوزان—هنا تمثل **القوى الترابطية**</span>
**<span dir="rtl">(</span>Associative
<span dir="rtl"></span>Strengths<span dir="rtl">)</span>**
—<span dir="rtl">التي تجعل متوسط مربعات جميع **الأخطاء**</span>
**(Errors)** <span dir="rtl">قريبًا من الصفر قدر الإمكان. هو عبارة عن
**خوارزمية**</span> **(Algorithm)** "<span dir="rtl">توافق
المنحنيات</span> (Curve-fitting)" <span dir="rtl">أو **الانحدار**
</span>**(Regression)** <span dir="rtl">التي تُستخدم على نطاق واسع في
**الهندسة**</span> **(Engineering)** <span dir="rtl">و**التطبيقات
العلمية** </span>**(Scientific Applications)** <span dir="rtl">(انظر
**الفصل**</span> 9.4<span dir="rtl">)</span>.

<span dir="rtl">كان نموذج **ريسكورلا-فاجنر**</span> **(Rescorla-Wagner
model)** <span dir="rtl">ذا تأثير كبير في تاريخ نظرية **تعلم
الحيوان**</span> **(Animal Learning Theory)** <span dir="rtl">لأنه أظهر
أن النظرية "الميكانيكية" يمكن أن تفسر الحقائق الأساسية حول
**التقييد**</span> **(Blocking)** <span dir="rtl">دون اللجوء إلى نظريات
معرفية أكثر تعقيدًا تتضمن، على سبيل المثال، إدراك **الحيوان**</span>
**(Animal)** <span dir="rtl">الصريح أن هناك مكون **محفز**</span>
**(Stimulus Component)** <span dir="rtl">آخر قد تم إضافته، ومن ثم مسح
**ذاكرته قصيرة الأمد**</span> **(Short-term Memory)**
<span dir="rtl">للخلف لإعادة تقييم العلاقات التنبؤية المتعلقة بـ
**المحفز غير المشروط** </span>**(Unconditioned Stimulus
(US))**<span dir="rtl">.</span> <span dir="rtl">أظهر نموذج
**ريسكورلا-فاجنر**</span> **(Rescorla-Wagner model)**
<span dir="rtl">كيف يمكن تعديل النظريات التقليدية حول تزامن **المحفزات**
</span>**(Stimuli)**<span dir="rtl">، والتي كانت تعتبر أن التزامن الزمني
للمحفزات كان شرطًا ضروريًا وكافيًا للتعليم، بطريقة بسيطة لتفسير **التقييد**
</span>**(Blocking) <span dir="rtl"></span>**(Moore and Schmajuk,
2008)<span dir="rtl">.</span>

<span dir="rtl">يوفر نموذج **ريسكورلا-فاجنر**</span> **(Rescorla-Wagner
model)** <span dir="rtl">تفسيرًا بسيطًا للتقييد وبعض الميزات الأخرى من
**التكييف الكلاسيكي** </span>**(Classical
Conditioning)**<span dir="rtl">، ولكنه ليس نموذجًا كاملاً أو مثاليًا
للتكييف الكلاسيكي. أفكار مختلفة تفسر مجموعة متنوعة من التأثيرات الأخرى
المرصودة، ولا يزال التقدم جارياً نحو فهم العديد من التفاصيل الدقيقة
للتكييف الكلاسيكي</span>. **<span dir="rtl">نموذج التفاضل الزمني</span>
<span dir="rtl">(</span>Temporal <span dir="rtl"></span>Difference (TD)
model<span dir="rtl">)</span>**<span dir="rtl">، الذي سنصفه بعد ذلك، ليس
أيضًا نموذجًا كاملاً أو مثاليًا للتكييف الكلاسيكي، لكنه يوسع نموذج
**ريسكورلا-فاجنر**</span> **(Rescorla-Wagner model)**
<span dir="rtl">لمعالجة كيفية تأثير العلاقات الزمنية بين
**المحفزات**</span> **(Stimuli)** <span dir="rtl">داخل وخارج التجارب على
**التعليم** </span>**(Learning)** <span dir="rtl">وكيف يمكن أن ينشأ
**التكييف من الدرجة العليا** </span>**(Higher-order
Conditioning)**<span dir="rtl">.</span>

**<u>14.2.3 <span dir="rtl">نموذج التفاضل الزمني</span> (The TD
Model)</u>**

<span dir="rtl">نموذج **التفاضل الزمني**</span> **(Temporal Difference
(TD) Model)** <span dir="rtl">هو نموذج يعمل في **الوقت الحقيقي**
</span>**(Real-time)**<span dir="rtl">، على عكس نموذج **ريسكورلا-فاجنر**
</span>**(Rescorla-Wagner Model)** <span dir="rtl">الذي يعتمد على مستوى
**التجربة** </span>**(Trial-level)**<span dir="rtl">.</span>
<span dir="rtl">تمثل خطوة واحدة</span> $`t`$ <span dir="rtl">في نموذج
**ريسكورلا-فاجنر** </span>**(Rescorla-Wagner Model)**
**<span dir="rtl">تجربة تكييف كاملة</span> (Complete Conditioning
Trial)**<span dir="rtl">.</span> <span dir="rtl">لا يطبق هذا النموذج على
التفاصيل المتعلقة بما يحدث خلال الفترة الزمنية التي تجري فيها
**التجربة** </span>**(Trial)**<span dir="rtl">، أو ما قد يحدث بين
**التجارب** </span>**(Trials)**<span dir="rtl">.</span>
<span dir="rtl">داخل كل **تجربة** </span>**(Trial)**<span dir="rtl">، قد
يواجه **الحيوان** </span>**(Animal)** <span dir="rtl">محفزات متنوعة تحدث
في أوقات محددة وتستمر لفترات معينة. تؤثر هذه العلاقات الزمنية بشكل كبير
على **التعليم** </span>**(Learning)**<span dir="rtl">.</span>
<span dir="rtl">كما أن نموذج **ريسكورلا-فاجنر
(**</span>**Rescorla-Wagner
<span dir="rtl"></span>Model<span dir="rtl">)
</span>**<span dir="rtl">لا يتضمن آلية **التكييف من الدرجة العليا**
</span>**(Higher-order Conditioning)**<span dir="rtl">، بينما في نموذج
**التفاضل الزمني** </span>**(TD Model)**<span dir="rtl">، يكون **التكييف
من الدرجة العليا (**</span>**Higher-order
<span dir="rtl"></span>Conditioning<span dir="rtl">)
</span>**<span dir="rtl">نتيجة طبيعية لفكرة **التعزيز الذاتي**</span>
**(Bootstrapping)** <span dir="rtl">التي تقوم عليها **خوارزميات**</span>
**(Algorithms)** <span dir="rtl">التفاضل الزمني</span>
(TD)<span dir="rtl">.</span>

<span dir="rtl">لوصف نموذج **التفاضل الزمني** </span>**(TD
Model)**<span dir="rtl">، نبدأ بصياغة نموذج **ريسكورلا-فاجنر**</span>
**<span dir="rtl">(</span>Rescorla-Wagner
Model<span dir="rtl">)</span>** <span dir="rtl">أعلاه، لكن هنا</span>
$`t`$ <span dir="rtl">تشير إلى **الخطوات الزمنية**</span> **(Time
Steps)** <span dir="rtl">داخل أو بين **التجارب**</span> **(Trials)**
<span dir="rtl">بدلاً من **التجارب الكاملة** </span>**(Complete
Trials)**<span dir="rtl">.</span> <span dir="rtl">فكر في الوقت
بين</span> $`t`$ <span dir="rtl">و</span>$`t + 1`$ <span dir="rtl">كفترة
زمنية صغيرة، لنقل 0.01 ثانية، وفكر في **التجربة**</span> **(Trial)**
<span dir="rtl">كسلسلة من **الحالات**
</span>**(States)**<span dir="rtl">، ترتبط كل منها بخطوة زمنية، حيث تمثل
**الحالة**</span> **(State)** <span dir="rtl">عند الخطوة</span> $`t`$
<span dir="rtl">تفاصيل كيفية تمثيل **المحفزات** </span>**(Stimuli)**
<span dir="rtl">في الوقت</span> $`t`$ <span dir="rtl">بدلاً من مجرد تسمية
لمكونات **المحفز الشرطي**</span> **<span dir="rtl">(</span>Conditioned
<span dir="rtl"></span>Stimulus (CS)<span dir="rtl">)</span>**
<span dir="rtl">الموجودة في **التجربة**
</span>**(Trial)**<span dir="rtl">.</span> <span dir="rtl">في الواقع،
يمكننا تمامًا التخلي عن فكرة **التجارب**
</span>**(Trials)**<span dir="rtl">.</span> <span dir="rtl">من وجهة نظر
**الحيوان** </span>**(Animal)**<span dir="rtl">، **التجربة**</span>
**(Trial)** <span dir="rtl">هي مجرد جزء من تجربته المستمرة في التفاعل مع
**بيئته** </span>**(Environment)**<span dir="rtl">.</span>
<span dir="rtl">وفقًا لنظرتنا المعتادة إلى **العميل**</span> **(Agent)**
<span dir="rtl">الذي يتفاعل مع **بيئته**
</span>**(Environment)**<span dir="rtl">، تخيل أن **الحيوان**</span>
**(Animal)** <span dir="rtl">يمر بتسلسل لا نهائي من **الحالات**
</span>**(States)**<span dir="rtl">، كل منها ممثل بـ **متجه ميزات**
</span>**(Feature Vector)** $`\mathbf{x(s)}`$<span dir="rtl">.</span>
<span dir="rtl">ومع ذلك، فإنه لا يزال من الملائم غالبًا الإشارة إلى
**التجارب**</span> **(Trials)** <span dir="rtl">كأجزاء من الزمن يتكرر
خلالها **أنماط المحفزات** </span>**(Patterns of Stimuli)**
<span dir="rtl">في تجربة</span>.

<span dir="rtl">لا تقتصر **ميزات الحالة**</span> **(State Features)**
<span dir="rtl">على وصف **المحفزات الخارجية**</span>
**<span dir="rtl">(</span>External Stimuli<span dir="rtl">)
</span>**<span dir="rtl">التي يواجهها **الحيوان**</span>
**(Animal)**<span dir="rtl">؛ يمكنها أن تصف **أنماط النشاط العصبي
(**</span>**Neural Activity
<span dir="rtl"></span>Patterns<span dir="rtl">)
</span>**<span dir="rtl">التي تنتجها **المحفزات الخارجية**</span>
**(External Stimuli)** <span dir="rtl">في دماغ **الحيوان**
</span>**(Animal)**<span dir="rtl">، وهذه الأنماط يمكن أن تعتمد على
**التاريخ** </span>**(History-dependent)**<span dir="rtl">، بمعنى أنها
قد تكون **أنماط مستمرة**</span> **(Persistent Patterns)**
<span dir="rtl">تنتجها تسلسلات من **المحفزات الخارجية**</span>
**<span dir="rtl">(</span>External
<span dir="rtl"></span>Stimuli<span dir="rtl">)</span>**.
<span dir="rtl">بالطبع، لا نعرف بالضبط ما هي هذه **أنماط النشاط العصبي**
</span>**(Neural Activity Patterns)**<span dir="rtl">، ولكن نموذج
**التفاضل الزمني**</span> **(TD Model)** <span dir="rtl">الذي يعمل في
الوقت الحقيقي يسمح لنا باستكشاف تأثيرات **التعليم**</span>
**(Learning)** <span dir="rtl">الناتجة عن الفرضيات المختلفة حول
**التمثيلات الداخلية**</span> **<span dir="rtl">(</span>Internal
<span dir="rtl"></span>Representations<span dir="rtl">)</span>**
<span dir="rtl">لـ **المحفزات الخارجية** </span>**(External
Stimuli)**<span dir="rtl">.</span> <span dir="rtl">لهذا السبب، لا يلتزم
نموذج **التفاضل الزمني**</span> **(TD Model)** <span dir="rtl">بأي
**تمثيل حالة معين** </span>**(Particular State
Representation)**<span dir="rtl">. بالإضافة إلى ذلك، لأن نموذج **التفاضل
الزمني**</span> **(TD Model)** <span dir="rtl">يتضمن **الخصم**
</span>**(Discounting)** <span dir="rtl">و**آثار الاستحقاق**</span>
**(Eligibility Traces)** <span dir="rtl">التي تمتد عبر الفترات الزمنية
بين **المحفزات** </span>**(Stimuli)**<span dir="rtl">، فإن النموذج يجعل
من الممكن أيضًا استكشاف كيفية تفاعل **الخصم**</span> **(Discounting)**
<span dir="rtl">و**آثار الاستحقاق**</span> **(Eligibility Traces)**
<span dir="rtl">مع **تمثيلات المحفزات**</span> **(Stimulus
Representations)** <span dir="rtl">في التنبؤ بنتائج تجارب **التكييف
الكلاسيكي** </span>**(Classical Conditioning
Experiments)**<span dir="rtl">.</span>

<span dir="rtl">فيما يلي سنصف بعض **تمثيلات الحالة**</span> **(State
Representations)** <span dir="rtl">التي تم استخدامها مع نموذج **التفاضل
الزمني**</span> **(TD Model)** <span dir="rtl">وبعض الآثار المترتبة
عليها، ولكن في الوقت الحالي نبقى على الحياد بشأن التمثيل ونفترض فقط أن
كل **حالة**</span> **(State)** $`s`$ <span dir="rtl">ممثلة بـ **متجه
ميزات** </span>**(Feature Vector)**
<span dir="rtl"></span>$`x(s) = (x1(s),x2(s),\ldots,xn(s))\top`$
<span dir="rtl">ثم تعطى القوة الترابطية الإجمالية المقابلة لحالة
**الحالة** </span>**(State)** <span dir="rtl"></span>s
<span dir="rtl"></span> <span dir="rtl">وفقًا للمعادلة (14.1)، تمامًا كما
في نموذج **ريسكورلا-فاجنر (**</span>**Rescorla-Wagner
<span dir="rtl"></span>Model<span dir="rtl">)</span>**<span dir="rtl">،
ولكن نموذج **التفاضل الزمني**</span> **(TD Model)** <span dir="rtl">يقوم
بتحديث **متجه القوة الترابطية (**</span>**Associative
<span dir="rtl"></span>Strength Vector<span dir="rtl">)</span>** $`w`$
<span dir="rtl">بطريقة مختلفة. مع تحويل</span> $`\mathbf{t}`$
<span dir="rtl">الآن للإشارة إلى **خطوة زمنية (**</span>**Time
<span dir="rtl"></span>Step<span dir="rtl">)</span>**
<span dir="rtl">بدلاً من **تجربة كاملة** </span>**(Complete
Trial)**<span dir="rtl">، يحكم نموذج **التفاضل الزمني  **
</span> **(TD Model** **<span dir="rtl">التعليم</span> (Learning)**
<span dir="rtl">وفقًا لهذا التحديث</span>:

``` math
w_{t + 1} = w_{t} + \alpha\,\delta_{t}\, z_{t}
```

<span dir="rtl">الذي يستبدل</span> $`xt(St)`$ <span dir="rtl"></span>
<span dir="rtl">في تحديث **ريسكورلا-فاجنر**</span> **(Rescorla-Wagner)**
<span dir="rtl">(المعادلة 14.2)  
بـ</span> $`zt`$<span dir="rtl">، وهو **متجه آثار الاستحقاق**
</span>**(Vector of Eligibility Traces)**<span dir="rtl">، وبدلاً
من</span> $`\delta t`$ <span dir="rtl"> في المعادلة (14.3)، هنا</span>
$`\delta t`$ <span dir="rtl"></span>​ <span dir="rtl">هو **خطأ التفاضل
الزمني** </span>**(TD Error)**<span dir="rtl">.</span>

``` math
\delta_{t} = R_{t + 1} + \gamma\widehat{v}\left( S_{t + 1},w_{t} \right) - \widehat{v}\left( S_{t},w_{t} \right)
```

<span dir="rtl">حيث أن</span> γ <span dir="rtl">هو **عامل الخصم**</span>
**(Discount Factor)** <span dir="rtl">ويتراوح بين 0 و1، و</span>$`Rt`$
<span dir="rtl">هو **هدف التنبؤ** </span>**(Prediction Target)**
<span dir="rtl">في الوقت</span> $`t`$<span dir="rtl">،
و</span>v^($`S_{t + 1}`$,wt) <span dir="rtl">و</span>v^(St,wt​)
<span dir="rtl">هما **القوى الترابطية الإجمالية** </span>**(Aggregate
Associative Strengths)** <span dir="rtl">في الوقتين</span> $`t + 1`$
<span dir="rtl">و</span>$`t`$ <span dir="rtl">كما هو معرف في  
المعادلة</span> (14.1)<span dir="rtl">.</span>

<span dir="rtl">كل مكون</span> $`i`$ <span dir="rtl">من **متجه آثار
الاستحقاق**</span> **(Eligibility-trace Vector)** $`zt`$
<span dir="rtl">يزيد أو ينقص وفقًا للمكون</span> $`xi(St)`$
<span dir="rtl">من **متجه الميزات** </span>**(Feature Vector)**
$`x(St)`$<span dir="rtl">، وإلا فإنه يتلاشى بمعدل تحدده</span>
$`\gamma\lambda`$ <span dir="rtl">كما يلي</span>:

``` math
z_{t + 1} = \gamma\lambda z_{t} + x\left( S_{t} \right)
```

<span dir="rtl">هنا،</span> λ <span dir="rtl">هو **بارامتر تلاشي آثار
الاستحقاق المعتاد**</span> **<span dir="rtl">(</span>Usual Eligibility
Trace Decay <span dir="rtl"></span>Parameter<span dir="rtl">)</span>**.
<span dir="rtl">لاحظ أنه إذا كان</span> $`\gamma = 0`$<span dir="rtl">،
فإن نموذج **التفاضل الزمني**</span> **(TD Model)** <span dir="rtl">يتقلص
إلى نموذج **ريسكورلا-فاجنر**</span> **(Rescorla-Wagner Model)**
<span dir="rtl">مع بعض الاستثناءات: أن معنى</span> $`t`$
<span dir="rtl">يختلف في كل حالة (فهو رقم التجربة في نموذج
**ريسكورلا-فاجنر**</span> **(Rescorla-Wagner Model)**
<span dir="rtl">وخطوة زمنية في نموذج **التفاضل الزمني** </span>**(TD
Model)**)<span dir="rtl">، وفي نموذج **التفاضل الزمني**</span> **(TD
Model)** <span dir="rtl">هناك تقدم بخطوة زمنية واحدة في هدف
التنبؤ</span> $`R`$<span dir="rtl">.</span> <span dir="rtl">يعادل نموذج
**التفاضل الزمني**</span> **(TD Model)** <span dir="rtl">الرؤية العكسية
لخوارزمية **شبه التدرج التفاضلي الزمني (**</span>**Semi-gradient
<span dir="rtl">  
</span>TD(λ) Algorithm<span dir="rtl">) </span>**<span dir="rtl">مع
**تقريب دالة خطية**</span> **(Linear Function Approximation)**
<span dir="rtl">انظر **الفصل** </span>**(**12)<span dir="rtl">، باستثناء
أن</span> $`Rt`$ <span dir="rtl"></span> <span dir="rtl">في النموذج لا
يجب أن يكون إشارة مكافأة كما هو الحال عندما تُستخدم خوارزمية **التفاضل
الزمني**</span> **(TD Algorithm)** <span dir="rtl">لتعلم **دالة
القيمة**</span> **(Value Function)** <span dir="rtl">من أجل تحسين
السياسة</span>.

**<u>14.2.4 <span dir="rtl">محاكاة نموذج التفاضل الزمني</span> (TD Model
Simulations)</u>**

<span dir="rtl">النماذج الزمنية الفورية مثل نموذج **التفاضل
الزمني**</span> **(TD Model)** <span dir="rtl">مثيرة للاهتمام بشكل رئيسي
لأنها تقدم تنبؤات لمجموعة واسعة من الحالات التي لا يمكن تمثيلها بواسطة
النماذج التي تعتمد على مستوى **التجربة** </span>**(Trial-level
Models)**<span dir="rtl">.</span> <span dir="rtl">تتعلق هذه الحالات
بتوقيت ومدد **المحفزات القابلة للتكييف** </span>**(Conditionable
Stimuli)**<span dir="rtl">، وتوقيت هذه **المحفزات**</span> **(Stimuli)**
<span dir="rtl">بالنسبة لتوقيت **المحفز غير المشروط**
</span>**(Unconditioned Stimulus (US))**<span dir="rtl">، وكذلك توقيت
وأشكال **الاستجابات الشرطية** </span>**(Conditioned Responses
(CRs))**<span dir="rtl">.على سبيل المثال، يجب أن يبدأ **المحفز غير
المشروط** </span>**(US)** <span dir="rtl">عمومًا بعد بداية **المحفز
المحايد**</span> **(Neutral Stimulus)** <span dir="rtl">حتى يحدث
**التكييف** </span>**(Conditioning)**<span dir="rtl">، مع اعتماد معدل
وفعالية **التعليم**</span> **(Learning)** <span dir="rtl">على **الفاصل
الزمني بين المحفزين** </span>**(Inter-stimulus Interval
(ISI))**<span dir="rtl">، أي الفاصل بين بدايات **المحفز الشرطي
(**</span>**Conditioned <span dir="rtl"></span>Stimulus
(CS)<span dir="rtl">) </span>**<span dir="rtl">و**المحفز غير المشروط**
</span>**(US)**<span dir="rtl">.</span> <span dir="rtl">عندما تظهر
**الاستجابات الشرطية** </span>**(CRs)**<span dir="rtl">، فإنها تبدأ
عمومًا قبل ظهور **المحفز غير المشروط**</span> **(US)**
<span dir="rtl">وتغير ملفاتها الزمنية أثناء **التعليم**
</span>**(Learning)**<span dir="rtl">. في **التكييف باستخدام محفزات
شرطية مركبة** </span>**(Compound CSs)**<span dir="rtl">، قد لا تبدأ جميع
**المحفزات المكونة**</span> **(Component Stimuli)**
<span dir="rtl">وتنتهي في نفس الوقت، حيث تشكل أحيانًا ما يسمى بـ **المركب
المتسلسل**</span> **(Serial Compound)** <span dir="rtl">حيث تحدث
**المحفزات المكونة**</span> **(Component Stimuli)** <span dir="rtl">في
تسلسل زمني. تجعل هذه الاعتبارات الزمنية من المهم التفكير في كيفية تمثيل
**المحفزات** </span>**(Stimuli)**<span dir="rtl">، وكيف تتكشف هذه
**التمثيلات**</span> **(Representations)** <span dir="rtl">بمرور الوقت
خلال وبين **التجارب** </span>**(Trials)**<span dir="rtl">، وكيف تتفاعل
مع **الخصم**</span> **(Discounting)** <span dir="rtl">و**آثار
الاستحقاق** </span>**(Eligibility Traces)**<span dir="rtl">.</span>

<img src="./media/image169.png"
style="width:6.26806in;height:4.64792in" />

**<span dir="rtl">الشكل 14.1: ثلاث تمثيلات للمحفزات</span> (Stimulus
Representations)** <span dir="rtl">(في الأعمدة) تُستخدم أحيانًا مع **نموذج
التفاضل الزمني** </span>**(TD Model)**<span dir="rtl">.</span>
<span dir="rtl">يمثل كل صف عنصرًا واحدًا من **تمثيل المحفزات**
</span>**(Stimulus Representation)**<span dir="rtl">.</span>
<span dir="rtl">تختلف **التمثيلات الثلاثة**</span> **(Three
Representations)** <span dir="rtl">وفقًا لتدرج **التعميم الزمني**
</span>**(Temporal Generalization Gradient)**<span dir="rtl">، حيث لا
يوجد تعميم بين النقاط الزمنية القريبة في **المركب المتسلسل
الكامل**</span> **(Complete Serial Compound)** <span dir="rtl">(العمود
الأيسر) ويوجد تعميم كامل بين النقاط الزمنية القريبة في **تمثيل الوجود**
</span>**(Presence Representation)** <span dir="rtl">(العمود الأيمن).
يحتل **تمثيل الميكرو محفزات**</span> **(Microstimulus Representation)**
<span dir="rtl">مكانًا متوسطًا. تحدد درجة **التعميم الزمني**</span>
**(Temporal Generalization)** <span dir="rtl">دقة **الحبيبات الزمنية**
</span>**(Temporal Granularity)** <span dir="rtl">التي يتم بها تعلم
**تنبؤات المحفز غير المشروط**</span> **<span dir="rtl">(</span>US
Predictions<span dir="rtl">)</span>**. <span dir="rtl">تم التكيف مع
تغييرات طفيفة من</span> **Learning & Behavior**<span dir="rtl">، تقييم
**نموذج التفاضل الزمني في التكييف الكلاسيكي** </span>**(Evaluating the
TD Model of Classical Conditioning)**<span dir="rtl">،  
(المجلد 40، 2012، الصفحة 311، بواسطة</span> E. A. Ludvig
<span dir="rtl">و</span> R. S. Sutton <span dir="rtl">و</span> E. J.
Kehoe. <span dir="rtl">بإذن من</span> Springer<span dir="rtl">).</span>

<span dir="rtl">يوضح **الشكل 14.1** ثلاثة من **تمثيلات المحفزات**</span>
**(Stimulus Representations)** <span dir="rtl">التي تم استخدامها في
استكشاف سلوك **نموذج التفاضل الزمني** </span>**(TD
Model)**<span dir="rtl">:</span> **<span dir="rtl">المركب المتسلسل
الكامل</span> (Complete Serial Compound (CSC))**<span dir="rtl">،
**تمثيل الميكرو محفزات**</span> **<span dir="rtl">(</span>Microstimulus
<span dir="rtl"></span>(MS)<span dir="rtl">)</span>**<span dir="rtl">،
و**تمثيل الوجود**</span> **(Presence Representations)**
<span dir="rtl">(</span>Ludvig, Sutton, and Kehoe,
2012<span dir="rtl">)</span>. <span dir="rtl">تختلف هذه
**التمثيلات**</span> **(Representations)** <span dir="rtl">في درجة
التعميم التي تفرضها بين النقاط الزمنية القريبة التي يكون خلالها
**المحفز**</span> **(Stimulus)** <span dir="rtl">حاضرًا</span>.

<span dir="rtl">أبسط **التمثيلات**</span> **(Representations)**
<span dir="rtl">الموضحة في **الشكل**</span> **(Figure) 14.1**
<span dir="rtl">هو **تمثيل الوجود** </span>**(Presence Representation)**
<span dir="rtl">في العمود الأيمن من الشكل. يحتوي هذا **التمثيل**
</span>**(Representation)** <span dir="rtl">على **ميزة واحدة**</span>
**(Single Feature)** <span dir="rtl">لكل **مكون محفز شرطي**
</span>**(Component CS)** <span dir="rtl">حاضر في **التجربة**
</span>**(Trial)**<span dir="rtl">، حيث تأخذ هذه **الميزة**</span>
**(Feature)** <span dir="rtl">القيمة 1 كلما كان هذا المكون موجودًا، و0 في
غير ذلك. على الرغم من أن **تمثيل الوجود  
(**</span>**Presence Representation<span dir="rtl">)
</span>**<span dir="rtl">ليس فرضية واقعية حول كيفية تمثيل
**المحفزات**</span> **(Stimuli)** <span dir="rtl">في دماغ **الحيوان**
</span>**(Animal)**<span dir="rtl">، إلا أن **نموذج التفاضل
الزمني**</span> **(TD Model)** <span dir="rtl">مع هذا **التمثيل**
</span>**(Representation)** <span dir="rtl">يمكن أن يُنتج العديد من
الظواهر الزمنية المرصودة في **التكييف الكلاسيكي** </span>**(Classical
Conditioning)**<span dir="rtl">.</span>

<span dir="rtl">بالنسبة لـ **تمثيل المركب المتسلسل الكامل**</span>
**(CSC Representation)** (<span dir="rtl">العمود الأيسر من **الشكل**
</span>**(Figure) 14.1**)<span dir="rtl">، فإن بداية كل **محفز
خارجي**</span> **(External Stimulus)** <span dir="rtl">تؤدي إلى سلسلة من
**الإشارات الداخلية**</span> **(Internal Signals)** <span dir="rtl">ذات
**المدة القصيرة**</span> **(Short-duration)** <span dir="rtl">والتي
تستمر حتى ينتهي **المحفز الخارجي** </span>**(External
Stimulus)**<span dir="rtl">.</span> <span dir="rtl">هذا يشبه افتراض أن
**الجهاز العصبي للحيوان**</span> **(Animal's Nervous System)**
<span dir="rtl">لديه ساعة تحتفظ بتوقيت دقيق أثناء تقديم **المحفزات**
</span>**(Stimuli)**<span dir="rtl">؛ ما يسميه المهندسون بـ "خط تأخير
مشبع</span> (Tapped Delay Line)"<span dir="rtl">.</span>
<span dir="rtl">على غرار **تمثيل الوجود** </span>**(Presence
Representation)**<span dir="rtl">، فإن **تمثيل المركب المتسلسل
الكامل**</span> **<span dir="rtl">(</span>CSC
<span dir="rtl"></span>Representation<span dir="rtl">)
</span>**<span dir="rtl">ليس فرضية واقعية حول كيفية تمثيل الدماغ
للمحفزات داخليًا، ولكن  
</span>Ludvig <span dir="rtl">وآخرون (2012) أطلقوا عليه **"خيال مفيد"**
لأنه يمكن أن يكشف تفاصيل حول كيفية عمل **نموذج التفاضل الزمني**</span>
**(TD Model)** <span dir="rtl">عندما يكون غير مقيد نسبيًا بتمثيل
المحفزات. يُستخدم **تمثيل المركب المتسلسل الكامل**</span> **(CSC
Representation)** <span dir="rtl">أيضًا في معظم نماذج التفاضل الزمني التي
تدرس **الخلايا العصبية المنتجة للدوبامين**</span> **(Dopamine-producing
Neurons)** <span dir="rtl">في الدماغ، وهو موضوع نتناوله في **الفصل**
</span>**15**<span dir="rtl">.</span> <span dir="rtl">يُنظر إلى **تمثيل
المركب المتسلسل الكامل  
(**</span>**CSC <span dir="rtl"></span>Representation<span dir="rtl">)
</span>**<span dir="rtl">غالبًا على أنه جزء أساسي من **نموذج التفاضل
الزمني** </span>**(TD Model)**<span dir="rtl">، على الرغم من أن هذا
الرأي خاطئ</span>.

<span dir="rtl">أما **تمثيل الميكرو محفزات**</span> **(MS
Representation)** <span dir="rtl">(العمود الأوسط من **الشكل**
</span>**(Figure) <span dir="rtl"></span>14.1**<span dir="rtl">)، فهو
يشبه **تمثيل المركب المتسلسل الكامل**</span> **(CSC Representation)**
<span dir="rtl">من حيث أن كل **محفز خارجي**</span> **(External
Stimulus)** <span dir="rtl">يؤدي إلى سلسلة من **المحفزات الداخلية
(**</span>**Internal
<span dir="rtl"></span>Stimuli<span dir="rtl">)</span>**<span dir="rtl">،
ولكن في هذه الحالة، **المحفزات الداخلية**</span> **(Internal Stimuli)**
**<span dir="rtl">الميكرو محفزات</span> (Microstimuli)**
<span dir="rtl">ليست ذات شكل محدود وغير متداخل؛ بل إنها تمتد بمرور الوقت
وتتداخل. مع مرور الوقت منذ بداية **المحفز**
</span>**(Stimulus)**<span dir="rtl">، تصبح مجموعات مختلفة من **الميكرو
محفزات** </span>**(Microstimuli)** <span dir="rtl">أكثر أو أقل نشاطًا،
ويصبح كل **ميكرو محفز**</span> **(Microstimulus)** <span dir="rtl">تالي
أكثر اتساعًا في الزمن ويصل إلى مستوى أقصى أقل. بالطبع، هناك العديد من
**تمثيلات الميكرو محفزات** </span>**(MS Representations)**
<span dir="rtl">اعتمادًا على طبيعة **الميكرو محفزات**
</span>**(Microstimuli)**<span dir="rtl">، وقد تم دراسة عدد من الأمثلة
على **تمثيلات الميكرو محفزات**</span> **(MS Representations)**
<span dir="rtl">في الأدبيات، في بعض الحالات جنبًا إلى جنب مع اقتراحات حول
كيفية إنتاج دماغ **الحيوان**</span> **(Animal's Brain)**
<span dir="rtl">لها  
(انظر **التعليقات الببليوغرافية والتاريخية**</span> **(Bibliographic and
Historical Comments)** <span dir="rtl">في نهاية هذا الفصل). تُعتبر
**تمثيلات الميكرو محفزات**</span> **(MS Representations)**
<span dir="rtl">أكثر واقعية من **تمثيل الوجود**</span> **(Presence
Representation)** <span dir="rtl">أو **تمثيل المركب المتسلسل الكامل
(**</span>**CSC
<span dir="rtl"></span>Representation<span dir="rtl">)</span>**
<span dir="rtl">كفرضيات حول التمثيلات العصبية للمحفزات، وتتيح ربط سلوك
**نموذج التفاضل الزمني**</span> **(TD Model)** <span dir="rtl">بمجموعة
أوسع من الظواهر المرصودة في تجارب **الحيوانات (**</span>**Animal
<span dir="rtl"></span>Experiments<span dir="rtl">)</span>**.
<span dir="rtl">على وجه الخصوص، من خلال افتراض أن سلاسل **الميكرو
محفزات** </span>**(Microstimuli)** <span dir="rtl">تنطلق من **المحفزات
غير المشروطة**</span> **(USs)** <span dir="rtl">وكذلك **المحفزات
الشرطية** </span>**(CSs)**<span dir="rtl">، ومن خلال دراسة التأثيرات
الكبيرة على **التعليم**</span> **(Learning)** <span dir="rtl">الناتجة عن
تفاعلات **الميكرو محفزات** </span>**(Microstimuli)**
<span dir="rtl">و**آثار الاستحقاق**</span> **(Eligibility Traces)**
<span dir="rtl">و**الخصم** </span>**(Discounting)**<span dir="rtl">،
يساعد **نموذج التفاضل الزمني**</span> **(TD Model)** <span dir="rtl">في
صياغة فرضيات تفسر العديد من الظواهر الدقيقة في **التكييف
الكلاسيكي**</span> **(Classical Conditioning)** <span dir="rtl">وكيف قد
ينتجها **دماغ الحيوان (**</span>**Animal's
<span dir="rtl"></span>Brain<span dir="rtl">)</span>**.
<span dir="rtl">سنتحدث أكثر عن هذا الموضوع في الأسفل، وخاصة في
**الفصل**</span> **15** <span dir="rtl">حيث نناقش  
**التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">و**علم الأعصاب**
</span>**(Neuroscience)**<span dir="rtl">.</span>

<span dir="rtl">حتى مع **تمثيل الوجود البسيط** </span>**(Simple Presence
Representation)**<span dir="rtl">، فإن **نموذج التفاضل الزمني**</span>
**(TD Model)** <span dir="rtl">يُنتج جميع الخصائص الأساسية لـ **التكييف
الكلاسيكي**</span> **<span dir="rtl">(</span>Classical
<span dir="rtl"></span>Conditioning<span dir="rtl">)</span>**
<span dir="rtl">التي يتم تفسيرها بواسطة **نموذج ريسكورلا-فاجنر**</span>
**<span dir="rtl">(</span>Rescorla-Wagner
<span dir="rtl"></span>**<img src="./media/image170.png"
style="width:3.04444in;height:2.95486in" />**Model<span dir="rtl">)</span>**<span dir="rtl">،
بالإضافة إلى ميزات **التكييف** </span>**(Conditioning)**
<span dir="rtl">التي تتجاوز نطاق النماذج المستندة إلى مستوى التجربة. على
سبيل المثال، كما ذكرنا سابقًا، فإن سمة واضحة من **سمات التكييف
الكلاسيكي** </span>**(Classical Conditioning)** <span dir="rtl">هي أن
**المحفز غير المشروط**</span> **(US)** <span dir="rtl">يجب أن يبدأ عادةً
بعد بداية **المحفز المحايد**</span> **<span dir="rtl">(</span>Neutral
<span dir="rtl"></span>Stimulus<span dir="rtl">)
</span>**<span dir="rtl">حتى يحدث **التكييف**
</span>**(Conditioning)**<span dir="rtl">، وبعد **التكييف**
</span>**(Conditioning)**<span dir="rtl">، تبدأ **الاستجابة
الشرطية**</span> **(CR)** <span dir="rtl">قبل ظهور **المحفز غير
المشروط** </span>**(US)**<span dir="rtl">.</span> <span dir="rtl">بعبارة
أخرى، يتطلب **التكييف** </span>**(Conditioning)** <span dir="rtl">عمومًا
وجود **فاصل زمني بين المحفزين**</span> **(ISI)** <span dir="rtl">إيجابي،
وتنبئ **الاستجابة الشرطية**</span> **(CR)** <span dir="rtl">بشكل عام
بظهور **المحفز غير المشروط** </span>**(US)**<span dir="rtl">.</span>

<span dir="rtl">تعتمد قوة **التكييف**</span> **(Conditioning Strength)**
<span dir="rtl">(مثل نسبة **الاستجابات الشرطية**</span> **(CRs)**
<span dir="rtl">التي يتم استدعاؤها بواسطة **المحفز الشرطي**
</span>**(CS)**<span dir="rtl">)</span> <span dir="rtl">على **الفاصل
الزمني بين المحفزين**</span> **(ISI)** <span dir="rtl">وتختلف بشكل كبير
بين الأنواع وأنظمة الاستجابة، لكنها تحتوي عادةً على الخصائص التالية: تكون
ضئيلة أو معدومة عند وجود **فاصل زمني بين المحفزين**</span> **(ISI)**
<span dir="rtl">صفر أو سالب، أي عندما يحدث بدء **المحفز غير
المشروط**</span> **(US)** <span dir="rtl">في نفس الوقت مع أو قبل بدء
**المحفز الشرطي**</span> **(CS)** (<span dir="rtl">على الرغم من أن
الأبحاث أظهرت أن القوة الترابطية أحيانًا تزداد قليلاً أو تصبح سلبية مع
**فواصل زمنية سالبة**</span> **<span dir="rtl">(</span>Negative
<span dir="rtl"></span>ISIs)**<span dir="rtl">)؛ تزداد إلى الحد الأقصى
عند **فاصل زمني إيجابي**</span> **(Positive ISI)** <span dir="rtl">حيث
يكون **التكييف** </span>**(Conditioning)** <span dir="rtl">أكثر فعالية؛
ثم تنخفض إلى الصفر بعد فترة زمنية تختلف بشكل كبير مع أنظمة الاستجابة.
يعتمد الشكل الدقيق لهذا الاعتماد في **نموذج التفاضل الزمني**</span>
**(TD Model)** <span dir="rtl">على قيم **البارامترات**</span>
**(Parameters)** <span dir="rtl">الخاصة به وتفاصيل **تمثيل
المحفزات**</span> **<span dir="rtl">(</span>Stimulus
<span dir="rtl"></span>Representation<span dir="rtl">)</span>**<span dir="rtl">،
لكن هذه الميزات الأساسية لاعتماد **الفاصل الزمني بين المحفزين**</span>
**<span dir="rtl">(</span>ISI-dependency<span dir="rtl">)</span>**
<span dir="rtl">هي خصائص جوهرية في **نموذج التفاضل الزمني** </span>**(TD
Model)**<span dir="rtl">.</span>

<span dir="rtl">أحد القضايا النظرية التي تنشأ مع **التكييف المركب
المتسلسل**</span> **<span dir="rtl">(</span>Serial-compound
<span dir="rtl"></span>Conditioning<span dir="rtl">)</span>**<span dir="rtl">،
وهو **التكييف**</span> **(Conditioning)** <span dir="rtl">باستخدام
**محفز شرطي مركب**</span> **<span dir="rtl">(</span>Compound
<span dir="rtl"></span>CS<span dir="rtl">)
</span>**<span dir="rtl">تتكون مكوناته في تسلسل، يتعلق بتسهيل
**الارتباطات البعيدة** </span>**(Remote
Associations)**<span dir="rtl">.</span> <span dir="rtl">لقد وُجد أنه إذا
تم ملء **الفاصل الزمني الفارغ** </span>**(Empty Trace Interval)**
<span dir="rtl">بين **المحفز الشرطي الأول** </span>**(First CS)**
(CSA_AA​) <span dir="rtl">و**المحفز غير المشروط**</span> **(US)**
<span dir="rtl">بـ **محفز شرطي ثانٍ** </span>**(Second CS)** (CSB_BB​)
<span dir="rtl">لتشكيل **محفز مركب متسلسل (**</span>**Serial-compound
Stimulus<span dir="rtl">)</span>**<span dir="rtl">، فإن **التكييف**
</span>**(Conditioning)** <span dir="rtl">لـ</span> CSA_AA​
<span dir="rtl">يتم تسهيله. كما هو موضح على اليمين، يُظهر **سلوك نموذج
التفاضل الزمني**</span> **(TD Model)** <span dir="rtl">مع **تمثيل
الوجود** </span>**(Presence Representation)** <span dir="rtl">في محاكاة
لمثل هذه التجربة، حيث تُعرض تفاصيل التوقيت أعلاه. متسقًا مع النتائج
التجريبية</span> (Kehoe, 1982)<span dir="rtl">، يُظهر **النموذج**</span>
**(Model)** <span dir="rtl">تسهيلًا لكل من معدل **التكييف**</span>
**(Conditioning Rate)** <span dir="rtl">ومستوى **التكييف**</span>
**(Conditioning Level)** <span dir="rtl">الأقصى لـ</span> CSA_AA​
<span dir="rtl">بسبب وجود</span> CSB_BB​

<img src="./media/image171.png"
style="width:2.66181in;height:2.66528in" /><span dir="rtl">تُعتبر تجربة
أجراها</span> **Egger <span dir="rtl">و</span>Miller (1962)**
<span dir="rtl">واحدة من العروض المعروفة حول تأثيرات العلاقات الزمنية
بين **المحفزات**</span> **(Stimuli)** <span dir="rtl">داخل
**التجربة**</span> **(Trial)** <span dir="rtl">على **التكييف**
</span>**(Conditioning)**<span dir="rtl">، والتي تضمنت استخدام **محفزين
شرطيين متداخلين** </span>**(Overlapping CSs)** <span dir="rtl">في تكوين
تأخيري، كما هو موضح على اليمين (الأعلى). على الرغم من أن</span> **CSB**
<span dir="rtl">كان في علاقة زمنية أفضل مع **المحفز غير المشروط**
</span>**(US)**<span dir="rtl">، إلا أن وجود</span> **CSA**
<span dir="rtl">قلل بشكل كبير من **التكييف** </span>**(Conditioning)**
<span dir="rtl">لـ</span> **CSB** <span dir="rtl">مقارنةً بالتجارب التي
كان فيها</span> **CSA** <span dir="rtl">غائبًا. مباشرة على اليمين يظهر
نفس النتيجة التي يتم إنتاجها بواسطة **نموذج التفاضل الزمني**</span>
**<span dir="rtl">(</span>TD
<span dir="rtl"></span>Model<span dir="rtl">)
</span>**<span dir="rtl">في محاكاة لهذه التجربة باستخدام **تمثيل
الوجود** </span>**(Presence Representation)**<span dir="rtl">.</span>

<img src="./media/image172.png"
style="width:2.69722in;height:2.74861in" /><span dir="rtl">يُفسر **نموذج
التفاضل الزمني**</span> **(TD Model)** **<span dir="rtl">التحجب</span>
(Blocking)** <span dir="rtl">لأنه قاعدة تعلم تصحيح الأخطاء مثل **نموذج
ريسكورلا-فاجنر** </span>**(Rescorla-Wagner
Model)**<span dir="rtl">.</span> <span dir="rtl">لكن، بخلاف تفسير نتائج
**التحجب الأساسية** </span>**(Basic Blocking Results)**<span dir="rtl">،
يتوقع **نموذج التفاضل الزمني**</span> **(TD Model)** <span dir="rtl">(مع
**تمثيل الوجود (**</span>**Presence
<span dir="rtl"></span>Representation<span dir="rtl">)</span>**
<span dir="rtl">وتمثيلات أكثر تعقيدًا أيضًا) أن **التحجب**
</span>**(Blocking)** <span dir="rtl">يتم عكسه إذا تم تحريك **المحفز
المحجوب**</span> **(Blocked Stimulus)** <span dir="rtl">إلى وقت سابق
بحيث يحدث بدايته قبل بداية **المحفز المحجب**</span>
**<span dir="rtl">(</span>Blocking
<span dir="rtl"></span>Stimulus<span dir="rtl">)
</span>**<span dir="rtl">(مثل</span> **CSA** <span dir="rtl">في الرسم
البياني على اليمين). تستحق هذه الخاصية من سلوك **نموذج التفاضل
الزمني**</span> **(TD Model)** <span dir="rtl">الاهتمام لأنها لم تكن قد
لوحظت في وقت تقديم النموذج. تذكر أنه في **التحجب**
</span>**(Blocking)**<span dir="rtl">، إذا كان **الحيوان**
</span>**(Animal)** <span dir="rtl">قد تعلم بالفعل أن **محفز شرطي
واحد**</span> **(CS)** <span dir="rtl">يتنبأ بـ **المحفز غير المشروط**
</span>**(US)**<span dir="rtl">، فإن تعلم أن **محفز شرطي ثانٍ**
</span>**(Newly-added Second CS)** <span dir="rtl">يتنبأ أيضًا بـ
**المحفز غير المشروط**</span> **(US)** <span dir="rtl">يتم تقليله بشكل
كبير، أي يتم تحجبه. لكن إذا بدأ **المحفز الشرطي الثاني المضاف حديثًا**
</span>**(Newly-added Second CS)** <span dir="rtl">في وقت أبكر من
**المحفز الشرطي المدرب مسبقًا** </span>**(Pretrained
CS)**<span dir="rtl">، فعندئذٍ وفقًا لـ **نموذج التفاضل الزمني**</span>
**(TD Model)** <span dir="rtl">لا يتم تحجب **التعليم**</span>
**(Learning)** <span dir="rtl">للمحفز الشرطي المضاف حديثًا. في الواقع، مع
استمرار التدريب وزيادة **القوة الترابطية** </span>**(Associative
Strength)** <span dir="rtl">للمحفز الشرطي المضاف حديثًا، يفقد **المحفز
الشرطي المدرب مسبقًا** </span>**(Pretrained CS)** **<span dir="rtl">القوة
الترابطية</span> (Associative Strength)**<span dir="rtl">.</span>

<span dir="rtl">يظهر سلوك **نموذج التفاضل الزمني**</span> **(TD Model)**
<span dir="rtl">في ظل هذه الظروف في الجزء السفلي من **الشكل**
</span>**(Figure) 14.2**<span dir="rtl">.</span> <span dir="rtl">تختلف
هذه التجربة المحاكية عن تجربة</span> **Egger-Miller**
<span dir="rtl">(أسفل الصفحة السابقة) في أن **المحفز الشرطي
الأقصر**</span> **(Shorter CS)** <span dir="rtl">الذي كان له بداية
متأخرة تلقى تدريبًا مسبقًا حتى أصبح مرتبطًا بالكامل بـ **المحفز غير
المشروط** </span>**(US)**<span dir="rtl">.</span> <span dir="rtl">قاد
هذا التنبؤ المفاجئ</span> **Kehoe <span dir="rtl">و</span>Schreurs
<span dir="rtl">و</span>Graham (1987)** <span dir="rtl">إلى إجراء
التجربة باستخدام تحضير غشاء الأرنب الغامض المدروس جيدًا. أكدت نتائجهم
تنبؤ النموذج، وأشاروا إلى أن النماذج غير التابعة لـ **التفاضل
الزمني**</span> **<span dir="rtl">(</span>Non-TD
<span dir="rtl"></span>Models<span dir="rtl">)
</span>**<span dir="rtl">تواجه صعوبة كبيرة في تفسير بياناتهم</span>.

<span dir="rtl">مع **نموذج التفاضل الزمني** </span>**(TD
Model)**<span dir="rtl">، يكون للمحفز التنبؤي الأسبق الأولوية على المحفز
التنبؤي المتأخر، لأن **نموذج التفاضل الزمني** </span>**(TD
Model)**<span dir="rtl">، مثل جميع طرق التنبؤ الموصوفة في هذا الكتاب،
يعتمد على فكرة **الارتجاع أو التعزيز الذاتي (**</span>**Backing-up
<span dir="rtl"></span>or
<span dir="rtl"></span>Bootstrapping<span dir="rtl">) تحديثات</span>**
**<span dir="rtl">القوى الترابطية</span>
<span dir="rtl">(</span>Associative
<span dir="rtl"></span>Strengths<span dir="rtl">)
</span>**<span dir="rtl">تنقل القوة في حالة معينة نحو القوة في حالات
لاحقة. من النتائج الأخرى للتعزيز الذاتي أن **نموذج التفاضل
الزمني**</span> **(TD Model)** <span dir="rtl">يوفر تفسيرًا لظاهرة
**التكييف من الدرجة العليا** </span>**(Higher-order
Conditioning)**<span dir="rtl">، وهي سمة من **سمات التكييف
الكلاسيكي**</span> **<span dir="rtl">(</span>Classical
<span dir="rtl"></span>Conditioning<span dir="rtl">)
</span>**<span dir="rtl">التي تتجاوز نطاق **نموذج ريسكورلا-فاجنر
(**</span>**Rescorla-Wagner
<span dir="rtl"></span>Model<span dir="rtl">)
</span>**<span dir="rtl">والنماذج المماثلة. كما وصفنا سابقًا، فإن
**التكييف من الدرجة العليا**</span>
**<span dir="rtl">(</span>Higher-order
<span dir="rtl"></span>Conditioning<span dir="rtl">)
</span>**<span dir="rtl">هو الظاهرة التي يمكن فيها أن يعمل **محفز شرطي**
</span>**(CS)** <span dir="rtl">تم تكييفه مسبقًا كـ **محفز غير
مشروط**</span> **(US)** <span dir="rtl">في تكييف محفز آخر محايد في
البداية. يوضح **الشكل**</span> **(Figure) 14.3** <span dir="rtl">سلوك
**نموذج التفاضل الزمني**</span> **(TD Model)** (<span dir="rtl">مرة أخرى
مع **تمثيل الوجود (**</span>**Presence
<span dir="rtl"></span>Representation<span dir="rtl">)</span>**)
<span dir="rtl">في تجربة **تكييف من الدرجة العليا
(**</span>**Higher-order
<span dir="rtl"></span>Conditioning<span dir="rtl">)</span>**
<span dir="rtl">في هذه الحالة هو **التكييف من الدرجة الثانية**
</span>**(Second-order** <img src="./media/image173.png"
style="width:3.00486in;height:3.19375in" />**Conditioning)**<span dir="rtl">.</span>
<span dir="rtl">في المرحلة الأولى (غير معروضة في الشكل)، يتم
تدريب</span> **CSB** <span dir="rtl">للتنبؤ بـ **المحفز غير
المشروط**</span> **(US)** <span dir="rtl">بحيث تزداد قوته الترابطية، هنا
إلى 1.65. في المرحلة الثانية، يتم إقران</span> **CSA**
<span dir="rtl">مع</span> **CSB** <span dir="rtl">في غياب **المحفز غير
المشروط** </span>**(US)**<span dir="rtl">، في الترتيب التسلسلي الموضح في
أعلى الشكل. يكتسب</span> **CSA** <span dir="rtl">قوة ترابطية على الرغم
من أنه لم يتم إقرانه أبدًا مع **المحفز غير المشروط**
</span>**(US)**<span dir="rtl">.</span> <span dir="rtl">مع استمرار
التدريب، تصل القوة الترابطية لـ</span> **CSA** <span dir="rtl">إلى
ذروتها ثم تنخفض لأن القوة الترابطية لـ</span> **CSB**<span dir="rtl">،
وهو المعزز الثانوي، تنخفض بحيث يفقد قدرته على توفير التعزيز الثانوي.
تنخفض القوة الترابطية لـ</span> **CSB** <span dir="rtl">لأن **المحفز غير
المشروط**</span> **(US)** <span dir="rtl">لا يحدث في هذه التجارب من
**التكييف من الدرجة العليا** </span>**(Higher-order Conditioning
Trials)** <span dir="rtl">هذه هي تجارب الانقراض لـ</span> **CSB**
<span dir="rtl">لأن علاقته التنبؤية بـ **المحفز غير المشروط**</span>
**(US)** <span dir="rtl">تتعطل بحيث تقل قدرته على العمل كمعزز. يتم رؤية
هذا النمط نفسه في تجارب الحيوانات. يجعل هذا الانقراض للتعزيز الشرطي في
تجارب **التكييف من الدرجة العليا**</span> **(Higher-order
Conditioning)** <span dir="rtl">من الصعب إثبات **التكييف من الدرجة
العليا**</span> **(Higher-order Conditioning)** <span dir="rtl">ما لم
يتم تحديث العلاقات التنبؤية الأصلية بشكل دوري عن طريق إدخال تجارب من
الدرجة الأولى أحيانًا</span>.

<span dir="rtl">ينتج **نموذج التفاضل الزمني**</span> **(TD Model)**
<span dir="rtl">تناظرًا بين **التكييف من الدرجة الثانية  
(**</span>**Second-order Conditioning<span dir="rtl">)
</span>**<span dir="rtl">و**التكييف من الدرجات العليا**</span>
**<span dir="rtl">(</span>Higher-order
<span dir="rtl"></span>Conditioning<span dir="rtl">)</span>**
<span dir="rtl">لأن</span>
$`\gamma v\hat{}(S_{t + 1},wt)\  - v\hat{}(St,wt)`$ <span dir="rtl">يظهر
في **خطأ التفاضل الزمني  **
</span>**(TD Error)** <span dir="rtl"></span>$`\delta t`$
<span dir="rtl">في المعادلة (14.5). هذا يعني أنه نتيجةً للتعليم السابق،
يمكن أن تختلف</span> $`\gamma v\hat{}(S_{t + 1},wt)`$
<span dir="rtl">عن</span> $`v\hat{}(St,wt)`$<span dir="rtl">، مما
يجعل</span> $`\delta t`$ <span dir="rtl">غير صفري (فرق زمني). هذا الفرق
له نفس حالة</span> $`R_{t + 1}`$ ​ <span dir="rtl">في المعادلة (14.5)،
مما يعني أنه بالنسبة للتعليم، لا يوجد فرق بين الفرق الزمني وظهور
**المحفز غير المشروط** </span>**(US)**<span dir="rtl">. في الواقع، هذه
الخاصية من **خوارزمية التفاضل الزمني**</span> **<span dir="rtl">  
</span>(TD Algorithm)** <span dir="rtl">هي واحدة من الأسباب الرئيسية
لتطويرها، والتي نفهمها الآن من خلال ارتباطها بـ **البرمجة
الديناميكية**</span> **<span dir="rtl">(</span>Dynamic
<span dir="rtl"></span>Programming<span dir="rtl">)
</span>**<span dir="rtl">كما هو موضح في **الفصل**
</span>**6**<span dir="rtl">.</span> <span dir="rtl">يرتبط التعزيز
الذاتي للقيم ارتباطًا وثيقًا بـ **التكييف من الدرجة الثانية**</span>
**(Second-order Conditioning)** <span dir="rtl">و**التكييف من الدرجات
العليا  
(**</span>**Higher-order Conditioning<span dir="rtl">)</span>**.

<span dir="rtl">في الأمثلة المتعلقة بسلوك **نموذج التفاضل
الزمني**</span> **(TD Model)** <span dir="rtl">التي تم وصفها أعلاه، قمنا
فقط بفحص التغيرات في **القوى الترابطية**</span> **(Associative
Strengths)** <span dir="rtl">لمكونات **المحفز الشرطي**
</span>**(CS)**<span dir="rtl">؛ ولم نتناول ما يتنبأ به النموذج حول
خصائص **الاستجابات الشرطية للحيوان**</span>
**<span dir="rtl">(</span>Conditioned <span dir="rtl"></span>Responses
(CRs)<span dir="rtl">)</span>**<span dir="rtl">:</span>
<span dir="rtl">توقيتها، شكلها، وكيفية تطورها على مدار **التجارب
التكييفية** </span>**(Conditioning Trials)**<span dir="rtl">.</span>
<span dir="rtl">تعتمد هذه الخصائص على النوع، **نظام الاستجابة**</span>
**<span dir="rtl">(</span>Response
<span dir="rtl"></span>System<span dir="rtl">)
</span>**<span dir="rtl">الذي يتم ملاحظته، ومعلمات **التجارب التكييفية**
</span>**(Conditioning Trials)**<span dir="rtl">، ولكن في العديد من
التجارب مع حيوانات مختلفة وأنظمة استجابة مختلفة، تزداد شدة **الاستجابة
الشرطية** </span>**(CR)** <span dir="rtl">أو احتمال حدوث **استجابة
شرطية**</span> **(CR)** <span dir="rtl">مع اقتراب الوقت المتوقع لحدوث
**المحفز غير المشروط** </span>**(US)**<span dir="rtl">. على سبيل المثال،
في **التكييف الكلاسيكي**</span> **(Classical Conditioning)**
<span dir="rtl">لاستجابة غشاء الأرنب الغامض التي ذكرناها سابقًا، على مدار
**التجارب التكييفية** </span>**(Conditioning Trials)**<span dir="rtl">،
يقل التأخير من بداية **المحفز الشرطي**</span> **(CS)**
<span dir="rtl">إلى الوقت الذي يبدأ فيه غشاء العين الغامض بالتحرك عبر
العين مع مرور التجارب، وتزداد سعة هذا الإغلاق التوقعي تدريجيًا على مدار
الفاصل الزمني بين **المحفز الشرطي**</span> **(CS)**
<span dir="rtl">و**المحفز غير المشروط**</span> **(US)**
<span dir="rtl">حتى يصل الغشاء إلى الإغلاق الأقصى في الوقت المتوقع لحدوث
**المحفز غير المشروط** </span>**(US)**<span dir="rtl">.</span>
<span dir="rtl">إن توقيت وشكل هذه **الاستجابة الشرطية** </span>**(CR)**
<span dir="rtl">أمر بالغ الأهمية لأهميتها التكيفية—فإغلاق العين مبكرًا
جدًا يقلل من الرؤية (على الرغم من أن غشاء العين الغامض شفاف جزئيًا)، في
حين أن إغلاقها متأخرًا يكون قليل الفائدة في الحماية. إن التقاط ميزات
**الاستجابة الشرطية**</span> **(CR)** <span dir="rtl">مثل هذه يمثل تحديًا
للنماذج المتعلقة بـ **التكييف الكلاسيكي**</span>
**<span dir="rtl">(</span>Classical
<span dir="rtl"></span>Conditioning<span dir="rtl">)</span>**.

<span dir="rtl">لا يتضمن **نموذج التفاضل الزمني**</span> **(TD Model)**
<span dir="rtl">كجزء من تعريفه أي آلية لترجمة مسار الوقت الخاص بتنبؤ
**المحفز غير المشروط** </span>**(US)**<span dir="rtl">،</span>
$`v\hat{}(St,wt)`$<span dir="rtl">، إلى **ملف يمكن مقارنته بخصائص
الاستجابة الشرطية للحيوان** </span>**(CR)**<span dir="rtl">.</span>
<span dir="rtl">أبسط اختيار هو السماح لمسار الوقت الخاص بـ **الاستجابة
الشرطية المحاكية**</span> **(Simulated CR)** <span dir="rtl">بمساواة
مسار الوقت الخاص بتنبؤ **المحفز غير المشروط**
</span>**(US)**<span dir="rtl">.</span> <span dir="rtl">في هذه الحالة،
تعتمد ميزات **الاستجابات الشرطية المحاكية**</span> **(Simulated CRs)**
<span dir="rtl">وكيفية تغيرها على مدار **التجارب التكييفية**</span>
**(Conditioning Trials)** <span dir="rtl">فقط على **تمثيل
المحفزات**</span> **<span dir="rtl">(</span>Stimulus
<span dir="rtl"></span>Representation<span dir="rtl">)
</span>**<span dir="rtl">المختار وقيم **بارامترات النموذج**
</span>**(Model’s Parameters)** $`\alpha`$<span dir="rtl">،</span>
γ<span dir="rtl">، و</span>λ<span dir="rtl">.</span>

<span dir="rtl">يُظهر **الشكل**</span> **(Figure) 14.4**
<span dir="rtl">مسارات تنبؤ **المحفز غير المشروط**</span> **(US)**
<span dir="rtl">في نقاط زمنية مختلفة أثناء **التعليم**</span>
**(Learning)** <span dir="rtl">باستخدام **التمثيلات الثلاثة**</span>
**(Three Representations)** <span dir="rtl">الموضحة في **الشكل**
</span>**(Figure) 14.1**<span dir="rtl">.</span> <span dir="rtl">في هذه
المحاكاة، حدث **المحفز غير المشروط**</span> **(US)** <span dir="rtl">بعد
25 خطوة زمنية من بداية **المحفز الشرطي**
</span>**(CS)**<span dir="rtl">، وكانت القيم</span>
α=0.05<span dir="rtl">،</span> λ=0.95<span dir="rtl">،
و</span>γ=0.97<span dir="rtl">.</span>

<span dir="rtl">مع **تمثيل المركب المتسلسل الكامل**</span> **(CSC
Representation)** <span dir="rtl">(الشكل 14.4 يسارًا)، تزداد منحنى تنبؤ
**المحفز غير المشروط**</span> **(US Prediction)** <span dir="rtl">الذي
تم تشكيله بواسطة **نموذج التفاضل الزمني  
(**</span>**TD Model<span dir="rtl">) </span>**<span dir="rtl">بشكل أسي
طوال الفترة الزمنية بين **المحفز الشرطي**</span> **(CS)**
<span dir="rtl">و**المحفز غير المشروط** </span>**(US)**
<span dir="rtl">حتى يصل إلى الحد الأقصى تمامًا عندما يحدث **المحفز غير
المشروط**</span> **(US)** <span dir="rtl">(عند الخطوة الزمنية 25). هذه
الزيادة الأسية هي نتيجة **الخصم**</span> **(Discounting)**
<span dir="rtl">في قاعدة التعليم الخاصة بـ **نموذج التفاضل الزمني**
</span>**(TD Model)**<span dir="rtl">.</span>

<span dir="rtl">مع **تمثيل الوجود**</span> **(Presence Representation)**
<span dir="rtl">(الشكل 14.4 في الوسط)، يكون تنبؤ **المحفز غير
المشروط**</span> **(US Prediction)** <span dir="rtl">ثابتًا تقريبًا أثناء
وجود **المحفز**</span> **(Stimulus)** <span dir="rtl">لأنه لا يوجد سوى
وزن واحد أو **قوة ترابطية**</span> **(Associative Strength)**
<span dir="rtl">ليتم تعلمها لكل **محفز**
</span>**(Stimulus)**<span dir="rtl">. وبالتالي، لا يمكن لـ **نموذج
التفاضل الزمني**</span> **(TD Model)** <span dir="rtl">مع **تمثيل الوجود
(**</span>**Presence
<span dir="rtl"></span>Representation<span dir="rtl">)</span>**
<span dir="rtl">إعادة إنشاء العديد من ميزات توقيت **الاستجابة الشرطية**
</span>**(CR Timing)**<span dir="rtl">.</span>

<span dir="rtl">أما مع **تمثيل الميكرو محفزات**</span> **(MS
Representation)** <span dir="rtl">(الشكل 14.4 يمينًا)، فإن تطور تنبؤ
**المحفز غير المشروط**</span> **(US Prediction)** <span dir="rtl">الخاص
بـ **نموذج التفاضل الزمني**</span> **(TD Model)** <span dir="rtl">يكون
أكثر تعقيدًا. بعد 200 **تجربة** </span>**(Trials)**<span dir="rtl">، يكون
ملف التنبؤ تقريبًا مع منحنى تنبؤ **المحفز غير المشروط** </span>**(US
Prediction Curve)** <span dir="rtl">الذي تم إنتاجه باستخدام **تمثيل
المركب المتسلسل الكامل**</span> **<span dir="rtl">(</span>CSC
<span dir="rtl"></span>Representation<span dir="rtl">)</span>**.

<img src="./media/image174.png"
style="width:6.26806in;height:1.79167in" />

<span dir="rtl">الشكل 14.4: مسار تنبؤ المحفز غير المشروط</span> (US
Prediction) <span dir="rtl">على مدار الاكتساب لنموذج التفاضل
الزمني</span> (TD Model) <span dir="rtl">مع ثلاث تمثيلات مختلفة
للمحفزات</span> <span dir="rtl">(</span>Stimulus
Representations<span dir="rtl">)</span>. <span dir="rtl">اليسار: مع
تمثيل المركب المتسلسل الكامل</span> (Complete Serial Compound
(CSC))<span dir="rtl">، يزداد تنبؤ المحفز غير المشروط</span> (US
Prediction) <span dir="rtl">بشكل أسي خلال الفاصل الزمني، ويصل إلى ذروته
عند وقت حدوث المحفز غير المشروط</span> (US)<span dir="rtl">.</span>
<span dir="rtl">عند النهاية العظمى (التجربة 200)، يصل تنبؤ المحفز غير
المشروط</span> (US Prediction) <span dir="rtl">إلى شدة المحفز غير
المشروط</span> (US Intensity) <span dir="rtl">(1 في هذه المحاكاة).
الوسط: مع تمثيل الوجود</span> (Presence Representation)<span dir="rtl">،
يتقارب تنبؤ المحفز غير المشروط</span> <span dir="rtl">(</span>US
<span dir="rtl"></span>Prediction<span dir="rtl">) إلى مستوى شبه ثابت.
يتم تحديد هذا المستوى الثابت بواسطة شدة المحفز غير المشروط</span> (US
Intensity) <span dir="rtl">وطول الفاصل الزمني بين المحفز الشرطي والمحفز
غير المشروط</span> <span dir="rtl">(</span>CS–US
<span dir="rtl"></span>Interval<span dir="rtl">)</span>.
<span dir="rtl">اليمين: مع تمثيل الميكرو محفزات</span> (Microstimulus
Representation)<span dir="rtl">، عند النهاية العظمى، يقارب نموذج التفاضل
الزمني</span> (TD Model) <span dir="rtl">المسار الزمني المتزايد أسيًا
الموضح مع المركب المتسلسل الكامل</span> (CSC) <span dir="rtl">من خلال
التوليف الخطي لمختلف الميكرو محفزات</span>
(Microstimuli)<span dir="rtl">.</span> <span dir="rtl">تم التكييف مع
تغييرات طفيفة من</span> Learning & Behavior<span dir="rtl">، تقييم نموذج
التفاضل الزمني في التكييف الكلاسيكي</span> (Evaluating the TD Model of
Classical Conditioning)<span dir="rtl">، (المجلد 40، 2012، بواسطة</span>
E. A. Ludvig <span dir="rtl">و</span> R. S. Sutton
<span dir="rtl">و</span> E. J. Kehoe. <span dir="rtl">بإذن من</span>
Springer<span dir="rtl">).</span>

<span dir="rtl">منحنيات **تنبؤ المحفز غير المشروط**</span> **(US
Prediction Curves)** <span dir="rtl">الموضحة في **الشكل**
</span>**(Figure) <span dir="rtl"></span>14.4** <span dir="rtl">لم تكن
مصممة لتطابق بدقة **ملفات الاستجابة الشرطية**</span> **(CR Profiles)**
<span dir="rtl">كما تتطور خلال **التكييف**</span> **(Conditioning)**
<span dir="rtl">في أي تجربة معينة على الحيوانات، ولكنها توضح التأثير
القوي الذي يمارسه **تمثيل المحفزات**</span> **(Stimulus
Representation)** <span dir="rtl">على التنبؤات المستمدة من **نموذج
التفاضل الزمني** </span>**(TD Model)**<span dir="rtl">.</span>
<span dir="rtl">علاوة على ذلك، على الرغم من أنه يمكننا فقط ذكره هنا، فإن
كيفية تفاعل **تمثيل المحفزات**</span> **(Stimulus Representation)**
<span dir="rtl">مع **الخصم**</span> **(Discounting)**
<span dir="rtl">و**آثار الاستحقاق**</span> **(Eligibility Traces)**
<span dir="rtl">أمر مهم في تحديد خصائص **منحنيات تنبؤ المحفز غير
المشروط**</span> **(US Prediction Profiles)** <span dir="rtl">التي
ينتجها **نموذج التفاضل الزمني** </span>**(TD
Model)**<span dir="rtl">.</span>

<span dir="rtl">بُعد آخر يتجاوز ما يمكننا مناقشته هنا هو تأثير **آليات
توليد الاستجابة المختلفة**</span> **<span dir="rtl">(</span>Different
<span dir="rtl"></span>Response-generation
Mechanisms<span dir="rtl">)</span>** <span dir="rtl">التي تحول **تنبؤات
المحفز غير المشروط**</span> **<span dir="rtl">(</span>US
<span dir="rtl"></span>Predictions<span dir="rtl">)</span>**
<span dir="rtl">إلى **ملفات الاستجابة الشرطية** </span>**(CR
Profiles)**<span dir="rtl">؛ الملفات المعروضة في **الشكل**
</span>**(Figure) 14.4** <span dir="rtl">هي **منحنيات تنبؤ المحفز غير
المشروط "الخام"**</span> **<span dir="rtl">(</span>Raw US Prediction
Profiles<span dir="rtl">)</span>**. <span dir="rtl">حتى دون أي افتراض
خاص حول كيفية قيام دماغ الحيوان بإنتاج **استجابات ظاهرة**
</span>**(Overt Responses)** <span dir="rtl">من **تنبؤات المحفز غير
المشروط** </span>**(US Predictions)**<span dir="rtl">، فإن الملفات في
**الشكل**</span> **(Figure) 14.4** <span dir="rtl">لتمثيلات **المركب
المتسلسل الكامل**</span> **(CSC)** <span dir="rtl">و**الميكرو
محفزات**</span> **(MS)** <span dir="rtl">تزداد مع اقتراب وقت **المحفز
غير المشروط**</span> **(US)** <span dir="rtl">وتصل إلى الحد الأقصى عند
وقت **المحفز غير المشروط** </span>**(US)**<span dir="rtl">، كما يُرى في
العديد من تجارب **التكييف على الحيوانات**</span>
**<span dir="rtl">(</span>Animal Conditioning
<span dir="rtl"></span>Experiments<span dir="rtl">)</span>**.

<span dir="rtl">يُظهر **نموذج التفاضل الزمني** </span>**(TD
Model)**<span dir="rtl">، عند دمجه مع **تمثيلات محفزات محددة**</span>
**<span dir="rtl">(</span>Particular <span dir="rtl"></span>Stimulus
Representations<span dir="rtl">)</span>** <span dir="rtl">و**آليات توليد
الاستجابة**</span> **<span dir="rtl">(</span>Response-generation
<span dir="rtl"></span>Mechanisms<span dir="rtl">)</span>**<span dir="rtl">،
قدرة على تفسير مجموعة واسعة بشكل مدهش من الظواهر المرصودة في تجارب
**التكييف الكلاسيكي**</span> **(Classical Conditioning Experiments)**
<span dir="rtl">على الحيوانات، ولكنه بعيد كل البعد عن كونه نموذجًا
مثاليًا. لتوليد تفاصيل أخرى من **التكييف الكلاسيكي**</span>
**<span dir="rtl">(</span>Classical
<span dir="rtl"></span>Conditioning<span dir="rtl">)</span>**<span dir="rtl">،
يحتاج النموذج إلى التوسيع، ربما عن طريق إضافة عناصر مستندة إلى النموذج
وآليات لتغيير بعض **بارامتراته**</span> **(Parameters)**
<span dir="rtl">بشكل تكيفي.</span> **<span dir="rtl">النماذج
البايزية</span> <span dir="rtl">(</span>Bayesian
<span dir="rtl"></span>Models<span dir="rtl">)</span>**<span dir="rtl">،
على سبيل المثال، تعمل ضمن إطار احتمالي يتم فيه تعديل تقديرات الاحتمالات
بناءً على التجربة. جميع هذه النماذج تساهم بشكل مفيد في فهمنا لـ **التكييف
الكلاسيكي**</span> **<span dir="rtl">(</span>Classical
<span dir="rtl"></span>Conditioning<span dir="rtl">)</span>**.

<span dir="rtl">ربما تكون الميزة الأكثر بروزًا في **نموذج التفاضل
الزمني**</span> **(TD Model)** <span dir="rtl">هي أنه يعتمد على
**نظرية** </span>**(Theory)** <span dir="rtl">النظرية التي وصفناها في
هذا الكتاب تقترح تفسيرًا لما يحاول **الجهاز العصبي للحيوان**
</span>**(Animal's Nervous System)** <span dir="rtl">القيام به أثناء
**التكييف** </span>**(Conditioning)**<span dir="rtl">:</span>
<span dir="rtl">إنه يحاول تشكيل تنبؤات دقيقة طويلة الأمد، تتسق مع القيود
المفروضة من قبل كيفية تمثيل **المحفزات**</span> **(Stimuli)**
<span dir="rtl">وكيفية عمل **الجهاز العصبي**</span> **(Nervous
System)**. <span dir="rtl">بعبارة أخرى، تقترح **تفسيرًا معياريًا**</span>
**<span dir="rtl">(</span>Normative
<span dir="rtl"></span>Account<span dir="rtl">)
</span>**<span dir="rtl">لـ **التكييف الكلاسيكي**</span> **(Classical
Conditioning)** <span dir="rtl">حيث يكون **التنبؤ طويل الأمد**
</span>**(Long-term Prediction)**<span dir="rtl">، بدلاً من التنبؤ
الفوري، هو الميزة الرئيسية</span>.

<span dir="rtl">يُعد تطوير **نموذج التفاضل الزمني للتكييف
الكلاسيكي**</span> **<span dir="rtl">(</span>TD Model of Classical
<span dir="rtl"></span>Conditioning<span dir="rtl">)
</span>**<span dir="rtl">مثالاً على الحالات التي كان الهدف الصريح فيها هو
**نمذجة بعض التفاصيل** </span>**(Model Some Details)**
<span dir="rtl">لسلوك **تعلم الحيوانات** </span>**(Animal Learning
Behavior)**<span dir="rtl">.</span> <span dir="rtl">بالإضافة إلى وضعه
كخوارزمية، فإن **التعليم بالتفاضل الزمني**</span> **(TD Learning)**
<span dir="rtl">هو أيضًا أساس هذا النموذج للجوانب المختلفة لـ **التعليم
البيولوجي** </span>**(Biological Learning)**<span dir="rtl">.</span>
<span dir="rtl">كما نناقش في **الفصل**</span>**15**<span dir="rtl">،
اتضح أيضًا أن **التعليم بالتفاضل الزمني**</span> **(TD Learning)**
<span dir="rtl">يدعم نموذجًا مؤثرًا لنشاط **الخلايا العصبية المنتجة
للدوبامين** </span>**(Dopamine-producing Neurons)**<span dir="rtl">، وهي
مادة كيميائية في دماغ الثدييات تشارك بعمق في **معالجة المكافآت**
</span>**(Reward Processing)**<span dir="rtl">.</span>
<span dir="rtl">هذه هي الحالات التي تتلاقى فيها **نظرية التعليم
المعزز**</span> **(Reinforcement Learning Theory)** <span dir="rtl">بشكل
دقيق مع البيانات السلوكية والعصبية للحيوانات</span>.

<span dir="rtl">ننتقل الآن إلى النظر في التوافقات بين **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">وسلوك
الحيوانات في تجارب **التكييف الأداتي** </span>**(Instrumental
Conditioning Experiments)**<span dir="rtl">، وهو النوع الرئيسي الآخر من
التجارب المخبرية التي يدرسها علماء **تعلم الحيوانات**</span>
**<span dir="rtl">(</span>Animal Learning
<span dir="rtl"></span>Psychologists<span dir="rtl">)</span>**.

**<u>14.3 <span dir="rtl">التكييف الأداتي</span> (Instrumental
Conditioning)</u>**

<span dir="rtl">في تجارب **التكييف الأداتي** </span>**(Instrumental
Conditioning)**<span dir="rtl">، يعتمد **التعليم**</span> **(Learning)**
<span dir="rtl">على عواقب السلوك: تقديم **محفز معزز**</span>
**(Reinforcing Stimulus)** <span dir="rtl">يعتمد على ما يفعله
**الحيوان** </span>**(Animal)**<span dir="rtl">.</span>
<span dir="rtl">في المقابل، في تجارب **التكييف الكلاسيكي**
</span>**(Classical Conditioning)**<span dir="rtl">، يتم تقديم **المحفز
المعزز**</span> **(Reinforcing Stimulus)** <span dir="rtl">وهو **المحفز
غير المشروط** </span>**(US)** <span dir="rtl">بشكل مستقل عن سلوك
**الحيوان** </span>**(Animal)**<span dir="rtl">.</span>
<span dir="rtl">يُعتبر **التكييف الأداتي**</span> **(Instrumental
Conditioning)** <span dir="rtl">عادةً مكافئًا لـ **التكييف الإجرائي**
</span>**(Operant Conditioning)**<span dir="rtl">، وهو المصطلح الذي قدمه
**ب. ف. سكينر  
(**</span>**B. F. Skinner<span dir="rtl">)</span>**
<span dir="rtl">(</span>1938<span dir="rtl">، 1963)</span>
<span dir="rtl">للإشارة إلى التجارب التي يعتمد فيها **التعزيز**
</span>**(Reinforcement)** <span dir="rtl">على السلوك. ومع ذلك، تختلف
التجارب والنظريات بين من يستخدمون هذين المصطلحين في عدد من الجوانب،
سنناقش بعضها أدناه. سنستخدم حصريًا مصطلح **التكييف الأداتي**
</span>**(Instrumental Conditioning)** <span dir="rtl">للإشارة إلى
التجارب التي يعتمد فيها **التعزيز** </span>**(Reinforcement)**
<span dir="rtl">على **السلوك**
</span>**(Behavior)**<span dir="rtl">.</span> <span dir="rtl">تعود جذور
**التكييف الأداتي (**</span>**Instrumental
<span dir="rtl"></span>Conditioning<span dir="rtl">)</span>**
<span dir="rtl">إلى التجارب التي أجراها عالم النفس الأمريكي **إدوارد
ثورندايك**</span> **<span dir="rtl">(</span>Edward
<span dir="rtl"></span>Thorndike<span dir="rtl">)
</span>**<span dir="rtl">قبل مئة عام من نشر الطبعة الأولى من هذا
الكتاب</span>.

<img src="./media/image175.png" style="width:3.46736in;height:2.55in" />

<span dir="rtl">لاحظ **ثورندايك**</span> **(Thorndike)**
<span dir="rtl">سلوك **القطط**</span> **(Cats)** <span dir="rtl">عندما
وُضعت في "صناديق الألغاز</span> (Puzzle Boxes)"<span dir="rtl">، مثل
الصندوق الموضح على اليمين، والتي يمكنها الهروب منها عبر القيام بأفعال
مناسبة. على سبيل المثال، يمكن لقطة فتح باب أحد الصناديق عن طريق أداء
سلسلة من ثلاث حركات منفصلة: الضغط على منصة في الجزء الخلفي من الصندوق،
وسحب خيط من خلال مخالبها، ودفع قضيب لأعلى أو لأسفل. عندما وُضعت القطة
لأول مرة في صندوق الألغاز، مع وجود الطعام مرئيًا في الخارج، أظهرت جميع
القطط تقريبًا باستثناء القليل منها "علامات واضحة على عدم الراحة" ونشاطًا
قويًا للغاية "لتحاول غريزيًا الهروب من
الاحتجاز"(</span>Thorndike<span dir="rtl">، 1898)</span>.

<span dir="rtl">في تجاربه مع قطط مختلفة وصناديق تحتوي على آليات هروب
مختلفة، سجل **ثورندايك** </span>**(Thorndike)** <span dir="rtl">الزمن
الذي استغرقته كل قطة للهروب عبر تجارب متعددة في كل صندوق. لاحظ أن الزمن
انخفض بشكل شبه دائم مع التجارب المتتالية، على سبيل المثال، من 300 ثانية
إلى 6 أو 7 ثوانٍ. وقد وصف سلوك القطط في صندوق الألغاز بهذا الشكل</span>:

<span dir="rtl"></span>"<span dir="rtl">القطة التي تخدش كل أنحاء الصندوق
في صراعها المندفع ستخدش على الأرجح الخيط أو الحلقة أو الزر بطريقة تؤدي
إلى فتح الباب. وبشكل تدريجي، سيتم القضاء على جميع الدوافع غير الناجحة،
وسيتم تثبيت الدافع المحدد الذي يؤدي إلى الفعل الناجح بفعل اللذة الناتجة.
حتى أنه، بعد العديد من المحاولات، ستقوم القطة، عندما توضع في الصندوق،
بخدش الزر أو الحلقة بطريقة محددة فورًا". (ثورندايك، 1898، ص. 13)</span>

<span dir="rtl">هذه التجارب وغيرها (بما في ذلك تجارب أجريت على الكلاب،
الكتاكيت، القرود، وحتى الأسماك) قادت **ثورندايك**</span> **(Thorndike)**
<span dir="rtl">إلى صياغة عدد من "قوانين</span>"
**<span dir="rtl">التعليم</span> (Learning)**<span dir="rtl">، كان
الأكثر تأثيرًا بينها هو **قانون الأثر** </span>**(Law of
Effect)**<span dir="rtl">، وهو نسخة منه اقتبسناها في **الفصل**</span>
**1** <span dir="rtl">(صفحة 15). يصف هذا القانون ما يُعرف عمومًا بـ
**التعليم من خلال المحاولة والخطأ** </span>**(Learning by Trial and
Error)**<span dir="rtl">. كما ذكرنا في
**الفصل**</span>**1**<span dir="rtl">، أثارت العديد من جوانب **قانون
الأثر**</span> **(Law of Effect)** <span dir="rtl">جدلاً، وتم تعديل
تفاصيله على مر السنين. ومع ذلك، يظل هذا القانون بشكل أو بآخر يعبر عن
مبدأ دائم في **التعليم** </span>**(Learning)**<span dir="rtl">.</span>

<span dir="rtl">تتوافق الميزات الأساسية لخوارزميات **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">مع ميزات
**التعليم عند الحيوانات**</span> **(Animal Learning)**
<span dir="rtl">التي يصفها **قانون الأثر** </span>**(Law of
Effect)**<span dir="rtl">.</span> <span dir="rtl">أولاً، خوارزميات
**التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">هي **انتقائية**
</span>**(Selectional)**<span dir="rtl">، بمعنى أنها تحاول عدة بدائل
وتختار من بينها من خلال مقارنة عواقبها. ثانيًا، خوارزميات **التعليم
المعزز** </span>**(Reinforcement Learning)** <span dir="rtl">هي
**ترابطية** </span>**(Associative)**<span dir="rtl">، بمعنى أن البدائل
التي يتم العثور عليها من خلال الانتقاء ترتبط بمواقف معينة، أو **حالات**
</span>**(States)**<span dir="rtl">، لتشكيل **سياسة**</span>
**(Policy)** <span dir="rtl">العميل. مثل **التعليم**</span>
**(Learning)** <span dir="rtl">الموصوف في **قانون الأثر** </span>**(Law
of Effect)**<span dir="rtl">، فإن **التعليم المعزز**
</span>**(Reinforcement Learning)** <span dir="rtl">ليس مجرد عملية إيجاد
**الأفعال**</span> **(Actions)** <span dir="rtl">التي تنتج مكافآت كبيرة،
بل أيضًا ربط هذه **الأفعال**</span> **(Actions)** <span dir="rtl">بـ
**الحالات**</span> **(Situations)** <span dir="rtl">أو **الحالات**
</span>**(States)**<span dir="rtl">. استخدم **ثورندايك**</span>
**(Thorndike)** <span dir="rtl">عبارة "التعليم من خلال الاختيار والربط"
(</span>Learning by <span dir="rtl"></span>Selecting and Connecting)
(Hilgard, 1956<span dir="rtl">).</span> <span dir="rtl">يُعد **الانتقاء
الطبيعي**</span> **<span dir="rtl">(</span>Natural
<span dir="rtl"></span>Selection<span dir="rtl">)</span>**
<span dir="rtl">في التطور مثالًا رئيسيًا على عملية **انتقائية**
</span>**(Selectional)**<span dir="rtl">، ولكنه ليس **ترابطيًا**
</span>**(Associative)** <span dir="rtl">(على الأقل كما يُفهم عادةً)؛
بينما **التعليم الخاضع للإشراف**</span>
**<span dir="rtl">(</span>Supervised
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**
<span dir="rtl">هو **ترابطي** </span>**(Associative)**<span dir="rtl">،
لكنه ليس **انتقائيًا**</span> **(Selectional)** <span dir="rtl">لأنه
يعتمد على تعليمات مباشرة تخبر العميل بكيفية تغيير سلوكه</span>.

<span dir="rtl">بمصطلحات حاسوبية، يصف **قانون الأثر**</span> **(Law of
Effect)** <span dir="rtl">طريقة أولية للجمع بين **البحث**
</span>**(Search)** <span dir="rtl">و**الذاكرة**
</span>**(Memory)**<span dir="rtl">:</span> <span dir="rtl">البحث في شكل
تجربة وانتقاء بين عدة **أفعال**</span> **(Actions)** <span dir="rtl">في
كل **حالة** </span>**(Situation)**<span dir="rtl">، و**الذاكرة**</span>
**(Memory)** <span dir="rtl">في شكل **ترابطات**</span>
**(Associations)** <span dir="rtl">تربط بين **الحالات**
</span>**(Situations)** <span dir="rtl">و**الأفعال**</span>
**(Actions)** <span dir="rtl">التي ثبت حتى الآن أنها تعمل بشكل أفضل في
تلك **الحالات** </span>**(Situations)**<span dir="rtl">.</span>
<span dir="rtl">يعد **البحث**</span> **(Search)**
<span dir="rtl">و**الذاكرة**</span> **(Memory)** <span dir="rtl">مكونات
أساسية في جميع خوارزميات **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">، سواء كانت **الذاكرة**</span> **(Memory)**
<span dir="rtl">تأخذ شكل **سياسة**</span> **(Policy)**
<span dir="rtl">العميل، أو **دالة القيمة** </span>**(Value
Function)**<span dir="rtl">، أو **نموذج البيئة**</span>
**<span dir="rtl">(</span>Environment Model<span dir="rtl">)</span>**.

<span dir="rtl">يعني احتياج خوارزمية **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">إلى **البحث**</span>
**(Search)** <span dir="rtl">أنه يتعين عليها **الاستكشاف**</span>
**(Explore)** <span dir="rtl">بطريقة ما</span>.

<span dir="rtl">من بين أبرز الباحثين في **تعلم الحيوانات**</span>
**(Animal Learning)** <span dir="rtl">الذين تأثروا بـ **قانون الأثر  
(**</span>**Law <span dir="rtl"></span>of Effect<span dir="rtl">)
</span>**<span dir="rtl">كان **كلارك هال**</span> **(Clark Hull)**
<span dir="rtl">(مثل،</span> Hull, 1943<span dir="rtl">)</span>
<span dir="rtl">و**ب. ف. سكينر  
(**</span>**B. F. Skinner**<span dir="rtl">) مثل، (</span>Skinner,
1938<span dir="rtl">).</span> <span dir="rtl">كان جوهر أبحاثهم هو فكرة
اختيار **السلوك** </span>**(Behavior)** <span dir="rtl">بناءً على عواقبه.
يشترك **التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">في بعض الميزات مع نظرية **هال**
</span>**(Hull)**<span dir="rtl">، التي تضمنت آليات شبيهة بـ **آثار
الاستحقاق**</span> **<span dir="rtl">(</span>Eligibility
<span dir="rtl"></span>Traces<span dir="rtl">)
</span>**<span dir="rtl">و**التعزيز الثانوي**</span> **(Secondary
Reinforcement)** <span dir="rtl">لتفسير القدرة على **التعليم**</span>
**(Learning)** <span dir="rtl">عندما يكون هناك فاصل زمني كبير بين
**الإجراء**</span> **(Action)** <span dir="rtl">و**المحفز المعزز**
</span>**(Reinforcing Stimulus)** <span dir="rtl">الناتج عنه (انظر
**الفصل** </span>**14.4**<span dir="rtl">)</span>. <span dir="rtl">كما
لعبت العشوائية دورًا في نظرية **هال**</span> **(Hull)**
<span dir="rtl">من خلال ما أسماه "التذبذب السلوكي</span> (Behavioral
Oscillation)" <span dir="rtl">لإدخال **السلوك الاستكشافي**</span>
**<span dir="rtl">(</span>Exploratory
<span dir="rtl"></span>Behavior<span dir="rtl">)</span>**.

<span dir="rtl">لم يكن **سكينر**</span> **(Skinner)**
<span dir="rtl">ملتزمًا بالكامل بجانب **الذاكرة**</span> **(Memory)**
<span dir="rtl">في **قانون الأثر**</span> **<span dir="rtl">(</span>Law
of <span dir="rtl"></span>Effect<span dir="rtl">)</span>**.
<span dir="rtl">فبينما كان يعارض فكرة الروابط الترابطية، ركز بدلاً من ذلك
على **الانتقاء** </span>**(Selection)** <span dir="rtl">من **السلوك
المنبعث تلقائيًا** </span>**(Spontaneously-emitted
Behavior)**<span dir="rtl">.</span> <span dir="rtl">قدم **سكينر**
</span>**(Skinner)** <span dir="rtl">مصطلح "التكييف الإجرائي</span>
(Operant Conditioning) <span dir="rtl">لتسليط الضوء على الدور الرئيسي
لآثار **الإجراء**</span> **(Action)** <span dir="rtl">على **بيئة
الحيوان** </span>**(Animal's Environment)**<span dir="rtl">.</span>
<span dir="rtl">على عكس تجارب **ثورندايك** </span>**(Thorndike)**
<span dir="rtl">وآخرين، التي تتكون من تسلسلات من **التجارب المنفصلة**
</span>**(Separate Trials)**<span dir="rtl">، سمحت تجارب **التكييف
الإجرائي**</span> **(Operant Conditioning)** <span dir="rtl">التي أجراها
**سكينر** </span>**(Skinner)** <span dir="rtl">لمواضيع الحيوانات بالتصرف
لفترات زمنية ممتدة دون انقطاع. اخترع **سكينر**</span> **(Skinner)**
<span dir="rtl">غرفة التكييف الإجرائي، التي تعرف الآن بـ "صندوق
سكينر</span> (Skinner Box)"<span dir="rtl">، وأبسط نسخة منها تحتوي على
رافعة أو مفتاح يمكن للحيوان الضغط عليه للحصول على مكافأة، مثل الطعام أو
الماء، يتم تسليمها وفقًا لقانون محدد جيدًا، يسمى **جدول التعزيز**
</span>**(Reinforcement Schedule)**<span dir="rtl">.</span>
<span dir="rtl">من خلال تسجيل العدد التراكمي للضغطات على الرافعة كدالة
للوقت، تمكن **سكينر**</span> **(Skinner)** <span dir="rtl">وأتباعه من
دراسة تأثير **جداول التعزيز المختلفة**</span> **(Different Reinforcement
Schedules)** <span dir="rtl">على معدل ضغط الحيوان على الرافعة. لم يتم
تطوير **نمذجة النتائج**</span> **(Modeling Results)** <span dir="rtl">من
تجارب كهذه باستخدام مبادئ **التعليم المعزز**</span> **(Reinforcement
Learning)** <span dir="rtl">التي نقدمها في هذا الكتاب بشكل جيد، ولكننا
نذكر بعض الاستثناءات في قسم **الملاحظات الببليوغرافية
والتاريخية**</span> **<span dir="rtl">(</span>Bibliographic and
<span dir="rtl"></span>Historical Remarks<span dir="rtl">)
</span>**<span dir="rtl">في نهاية هذا **الفصل**</span>.

<span dir="rtl">إحدى مساهمات **سكينر**</span> **(Skinner)**
<span dir="rtl">الأخرى كانت نتيجة إدراكه لفعالية **تدريب
الحيوان**</span> **<span dir="rtl">(</span>Training
<span dir="rtl"></span>an Animal<span dir="rtl">)
</span>**<span dir="rtl">من خلال تعزيز **التقريبات المتتالية**</span>
**(Successive Approximations)** <span dir="rtl">للسلوك المطلوب، وهي
عملية أطلق عليها **التشكيل**
</span>**(Shaping)**<span dir="rtl">.</span> <span dir="rtl">على الرغم
من أن هذه التقنية كانت مستخدمة من قبل آخرين، بما في ذلك **سكينر**</span>
**(Skinner)** <span dir="rtl">نفسه، إلا أن أهميتها أصبحت واضحة له عندما
كان هو وزملاؤه يحاولون تدريب حمامة على لعب البولينغ عن طريق دفع كرة
خشبية بمنقارها. بعد الانتظار لفترة طويلة دون رؤية أي ضربة يمكنهم
تعزيزها، قرروا تعزيز أي استجابة لها أدنى تشابه مع الضربة ربما في البداية
مجرد النظر إلى الكرة—ثم اختيار الاستجابات التي تقترب بشكل أكبر من الشكل
النهائي. كانت النتيجة مدهشة لهم. في غضون دقائق قليلة، كانت الكرة تتقافز
على جدران الصندوق وكأن الحمامة كانت بطلة في لعبة الاسكواش.</span>
(Skinner, 1958, p. 94)

<span dir="rtl">لم تتعلم الحمامة سلوكًا غير مألوف للحمامات فقط، بل تعلمته
بسرعة من خلال عملية تفاعلية يتغير فيها سلوكها و**شروط التعزيز**</span>
**(Reinforcement Contingencies)** <span dir="rtl">استجابة لبعضهما البعض.
قارن **سكينر**</span> **(Skinner)** <span dir="rtl">عملية تعديل **شروط
التعزيز**</span> **(Reinforcement Contingencies)** <span dir="rtl">بعمل
نحات يقوم بتشكيل الصلصال إلى الشكل المطلوب. يعد **التشكيل**</span>
**(Shaping)** <span dir="rtl">تقنية قوية لأنظمة **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">الحاسوبية
أيضًا. عندما يكون من الصعب على **العميل**</span> **(Agent)**
<span dir="rtl">الحصول على أي إشارة مكافأة غير صفرية على الإطلاق، سواء
بسبب ندرة المواقف المعززة أو عدم الوصول إليها نظرًا للسلوك الأولي، فإن
البدء بمشكلة أسهل وزيادة صعوبتها تدريجيًا مع تعلم **العميل**</span>
**(Agent)** <span dir="rtl">يمكن أن يكون استراتيجية فعالة، وأحيانًا لا
غنى عنها</span>.

<span dir="rtl">مفهوم من علم النفس ذو صلة خاصة في سياق **التكييف
الأداتي** </span>**(Instrumental Conditioning)** <span dir="rtl">هو
**الدافعية** </span>**(Motivation)**<span dir="rtl">، والتي تشير إلى
العمليات التي تؤثر على اتجاه وقوة، أو حيوية، **السلوك**
</span>**(Behavior)**<span dir="rtl">.</span> <span dir="rtl">على سبيل
المثال، كانت **قطط ثورندايك**</span> **(Thorndike's Cats)**
<span dir="rtl">مدفوعة للهروب من صناديق الألغاز لأنها كانت ترغب في
الحصول على الطعام الموجود خارج الصندوق مباشرة. كان تحقيق هذا الهدف مجزيًا
لها وعزز **الأفعال**</span> **(Actions)** <span dir="rtl">التي سمحت لها
بالهروب. من الصعب ربط مفهوم **الدافعية**
</span>**(Motivation)**<span dir="rtl">، الذي له أبعاد عديدة، بطريقة
دقيقة مع منظور **التعليم المعزز** </span>**(Reinforcement Learning)**
<span dir="rtl">الحسابي، ولكن هناك روابط واضحة مع بعض أبعاده</span>.

<span dir="rtl">من ناحية، فإن **إشارة المكافأة**</span> **(Reward
Signal)** <span dir="rtl">الخاصة بـ **عميل التعليم المعزز**
</span>**(Reinforcement Learning Agent)** <span dir="rtl">هي الأساس
لدافعيته: العميل مدفوع لزيادة إجمالي المكافأة التي يحصل عليها على المدى
الطويل. إذن، أحد الجوانب الرئيسية للدافعية هو ما يجعل تجربة العميل
مجزية. في **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">، تعتمد إشارات المكافأة على **حالة بيئة
العميل**</span> **(State of the Agent’s Environment)**
<span dir="rtl">وعلى **أفعال العميل** </span>**(Agent’s
Actions)**<span dir="rtl">. علاوة على ذلك، كما أشرنا في **الفصل**
</span>**1**<span dir="rtl">، فإن **حالة بيئة العميل**</span>
**<span dir="rtl">(</span>State of the Agent’s
<span dir="rtl"></span>Environment<span dir="rtl">)
</span>**<span dir="rtl">لا تشمل فقط المعلومات المتعلقة بما هو خارجي
بالنسبة للجهاز، مثل الكائن الحي أو الروبوت الذي يحتوي على العميل، ولكن
أيضًا ما هو داخلي لهذا الجهاز. بعض مكونات الحالة الداخلية تتوافق مع ما
يسميه علماء النفس **الحالة الدافعية للحيوان**</span>
**<span dir="rtl">(</span>Animal’s Motivational
<span dir="rtl"></span>State<span dir="rtl">)</span>**<span dir="rtl">،
والتي تؤثر على ما هو مجزٍ للحيوان. على سبيل المثال، سيشعر الحيوان بمكافأة
أكبر عند تناول الطعام عندما يكون جائعًا مقارنةً بوقت إنهاء وجبة مشبعة
للتو. مفهوم **الاعتماد على الحالة** </span>**(State Dependence)**
<span dir="rtl">واسع بما يكفي للسماح بالعديد من أنواع التأثيرات التي
تعدل إنتاج إشارات المكافأة</span>.

<span dir="rtl">توفر **دوال القيمة**</span> **(Value Functions)**
<span dir="rtl">رابطًا آخر لمفهوم **الدافعية**</span> **(Motivation)**
<span dir="rtl">كما يفهمها علماء النفس. إذا كان الدافع الأساسي لاختيار
**الإجراء**</span> **(Action)** <span dir="rtl">هو الحصول على أكبر قدر
ممكن من المكافآت، فإن **عميل التعليم المعزز**</span> **(Reinforcement
Learning Agent)** <span dir="rtl">الذي يختار **الأفعال**</span>
**(Actions)** <span dir="rtl">باستخدام **دالة القيمة** </span>**(Value
Function)**<span dir="rtl">، يكون لديه دافع أقرب وهو الارتفاع في تدرج
**دالة القيمة الخاصة به** </span>**(Value Function)**<span dir="rtl">،
أي اختيار **الأفعال**</span> **(Actions)** <span dir="rtl">التي يُتوقع أن
تؤدي إلى **الحالات**</span> **(States)** <span dir="rtl">التالية الأعلى
قيمة (أو ما يعادل ذلك، اختيار **الأفعال** </span>**(Actions)**
<span dir="rtl">ذات أعلى قيم فعل). بالنسبة لهؤلاء العملاء، تعتبر **دوال
القيمة**</span> **(Value Functions)** <span dir="rtl">القوة الدافعة
الرئيسية التي تحدد اتجاه **سلوكهم**
</span>**(Behavior)**<span dir="rtl">.</span>

<span dir="rtl">جانب آخر من **الدافعية**</span> **(Motivation)**
<span dir="rtl">هو أن **الحالة الدافعية للحيوان**</span>
**<span dir="rtl">(</span>Animal’s <span dir="rtl"></span>Motivational
State<span dir="rtl">) </span>**<span dir="rtl">لا تؤثر فقط على
**التعليم** </span>**(Learning)**<span dir="rtl">، ولكنها تؤثر أيضًا على
قوة أو حيوية **السلوك**</span> **(Behavior)** <span dir="rtl">بعد
**التعليم** </span>**(Learning)**<span dir="rtl">.</span>
<span dir="rtl">على سبيل المثال، بعد تعلم إيجاد الطعام في صندوق الهدف في
المتاهة، سيجري الجرذ الجائع بشكل أسرع إلى صندوق الهدف مقارنةً بجرذ ليس
جائعًا. هذا الجانب من **الدافعية**</span> **(Motivation)**
<span dir="rtl">لا يرتبط بسهولة مع إطار العمل الخاص بـ **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">الذي نقدمه
هنا، ولكن في قسم **الملاحظات الببليوغرافية والتاريخية**</span>
**(Bibliographical and Historical Remarks)** <span dir="rtl">في نهاية
هذا **الفصل** </span>**(Chapter)**<span dir="rtl">، نشير إلى عدة منشورات
تقترح نظريات حول **حيوية السلوك**</span> **(Behavioral Vigor)**
<span dir="rtl">استنادًا إلى **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">.</span>

<span dir="rtl">ننتقل الآن إلى موضوع **التعليم**</span> **(Learning)**
<span dir="rtl">عندما تحدث **المحفزات المعززة**</span>
**<span dir="rtl">(</span>Reinforcing
<span dir="rtl"></span>Stimuli<span dir="rtl">)
</span>**<span dir="rtl">بعد فترة طويلة من الأحداث التي تعززها. تتوافق
الآليات المستخدمة في خوارزميات **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">لتمكين **التعليم**</span>
**(Learning)** <span dir="rtl">مع **التعزيز المتأخر** </span>**(Delayed
Reinforcement)** <span dir="rtl">مثل **آثار الاستحقاق**</span>
**(Eligibility Traces)** <span dir="rtl">و**التعليم بالتفاضل الزمني**
</span>**(TD Learning)** <span dir="rtl">بشكل وثيق مع فرضيات علماء النفس
حول كيفية تعلم الحيوانات في ظل هذه الظروف</span>.

**<u>14.4 <span dir="rtl">التعزيز المتأخر</span> (Delayed
Reinforcement)</u>**

**<span dir="rtl">قانون الأثر</span> (Law of Effect)**
<span dir="rtl">يتطلب تأثيرًا رجعيًا على الروابط، وقد واجه بعض النقاد
الأوائل للقانون صعوبة في تصور كيفية تأثير الحاضر على شيء حدث في الماضي.
ازداد هذا القلق بسبب حقيقة أن **التعليم**</span> **(Learning)**
<span dir="rtl">يمكن أن يحدث حتى عندما يكون هناك تأخير كبير بين
**الإجراء** </span>**(Action)** <span dir="rtl">والمكافأة أو العقوبة
الناتجة. وبالمثل، في **التكييف الكلاسيكي** </span>**(Classical
Conditioning)**<span dir="rtl">، يمكن أن يحدث **التعليم**</span>
**(Learning)** <span dir="rtl">عندما يتبع **بداية المحفز غير
المشروط**</span> **(US Onset)** **<span dir="rtl">نهاية المحفز
الشرطي</span> (CS Offset)** <span dir="rtl">بفاصل زمني غير ضئيل. نسمي
هذا **مشكلة التعزيز المتأخر**</span> **<span dir="rtl">(</span>Problem
<span dir="rtl"></span>of Delayed
Reinforcement<span dir="rtl">)</span>**<span dir="rtl">، وهي مرتبطة بما
أطلق عليه **مينسكي**</span> **(Minsky) <span dir="rtl">عام 1961</span>**
<span dir="rtl"></span>"<span dir="rtl">مشكلة تخصيص الفضل</span>
(Credit-assignment Problem) <span dir="rtl">لأنظمة التعليم": كيف توزع
الفضل في النجاح بين العديد من القرارات التي قد تكون ساهمت في تحقيقه؟
تشمل خوارزميات **التعليم المعزز** </span>**(Reinforcement Learning)**
<span dir="rtl">المقدمة في هذا الكتاب آليتين أساسيتين لمعالجة هذه
المشكلة. الأولى هي استخدام **آثار الاستحقاق** </span>**(Eligibility
Traces)**<span dir="rtl">، والثانية هي استخدام **طرق التفاضل
الزمني**</span> **(TD Methods)** <span dir="rtl">لتعلم **دوال
القيمة**</span> **(Value Functions)** <span dir="rtl">التي توفر تقييمات
شبه فورية للأفعال (في مهام مثل تجارب التكييف الأداتي) أو التي توفر أهداف
تنبؤ فورية (في مهام مثل تجارب التكييف الكلاسيكي). كلا هاتين الطريقتين
تتوافقان مع آليات مماثلة مقترحة في نظريات **تعلم الحيوانات**
</span>**(Animal Learning)**<span dir="rtl">.</span>

<span dir="rtl">أشار **بافلوف**</span> **(Pavlov) <span dir="rtl">عام
1927</span>** <span dir="rtl">إلى أن كل **محفز**</span> **(Stimulus)**
<span dir="rtl">يجب أن يترك أثرًا في **الجهاز العصبي**</span> **(Nervous
System)** <span dir="rtl">يستمر لبعض الوقت بعد انتهاء **المحفز**</span>
**(Stimulus)**<span dir="rtl">، واقترح أن **آثار المحفزات**</span>
**(Stimulus Traces)** <span dir="rtl">تجعل **التعليم**</span>
**(Learning)** <span dir="rtl">ممكنًا عندما يكون هناك فجوة زمنية بين
**نهاية المحفز الشرطي**</span> **(CS Offset)** <span dir="rtl">و**بداية
المحفز غير المشروط** </span>**(US Onset)**<span dir="rtl">.</span>
<span dir="rtl">حتى يومنا هذا، يُطلق على **التكييف**</span>
**(Conditioning)** <span dir="rtl">في ظل هذه الظروف **التكييف
الأثري**</span> **<span dir="rtl">(</span>Trace
<span dir="rtl"></span>Conditioning<span dir="rtl">)
</span>**<span dir="rtl">(صفحة 344). بافتراض بقاء أثر **المحفز
الشرطي**</span> **(CS)** <span dir="rtl">عندما يصل **المحفز غير
المشروط** </span>**(US)**<span dir="rtl">، يحدث **التعليم**</span>
**(Learning)** <span dir="rtl">من خلال الحضور المتزامن للأثر و**المحفز
غير المشروط** </span>**(US)**<span dir="rtl">.</span>
<span dir="rtl">نناقش بعض المقترحات لآليات الأثر في **الجهاز العصبي**
</span>**(Nervous System)** <span dir="rtl">في **الفصل**
</span>**15**<span dir="rtl">.</span>

<span dir="rtl">تم اقتراح **آثار المحفزات**</span> **(Stimulus Traces)**
<span dir="rtl">أيضًا كوسيلة لربط الفاصل الزمني بين **الأفعال**</span>
**(Actions)** <span dir="rtl">والمكافآت أو العقوبات الناتجة في **التكييف
الأداتي** </span>**(Instrumental Conditioning)** <span dir="rtl">على
سبيل المثال، في نظرية **هال**</span> **(Hull)** <span dir="rtl">المؤثرة
في **التعليم** </span>**(Learning)**<span dir="rtl">، كانت "آثار
المحفزات الكبيرة</span> (Molar Stimulus Traces)" <span dir="rtl">مسؤولة
عن ما أسماه **تدرج الهدف للحيوان**</span>
**<span dir="rtl">(</span>Animal’s <span dir="rtl"></span>Goal
Gradient<span dir="rtl">)</span>**<span dir="rtl">، وهو وصف لكيفية
انخفاض القوة القصوى للاستجابة المكتسبة أداتيًا مع زيادة تأخير
التعزيز</span> (Hull, 1932, 1943)<span dir="rtl">.</span>
<span dir="rtl">افترض **هال**</span> **(Hull)** <span dir="rtl">أن
**أفعال الحيوان**</span> **<span dir="rtl">(</span>Animal’s
<span dir="rtl"></span>Actions<span dir="rtl">)
</span>**<span dir="rtl">تترك **محفزات داخلية**</span> **(Internal
Stimuli)** <span dir="rtl">تتلاشى آثارها بشكل أسي كدالة للوقت منذ القيام
بالفعل. بالنظر إلى بيانات **تعلم الحيوانات**</span> **(Animal
Learning)** <span dir="rtl">المتاحة في ذلك الوقت، افترض أن الآثار تتلاشى
فعليًا إلى الصفر بعد 30 إلى 40 ثانية</span>.

<span dir="rtl">تشبه **آثار الاستحقاق**</span> **(Eligibility Traces)**
<span dir="rtl">المستخدمة في الخوارزميات الموصوفة في هذا الكتاب **آثار
هال** </span>**(Hull’s Traces)**<span dir="rtl">:</span>
<span dir="rtl">فهي آثار متلاشية للزيارات السابقة **للحالات**</span>
**(State Visitations)** <span dir="rtl">أو **لأزواج
الحالة–الفعل**</span> **(State–Action Pairs)** <span dir="rtl">السابقة.
تم تقديم **آثار الاستحقاق**</span> **<span dir="rtl">(</span>Eligibility
<span dir="rtl"></span>Traces<span dir="rtl">)
</span>**<span dir="rtl">من قبل **كلوبف**</span> **(Klopf)
<span dir="rtl">عام 1972</span>** <span dir="rtl">في نظريته العصبية التي
اعتبر فيها أنها آثار ممتدة زمنيًا للنشاط السابق عند **المشابك العصبية**
</span>**(Synapses)**<span dir="rtl">، وهي الروابط بين **الخلايا
العصبية** </span>**(Neurons)**<span dir="rtl">.</span>
<span dir="rtl">آثار **كلوبف**</span> **(Klopf’s Traces)**
<span dir="rtl">أكثر تعقيدًا من الآثار المتلاشية أسيًا التي تستخدمها
خوارزمياتنا، وسنناقش هذا بشكل أكبر عند تناول نظريته في **الفصل**
</span>**15.9<span dir="rtl">.</span>**

<span dir="rtl">لتفسير **تدرجات الهدف**</span> **(Goal Gradients)**
<span dir="rtl">التي تمتد على فترات زمنية أطول من تلك التي تغطيها **آثار
المحفزات** </span>**(Stimulus Traces)**<span dir="rtl">، اقترح
**هال**</span> **(Hull) <span dir="rtl">عام 1943</span>**
<span dir="rtl">أن التدرجات الأطول ناتجة عن **التعزيز الشرطي**</span>
**(Conditioned Reinforcement)** <span dir="rtl">الذي يتراجع من الهدف،
وهي عملية تعمل بالتوازي مع **آثار المحفزات الكبيرة**</span> **(Molar
Stimulus Traces)** <span dir="rtl">الخاصة به. أظهرت تجارب الحيوانات أنه
إذا كانت الظروف مواتية لتطوير **التعزيز الشرطي**</span>
**<span dir="rtl">(</span>Conditioned
<span dir="rtl"></span>Reinforcement<span dir="rtl">)
</span>**<span dir="rtl">خلال فترة التأخير، فإن **التعليم**</span>
**(Learning)** <span dir="rtl">لا يقل مع زيادة التأخير بقدر ما يحدث في
ظل الظروف التي تعيق **التعزيز الثانوي** </span>**(Secondary
Reinforcement)**<span dir="rtl">.</span> <span dir="rtl">يُفضل **التعزيز
الشرطي** </span>**(Conditioned Reinforcement)** <span dir="rtl">إذا كانت
هناك **محفزات**</span> **(Stimuli)** <span dir="rtl">تحدث بانتظام خلال
فترة التأخير. عندها يبدو كما لو أن المكافأة ليست مؤجلة فعليًا لأن هناك
**تعزيزًا شرطيًا (**</span>**Conditioned
<span dir="rtl"></span>Reinforcement<span dir="rtl">)
</span>**<span dir="rtl">أكثر إلحاحًا. لذلك تخيل **هال**</span>
**(Hull)** <span dir="rtl">أن هناك **تدرجًا أوليًا (**</span>**Primary
<span dir="rtl"></span>Gradient<span dir="rtl">)
</span>**<span dir="rtl">يعتمد على تأخير **التعزيز الأولي**</span>
**(Primary Reinforcement)** <span dir="rtl">يتم بواسطة **آثار المحفزات**
</span>**(Stimulus Traces)**<span dir="rtl">، وأن هذا يتم تعديله
تدريجيًا، وإطالته، من خلال **التعزيز الشرطي** </span>**(Conditioned
Reinforcement)**<span dir="rtl">.</span>

<span dir="rtl">الخوارزميات المقدمة في هذا الكتاب التي تستخدم كل من
**آثار الاستحقاق** </span>**(Eligibility Traces)**
<span dir="rtl">و**دوال القيمة**</span> **(Value Functions)**
<span dir="rtl">لتمكين **التعليم**</span> **(Learning)**
<span dir="rtl">مع **التعزيز المتأخر**</span>
**<span dir="rtl">(</span>Delayed
<span dir="rtl"></span>Reinforcement<span dir="rtl">)</span>**
<span dir="rtl">تتوافق مع فرضية **هال**</span> **(Hull)**
<span dir="rtl">حول كيفية قدرة الحيوانات على **التعليم**
</span>**(Learning)** <span dir="rtl">في ظل هذه الظروف. تُوضح **بنية
الممثل–الناقد** </span>**(Actor–Critic Architecture)**
<span dir="rtl">التي نوقشت في الأقسام 13.5، 15.7، و15.8 هذه المطابقة
بشكل أوضح. يستخدم **الناقد** </span>**(Critic)**
<span dir="rtl">خوارزمية **التفاضل الزمني**</span> **(TD Algorithm)**
<span dir="rtl">لتعلم **دالة القيمة**</span> **(Value Function)**
<span dir="rtl">المرتبطة بسلوك النظام الحالي، أي لتنبؤ **العائد**</span>
**(Return)** <span dir="rtl">الخاص بـ **السياسة الحالية**
</span>**(Current Policy)**<span dir="rtl">. يقوم **الممثل**</span>
**(Actor)** <span dir="rtl">بتحديث **السياسة الحالية**</span> **(Current
Policy)** <span dir="rtl">بناءً على **تنبؤات الناقد** </span>**(Critic's
Predictions)**<span dir="rtl">، أو بشكل أكثر دقة، بناءً على **التغيرات في
تنبؤات الناقد (**</span>**Changes <span dir="rtl"></span>in the Critic's
Predictions<span dir="rtl">)</span>**. <span dir="rtl">يعمل **خطأ
التفاضل الزمني**</span> **(TD Error)** <span dir="rtl">الناتج عن
**الناقد** </span>**(Critic)** <span dir="rtl">كإشارة **تعزيز
شرطي**</span> **(Conditioned Reinforcement Signal)** <span dir="rtl">لـ
**الممثل** </span>**(Actor)**<span dir="rtl">، مما يوفر تقييمًا فوريًا
للأداء حتى عندما تكون **إشارة المكافأة الأولية** </span>**(Primary
Reward Signal)** <span dir="rtl">نفسها متأخرة بشكل كبير. الخوارزميات
التي تقدر **دوال قيمة الأفعال (**</span>**Action-Value
<span dir="rtl"></span>Functions<span dir="rtl">)</span>**<span dir="rtl">،
مثل</span> **(Q-learning)** <span dir="rtl">و**سارسا**
</span>**(Sarsa)**<span dir="rtl">، تستخدم أيضًا مبادئ **التعليم بالتفاضل
الزمني**</span> **(TD Learning)** <span dir="rtl">لتمكين
**التعليم**</span> **(Learning)** <span dir="rtl">مع **التعزيز
المتأخر**</span> **<span dir="rtl">(</span>Delayed
<span dir="rtl"></span>Reinforcement<span dir="rtl">)
</span>**<span dir="rtl">من خلال **التعزيز الشرطي**
</span>**(Conditioned Reinforcement)**<span dir="rtl">.</span>
<span dir="rtl">التشابه الوثيق بين **التعليم بالتفاضل الزمني**</span>
**(TD Learning)** <span dir="rtl">ونشاط **الخلايا العصبية المنتجة
للدوبامين** </span>**(Dopamine Producing Neurons)** <span dir="rtl">الذي
نناقشه في **الفصل**</span> **15** <span dir="rtl">يوفر دعمًا إضافيًا
للروابط بين خوارزميات **التعليم المعزز**</span> **(Reinforcement
Learning)** <span dir="rtl">وهذا الجانب من **نظرية التعليم الخاصة بـ
هال** </span>**(Hull's Learning Theory)**<span dir="rtl">.</span>

**<u>14.5 <span dir="rtl">الخرائط المعرفية</span> (Cognitive Maps)</u>**

<span dir="rtl">تستخدم خوارزميات **التعليم المعزز القائم على
النموذج**</span> **<span dir="rtl">(</span>Model-based Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">) نماذج بيئية</span>
(Environment Models)** <span dir="rtl">تحتوي على عناصر مشتركة مع ما
يسميه علماء النفس **الخرائط المعرفية** </span>**(Cognitive
Maps)**<span dir="rtl">.</span> <span dir="rtl">تذكر من مناقشتنا حول
**التخطيط والتعليم** </span>**(Planning and Learning)**
<span dir="rtl">في **الفصل** </span>**8** <span dir="rtl">أنه يقصد بـ
**النموذج البيئي (**</span>**Environment
<span dir="rtl"></span>Model<span dir="rtl">)
</span>**<span dir="rtl">أي شيء يمكن لـ **العميل**</span> **(Agent)**
<span dir="rtl">استخدامه للتنبؤ بكيفية استجابة **البيئة**
</span>**(Environment)** <span dir="rtl">لأفعاله من حيث **الانتقالات
الحركية**</span> **(State Transitions)** <span dir="rtl">و**المكافآت**
</span>**(Rewards)**<span dir="rtl">، ويقصد بـ **التخطيط**</span>
**(Planning)** <span dir="rtl">أي عملية تحسب **السياسة**</span>
**(Policy)** <span dir="rtl">من نموذج كهذا. تتكون **النماذج
البيئية**</span> **(Environment Models)** <span dir="rtl">من جزأين: جزء
**انتقالات الحالة  
(**</span>**State-Transition Part<span dir="rtl">)
</span>**<span dir="rtl">يشفر المعرفة حول تأثير **الأفعال**</span>
**(Actions)** <span dir="rtl">على تغييرات **الحالة**
</span>**(State)**<span dir="rtl">، وجزء **نموذج المكافأة**</span>
**(Reward Model Part)** <span dir="rtl">يشفر المعرفة حول **إشارات
المكافأة** </span>**(Reward Signals)** <span dir="rtl">المتوقعة لكل
**حالة**</span> **(State)** <span dir="rtl">أو لكل **زوج حالة–فعل  
(**</span>**State–Action Pair<span dir="rtl">)</span>**.
<span dir="rtl">تختار خوارزمية تعتمد على النموذج **الأفعال**</span>
**(Actions)** <span dir="rtl">باستخدام نموذج للتنبؤ بعواقب المسارات
المحتملة من **الأفعال**</span> **(Actions)** <span dir="rtl">من حيث
**الحالات المستقبلية  
(**</span>**Future States<span dir="rtl">)
</span>**<span dir="rtl">وإشارات **المكافأة**</span> **(Reward
Signals)** <span dir="rtl">المتوقعة أن تنشأ من تلك **الحالات**
</span>**(States)**<span dir="rtl">.</span> <span dir="rtl">أبسط نوع من
**التخطيط**</span> **(Planning)** <span dir="rtl">هو مقارنة العواقب
المتوقعة لمجموعات من تسلسلات **القرارات "المتخيلة"** </span>**(Imagined
Sequences of Decisions)**<span dir="rtl">.</span>

<span dir="rtl">تساؤلات حول ما إذا كانت الحيوانات تستخدم **نماذج بيئية**
</span>**(Environment Models)**<span dir="rtl">، وإذا كان الأمر كذلك،
كيف تكون هذه النماذج وكيف تتعلمها، لعبت دورًا مؤثرًا في تاريخ أبحاث **تعلم
الحيوانات** </span>**(Animal Learning
Research)**<span dir="rtl">.</span> <span dir="rtl">تحدى بعض الباحثين
النظرة السائدة حينذاك **للمنبه–الاستجابة** </span>**(Stimulus-Response
(S–R))** <span dir="rtl">في **التعليم والسلوك** </span>**(Learning and
Behavior)**<span dir="rtl">، والتي تتوافق مع أبسط طرق **التعليم الخالي
من النموذج**</span> **(Model-Free Learning)** <span dir="rtl">للسياسات،
من خلال إثبات **التعليم الكامن** </span>**(Latent
Learning)**<span dir="rtl">.</span> <span dir="rtl">في أول تجربة
**للتعليم الكامن** </span>**(Latent Learning)**<span dir="rtl">، تم وضع
مجموعتين من الفئران في متاهة. بالنسبة للمجموعة التجريبية، لم تكن هناك
**مكافأة** </span>**(Reward)** <span dir="rtl">خلال المرحلة الأولى من
التجربة، ولكن تم إدخال الطعام فجأة في **صندوق الهدف** </span>**(Goal
Box)** <span dir="rtl">في المتاهة عند بداية المرحلة الثانية. بالنسبة
للمجموعة الضابطة، كان الطعام موجودًا في **صندوق الهدف**</span> **(Goal
Box)** <span dir="rtl">خلال كلا المرحلتين. كان السؤال هو ما إذا كانت
الفئران في المجموعة التجريبية قد تعلمت أي شيء خلال المرحلة الأولى في
غياب **مكافأة الطعام  
(**</span>**Food Reward<span dir="rtl">)</span>**. <span dir="rtl">على
الرغم من أن الفئران التجريبية لم تظهر الكثير من **التعليم**
</span>**(Learning)** <span dir="rtl">خلال المرحلة الأولى غير المعززة،
إلا أنها بمجرد اكتشافها للطعام الذي تم إدخاله في المرحلة الثانية، كانت
قادرة على اللحاق بسرعة بالفئران في المجموعة الضابطة. تم الاستنتاج بأنه
"خلال فترة عدم المكافأة، كانت الفئران \[في المجموعة التجريبية\] تطور
**تعلمًا كامنًا**</span> **(Latent Learning)** <span dir="rtl">للمتاهة
يمكنها استخدامه بمجرد تقديم المكافأة"</span> (Blodgett,
1929)<span dir="rtl">.</span>

<span dir="rtl">يرتبط **التعليم الكامن**</span> **(Latent Learning)**
<span dir="rtl">ارتباطًا وثيقًا بعالم النفس **إدوارد تولمان**</span>
**<span dir="rtl">(</span>Edward
<span dir="rtl"></span>Tolman<span dir="rtl">)</span>**<span dir="rtl">،
الذي فسّر هذه النتيجة، ونتائج أخرى مشابهة، على أنها تظهر أن الحيوانات
يمكنها تعلم "خريطة معرفية للبيئة</span> (Cognitive Map of the
Environment) <span dir="rtl">في غياب **المكافآت** </span>**(Rewards)**
<span dir="rtl">أو **العقوبات** </span>**(Penalties)**<span dir="rtl">،
وأنها يمكنها استخدام هذه **الخريطة**</span> **(Map)**
<span dir="rtl">لاحقًا عندما تكون محفزة لتحقيق هدف</span> (Tolman,
1948)<span dir="rtl">.</span> <span dir="rtl">يمكن أن تسمح **الخريطة
المعرفية**</span> **(Cognitive Map)** <span dir="rtl">أيضًا للفأر بتخطيط
مسار إلى **الهدف**</span> **(Goal)** <span dir="rtl">يختلف عن المسار
الذي استخدمه الفأر في استكشافه الأولي. تفسيرات نتائج كهذه أدت إلى الجدل
المستمر الذي يكمن في قلب الانقسام بين **السلوكية**
</span>**(Behaviorist)** <span dir="rtl">و**المعرفية**</span>
**(Cognitive)** <span dir="rtl">في علم النفس. في المصطلحات الحديثة، لا
تقتصر **الخرائط المعرفية**</span> **(Cognitive Maps)**
<span dir="rtl">على نماذج **التخطيطات المكانية** </span>**(Spatial
Layouts)** <span dir="rtl">فحسب، بل تشمل بشكل عام **نماذج بيئية**
</span>**(Environment Models)**<span dir="rtl">، أو **نماذج مساحة المهمة
للحيوان** </span>**(Animal’s Task Space
Models)**<span dir="rtl">.</span> <span dir="rtl">تفسير **الخرائط
المعرفية** </span>**(Cognitive Maps)** <span dir="rtl">لتجارب **التعليم
الكامن**</span> **(Latent Learning Experiments)** <span dir="rtl">يشبه
الادعاء بأن الحيوانات تستخدم خوارزميات قائمة على النموذج</span>
(Model-based Algorithms)<span dir="rtl">، وأن **النماذج البيئية**
</span>**(Environment Models)** <span dir="rtl">يمكن تعلمها حتى بدون
**مكافآت**</span> **(Rewards)** <span dir="rtl">أو **عقوبات**
</span>**(Penalties)** <span dir="rtl">صريحة. ثم تُستخدم
**النماذج**</span> **(Models)** <span dir="rtl">في **التخطيط**</span>
**(Planning)** <span dir="rtl">عندما تكون **الحيوانات**</span>
**(Animals)** <span dir="rtl">محفزة بظهور **المكافآت**</span>
**(Rewards)** <span dir="rtl">أو **العقوبات**
</span>**(Penalties)**<span dir="rtl">.</span>

<span dir="rtl">شرح **تولمان**</span> **(Tolman)** <span dir="rtl">كيف
تتعلم الحيوانات **الخرائط المعرفية**</span> **(Cognitive Maps)**
<span dir="rtl">بأنه يتم تعلم **الارتباطات بين المحفزات**</span>
**(Stimulus-Stimulus or** $`\mathbf{S–S}`$ **Associations)**
<span dir="rtl">من خلال تجربة تسلسلات **المحفزات**</span> **(Stimuli)**
<span dir="rtl">أثناء استكشاف **البيئة**
</span>**(Environment)**<span dir="rtl">.</span> <span dir="rtl">في علم
النفس، يُطلق على هذا **نظرية التوقع** </span>**(Expectancy
Theory)**<span dir="rtl">:</span> <span dir="rtl">بناءً على **الارتباطات
بين المحفزات  
(**</span>$`\mathbf{S–S}`$
<span dir="rtl"></span>**Associations<span dir="rtl">)</span>**<span dir="rtl">،
يولد حدوث **محفز**</span> **(Stimulus)** <span dir="rtl">توقعًا حول
**المحفز**</span> **(Stimulus)** <span dir="rtl">الذي سيأتي بعد ذلك.
يشبه هذا ما يسميه مهندسو التحكم **تحديد النظام** </span>**(System
Identification)**<span dir="rtl">، حيث يتم تعلم نموذج لنظام ذو
ديناميكيات غير معروفة من أمثلة تدريبية مسماة. في أبسط النسخ **المتقطعة
الزمن** </span>**(Discrete-time Versions)**<span dir="rtl">، تكون أمثلة
التدريب هي **أزواج** </span>$`\mathbf{S–S'}`$<span dir="rtl">،
حيث</span> $`\mathbf{S}`$ <span dir="rtl">تمثل **حالة** </span>**(State
<span dir="rtl"></span>**<span dir="rtl">و</span>$`S\mathbf{'}`$<span dir="rtl">،
الحالة التالية، هي التسمية. عند ملاحظة</span>
$`\mathbf{S}`$<span dir="rtl">، يُنشئ **النموذج**</span> **(Model)**
**"<span dir="rtl">توقعًا</span> (Expectation)
<span dir="rtl"></span>**<span dir="rtl">بأن</span> $`\mathbf{S'}`$
<span dir="rtl">ستلاحظ بعد ذلك. تكون النماذج الأكثر فائدة في **التخطيط**
</span>**(Planning)** <span dir="rtl">تشمل **الأفعال**</span>
**(Actions)** <span dir="rtl">أيضًا، بحيث تكون الأمثلة مثل</span>
$`\mathbf{SA–S'}`$<span dir="rtl">، حيث</span> $`\mathbf{S'}`$
<span dir="rtl"></span> <span dir="rtl">متوقعة عندما يتم تنفيذ
**الفعل**</span> $`\mathbf{A}`$ **(Action** $`\mathbf{A}`$**)**
<span dir="rtl">في **الحالة** </span>$`\mathbf{S}`$
<span dir="rtl"></span>**(State**
$`\mathbf{S}`$**)**<span dir="rtl">.</span> <span dir="rtl">من المفيد
أيضًا تعلم كيفية توليد **البيئة** </span>**(Environment)**
**<span dir="rtl">للمكافآت</span> (Rewards)**<span dir="rtl">.</span>
<span dir="rtl">في هذه الحالة، تكون الأمثلة من الشكل</span>
$`\mathbf{S–R}`$ <span dir="rtl">أو  
</span>$`\mathbf{SA–R}`$<span dir="rtl">، حيث</span> $`\mathbf{R}`$
<span dir="rtl">هي **إشارة مكافأة**</span> **(Reward Signal)**
<span dir="rtl">مرتبطة بـ</span> $`\mathbf{S}`$ <span dir="rtl">أو
**زوج**</span> $`\mathbf{SA}`$ **(SA Pair)**. <span dir="rtl">هذه كلها
أشكال من **التعليم الخاضع للإشراف**</span> **(Supervised Learning)**
<span dir="rtl">والتي من خلالها يمكن **لعميل** </span>**(Agent)**
<span dir="rtl">أن يكتسب **خرائط معرفية**</span> **(Cognitive-like
Maps)** <span dir="rtl">سواء حصل على أي **إشارات مكافأة غير
صفرية**</span> **(Non-zero Reward Signals)** <span dir="rtl">أثناء
استكشاف **بيئته**</span> **(Environment)** <span dir="rtl">أم لا</span>.

**<u>14.6 <span dir="rtl">السلوك المعتاد والموجه نحو الهدف</span>
(Habitual and Goal-directed Behavior)</u>**

<span dir="rtl">التمييز بين **خوارزميات التعليم المعزز الخالية من
النموذج**</span> **<span dir="rtl">(</span>Model-free Reinforcement
<span dir="rtl"></span>Learning Algorithms<span dir="rtl">)
</span>**<span dir="rtl">وخوارزميات **التعليم المعزز القائم على
النموذج**</span> **<span dir="rtl">(</span>Model-based
<span dir="rtl"></span>Reinforcement Learning
Algorithms<span dir="rtl">)</span>** <span dir="rtl">يتوافق مع التمييز
الذي يضعه علماء النفس بين **التحكم المعتاد**</span> **(Habitual
Control)** <span dir="rtl">و**التحكم الموجه نحو الهدف**
</span>**(Goal-directed Control)** <span dir="rtl">لأنماط السلوك
المكتسبة.</span> **<span dir="rtl">العادات</span> (Habits)**
<span dir="rtl">هي أنماط سلوكية يتم تحفيزها بواسطة **المحفزات المناسبة**
</span>**(Appropriate Stimuli)** <span dir="rtl">ثم يتم تنفيذها تلقائيًا
إلى حد ما.</span> **<span dir="rtl">السلوك الموجه نحو الهدف  
(</span>Goal-directed
Behavior<span dir="rtl">)</span>**<span dir="rtl">، وفقًا لاستخدام علماء
النفس لهذا المصطلح، يكون **هادفًا** </span>**(Purposeful)**
<span dir="rtl">بمعنى أنه يتحكم فيه معرفة **قيمة الأهداف**</span>
**(Value of Goals)** <span dir="rtl">والعلاقة بين **الأفعال**</span>
**(Actions)** <span dir="rtl">وعواقبها. يقال أحيانًا إن
**العادات**</span> **(Habits)** <span dir="rtl">يتم التحكم فيها بواسطة
**المحفزات السابقة**</span> **(Antecedent Stimuli)**<span dir="rtl">،
بينما يقال إن **السلوك الموجه نحو الهدف**</span>
**<span dir="rtl">(</span>Goal-directed
<span dir="rtl"></span>Behavior<span dir="rtl">)</span>**
<span dir="rtl">يتم التحكم فيه بواسطة **عواقبه**
</span>**(Consequences)** (Dickinson, 1980, 1985)<span dir="rtl">. يتمتع
**التحكم الموجه نحو الهدف**</span> **(Goal-directed Control)**
<span dir="rtl">بميزة القدرة على تغيير **سلوك الحيوان**</span>
**(Animal’s Behavior)** <span dir="rtl">بسرعة عندما تقوم
**البيئة**</span> **(Environment)** <span dir="rtl">بتغيير طريقتها في
الاستجابة لأفعال الحيوان. في حين أن **السلوك المعتاد**</span>
**(Habitual Behavior)** <span dir="rtl">يستجيب بسرعة **للبيئة المألوفة**
</span>**(Accustomed Environment)**<span dir="rtl">، فإنه غير قادر على
التكيف بسرعة مع التغييرات في **البيئة**
</span>**(Environment)**<span dir="rtl">.</span> <span dir="rtl">من
المحتمل أن يكون تطور **التحكم السلوكي الموجه نحو الهدف**
</span>**(Goal-directed Behavioral Control)** <span dir="rtl">تقدمًا
كبيرًا في تطور **الذكاء الحيواني**</span>
**<span dir="rtl">(</span>Animal
<span dir="rtl"></span>Intelligence<span dir="rtl">)</span>**.

**<span dir="rtl">الشكل</span> (Figure) 14.5** <span dir="rtl">يوضح
الفرق بين **استراتيجيات اتخاذ القرار الخالية من النموذج  
(**</span>**Model-free Decision Strategies<span dir="rtl">)</span>**
<span dir="rtl">و**القائمة على النموذج**</span>
**<span dir="rtl">(</span>Model-based Decision
<span dir="rtl"></span>Strategies<span dir="rtl">)
</span>**<span dir="rtl">في مهمة افتراضية حيث يجب على الجرذ التنقل في
متاهة تحتوي على **صناديق هدف مميزة**</span> **(Distinctive Goal
Boxes)**<span dir="rtl">، كل منها يقدم **مكافأة مرتبطة**
</span>**(Associated Reward)** <span dir="rtl">بالحجم الموضح</span>
<span dir="rtl">(**الشكل 14.5 الأعلى**).</span> <span dir="rtl">بدءًا
من</span> $`S1`$<span dir="rtl">، يجب على الجرذ أن يختار أولاً بين
اليسار</span> ($`L`$) <span dir="rtl">أو اليمين</span> ($`R`$)
<span dir="rtl">ثم عليه أن يختار بين</span> $`L`$
<span dir="rtl">أو</span> $`R`$ <span dir="rtl">مرة أخرى عند</span>
$`S2`$ <span dir="rtl">أو</span> $`S3`$ <span dir="rtl">للوصول إلى أحد
**صناديق الهدف** </span>**(Goal Boxes)**<span dir="rtl">.</span>
<span dir="rtl">تمثل **صناديق الهدف**</span> **(Goal Boxes)**
**<span dir="rtl">الحالات النهائية  
(</span>Terminal <span dir="rtl"></span>States<span dir="rtl">)</span>**
<span dir="rtl">لكل **حلقة من المهام**</span> **(Episodic Task)**
<span dir="rtl">الخاصة بالجرذ. تعتمد **الاستراتيجية الخالية من
النموذج**</span> **(Model-free Strategy)** (**<span dir="rtl">الشكل 14.5
أسفل اليسار</span>**) <span dir="rtl">على القيم المخزنة **لأزواج
الحالة–الفعل** </span>**(State–Action Pairs)**<span dir="rtl">.</span>
<span dir="rtl">هذه **قيم الأفعال**</span> **(Action Values)**
<span dir="rtl">هي تقديرات **لأعلى عائد**</span> **(Highest Return)**
<span dir="rtl">يمكن أن يتوقعه الجرذ لكل **فعل**</span> **(Action)**
<span dir="rtl">يتم اتخاذه من كل **حالة**</span> **(State)**
<span dir="rtl">(غير نهائية). يتم الحصول عليها على مدى العديد من
المحاولات لتشغيل المتاهة من البداية إلى النهاية. عندما تصبح تقديرات
**قيم الأفعال**</span> **(Action Values)** <span dir="rtl">جيدة بما يكفي
لتقدير **العوائد المثلى** </span>**(Optimal Returns)**<span dir="rtl">،
يحتاج الجرذ فقط إلى اختيار **الفعل**</span> **(Action)**
<span dir="rtl">ذو **القيمة الأكبر** </span>**(Largest Action Value)**
<span dir="rtl">في كل **حالة**</span> **(State)** <span dir="rtl">لاتخاذ
**قرارات مثلى** </span>**(Optimal Decisions)**<span dir="rtl">. في هذه
الحالة، عندما تصبح تقديرات **قيمة الفعل**</span> **(Action-Value
Estimates)** <span dir="rtl">دقيقة بما يكفي، يختار الجرذ</span> $`L`$
<span dir="rtl">من</span> $`S1`$ <span dir="rtl">و</span>$`R`$
<span dir="rtl">من</span> $`S2`$ <span dir="rtl">للحصول على **أقصى
عائد**</span> **(Maximum Return)** <span dir="rtl">قدره 4. قد تعتمد
**استراتيجية خالية من النموذج**</span> **(Model-free Strategy)**
<span dir="rtl">أخرى ببساطة على **سياسة مخزنة** </span>**(Cached
Policy)** <span dir="rtl">بدلاً من **قيم الأفعال** </span>**(Action
Values)**<span dir="rtl">، مما يؤدي إلى إنشاء روابط مباشرة من</span>
$`S1`$ <span dir="rtl">إلى</span> $`L`$ <span dir="rtl">ومن</span>
$`S2`$ <span dir="rtl">إلى</span> $`R`$<span dir="rtl">.</span>
<span dir="rtl">في أي من هاتين الاستراتيجيتين، لا تعتمد **القرارات**
</span>**(Decisions)** <span dir="rtl">على **نموذج بيئي**
</span>**(Environment Model)**<span dir="rtl">.</span>
<span dir="rtl">لا حاجة لاستشارة **نموذج انتقالات الحالة  
(**</span>**State-Transition
Model<span dir="rtl">)</span>**<span dir="rtl">، ولا يتطلب وجود أي اتصال
بين خصائص **صناديق الهدف (**</span>**Goal
Boxes<span dir="rtl">)</span>** <span dir="rtl">و**المكافآت**</span>
**(Rewards)** <span dir="rtl">التي تقدمها</span>.

<img src="./media/image176.png"
style="width:6.26806in;height:3.16528in" />

**<span dir="rtl">الشكل</span> (Figure) 14.5**<span dir="rtl">:</span>
<span dir="rtl">استراتيجيات **التعليم القائم على النموذج**
</span>**(Model-based Learning)** <span dir="rtl">و**التعليم الخالي من
النموذج**</span> **(Model-free Learning)** <span dir="rtl">لحل مشكلة
افتراضية لاختيار **الأفعال المتسلسلة** </span>**(Sequential
Action-Selection)**<span dir="rtl">.</span>
<span dir="rtl">**الأعلى**:</span> <span dir="rtl">جرذ يتنقل في متاهة
تحتوي على **صناديق هدف مميزة** </span>**(Distinctive Goal
Boxes)**<span dir="rtl">، كل منها مرتبط بـ **مكافأة**</span>
**(Reward)** <span dir="rtl">ذات قيمة محددة كما هو موضح</span>.
**<span dir="rtl">أسفل اليسار</span>**: <span dir="rtl">تعتمد
**الاستراتيجية الخالية من النموذج (**</span>**Model-free
<span dir="rtl"></span>Strategy<span dir="rtl">)</span>**
<span dir="rtl">على قيم الأفعال المخزنة لكل **زوج حالة–فعل**</span>
**(State–Action Pair)** <span dir="rtl">يتم الحصول عليها عبر العديد من
تجارب التعليم. لاتخاذ القرارات، يحتاج الجرذ فقط إلى اختيار **الفعل**
</span>**(Action)** <span dir="rtl">الذي يمتلك أعلى **قيمة
للفعل**</span> **(Action Value)** <span dir="rtl">في كل **حالة**
</span>**(State)**<span dir="rtl">.</span> <span dir="rtl">**أسفل
اليمين**:</span> <span dir="rtl">في **استراتيجية قائمة على النموذج**
</span>**(Model-based Strategy)**<span dir="rtl">، يتعلم الجرذ **نموذج
البيئة (**</span>**Environment
<span dir="rtl"></span>Model<span dir="rtl">)</span>**<span dir="rtl">،
الذي يتكون من معرفة **انتقالات الحالة–الفعل–الحالة التالية
(**</span>**State–Action-Next-State Transitions<span dir="rtl">)
</span>**<span dir="rtl">و**نموذج المكافأة**</span> **(Reward Model)**
<span dir="rtl">الذي يتألف من معرفة **المكافأة** </span>**(Reward)**
<span dir="rtl">المرتبطة بكل **صندوق هدف مميز** </span>**(Distinctive
Goal Box)**<span dir="rtl">.</span> <span dir="rtl">يمكن للجرذ أن يقرر
أي طريق يسلك في كل **حالة**</span> **(State)** <span dir="rtl">باستخدام
**النموذج**</span> **(Model)** <span dir="rtl">لمحاكاة تسلسلات اختيارات
الأفعال لإيجاد مسار يوفر أعلى **عائد**
</span>**(Return)**<span dir="rtl">. (مقتبس من</span> Trends in
Cognitive Science<span dir="rtl">، المجلد 10، العدد 8،</span> Y. Niv, D.
Joel, and P. Dayan, A Normative Perspective on
Motivation<span dir="rtl">، ص. 376، 2006، بإذن من</span>
Elsevier<span dir="rtl">).</span>

<span dir="rtl">**الشكل 14.5 (أسفل اليمين)** يوضح استراتيجية قائمة على
النموذج</span> (Model-based Strategy)<span dir="rtl">. تستخدم هذه
الاستراتيجية **نموذج البيئة**</span> **(Environment Model)**
<span dir="rtl">الذي يتكون من **نموذج انتقال الحالة**</span>
**(State-Transition Model)** <span dir="rtl">و**نموذج المكافأة**
</span>**(Reward Model)**<span dir="rtl">.</span> <span dir="rtl">يُظهر
**نموذج انتقال الحالة**</span> **(State-Transition Model)**
<span dir="rtl">على شكل **شجرة قرار** </span>**(Decision
Tree)**<span dir="rtl">، ويربط **نموذج المكافأة**</span> **(Reward
Model)** <span dir="rtl">بين الميزات المميزة **لصناديق الهدف**</span>
**(Goal Boxes)** <span dir="rtl">والمكافآت التي يمكن العثور عليها في كل
منها. (المكافآت المرتبطة **بالحالات**</span> **(States)** $`S1`$
<span dir="rtl">و</span>$`S2`$ <span dir="rtl">و</span>$`S3`$
<span dir="rtl">هي أيضًا جزء من **نموذج المكافأة** </span>**(Reward
Model)**<span dir="rtl">، ولكنها هنا صفرية وليست معروضة). يمكن **لوكيل
قائم على النموذج**</span> **(Model-based Agent)** <span dir="rtl">أن
يقرر أي طريق يسلك في كل **حالة**</span> **(State)**
<span dir="rtl">باستخدام **النموذج**</span> **(Model)**
<span dir="rtl">لمحاكاة تسلسلات اختيارات **الأفعال**</span> **(Action
Choices)** <span dir="rtl">لإيجاد مسار يوفر أعلى **عائد**
</span>**(Return)**<span dir="rtl">.</span> <span dir="rtl">في هذه
الحالة، يكون **العائد**</span> **(Return)** <span dir="rtl">هو المكافأة
التي يتم الحصول عليها من النتيجة في نهاية المسار. هنا، مع وجود نموذج
دقيق بما فيه الكفاية، سيختار الجرذ الاتجاه</span> $`L`$
<span dir="rtl">ثم</span> $`R`$ <span dir="rtl">للحصول على مكافأة بقيمة
4. مقارنة العوائد المتوقعة من المسارات المحاكية هو شكل بسيط من
**التخطيط** </span>**(Planning)**<span dir="rtl">، والذي يمكن القيام به
بطرق متنوعة كما نوقش في **الفصل**</span>.

<span dir="rtl">عندما تتغير **بيئة وكيل خالي من النموذج**</span>
**(Model-free Agent)** <span dir="rtl">في طريقة استجابتها لأفعال
**الوكيل** </span>**(Agent)**<span dir="rtl">، يتعين على
**الوكيل**</span> **(Agent)** <span dir="rtl">اكتساب تجربة جديدة في
البيئة المتغيرة يتم خلالها تحديث **السياسة**</span> **(Policy)**
<span dir="rtl">و/أو **دالة القيمة** </span>**(Value
Function)**<span dir="rtl">.</span> <span dir="rtl">في الاستراتيجية
الخالية من النموذج الموضحة في **الشكل 14.5 (أسفل اليسار)**، على سبيل
المثال، إذا حدث تغيير في أحد **صناديق الهدف**</span> **(Goal Boxes)**
<span dir="rtl">ليقدم مكافأة مختلفة، سيحتاج الجرذ إلى عبور المتاهة عدة
مرات ربما ليختبر المكافأة الجديدة عند الوصول إلى ذلك **صندوق الهدف**
</span>**(Goal Box)**<span dir="rtl">، وفي الوقت نفسه يقوم بتحديث إما
**السياسة**</span> **(Policy)** <span dir="rtl">أو **دالة قيمة
الأفعال  **
</span>**(Action-Value Function)** <span dir="rtl">(أو كلاهما) بناءً على
هذه التجربة. النقطة الرئيسية هي أن **الوكيل الخالي من النموذج**</span>
**<span dir="rtl">(</span>Model-free
<span dir="rtl"></span>Agent<span dir="rtl">)</span>**
<span dir="rtl">لكي يغير **الفعل**</span> **(Action)**
<span dir="rtl">الذي تحدده **السياسة** </span>**(Policy)**
<span dir="rtl">في **حالة**</span> **(State)** <span dir="rtl">معينة، أو
ليغير **قيمة الفعل**</span> **(Action Value)** <span dir="rtl">المرتبطة
بتلك **الحالة** </span>**(State)**<span dir="rtl">، يجب عليه الانتقال
إلى تلك **الحالة** </span>**(State)**<span dir="rtl">، والقيام
**بالفعل**</span> **(Act)** <span dir="rtl">من خلالها، ربما عدة مرات،
واختبار نتائج **أفعاله** </span>**(Actions)**<span dir="rtl">.</span>

<span dir="rtl">يمكن لـ **الوكيل القائم على النموذج**</span>
**(Model-based Agent)** <span dir="rtl">التكيف مع التغييرات في **بيئته**
</span>**(Environment)** <span dir="rtl">دون الحاجة إلى هذا النوع من
"التجربة الشخصية" مع **الحالات** </span>**(States)**
<span dir="rtl">و**الأفعال** </span>**(Actions)**
<span dir="rtl">المتأثرة بالتغيير. يحدث التغيير في **نموذجه**</span>
**(Model)** <span dir="rtl">تلقائيًا (من خلال **التخطيط**
</span>**(Planning)**<span dir="rtl">)</span> <span dir="rtl">ليغير
**السياسة**</span> **(Policy)** <span dir="rtl">الخاصة به. يمكن
**للتخطيط**</span> **(Planning)** <span dir="rtl">تحديد عواقب التغييرات
في **البيئة**</span> **(Environment)** <span dir="rtl">التي لم يتم ربطها
معًا في تجربة **الوكيل** </span>**(Agent)**<span dir="rtl">.</span>
<span dir="rtl">على سبيل المثال، بالإشارة مرة أخرى إلى مهمة المتاهة في
**الشكل 14.5**، تخيل أن جرذًا مع **نموذج انتقال ومكافأة**</span>
**(Transition and Reward Model)** <span dir="rtl">تعلمه سابقًا تم وضعه
مباشرة في **صندوق الهدف** </span>**(Goal Box)** <span dir="rtl">إلى
يمين</span> $`S2`$ <span dir="rtl">ليجد أن المكافأة المتاحة هناك أصبحت
الآن بقيمة 1 بدلاً من 4. سيتغير **نموذج المكافأة**</span> **(Reward
Model)** <span dir="rtl">الخاص بالجرذ حتى وإن لم تتضمن **اختيارات
الأفعال**</span> **<span dir="rtl">(</span>Action
<span dir="rtl"></span>Choices<span dir="rtl">)</span>**
<span dir="rtl">اللازمة للوصول إلى ذلك **صندوق الهدف**</span> **(Goal
Box)** <span dir="rtl">في المتاهة. ستسمح عملية **التخطيط**</span>
**(Planning)** <span dir="rtl">بتطبيق المعرفة الجديدة حول المكافأة على
**التنقل في المتاهة**</span> **<span dir="rtl">(</span>Maze
<span dir="rtl"></span>Running<span dir="rtl">)
</span>**<span dir="rtl">دون الحاجة إلى تجربة إضافية في المتاهة؛ في هذه
الحالة، سيغير **السياسة** </span>**(Policy)** <span dir="rtl">ليختار
الاتجاه الصحيح عند كل من</span> $`S1`$ <span dir="rtl">و</span>$`S3`$
<span dir="rtl">للحصول على **عائد**</span> **(Return)**
<span dir="rtl">بقيمة 3</span>.

<span dir="rtl">تمامًا هذه الفكرة هي أساس تجارب **إلغاء القيمة**</span>
**(Outcome-Devaluation Experiments)** <span dir="rtl">مع **الحيوانات**
</span>**(Animals)**<span dir="rtl">.</span> <span dir="rtl">توفر نتائج
هذه التجارب نظرة عميقة حول ما إذا كان الحيوان قد تعلم **عادة**
</span>**(Habit)** <span dir="rtl">أو إذا كان **سلوكه**</span>
**(Behavior)** <span dir="rtl">تحت **التحكم الموجه نحو الهدف
(**</span>**Goal-directed
<span dir="rtl"></span>Control<span dir="rtl">)</span>**.
<span dir="rtl">تشبه تجارب **إلغاء القيمة**</span>
**(Outcome-Devaluation Experiments)** <span dir="rtl">تجارب **التعليم
الكامن**</span> **(Latent-Learning Experiments)** <span dir="rtl">من حيث
أن **المكافأة**</span> **(Reward)** <span dir="rtl">تتغير من مرحلة إلى
أخرى. بعد مرحلة تعلم أولية يتم فيها تقديم مكافأة، يتم تغيير **قيمة
المكافأة**</span> **<span dir="rtl">(</span>Reward
<span dir="rtl"></span>Value<span dir="rtl">)
</span>**<span dir="rtl">للنتيجة، بما في ذلك تحويلها إلى صفر أو حتى إلى
قيمة سلبية</span>.

<span dir="rtl">أُجريت تجربة مبكرة ومهمة من هذا النوع بواسطة **آدامز
وديكنسون** </span>**(Adams and Dickinson) <span dir="rtl">عام
1981</span>**<span dir="rtl">.</span> <span dir="rtl">قاموا بتدريب
الجرذان من خلال **التكييف الأداتي** </span>**(Instrumental
Conditioning)** <span dir="rtl">حتى أصبحت الجرذان تضغط على الرافعة بحماس
للحصول على حبيبات السكروز في غرفة التدريب. ثم وُضعت الجرذان في نفس الغرفة
مع سحب الرافعة، وتم إتاحة الطعام لها بشكل غير مرتبط بأفعالها، مما يعني
أن الحبيبات كانت متاحة لها بغض النظر عن أفعالها. بعد 15 دقيقة من هذا
الوصول الحر إلى الحبيبات، تم حقن مجموعة من الجرذان بالسم المسبب للغثيان
**كلوريد الليثيوم**</span> **<span dir="rtl">(</span>Lithium
<span dir="rtl"></span>Chloride<span dir="rtl">)</span>**.
<span dir="rtl">تم تكرار هذه العملية لثلاث جلسات، في الجلسة الأخيرة منها
لم تستهلك أي من الجرذان المحقونة أيًا من الحبيبات المتاحة بشكل غير مرتبط
بأفعالها، مما يشير إلى أن **قيمة المكافأة** </span>**(Reward Value)**
<span dir="rtl">للحبيبات قد انخفضت أي أن الحبيبات قد فقدت قيمتها. في
المرحلة التالية التي جرت بعد يوم واحد، تم وضع الجرذان مرة أخرى في الغرفة
ومنحت جلسة تدريب على **الإطفاء** </span>**(Extinction
Training)**<span dir="rtl">، مما يعني أن رافعة الاستجابة كانت في مكانها
ولكنها كانت منفصلة عن موزع الحبيبات، بحيث لا يؤدي الضغط عليها إلى تحرير
الحبيبات. كان السؤال هو ما إذا كانت الجرذان التي انخفضت **قيمة
المكافأة**</span> **(Reward Value)** <span dir="rtl">للحبيبات لديها
ستضغط على الرافعة أقل من الجرذان التي لم تنخفض **قيمة المكافأة**</span>
**(Reward Value)** <span dir="rtl">للحبيبات لديها، حتى دون تجربة
**المكافأة المنخفضة القيمة**</span> **(Devalued Reward)**
<span dir="rtl">نتيجة للضغط على الرافعة. اتضح أن الجرذان المحقونة كانت
معدلات استجابتها أقل بشكل كبير من الجرذان غير المحقونة منذ بداية تجارب
الإطفاء</span>.

<span dir="rtl">خلص **آدامز وديكنسون**</span> **(Adams and Dickinson)**
<span dir="rtl">إلى أن الجرذان المحقونة ربطت بين الضغط على الرافعة
والغثيان الناتج عن طريق **خريطة معرفية**</span> **(Cognitive Map)**
<span dir="rtl">تربط بين الضغط على الرافعة والحبيبات، وبين الحبيبات
والغثيان. وبالتالي، في تجارب الإطفاء، "علمت" الجرذان أن عواقب الضغط على
الرافعة ستكون شيئًا لا تريده، لذا قللت من الضغط على الرافعة منذ البداية.
النقطة المهمة هي أنها قللت من الضغط على الرافعة دون أن تجرب مباشرةً الضغط
على الرافعة يليه الشعور بالغثيان: لم تكن هناك رافعة موجودة عندما أصيبت
بالمرض. يبدو أنها كانت قادرة على الجمع بين معرفة نتيجة اختيار
**سلوكي**</span> **(Behavioral Choice)** <span dir="rtl">(الضغط على
الرافعة سيتبعه الحصول على حبيبة) مع **قيمة المكافأة**</span> **(Reward
Value)** <span dir="rtl">للنتيجة (يجب تجنب الحبيبات) وبالتالي يمكنها
تغيير **سلوكها** </span>**(Behavior)** <span dir="rtl">وفقًا لذلك. لا
يتفق كل علماء النفس مع هذا التفسير "**المعرفي**" لهذا النوع من التجارب،
وهو ليس الطريقة الوحيدة الممكنة لشرح هذه النتائج، ولكن تفسير **التخطيط
القائم على النموذج** </span>**(Model-based Planning)**
<span dir="rtl">مقبول على نطاق واسع</span>.

<span dir="rtl">لا يوجد ما يمنع **الوكيل**</span> **(Agent)**
<span dir="rtl">من استخدام كل من **الخوارزميات الخالية من
النموذج**</span> **<span dir="rtl">(</span>Model-free
Algorithms<span dir="rtl">)</span>** <span dir="rtl">و**الخوارزميات
القائمة على النموذج** </span>**(Model-based
Algorithms)**<span dir="rtl">، وهناك أسباب وجيهة لاستخدام كلاهما. نحن
نعلم من تجربتنا الخاصة أنه مع التكرار الكافي، يميل **السلوك الموجه نحو
الهدف**</span> **(Goal-directed Behavior)** <span dir="rtl">إلى التحول
إلى **سلوك معتاد** </span>**(Habitual Behavior)**<span dir="rtl">. تُظهر
التجارب أن هذا يحدث للجرذان أيضًا. قام **آدامز**</span> **(Adams)
<span dir="rtl">عام 1982</span>** <span dir="rtl">بإجراء تجربة لمعرفة ما
إذا كان التدريب الموسع سيحول **السلوك الموجه نحو الهدف  
(**</span>**Goal-directed Behavior<span dir="rtl">)</span>**
<span dir="rtl">إلى **سلوك معتاد** </span>**(Habitual
Behavior)**<span dir="rtl">.</span> <span dir="rtl">قام بذلك عن طريق
مقارنة تأثير **إلغاء القيمة**</span> **(Outcome Devaluation)**
<span dir="rtl">على الجرذان التي خضعت لكميات مختلفة من التدريب. إذا كان
التدريب الموسع يجعل الجرذان أقل حساسية **لإلغاء القيمة**
</span>**(Devaluation)** <span dir="rtl">مقارنة بالجرذان التي تلقت
تدريبًا أقل، فسيكون هذا دليلًا على أن التدريب الموسع يجعل **السلوك**
</span>**(Behavior)** <span dir="rtl">أكثر اعتيادًا. تبعت تجربة آدامز عن
كثب تجربة **آدامز وديكنسون**</span> **<span dir="rtl">(</span>Adams and
<span dir="rtl"></span>Dickinson<span dir="rtl">) عام 1981</span>**
<span dir="rtl">التي تم وصفها للتو. مبسطًا قليلاً، تم تدريب الجرذان في
مجموعة واحدة حتى قامت بـ 100 ضغطة على الرافعة معززة، وتم تدريب الجرذان
في المجموعة الأخرى—المجموعة التي خضعت للتدريب المفرط حتى قامت بـ 500
ضغطة على الرافعة معززة. بعد هذا التدريب، تم تخفيض **قيمة
المكافأة**</span> **(Reward Value)** <span dir="rtl">للحبيبات (باستخدام
حقن **كلوريد الليثيوم** </span>**(Lithium
Chloride)**<span dir="rtl">)</span> <span dir="rtl">للجرذان في كلا
المجموعتين. ثم تم إعطاء كلتا المجموعتين جلسة تدريب على **الإطفاء**
</span>**(Extinction Training)**<span dir="rtl">. كان سؤال آدامز هو ما
إذا كان **إلغاء القيمة**</span> **(Devaluation)** <span dir="rtl">سيؤثر
على معدل الضغط على الرافعة للجرذان التي خضعت للتدريب المفرط أقل مما كان
عليه بالنسبة للجرذان التي لم تخضع للتدريب المفرط، مما سيكون دليلًا على أن
التدريب الموسع يقلل من الحساسية **لإلغاء القيمة** </span>**(Outcome
Devaluation)**<span dir="rtl">.</span> <span dir="rtl">اتضح أن **إلغاء
القيمة**</span> **(Devaluation)** <span dir="rtl">قلل بشدة من معدل الضغط
على الرافعة للجرذان التي لم تخضع للتدريب المفرط. أما بالنسبة للجرذان
التي خضعت للتدريب المفرط، فقد كان **لإلغاء القيمة**</span>
**(Devaluation)** <span dir="rtl">تأثير ضئيل على ضغطها على الرافعة؛ في
الواقع، إذا كان هناك أي تأثير، فقد جعلها أكثر حماسًا. (تضمنت التجربة
الكاملة مجموعات ضابطة أظهرت أن كميات التدريب المختلفة لم تؤثر بحد ذاتها
بشكل كبير على معدلات الضغط على الرافعة بعد التعليم). أشارت هذه النتيجة
إلى أنه بينما كانت الجرذان التي لم تخضع للتدريب المفرط تتصرف بطريقة
موجهة نحو الهدف حساسة لمعرفة نتائج أفعالها، كانت الجرذان التي خضعت
للتدريب المفرط قد طورت **عادة الضغط على الرافعة**
</span>**(Lever-Pressing Habit)**<span dir="rtl">.</span>

<span dir="rtl">يقدم النظر إلى هذه النتائج ونتائج أخرى مماثلة من منظور
حسابي نظرة عميقة حول سبب توقع أن تتصرف **الحيوانات**</span>
**(Animals)** <span dir="rtl">بشكل معتاد في بعض الظروف، وبطريقة موجهة
نحو الهدف في ظروف أخرى، ولماذا تتحول من نمط تحكم إلى آخر مع استمرار
**التعليم** </span>**(Learning)**<span dir="rtl">.</span>
<span dir="rtl">في حين أنه من المؤكد أن **الحيوانات**</span>
**(Animals)** <span dir="rtl">تستخدم خوارزميات لا تتطابق تمامًا مع تلك
التي قدمناها في هذا الكتاب، يمكن للمرء أن يكتسب نظرة حول **سلوك
الحيوانات**</span> **(Animal Behavior)** <span dir="rtl">من خلال النظر
في المقايضات التي تلمح إليها خوارزميات **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">المختلفة. فكرة طورها علماء
الأعصاب الحاسوبيون **داو، نيف، ودايان**</span> **(Daw, Niv, and Dayan)
<span dir="rtl">عام 2005</span>** <span dir="rtl">هي أن
**الحيوانات**</span> **(Animals)** <span dir="rtl">تستخدم كل من
**العمليات الخالية من النموذج (**</span>**Model-free
Processes<span dir="rtl">) </span>**<span dir="rtl">و**العمليات القائمة
على النموذج** </span>**(Model-based Processes)**<span dir="rtl">.</span>
<span dir="rtl">كل عملية تقترح **فعلًا**
</span>**(Action)**<span dir="rtl">، ويتم اختيار الفعل للتنفيذ بناءً على
العملية التي يُحكم عليها بأنها الأكثر جدارة بالثقة من الاثنتين كما تحددها
مقاييس الثقة التي يتم الحفاظ عليها طوال فترة **التعليم**
</span>**(Learning)**<span dir="rtl">.</span> <span dir="rtl">في المراحل
المبكرة من **التعليم** </span>**(Learning)**<span dir="rtl">، تكون عملية
**التخطيط**</span> **(Planning Process)** <span dir="rtl">الخاصة بنظام
قائم على النموذج أكثر جدارة بالثقة لأنها تربط بين **التنبؤات قصيرة
المدى**</span> **<span dir="rtl">(</span>Short-Term
<span dir="rtl"></span>Predictions<span dir="rtl">)
</span>**<span dir="rtl">التي يمكن أن تصبح دقيقة مع خبرة أقل مقارنة
**بالتنبؤات طويلة المدى**</span> **<span dir="rtl">(</span>Long-Term
Predictions<span dir="rtl">) </span>**<span dir="rtl">الخاصة بالعملية
الخالية من النموذج. ولكن مع استمرار **التجربة**
</span>**(Experience)**<span dir="rtl">، تصبح العملية الخالية من النموذج
أكثر جدارة بالثقة لأن **التخطيط** </span>**(Planning)**
<span dir="rtl">عرضة للوقوع في أخطاء بسبب عدم دقة النموذج والاختصارات
الضرورية لجعل **التخطيط** </span>**(Planning)** <span dir="rtl">ممكنًا،
مثل أشكال مختلفة من "تشذيب الشجرة</span>
(Tree-Pruning)"<span dir="rtl">:</span> <span dir="rtl">إزالة الفروع غير
الواعدة من شجرة البحث. وفقًا لهذه الفكرة، يمكن توقع حدوث تحول من **السلوك
الموجه نحو الهدف**</span> **<span dir="rtl">(</span>Goal-directed
<span dir="rtl"></span>Behavior<span dir="rtl">)
</span>**<span dir="rtl">إلى **السلوك المعتاد**</span> **(Habitual
Behavior)** <span dir="rtl">مع تراكم المزيد من **التجربة**
</span>**(Experience)**<span dir="rtl">. تم اقتراح أفكار أخرى حول كيفية
تحكيم **الحيوانات**</span> **(Animals)** <span dir="rtl">بين **التحكم
الموجه نحو الهدف**</span> **(Goal-directed Control)**
<span dir="rtl">و**التحكم المعتاد** </span>**(Habitual
Control)**<span dir="rtl">، ويستمر كل من **البحث السلوكي**</span>
**(Behavioral Research)** <span dir="rtl">و**أبحاث علم الأعصاب**</span>
**<span dir="rtl">(</span>Neuroscience
<span dir="rtl"></span>Research<span dir="rtl">)
</span>**<span dir="rtl">في فحص هذا الموضوع والأسئلة ذات الصلة</span>.

<span dir="rtl">يثبت التمييز بين **الخوارزميات الخالية من
النموذج**</span> **(Model-free Algorithms)**
<span dir="rtl">و**الخوارزميات القائمة على النموذج**</span>
**(Model-based Algorithms)** <span dir="rtl">أنه مفيد لهذا النوع من
الأبحاث. يمكن للمرء أن يفحص الآثار الحسابية لهذه الأنواع من
**الخوارزميات**</span> **(Algorithms)** <span dir="rtl">في بيئات مجردة
تكشف عن المزايا والقيود الأساسية لكل نوع. هذا يخدم كلا من اقتراح وتوضيح
الأسئلة التي توجه تصميم التجارب اللازمة لتعميق فهم علماء النفس **للتحكم
السلوكي المعتاد والموجه نحو الهدف**</span>
**<span dir="rtl">(</span>Habitual and
<span dir="rtl"></span>Goal-directed Behavioral
Control<span dir="rtl">)</span>**.

**<u>14.7 <span dir="rtl">الملخص</span> (Summary)</u>**

<span dir="rtl">هدفنا في هذا **الفصل**</span> **(Chapter)**
<span dir="rtl">كان مناقشة التوافقات بين **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">والدراسة التجريبية **لتعلم الحيوانات في علم
النفس**</span> **<span dir="rtl">(</span>Animal Learning in
<span dir="rtl"></span>Psychology<span dir="rtl">)</span>**.
<span dir="rtl">لقد أكدنا في البداية أن **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">كما هو موضح في هذا الكتاب
ليس مخصصًا لنمذجة تفاصيل **سلوك الحيوانات** </span>**(Animal
Behavior)**<span dir="rtl">.</span> <span dir="rtl">إنه إطار **حسابي
مجرد**</span> **(Abstract Computational Framework)**
<span dir="rtl">يستكشف الحالات المثالية من منظور **الذكاء الاصطناعي
والهندسة** </span>**(Artificial Intelligence and
Engineering)**<span dir="rtl">.</span> <span dir="rtl">ولكن العديد من
**الخوارزميات الأساسية للتعليم المعزز**</span>
**<span dir="rtl">(</span>Basic Reinforcement Learning
<span dir="rtl"></span>Algorithms<span dir="rtl">)
</span>**<span dir="rtl">كانت مستوحاة من **النظريات النفسية**
</span>**(Psychological Theories)**<span dir="rtl">، وفي بعض الحالات،
ساهمت هذه **الخوارزميات**</span> **(Algorithms)** <span dir="rtl">في
تطوير نماذج جديدة **لتعلم الحيوانات**</span> **(Animal Learning
Models)**<span dir="rtl">. هذا الفصل وصف أبرز هذه التوافقات</span>.

<span dir="rtl">التمييز في **التعليم المعزز**</span> **(Reinforcement
Learning)** <span dir="rtl">بين **الخوارزميات الخاصة بالتنبؤ**
</span>**(Algorithms for Prediction)** <span dir="rtl">و**الخوارزميات
الخاصة بالتحكم** </span>**(Algorithms for Control)**
<span dir="rtl">يتوازى مع التمييز في نظرية تعلم الحيوانات بين **التكييف
الكلاسيكي أو البافلوفي (**</span>**Classical or
<span dir="rtl"></span>Pavlovian Conditioning<span dir="rtl">)
</span>**<span dir="rtl">و**التكييف الأداتي** </span>**(Instrumental
Conditioning)**<span dir="rtl">.</span> <span dir="rtl">الفرق الرئيسي
بين **تجارب التكييف الأداتي**</span> **(Instrumental Conditioning
Experiments)** <span dir="rtl">و**التكييف الكلاسيكي**</span>
**(Classical Conditioning Experiments)** <span dir="rtl">هو أنه في
الأولى، تكون **المكافأة المعززة**</span> **(Reinforcing Stimulus)**
<span dir="rtl">مشروطة **بسلوك الحيوان** </span>**(Animal’s
Behavior)**<span dir="rtl">، بينما في الأخيرة، لا تكون كذلك.</span>
**<span dir="rtl">التعليم للتنبؤ عبر خوارزمية التفاضل الزمني</span> (TD
Algorithm)** <span dir="rtl">يتوافق مع **التكييف الكلاسيكي**</span>
**(Classical Conditioning)**<span dir="rtl">، وقد وصفنا **نموذج التفاضل
الزمني**</span> **<span dir="rtl">(</span>TD
<span dir="rtl"></span>Model<span dir="rtl">)
</span>**<span dir="rtl">للتكييف الكلاسيكي كأحد الأمثلة التي توضح كيف أن
مبادئ **التعليم المعزز** </span>**(Reinforcement Learning)**
<span dir="rtl">تفسر بعض تفاصيل **سلوك تعلم الحيوانات**</span>
**<span dir="rtl">(</span>Animal <span dir="rtl"></span>Learning
Behavior<span dir="rtl">)</span>**. <span dir="rtl">هذا النموذج يعمم
**نموذج رسكورلا-فاجنر**</span> **<span dir="rtl">(</span>Rescorla–Wagner
<span dir="rtl"></span>Model<span dir="rtl">)
</span>**<span dir="rtl">المؤثر من خلال تضمين البعد الزمني حيث تؤثر
الأحداث داخل التجارب الفردية على **التعليم**
</span>**(Learning)**<span dir="rtl">، ويوفر تفسيرًا **للتكييف من الدرجة
الثانية** </span>**(Second-Order Conditioning)**<span dir="rtl">، حيث
يصبح **المنبه المعزز**</span> **(Reinforcing Stimuli)**
<span dir="rtl">معززًا بحد ذاته. كما أنه يمثل أساسًا لرؤية مؤثرة **لنشاط
الخلايا العصبية المنتجة للدوبامين في الدماغ**</span>
**<span dir="rtl">(</span>Activity of Dopamine Neurons
<span dir="rtl"></span>in the
Brain<span dir="rtl">)</span>**<span dir="rtl">، وهو موضوع سنتناوله في
**الفصل 15**</span>.

**<span dir="rtl">التعليم من خلال التجربة والخطأ</span> (Learning by
Trial and Error)** <span dir="rtl">هو الأساس في جانب التحكم **للتعليم
المعزز** </span>**(Reinforcement Learning)**<span dir="rtl">.</span>
<span dir="rtl">قدمنا بعض التفاصيل حول تجارب **ثورندايك**
</span>**(Thorndike)** <span dir="rtl">مع القطط والحيوانات الأخرى التي
قادت إلى **قانون الأثر** </span>**(Law of Effect)**<span dir="rtl">،
الذي ناقشناه هنا وفي **الفصل 1**</span> <span dir="rtl">(الصفحة 15).
أشرنا إلى أنه في **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**<span dir="rtl">،
لا يجب أن تقتصر **الاستكشافات**</span> **(Exploration)**
<span dir="rtl">على "التخبط الأعمى  
(</span>Blind Groping<span dir="rtl">)"</span>; <span dir="rtl">يمكن
توليد التجارب بطرق متقدمة باستخدام المعرفة الفطرية والمكتسبة سابقًا طالما
كان هناك بعض **الاستكشاف**
</span>**(Exploration)**<span dir="rtl">.</span> <span dir="rtl">ناقشنا
طريقة التدريب التي سماها **ب. ف. سكينر**</span> **(B. F. Skinner)**
**<span dir="rtl">التشكيل</span> (Shaping)** <span dir="rtl">حيث يتم
تعديل شروط المكافأة تدريجيًا لتدريب **الحيوان**</span> **(Animal)**
<span dir="rtl">على تقريب السلوك المطلوب بشكل متزايد</span>.
**<span dir="rtl">التشكيل</span> (Shaping)** <span dir="rtl">ليس فقط لا
غنى عنه في تدريب **الحيوانات** </span>**(Animal
Training)**<span dir="rtl">، ولكنه أيضًا أداة فعالة لتدريب **وكلاء
التعليم المعزز** </span>**(Reinforcement Learning
Agents)**<span dir="rtl">.</span> <span dir="rtl">هناك أيضًا صلة بفكرة
**الحالة التحفيزية للحيوان** </span>**(Animal’s Motivational
State)**<span dir="rtl">، التي تؤثر على ما سيقترب منه **الحيوان**</span>
**(Animal)** <span dir="rtl">أو يتجنبه وما هي الأحداث التي تكون **مكافئة
أو معاقبة**</span> **(Rewarding or Punishing)**
<span dir="rtl">للحيوان</span>.

<span dir="rtl">تشمل **خوارزميات التعليم المعزز**</span>
**(Reinforcement Learning Algorithms)** <span dir="rtl">المقدمة في هذا
الكتاب آليتين أساسيتين لمعالجة مشكلة **التعزيز المتأخر**
</span>**(Delayed Reinforcement)**<span dir="rtl">:</span>
**<span dir="rtl">آثار الاستحقاق</span> (Eligibility Traces)**
<span dir="rtl">و**دوال القيمة**</span> **(Value Functions)**
<span dir="rtl">التي يتم تعلمها عبر **خوارزميات التفاضل الزمني**
</span>**(TD Algorithms)**<span dir="rtl">.</span> <span dir="rtl">كلتا
الآليتين لهما سوابق في **نظريات تعلم الحيوانات** </span>**(Theories of
Animal Learning)**<span dir="rtl">.</span> <span dir="rtl">تشبه **آثار
الاستحقاق** </span>**(Eligibility Traces)** **<span dir="rtl">آثار
المنبهات</span> (Stimulus Traces)** <span dir="rtl">في النظريات المبكرة،
و**دوال القيمة**</span> **(Value Functions)** <span dir="rtl">تقابل دور
**التعزيز الثانوي**</span> **(Secondary Reinforcement)**
<span dir="rtl">في توفير تقييمات **فورية تقريبًا**</span>
**<span dir="rtl">(</span>Nearly <span dir="rtl"></span>Immediate
Evaluative Feedback<span dir="rtl">)</span>**.

<span dir="rtl">التوافق التالي الذي تناوله الفصل هو ذلك بين **نماذج
البيئة**</span> **(Environment Models)** <span dir="rtl">في **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">وما يسميه
علماء النفس **الخرائط المعرفية (**</span>**Cognitive
<span dir="rtl"></span>Maps<span dir="rtl">)</span>**.
<span dir="rtl">تجارب أجريت في منتصف القرن العشرين زعمت أنها تظهر قدرة
**الحيوانات** </span>**(Animals)** <span dir="rtl">على تعلم **الخرائط
المعرفية**</span> **(Cognitive Maps)** <span dir="rtl">كبدائل أو كإضافات
**لارتباطات الحالة-الفعل** </span>**(State–Action
Associations)**<span dir="rtl">، ثم استخدامها لاحقًا لتوجيه **السلوك**
</span>**(Behavior)**<span dir="rtl">، خاصة عندما تتغير
**البيئة**</span> **(Environment)** <span dir="rtl">بشكل غير متوقع. تشبه
**نماذج البيئة**</span> **<span dir="rtl">(</span>Environment
<span dir="rtl"></span>Models<span dir="rtl">)
</span>**<span dir="rtl">في **التعليم المعزز**</span> **(Reinforcement
Learning)** **<span dir="rtl">الخرائط المعرفية</span>
<span dir="rtl">(</span>Cognitive
<span dir="rtl"></span>Maps<span dir="rtl">) </span>**<span dir="rtl">في
أنها يمكن أن تتعلم بواسطة **طرق التعليم الخاضع للإشراف**</span>
**<span dir="rtl">(</span>Supervised Learning
<span dir="rtl"></span>Methods<span dir="rtl">)
</span>**<span dir="rtl">دون الاعتماد على **إشارات المكافأة**
</span>**(Reward Signals)**<span dir="rtl">، ثم يمكن استخدامها لاحقًا
**لتخطيط السلوك** </span>**(Plan Behavior)**<span dir="rtl">.</span>

<span dir="rtl">التمييز بين **الخوارزميات الخالية من النموذج**</span>
**(Model-Free Algorithms)** <span dir="rtl">و**الخوارزميات القائمة على
النموذج**</span> **(Model-Based Algorithms)** <span dir="rtl">في
**التعليم المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">يتوافق مع التمييز في علم النفس بين **السلوك
المعتاد**</span> **(Habitual Behavior)** <span dir="rtl">و**السلوك
الموجه نحو الهدف** </span>**(Goal-directed
Behavior)**<span dir="rtl">.</span> <span dir="rtl">تتخذ **الخوارزميات
الخالية من النموذج** </span>**(Model-Free Algorithms)**
<span dir="rtl">القرارات من خلال الوصول إلى المعلومات المخزنة في
**السياسة** </span>**(Policy)** <span dir="rtl">أو **دالة قيمة الفعل**
</span>**(Action-Value Function)**<span dir="rtl">، بينما تختار **الطرق
القائمة على النموذج**</span> **(Model-Based Methods)**
**<span dir="rtl">الأفعال</span> (Actions)** <span dir="rtl">كنتيجة
**للتخطيط المسبق** </span>**(Planning Ahead)** <span dir="rtl">باستخدام
**نموذج لبيئة الوكيل**</span> **<span dir="rtl">(</span>Model of the
Agent’s <span dir="rtl"></span>Environment<span dir="rtl">)</span>**.
<span dir="rtl">توفر **تجارب إلغاء القيمة**
</span>**(Outcome-Devaluation Experiments)** <span dir="rtl">معلومات حول
ما إذا كان **سلوك الحيوان**</span> **(Animal’s Behavior)**
<span dir="rtl">معتادًا أو تحت **التحكم الموجه نحو الهدف**
</span>**(Goal-Directed Control)**<span dir="rtl">.</span>
<span dir="rtl">ساعدت نظرية **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">في توضيح التفكير حول هذه القضايا</span>.

<span dir="rtl">من الواضح أن **تعلم الحيوانات**</span> **(Animal
Learning)** <span dir="rtl">يسهم في **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**<span dir="rtl">،
ولكن كنوع من **التعليم الآلي** </span>**(Machine
Learning)**<span dir="rtl">، يهدف **التعليم المعزز**
</span>**(Reinforcement Learning)** <span dir="rtl">إلى تصميم وفهم
**خوارزميات تعلم فعالة (**</span>**Effective Learning
<span dir="rtl"></span>Algorithms<span dir="rtl">)</span>**<span dir="rtl">،
وليس إلى تكرار أو تفسير تفاصيل **سلوك الحيوانات** </span>**(Animal
Behavior)**<span dir="rtl">.</span> <span dir="rtl">ركزنا على جوانب
**تعلم الحيوانات**</span> **(Animal Learning)** <span dir="rtl">التي
ترتبط بوضوح بطرق حل مشاكل **التنبؤ** </span>**(Prediction)**
<span dir="rtl">و**التحكم** </span>**(Control)**<span dir="rtl">، مع
تسليط الضوء على تدفق الأفكار المثمر بين **التعليم المعزز**
</span>**(Reinforcement Learning)** <span dir="rtl">و**علم
النفس**</span> **(Psychology)** <span dir="rtl">دون التعمق في العديد من
التفاصيل السلوكية والجدالات التي شغلت انتباه الباحثين في **تعلم
الحيوانات** </span>**(Animal Learning)**<span dir="rtl">. من المرجح أن
يستفيد التطوير المستقبلي **لنظرية وخوارزميات التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement <span dir="rtl"></span>Learning
Theory and Algorithms<span dir="rtl">) </span>**<span dir="rtl">من
الروابط مع العديد من الميزات الأخرى **لتعلم الحيوانات**</span> **(Animal
Learning)** <span dir="rtl">مع تزايد تقدير الفائدة الحسابية لهذه
الميزات. نتوقع أن يستمر تدفق الأفكار بين **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">و**علم النفس**</span>
**(Psychology)** <span dir="rtl">في جلب الفوائد لكلا التخصصين</span>.

<span dir="rtl">العديد من الروابط بين **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">ومجالات **علم النفس**
</span>**(Psychology)** <span dir="rtl">و**العلوم السلوكية
الأخرى**</span> **(Other Behavioral Sciences)** <span dir="rtl">تتجاوز
نطاق هذا الفصل. نحن نتجنب بشكل كبير مناقشة الروابط مع **علم النفس الخاص
باتخاذ القرارات** </span>**(Psychology of Decision
Making)**<span dir="rtl">، الذي يركز على كيفية اختيار **الأفعال**
</span>**(Actions)**<span dir="rtl">، أو كيفية اتخاذ **القرارات**
</span>**(Decisions)**<span dir="rtl">، بعد أن يحدث **التعليم**
</span>**(Learning)**<span dir="rtl">.</span> <span dir="rtl">نحن أيضًا
لا نناقش الروابط مع الجوانب البيئية والتطورية **للسلوك**</span>
**(Behavior)** <span dir="rtl">التي يدرسها علماء **السلوك**
</span>**(Ethologists)** <span dir="rtl">و**علماء البيئة السلوكية**
</span>**(Behavioral Ecologists)**<span dir="rtl">:</span>
<span dir="rtl">كيف تتفاعل **الحيوانات**</span> **(Animals)**
<span dir="rtl">مع بعضها البعض ومع بيئتها المادية، وكيف يساهم
**سلوكها**</span> **(Behavior)** <span dir="rtl">في **اللياقة التطورية**
</span>**(Evolutionary Fitness)**<span dir="rtl">.</span>
<span dir="rtl">تحتل **التحسين**
</span>**(Optimization)**<span dir="rtl">، و**عمليات اتخاذ القرار
الماركوفية** </span>**(MDPs)**<span dir="rtl">، و**البرمجة
الديناميكية**</span> **(Dynamic Programming)** <span dir="rtl">مكانة
بارزة في هذه المجالات، وتركيزنا على تفاعل **الوكيل**</span> **(Agent)**
<span dir="rtl">مع **البيئات الديناميكية**</span> **(Dynamic
Environments)** <span dir="rtl">يرتبط بدراسة **سلوك الوكيل**</span>
**(Agent Behavior)** <span dir="rtl">في **البيئات المعقدة**
</span>**(Complex Ecologies)**<span dir="rtl">.</span>
**<span dir="rtl">التعليم المعزز متعدد الوكلاء</span> (Multi-Agent
Reinforcement Learning)**<span dir="rtl">، الذي لم يتم تناوله في هذا
الكتاب، له روابط بالجوانب الاجتماعية **للسلوك**
</span>**(Behavior)**<span dir="rtl">.</span> <span dir="rtl">على الرغم
من عدم تناولها هنا، يجب عدم تفسير **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">على أنه يتجاهل المنظورات
التطورية. لا شيء في **التعليم المعزز**</span> **(Reinforcement
Learning)** <span dir="rtl">يوحي **بنظرية اللوح الفارغ  
(**</span>**Tabula <span dir="rtl"></span>Rasa<span dir="rtl">)
</span>**<span dir="rtl">حول **التعليم والسلوك** </span>**(Learning and
Behavior)**<span dir="rtl">.</span> <span dir="rtl">في الواقع، سلطت
التجارب مع التطبيقات الهندسية الضوء على أهمية بناء **نظم التعليم
المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning Systems<span dir="rtl">)</span>**
<span dir="rtl">المعرفة التي تشبه ما توفره **التطورات**</span>
**(Evolution)** **<span dir="rtl">للحيوانات</span>
(Animals)**<span dir="rtl">.</span>

<span dir="rtl">الفصل الخامس عشر:  
علم الأعصاب</span> (Neuroscience)

<span dir="rtl">علم الأعصاب</span> (Neuroscience) <span dir="rtl">هو
الدراسة متعددة التخصصات لأنظمة الأعصاب  
(</span>Nervous Systems<span dir="rtl">)</span>: <span dir="rtl">كيف
تنظم **وظائف الجسم** </span>**(Bodily Functions)**<span dir="rtl">،
وتتحكم في **السلوك** </span>**(Behavior)**<span dir="rtl">، وتغير على مر
الزمن نتيجة للتطور</span> (Development)<span dir="rtl">، **التعليم**
</span>**(Learning)**<span dir="rtl">، والشيخوخة</span>
(Aging)<span dir="rtl">؛ وكيف تجعل الآليات الخلوية والجزيئية
(</span>Cellular and Molecular
<span dir="rtl"></span>Mechanisms<span dir="rtl">) هذه
**الوظائف**</span> **(Functions)** <span dir="rtl">ممكنة. أحد أكثر
الجوانب إثارة في **التعليم المعزز** </span>**(Reinforcement Learning)**
<span dir="rtl">هو الأدلة المتزايدة من **علم الأعصاب**</span>
**(Neuroscience)** <span dir="rtl">التي تشير إلى أن **أنظمة
الأعصاب**</span> **(Nervous Systems)** <span dir="rtl">لدى البشر والعديد
من الحيوانات الأخرى تطبق **الخوارزميات**</span> **(Algorithms)**
<span dir="rtl">التي تتطابق بشكل ملحوظ مع **خوارزميات التعليم المعزز**
</span>**(Reinforcement Learning Algorithms)**<span dir="rtl">. الهدف
الرئيسي من هذا **الفصل**</span> **(Chapter)** <span dir="rtl">هو شرح هذه
التشابهات وما تشير إليه حول الأساس العصبي للتعليم المرتبط
بالمكافأة</span> <span dir="rtl">(</span>Reward-Related
Learning<span dir="rtl">)</span> <span dir="rtl">في الحيوانات</span>.

<span dir="rtl">أكثر نقطة اتصال ملحوظة بين **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">و**علم الأعصاب**
</span>**(Neuroscience)** <span dir="rtl">تتعلق بالدوبامين</span>
(Dopamine)<span dir="rtl">، وهو **مادة كيميائية**</span> **(Chemical)**
<span dir="rtl">تلعب دورًا عميقًا في **معالجة المكافآت**</span> **(Reward
Processing)** <span dir="rtl">في أدمغة الثدييات. يبدو أن **الدوبامين**
</span>**(Dopamine)** <span dir="rtl">ينقل أخطاء التفاضل الزمني</span>
(Temporal Difference (TD) Errors) <span dir="rtl">إلى **الهياكل
الدماغية**</span> **(Brain Structures)** <span dir="rtl">حيث يحدث
**التعليم**</span> **(Learning)** <span dir="rtl">واتخاذ القرارات</span>
<span dir="rtl">(</span>Decision
<span dir="rtl"></span>Making<span dir="rtl">)</span>.
<span dir="rtl">يتم التعبير عن هذا التشابه من خلال فرضية خطأ تنبؤ
المكافأة</span> <span dir="rtl">(</span>Reward Prediction
<span dir="rtl"></span>Error Hypothesis<span dir="rtl">) لنشاط خلايا
الدوبامين العصبية، وهي فرضية نتجت عن التقارب بين **التعليم المعزز
الحاسوبي**</span> **(Computational Reinforcement Learning)**
<span dir="rtl">ونتائج تجارب **علم الأعصاب**
</span>**(Neuroscience)**<span dir="rtl">.</span> <span dir="rtl">في هذا
**الفصل**</span> **(Chapter)** <span dir="rtl">نناقش هذه الفرضية،
والاكتشافات العصبية التي أدت إليها، ولماذا تُعتبر مساهمة مهمة في فهم
أنظمة مكافأة الدماغ</span> <span dir="rtl">(</span>Brain Reward
<span dir="rtl"></span>Systems<span dir="rtl">)</span>.
<span dir="rtl">كما نناقش التشابهات بين **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">و**علم الأعصاب**</span>
**(Neuroscience)** <span dir="rtl">التي تكون أقل إثارة من هذا التشابه
بين الدوبامين وخطأ التفاضل الزمني</span>
(Dopamine/TD-Error)<span dir="rtl">، ولكنها توفر أدوات مفاهيمية مفيدة
للتفكير في التعليم القائم على المكافأة في الحيوانات. بعض عناصر **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">لديها
القدرة على التأثير في دراسة **أنظمة الأعصاب** </span>**(Nervous
Systems)**<span dir="rtl">، ولكن ارتباطاتها بـ **علم الأعصاب**
</span>**(Neuroscience)** <span dir="rtl">لا تزال غير متطورة بشكل كبير.
نناقش في هذا **الفصل**</span> **(Chapter)** <span dir="rtl">عدة من هذه
الروابط المتطورة التي نعتقد أنها ستزداد أهمية بمرور الوقت</span>.

<span dir="rtl">كما أوضحنا في **قسم التاريخ**</span> **(History
Section)** <span dir="rtl">في **الفصل التمهيدي لهذا الكتاب**
</span>**(Introduction Chapter)** <span dir="rtl">(الفصل 1.7)، تأثرت
العديد من جوانب **التعليم المعزز** </span>**(Reinforcement Learning)**
<span dir="rtl">بـ **علم الأعصاب**
</span>**(Neuroscience)**<span dir="rtl">.</span> <span dir="rtl">الهدف
الثاني من هذا **الفصل**</span> **(Chapter)** <span dir="rtl">هو تعريف
القراء بالأفكار حول **وظيفة الدماغ**</span> **(Brain Function)**
<span dir="rtl">التي ساهمت في نهجنا تجاه **التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">.</span>
<span dir="rtl">بعض عناصر **التعليم المعزز** </span>**(Reinforcement
Learning)** <span dir="rtl">تكون أسهل في الفهم عند رؤيتها في ضوء نظريات
**وظيفة الدماغ** </span>**(Brain Function)**<span dir="rtl">. هذا ينطبق
بشكل خاص على فكرة أثر الاستحقاق</span> (Eligibility
Trace)<span dir="rtl">، وهو أحد الآليات الأساسية لـ **التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">، التي نشأت كخاصية
مفترضة للتشابكات العصبية</span> (Synapses)<span dir="rtl">، وهي الهياكل
التي تتواصل من خلالها خلايا الأعصاب</span> (Neurons) <span dir="rtl">مع
بعضها البعض</span>.

<span dir="rtl">في هذا **الفصل** </span>**(Chapter)**<span dir="rtl">،
لا نتعمق بشكل كبير في التعقيد الهائل لأنظمة الأعصاب</span>
<span dir="rtl">(</span>Neural
<span dir="rtl"></span>Systems<span dir="rtl">) التي تكمن وراء **التعليم
القائم على المكافأة**</span> **(Reward-Based Learning)**
<span dir="rtl">في الحيوانات؛ هذا **الفصل**</span> **(Chapter)**
<span dir="rtl">قصير جدًا، ونحن لسنا علماء أعصاب. لا نحاول وصف أو حتى
تسمية العديد من الهياكل الدماغية</span> (Brain Structures)
<span dir="rtl">والمسارات</span> (Pathways)<span dir="rtl">، أو أي من
الآليات الجزيئية</span> (Molecular Mechanisms) <span dir="rtl">التي
يُعتقد أنها تشارك في هذه العمليات. كما أننا لا ننصف الفرضيات والنماذج
التي تعتبر بدائل لتلك التي تتماشى بشكل جيد مع **التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">. لا يجب أن يكون
مفاجئًا أن هناك آراء مختلفة بين الخبراء في هذا المجال. نحن لا نقدم سوى
لمحة عن هذه القصة المثيرة والمستمرة في التطور. نأمل مع ذلك أن يقنعك هذا
**الفصل**</span> **(Chapter)** <span dir="rtl">بأن قناة مثمرة للغاية قد
ظهرت، تربط بين **التعليم المعزز** </span>**(Reinforcement Learning)**
<span dir="rtl">وأساساته النظرية وبين **علم الأعصاب**</span>
**(Neuroscience)** <span dir="rtl">الخاص بالتعليم القائم على المكافأة في
الحيوانات</span>.

<span dir="rtl">توجد العديد من المنشورات الممتازة التي تغطي الروابط بين
**التعليم المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**
<span dir="rtl">و**علم الأعصاب**
</span>**(Neuroscience)**<span dir="rtl">، وقد نقتبس بعضها في القسم
الأخير من هذا **الفصل** </span>**(Chapter)**<span dir="rtl">.</span>
<span dir="rtl">تختلف معالجتنا عن معظم هذه المنشورات لأننا نفترض إلمامًا
بـ **التعليم المعزز** </span>**(Reinforcement Learning)**
<span dir="rtl">كما تم تقديمه في الفصول السابقة من هذا **الكتاب**
</span>**(Book)**<span dir="rtl">، لكننا لا نفترض معرفة مسبقة بـ **علم
الأعصاب** </span>**(Neuroscience)**<span dir="rtl">.</span>
<span dir="rtl">نبدأ بمقدمة موجزة عن المفاهيم العصبية اللازمة لفهم أساسي
لما سيأتي بعد ذلك</span>.

**<u>15.1 <span dir="rtl">أساسيات علم الأعصاب</span> (Neuroscience
Basics)</u>**

<span dir="rtl">بعض المعلومات الأساسية حول **أنظمة الأعصاب**</span>
**(Nervous Systems)** <span dir="rtl">ستكون مفيدة لمتابعة ما سنغطيه في
هذا **الفصل** </span>**(Chapter)**<span dir="rtl">.</span>
<span dir="rtl">المصطلحات التي سنشير إليها لاحقًا ستكون مائلة. تخطي هذا
**القسم**</span> **(Section)** <span dir="rtl">لن يشكل مشكلة إذا كنت
تملك معرفة أولية بـ **علم الأعصاب**
</span>**(Neuroscience)**<span dir="rtl">.</span>

**<span dir="rtl">الخلايا العصبية</span> (Neurons)** <span dir="rtl">هي
المكونات الرئيسية لـ **أنظمة الأعصاب** </span>**(Nervous
Systems)**<span dir="rtl">، وهي **خلايا**</span> **(Cells)**
<span dir="rtl">متخصصة في معالجة ونقل المعلومات باستخدام إشارات كهربائية
وكيميائية. تأتي **الخلايا العصبية**</span> **(Neurons)**
<span dir="rtl">بأشكال عديدة، ولكن عادة ما تحتوي على **جسم الخلية**
</span>**(Cell Body)**<span dir="rtl">، و**التغصنات**
</span>**(Dendrites)**<span dir="rtl">، و**محور عصبي واحد**
</span>**(Single Axon)**<span dir="rtl">.</span>
**<span dir="rtl">التغصنات</span> (Dendrites)** <span dir="rtl">هي هياكل
تتفرع من **جسم الخلية**</span> **(Cell Body)** <span dir="rtl">لتستقبل
المدخلات من **خلايا عصبية**</span> **(Neurons)** <span dir="rtl">أخرى
(أو لتستقبل أيضًا إشارات خارجية في حالة **الخلايا العصبية الحسية**
</span>**(Sensory Neurons)**<span dir="rtl">).</span>
**<span dir="rtl">المحور العصبي</span> (Axon)** <span dir="rtl">هو ليف
ينقل مخرجات **الخلايا العصبية**</span> **(Neuron)** <span dir="rtl">إلى
**خلايا عصبية**</span> **(Neurons)** <span dir="rtl">أخرى (أو إلى
العضلات أو الغدد). تتكون مخرجات **الخلية العصبية**</span> **(Neuron)**
<span dir="rtl">من تسلسلات من النبضات الكهربائية التي تسمى **جهود
الفعل**</span> **(Action Potentials)** <span dir="rtl">التي تسير على طول
**المحور العصبي** </span>**(Axon)**<span dir="rtl">.</span>
<span dir="rtl">تُسمى **جهود الفعل**</span> **(Action Potentials)**
<span dir="rtl">أيضًا **النبضات العصبية**
</span>**(Spikes)**<span dir="rtl">، ويُقال إن **الخلية العصبية**</span>
**(Neuron)** <span dir="rtl">تطلق النبضة عندما تولد نبضة عصبية. في نماذج
الشبكات العصبية</span> (Neural Networks)<span dir="rtl">، من الشائع
استخدام **الأعداد الحقيقية** </span>**(Real Numbers)**
<span dir="rtl">لتمثيل معدل إطلاق **الخلية العصبية** </span>**(Neuron's
Firing Rate)**<span dir="rtl">، وهو متوسط عدد النبضات العصبية لكل وحدة
زمنية معينة</span>.

<span dir="rtl">يمكن أن يتفرع **محور الخلية العصبية**</span> **(Neuron's
Axon)** <span dir="rtl">بشكل واسع بحيث تصل **جهود الفعل**
</span>**(Action Potentials)** <span dir="rtl">الخاصة بـ **الخلية
العصبية**</span> **(Neuron)** <span dir="rtl">إلى العديد من الأهداف.
تُسمى البنية المتفرعة لـ **محور الخلية العصبية**</span> **(Neuron's
Axon)** **<span dir="rtl">تفرع المحور العصبي</span> (Axonal
Arbor)**<span dir="rtl">. وبما أن توصيل **جهود الفعل**</span> **(Action
Potential Conduction)** <span dir="rtl">هو عملية نشطة، تشبه إلى حد ما
احتراق الفتيل، فعندما يصل **جهد الفعل**</span> **(Action Potential)**
<span dir="rtl">إلى نقطة تفرع **المحور العصبي** </span>**(Axonal Branch
Point)** <span dir="rtl">فإنه "يشعل</span>" **<span dir="rtl">جهود
الفعل</span> (Action Potentials)** <span dir="rtl">في جميع الفروع
المنطلقة (على الرغم من أن الانتشار إلى فرع معين قد يفشل أحيانًا). نتيجة
لذلك، يمكن لنشاط **خلية عصبية**</span> **(Neuron)** <span dir="rtl">ذات
**تفرع محور عصبي كبير**</span> **(Large Axonal Arbor)**
<span dir="rtl">أن يؤثر على العديد من المواقع المستهدفة</span>.

**<span dir="rtl">المشبك العصبي</span> (Synapse)** <span dir="rtl">هو
هيكل يوجد عادةً في نهاية فرع المحور العصبي</span> (Axon Branch)
<span dir="rtl">ويقوم بوساطة الاتصال بين **الخلايا العصبية**
</span>**(Neurons)**<span dir="rtl">.</span> <span dir="rtl">يقوم
**المشبك العصبي**</span> **(Synapse)** <span dir="rtl">بنقل المعلومات من
**محور الخلية العصبية قبل المشبكية**</span> **(Presynaptic Neuron's
Axon)** <span dir="rtl">إلى تغصن أو جسم الخلية **بعد المشبكية**
</span>**(Postsynaptic Neuron)**<span dir="rtl">.</span>
<span dir="rtl">مع بعض الاستثناءات، يطلق **المشبك العصبي**</span>
**(Synapse)** **<span dir="rtl">ناقلًا عصبيًا كيميائيًا</span> (Chemical
Neurotransmitter)** <span dir="rtl">عند وصول **جهد الفعل**</span>
**(Action Potential)** <span dir="rtl">من **الخلية العصبية قبل المشبكية
(**</span>**Presynaptic
<span dir="rtl"></span>Neuron<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">(الاستثناءات هي حالات الربط الكهربائي المباشر بين
**الخلايا العصبية** </span>**(Neurons)**<span dir="rtl">، ولكننا لن
نتناولها هنا). تنتشر جزيئات **الناقل العصبي**</span>
**(Neurotransmitter)** <span dir="rtl">التي تطلق من الجانب قبل
المشبكي</span> (Presynaptic Side) <span dir="rtl">للمشبك عبر **الفجوة
المشبكية** </span>**(Synaptic Cleft)**<span dir="rtl">، وهي المسافة
الصغيرة جدًا بين نهاية الخلية قبل المشبكية و**الخلية العصبية بعد
المشبكية**</span> **<span dir="rtl">(</span>Postsynaptic
<span dir="rtl"></span>Neuron<span dir="rtl">)</span>**<span dir="rtl">،
ثم ترتبط بالمستقبلات على سطح الخلية بعد المشبكية</span> (Postsynaptic
Neuron) <span dir="rtl">لتحفيز أو تثبيط نشاطها في توليد النبضات
العصبية</span> (Spike-Generating Activity)<span dir="rtl">، أو لتعديل
سلوكها بطرق أخرى. يمكن أن يرتبط **ناقل عصبي معين**</span>
**(Neurotransmitter)** <span dir="rtl">بعدة أنواع مختلفة من
**المستقبلات** </span>**(Receptors)**<span dir="rtl">، بحيث ينتج كل نوع
تأثيرًا مختلفًا على **الخلية العصبية بعد المشبكية** </span>**(Postsynaptic
Neuron)**<span dir="rtl">.</span> <span dir="rtl">على سبيل المثال، هناك
خمسة أنواع مختلفة على الأقل من المستقبلات التي يمكن من خلالها أن يؤثر
**الناقل العصبي الدوبامين**</span> **(Dopamine)** <span dir="rtl">على
**الخلية العصبية بعد المشبكية** </span>**(Postsynaptic
Neuron)**<span dir="rtl">.</span> <span dir="rtl">تم تحديد العديد من
**المواد الكيميائية** </span>**(Chemicals)** <span dir="rtl">المختلفة كـ
**ناقلات عصبية**</span> **(Neurotransmitters)** <span dir="rtl">في
**أنظمة الأعصاب** </span>**(Nervous Systems)** <span dir="rtl">لدى
الحيوانات</span>.

**<span dir="rtl">النشاط الخلفي للخلية العصبية</span> (Neuron’s
Background Activity)** <span dir="rtl">هو مستوى نشاطها، عادة معدل
إطلاقها للنبضات، عندما لا يبدو أن **الخلية العصبية**</span> **(Neuron)**
<span dir="rtl">مدفوعة بمدخلات مشبكية مرتبطة بالمهمة التي تهم المجرب،
على سبيل المثال، عندما لا يرتبط نشاط الخلية بمحفز معين مقدم لموضوع
التجربة. يمكن أن يكون **النشاط الخلفي**</span> **(Background Activity)**
<span dir="rtl">غير منتظم بسبب المدخلات من الشبكة العصبية الأوسع، أو
بسبب الضجيج داخل **الخلية العصبية**</span> **(Neuron)**
<span dir="rtl">أو مشابكها. أحيانًا يكون **النشاط الخلفي**</span>
**(Background Activity)** <span dir="rtl">نتيجة عمليات ديناميكية داخلية
خاصة بـ **الخلية العصبية** </span>**(Neuron)**<span dir="rtl">.</span>
**<span dir="rtl">النشاط الفازي للخلية العصبية (</span>Neuron’s Phasic
<span dir="rtl"></span>Activity<span dir="rtl">)</span>**<span dir="rtl">،
على النقيض من **نشاطها الخلفي** </span>**(Background
Activity)**<span dir="rtl">، يتكون من انفجارات من النشاط النبضي عادة ما
تسببها المدخلات المشبكية. أما **النشاط التوني للخلية العصبية  
(**</span>**Neuron’s Tonic Activity<span dir="rtl">)</span>**
<span dir="rtl">فهو النشاط الذي يتغير ببطء وغالبًا بطريقة متدرجة، سواء
كان كنشاط خلفي أو لا</span>.

**<span dir="rtl">قوة أو فعالية المشبك العصبي</span> (Synapse’s
Efficacy)** <span dir="rtl">هي مدى تأثير **الناقل العصبي**
</span>**(Neurotransmitter)** <span dir="rtl">الذي يتم إطلاقه عند
**المشبك**</span> **(Synapse)** <span dir="rtl">على **الخلية العصبية بعد
المشبكية** </span>**(Postsynaptic Neuron)**<span dir="rtl">.</span>
<span dir="rtl">واحدة من الطرق التي يمكن أن يتغير بها **النظام العصبي**
</span>**(Nervous System)** <span dir="rtl">من خلال التجربة هي من خلال
التغيرات في **فعالية المشابك العصبية** </span>**(Synaptic Efficacies)**
<span dir="rtl">نتيجة لتراكيب معينة من نشاط الخلايا العصبية قبل وبعد
المشبكية</span> (Presynaptic and Postsynaptic Neurons)<span dir="rtl">،
وأحيانًا بوجود **معدل عصبي** </span>**(Neuromodulator)**<span dir="rtl">،
وهو **ناقل عصبي**</span> **(Neurotransmitter)** <span dir="rtl">له
تأثيرات أخرى غير أو بالإضافة إلى التحفيز أو التثبيط السريع
المباشر</span>.

<span dir="rtl">تحتوي الأدمغة على عدة **أنظمة تنظيم عصبي مختلفة**</span>
**(Neuromodulation Systems)** <span dir="rtl">تتكون من مجموعات من
**الخلايا العصبية**</span> **(Neurons)** <span dir="rtl">ذات تفرعات
واسعة لمحاورها العصبية</span> <span dir="rtl">(</span>Axonal
<span dir="rtl"></span>Arbors<span dir="rtl">)، مع استخدام كل نظام
**ناقلًا عصبيًا مختلفًا** </span>**(Different
Neurotransmitter)**<span dir="rtl">.</span> <span dir="rtl">يمكن أن تؤثر
**الأنظمة العصبية التنظيمية**</span> **(Neuromodulation Systems)**
<span dir="rtl">على وظيفة الدوائر العصبية، وتوسط التحفيز، والتنبيه،
والانتباه، والذاكرة، والمزاج، والعاطفة، والنوم، ودرجة حرارة الجسم. المهم
هنا هو أن **النظام التنظيمي العصبي**</span> **(Neuromodulatory System)**
<span dir="rtl">يمكن أن يوزع شيئًا مثل إشارة عددية</span> (Scalar
Signal)<span dir="rtl">، مثل إشارة التعزيز</span> (Reinforcement
Signal)<span dir="rtl">، لتغيير عمل المشابك في المواقع الموزعة بشكل واسع
والتي تكون حاسمة للتعليم</span>.

<span dir="rtl">القدرة على تغيير **فعالية المشابك العصبية**</span>
**(Synaptic Efficacies)** <span dir="rtl">تُسمى **اللدونة المشبكية**
</span>**(Synaptic Plasticity)**<span dir="rtl">. إنها واحدة من الآليات
الأساسية المسؤولة عن **التعليم** </span>**(Learning)**<span dir="rtl">.
**البارامترات**</span> **(Parameters)** <span dir="rtl">أو
**الأوزان**</span> **(Weights)** <span dir="rtl">التي تعدلها
**الخوارزميات التعليمية**</span> **<span dir="rtl">(</span>Learning
<span dir="rtl"></span>Algorithms<span dir="rtl">)</span>**
<span dir="rtl">تتوافق مع **فعالية المشابك العصبية** </span>**(Synaptic
Efficacies)**<span dir="rtl">.</span> <span dir="rtl">كما سنفصل أدناه،
فإن **تعديل اللدونة المشبكية**</span> **(Modulation of Synaptic
Plasticity)** <span dir="rtl">عبر **الناقل العصبي الدوبامين**</span>
**(Neuromodulator Dopamine)** <span dir="rtl">هو آلية محتملة لكيفية
تنفيذ الدماغ **الخوارزميات التعليمية**</span> **(Learning Algorithms)**
<span dir="rtl">مثل العديد من تلك الموصوفة في هذا **الكتاب**
</span>**(Book)**<span dir="rtl">.</span>

**<u>15.2 <span dir="rtl">إشارات المكافأة، إشارات التعزيز، القيم، وأخطاء
التنبؤ</span> <span dir="rtl">(</span>Reward Signals, Reinforcement
Signals, Values, and Prediction Errors<span dir="rtl">)</span></u>**

<span dir="rtl">تبدأ الروابط بين **علم الأعصاب**</span>
**(Neuroscience)** <span dir="rtl">و**التعليم المعزز الحاسوبي**</span>
**<span dir="rtl">(</span>Computational
<span dir="rtl"></span>Reinforcement Learning<span dir="rtl">)
</span>**<span dir="rtl">كتشابهات بين الإشارات في الدماغ والإشارات التي
تلعب أدوارًا بارزة في نظرية **التعليم المعزز**</span> **(Reinforcement
Learning Theory)** <span dir="rtl">و**الخوارزميات**
</span>**(Algorithms)**<span dir="rtl">. في **الفصل3**، قلنا إن أي مشكلة
تتعلق بتعلم السلوك الموجه نحو الهدف يمكن تقليلها إلى ثلاث إشارات تمثل
**الإجراءات** </span>**(Actions)**<span dir="rtl">، **الحالات**
</span>**(States)**<span dir="rtl">، و**المكافآت**
</span>**(Rewards)**<span dir="rtl">.</span> <span dir="rtl">ومع ذلك،
لشرح الروابط التي تم إنشاؤها بين **علم الأعصاب**</span>
**(Neuroscience)** <span dir="rtl">و**التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">، علينا أن نكون أقل
تجريدًا من ذلك وأن نأخذ في الاعتبار إشارات أخرى في **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">تتوافق بطرق
معينة مع الإشارات في الدماغ. بالإضافة إلى **إشارات المكافأة**
</span>**(Reward Signals)**<span dir="rtl">، تشمل هذه أيضًا **إشارات
التعزيز** </span>**(Reinforcement Signals)** <span dir="rtl">(التي نرى
أنها تختلف عن إشارات المكافأة)، **إشارات القيمة** </span>**(Value
Signals)**<span dir="rtl">، والإشارات التي تنقل **أخطاء التنبؤ**
</span>**(Prediction Errors)**<span dir="rtl">.</span>
<span dir="rtl">عندما نُسمّي إشارة بوظيفتها بهذه الطريقة، نفعل ذلك في سياق
**نظرية التعليم المعزز**</span> **<span dir="rtl">(</span>Reinforcement
Learning <span dir="rtl"></span>Theory<span dir="rtl">)</span>**
<span dir="rtl">حيث تتوافق الإشارة مع مصطلح في معادلة أو **خوارزمية**
</span>**(Algorithm)**<span dir="rtl">.</span> <span dir="rtl">من ناحية
أخرى، عندما نشير إلى إشارة في الدماغ، نعني حدثًا فسيولوجيًا مثل انفجار من
**جهود الفعل**</span> **<span dir="rtl">(</span>Action
<span dir="rtl"></span>Potentials<span dir="rtl">)
</span>**<span dir="rtl">أو إفراز **ناقل عصبي**
</span>**(Neurotransmitter)**<span dir="rtl">.</span>
<span dir="rtl">تسمية إشارة عصبية بوظيفتها، مثل تسمية **النشاط
الفازي**</span> **(Phasic Activity)** <span dir="rtl">لخلية الدوبامين
العصبية بأنها إشارة تعزيز، يعني أن الإشارة العصبية تتصرف مثل، وتُفترض
أنها تعمل مثل، الإشارة النظرية المقابلة</span>.

<span dir="rtl">كشف الأدلة على هذه التوافقات يتطلب مواجهة العديد من
التحديات. يمكن العثور على **النشاط العصبي** </span>**(Neural Activity)**
<span dir="rtl">المرتبط بمعالجة المكافآت</span> (Reward Processing)
<span dir="rtl">في كل جزء تقريبًا من الدماغ، ومن الصعب تفسير النتائج بشكل
غير قابل للالتباس لأن تمثيلات إشارات المكافأة المختلفة تميل إلى أن تكون
مرتبطة بشكل كبير مع بعضها البعض. تحتاج التجارب إلى أن تكون مصممة بعناية
للسماح بتمييز نوع واحد من إشارات المكافأة المرتبطة بأي درجة من اليقين عن
غيرها - أو عن وفرة من الإشارات الأخرى غير المرتبطة بمعالجة المكافآت. على
الرغم من هذه الصعوبات، تم إجراء العديد من التجارب بهدف التوفيق بين جوانب
**نظرية التعليم المعزز** </span>**(Reinforcement Learning Theory)**
<span dir="rtl">و**الخوارزميات**</span> **(Algorithms)**
<span dir="rtl">مع الإشارات العصبية</span> (Neural
Signals)<span dir="rtl">، وقد تم إنشاء بعض الروابط المقنعة. للتحضير لفحص
هذه الروابط، في بقية هذا **القسم**</span> **(Section)**
<span dir="rtl">نذكّر القارئ بما تعنيه إشارات المكافأة المختلفة وفقًا لـ
**نظرية التعليم المعزز** </span>**(Reinforcement Learning
Theory)**<span dir="rtl">.</span>

<span dir="rtl">في **تعليقاتنا على المصطلحات**</span> **(Comments on
Terminology)** <span dir="rtl">في نهاية **الفصل السابق**
</span>**(Previous Chapter)**<span dir="rtl">، قلنا إن</span> $`Rt`$
<span dir="rtl"></span>​ <span dir="rtl">يشبه إشارة المكافأة في دماغ
الحيوان وليس كائنًا أو حدثًا في بيئة الحيوان. في **التعليم المعزز**</span>
**(Reinforcement Learning)**<span dir="rtl">، تحدد إشارة المكافأة (إلى
جانب بيئة **الوكيل** </span>**(Agent's
Environment)**<span dir="rtl">)</span> <span dir="rtl">المشكلة التي
يحاول وكيل **التعليم المعزز** </span>**(Reinforcement Learning)**
<span dir="rtl">حلها. من هذه الناحية، يشبه</span> $`Rt`$
<span dir="rtl">إشارة في دماغ الحيوان تقوم بتوزيع **المكافأة
الأولية**</span> **(Primary Reward)** <span dir="rtl">إلى مواقع في جميع
أنحاء الدماغ. ولكن من غير المحتمل أن توجد إشارة مكافأة رئيسية موحدة
مثل</span> $`Rt`$ <span dir="rtl">في دماغ الحيوان. من الأفضل التفكير
في</span> $`Rt`$ <span dir="rtl">على أنه تجريد يلخص التأثير العام لعدد
كبير من الإشارات العصبية الناتجة عن العديد من الأنظمة في الدماغ التي
تقيم الصفات المكافئة أو العقابية للإحساسات والحالات</span>.

**<span dir="rtl">إشارات التعزيز</span> (Reinforcement Signals)**
<span dir="rtl">في **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">تختلف عن **إشارات المكافأة** </span>**(Reward
Signals)**<span dir="rtl">.</span> <span dir="rtl">وظيفة إشارة التعزيز
هي توجيه التغييرات التي تُجريها **الخوارزمية التعليمية**</span>
**(Learning Algorithm)** <span dir="rtl">في **سياسة الوكيل**
</span>**(Agent's Policy)**<span dir="rtl">، **تقديرات القيم**
</span>**(Value Estimates)**<span dir="rtl">، أو **نماذج البيئة**</span>
**<span dir="rtl">(</span>Environment
<span dir="rtl"></span>Models<span dir="rtl">)</span>**.
<span dir="rtl">على سبيل المثال، بالنسبة لطريقة **التفاضل الزمني**
</span>**(TD Method)**<span dir="rtl">، تكون إشارة التعزيز عند
الزمن</span> t <span dir="rtl">هي **خطأ التفاضل الزمني** </span>**(TD
Error)**

``` math
\delta_{t - 1} = R_{t} + \gamma V\left( S_{t} \right) - V\left( S_{t - 1} \right)
```

<span dir="rtl">إشارة التعزيز</span> (Reinforcement Signal)
<span dir="rtl">لبعض **الخوارزميات**</span> **(Algorithms)**
<span dir="rtl">قد تكون ببساطة إشارة المكافأة</span> (Reward
Signal)<span dir="rtl">، ولكن بالنسبة لمعظم **الخوارزميات**</span>
**(Algorithms)** <span dir="rtl">التي نناقشها، تكون إشارة التعزيز هي
إشارة المكافأة المعدلة بمعلومات أخرى، مثل **تقديرات القيم**</span>
**<span dir="rtl">(</span>Value
<span dir="rtl"></span>Estimates<span dir="rtl">)
</span>**<span dir="rtl">في أخطاء التفاضل الزمني</span> (TD
Errors)<span dir="rtl">.</span>

**<span dir="rtl">تقديرات قيم الحالات</span> (State Values)**
<span dir="rtl">أو **تقديرات قيم الإجراءات** </span>**(Action
Values)**<span dir="rtl">، أي</span> $`V`$ <span dir="rtl">أو</span>
$`Q`$<span dir="rtl">، تحدد ما هو جيد أو سيئ للوكيل</span> (Agent)
<span dir="rtl">على المدى الطويل. فهي **تنبؤات**
</span>**(Predictions)** <span dir="rtl">بإجمالي المكافأة التي يمكن أن
يتوقع الوكيل الحصول عليها في المستقبل. يقوم الوكلاء باتخاذ قرارات جيدة
من خلال اختيار الإجراءات التي تؤدي إلى حالات ذات **قيم حالة
تقديرية**</span> **<span dir="rtl">(</span>Estimated State
<span dir="rtl"></span>Values<span dir="rtl">)
</span>**<span dir="rtl">أكبر، أو من خلال اختيار الإجراءات ذات **قيم
الإجراء التقديرية**</span> **<span dir="rtl">(</span>Estimated Action
<span dir="rtl"></span>Values<span dir="rtl">)
</span>**<span dir="rtl">الأكبر</span>.

**<span dir="rtl">أخطاء التنبؤ</span> (Prediction Errors)**
<span dir="rtl">تقيس الفجوات بين **الإشارات**</span> **(Signals)**
<span dir="rtl">أو **الإحساسات** </span>**(Sensations)**
<span dir="rtl">المتوقعة والفعلية.</span> **<span dir="rtl">أخطاء تنبؤ
المكافأة</span> <span dir="rtl">(</span>Reward Prediction Errors,
<span dir="rtl"></span>RPEs<span dir="rtl">)
</span>**<span dir="rtl">تقيس تحديدًا الفجوات بين إشارة المكافأة المتوقعة
وتلك التي تم استلامها، بحيث تكون موجبة عندما تكون إشارة المكافأة أكبر من
المتوقع، وسالبة في الحالة الأخرى. أخطاء التفاضل الزمني  
(</span>TD Errors<span dir="rtl">) مثل المعادلة (6.5) هي أنواع خاصة من
**أخطاء تنبؤ المكافأة**</span> **(RPEs)** <span dir="rtl">التي تشير إلى
الفجوات بين التوقعات الحالية والسابقة للمكافأة على المدى الطويل. عندما
يشير علماء الأعصاب إلى **أخطاء تنبؤ المكافأة**</span> **(RPEs)**
<span dir="rtl">فإنهم عمومًا (وإن لم يكن دائمًا) يقصدون **أخطاء تنبؤ
التفاضل الزمني** </span>**(TD RPEs)**<span dir="rtl">، والتي نسميها
ببساطة أخطاء التفاضل الزمني</span> (TD Errors) <span dir="rtl">طوال هذا
**الفصل** </span>**(Chapter)**<span dir="rtl">. أيضًا في هذا **الفصل**
</span>**(Chapter)**<span dir="rtl">، يشير **خطأ التفاضل الزمني**</span>
**(TD Error)** <span dir="rtl">عمومًا إلى خطأ لا يعتمد على **الإجراءات**
</span>**(Actions)**<span dir="rtl">، على عكس أخطاء التفاضل الزمني
المستخدمة في تعلم **قيم الإجراءات**</span> **(Action-Values)**
<span dir="rtl">بواسطة **الخوارزميات**</span> **(Algorithms)**
<span dir="rtl">مثل **سارسا** </span>**(Sarsa)**
<span dir="rtl">و</span>**Q-learning**<span dir="rtl">.</span>
<span dir="rtl">وذلك لأن الروابط الأكثر شهرة بـ **علم الأعصاب**</span>
**(Neuroscience)** <span dir="rtl">تم تحديدها من حيث **أخطاء التفاضل
الزمني غير المعتمدة على الإجراءات** </span>**(Action-Free TD
Errors)**<span dir="rtl">، ولكننا لا نستبعد إمكانية وجود روابط مماثلة
تتعلق بأخطاء التفاضل الزمني المعتمدة على الإجراءات.</span>
<span dir="rtl">(أخطاء التفاضل الزمني لتوقع الإشارات الأخرى غير المكافآت
مفيدة أيضًا، لكن هذه الحالة لن تشغلنا هنا. انظر، على سبيل المثال،</span>
Modayil, White, and Sutton, 2014<span dir="rtl">).</span>

<span dir="rtl">يمكن للمرء أن يطرح العديد من الأسئلة حول الروابط بين
بيانات **علم الأعصاب**</span> **<span dir="rtl">(</span>Neuroscience
<span dir="rtl"></span>Data<span dir="rtl">)
</span>**<span dir="rtl">وهذه الإشارات المعرفة نظريًا. هل الإشارة
المرصودة تشبه إشارة المكافأة</span> (Reward Signal)<span dir="rtl">، أو
إشارة القيمة</span> (Value Signal)<span dir="rtl">، أو خطأ التنبؤ</span>
(Prediction Error)<span dir="rtl">، أو إشارة التعزيز</span>
(Reinforcement Signal)<span dir="rtl">، أو شيئًا مختلفًا تمامًا؟ وإذا كانت
إشارة خطأ، هل هي **خطأ تنبؤ المكافأة** </span>**(RPE)**<span dir="rtl">،
أو **خطأ التفاضل الزمني** </span>**(TD Error)**<span dir="rtl">، أو خطأ
أبسط مثل **خطأ ريسكورلا-فاغنر** </span>**(Rescorla-Wagner Error)**
<span dir="rtl">في المعادلة (14.3)؟ وإذا كانت **خطأ التفاضل الزمني  
(**</span>**TD
<span dir="rtl"></span>Error<span dir="rtl">)</span>**<span dir="rtl">،
هل يعتمد على الإجراءات مثل **خطأ التفاضل الزمني في**</span>
**Q-learning** <span dir="rtl">أو **سارسا**
</span>**(Sarsa)**<span dir="rtl">؟ كما أشير سابقًا، فإن فحص الدماغ
للإجابة على أسئلة من هذا النوع صعب للغاية. لكن الأدلة التجريبية تشير إلى
أن ناقلًا عصبيًا واحدًا، وهو **الناقل العصبي الدوبامين**
</span>**(Dopamine)**<span dir="rtl">، يشير إلى **أخطاء تنبؤ المكافأة**
</span>**(RPEs)**<span dir="rtl">، وعلاوة على ذلك، فإن النشاط
الفازي</span> (Phasic Activity) <span dir="rtl">للخلايا العصبية المنتجة
للدوبامين ينقل في الواقع **أخطاء التفاضل الزمني**</span> **(TD Errors)**
<span dir="rtl">(انظر **الفصل 15.1**</span> <span dir="rtl">لتعريف
**النشاط الفازي** </span>**(Phasic Activity)**<span dir="rtl">).</span>
<span dir="rtl">وقد أدت هذه الأدلة إلى فرضية **خطأ تنبؤ المكافأة لنشاط
خلايا الدوبامين العصبية**</span> **<span dir="rtl">(</span>Reward
Prediction Error Hypothesis of <span dir="rtl"></span>Dopamine Neuron
Activity<span dir="rtl">)</span>**<span dir="rtl">، التي سنقوم بوصفها
بعد ذلك</span>.

**<u>15.3 <span dir="rtl">فرضية خطأ تنبؤ المكافأة</span> (The Reward
Prediction Error Hypothesis)</u>**

<span dir="rtl">فرضية **خطأ تنبؤ المكافأة**</span> **(Reward Prediction
Error Hypothesis)** <span dir="rtl">لنشاط **خلايا الدوبامين
العصبية**</span> **(Dopamine Neuron Activity)** <span dir="rtl">تقترح أن
إحدى وظائف **النشاط الفازي  
(**</span>**Phasic Activity<span dir="rtl">) </span>**<span dir="rtl">لـ
**الخلايا العصبية المنتجة للدوبامين**</span>
**<span dir="rtl">(</span>Dopamine-Producing
<span dir="rtl"></span>Neurons<span dir="rtl">)
</span>**<span dir="rtl">في الثدييات هي إيصال خطأ بين تقدير قديم وجديد
للمكافأة المتوقعة في المستقبل إلى المناطق المستهدفة في جميع أنحاء
الدماغ. تم التصريح بهذه الفرضية لأول مرة (وإن لم يكن بهذه الكلمات
بالضبط) من قبل **مونتاغ، دايان، وسجنوسكي**</span>
**<span dir="rtl">(</span>Montague, Dayan, and
<span dir="rtl"></span>Sejnowski<span dir="rtl">)</span>**
<span dir="rtl"></span>(1996)<span dir="rtl">، الذين أظهروا كيف أن مفهوم
**خطأ التفاضل الزمني**</span> **(TD Error)** <span dir="rtl">من
**التعليم المعزز** </span>**(Reinforcement Learning)**
<span dir="rtl">يفسر العديد من ميزات **النشاط الفازي**</span> **(Phasic
Activity)** <span dir="rtl">لـ **خلايا الدوبامين العصبية**</span>
**(Dopamine Neurons)** <span dir="rtl">في الثدييات. التجارب التي أدت إلى
هذه الفرضية أجريت في الثمانينيات وأوائل التسعينيات في مختبر عالم الأعصاب
**وولفرام شولتز** </span>**(Wolfram Schultz)**<span dir="rtl">. يصف
**القسم 15.4**</span> <span dir="rtl">هذه التجارب المؤثرة، ويشرح **القسم
15.6**</span> <span dir="rtl">كيف تتماشى نتائج هذه التجارب مع **أخطاء
التفاضل الزمني** </span>**(TD Errors)**<span dir="rtl">، كما يشمل **قسم
الملاحظات البيبليوغرافية والتاريخية**</span> **(Bibliographical and
Historical Remarks Section)** <span dir="rtl">في نهاية هذا
**الفصل**</span> <span dir="rtl">دليلًا للمؤلفات المتعلقة بتطوير هذه
الفرضية المؤثرة</span>.

**<span dir="rtl">مونتاغ وآخرون</span> (Montague et al.)** (1996)
<span dir="rtl">قارنوا بين **أخطاء التفاضل الزمني** </span>**(TD
Errors)** <span dir="rtl">في نموذج</span> TD <span dir="rtl">للتكييف
الكلاسيكي مع **النشاط الفازي**</span> **(Phasic Activity)**
<span dir="rtl">لـ **الخلايا العصبية المنتجة للدوبامين**</span>
**(Dopamine-Producing Neurons)** <span dir="rtl">أثناء تجارب التكييف
الكلاسيكي. تذكر من **القسم 14.2**</span> <span dir="rtl">أن نموذج</span>
TD <span dir="rtl">للتكييف الكلاسيكي هو بشكل أساسي **خوارزمية**
</span>**TD <span dir="rtl">ذات النزول شبه التدرجي</span> (Semi-Gradient
Descent TD(λ) Algorithm)** <span dir="rtl">مع **تقريب دالة خطية**</span>
**<span dir="rtl">(</span>Linear <span dir="rtl"></span>Function
Approximation<span dir="rtl">)</span>**. <span dir="rtl">قام **مونتاغ
وآخرون**</span> **(Montague et al.)** <span dir="rtl">بعمل عدة افتراضات
لإعداد هذه المقارنة. أولًا، لأن **خطأ التفاضل الزمني**</span> **(TD
Error)** <span dir="rtl">يمكن أن يكون سالبًا، ولكن **الخلايا
العصبية**</span> **(Neurons)** <span dir="rtl">لا يمكن أن يكون لها معدل
إطلاق سلبي، فقد افترضوا أن الكمية المقابلة لنشاط **خلية الدوبامين
العصبية**</span> **(Dopamine Neuron Activity)**
<span dir="rtl">هي</span> $`\delta t - 1 + bt`$ ​<span dir="rtl">،
حيث</span> $`bt`$ <span dir="rtl">هو معدل إطلاق الخلية الخلفي</span>
(Background Firing Rate) <span dir="rtl">للخلية</span>.
**<span dir="rtl">خطأ التفاضل الزمني السالب</span> (Negative TD Error)**
<span dir="rtl">يتوافق مع انخفاض في معدل إطلاق **خلية الدوبامين
العصبية** </span>**(Dopamine Neuron's Firing Rate)** <span dir="rtl">إلى
ما دون معدلها الخلفي</span>.

<span dir="rtl">كان هناك افتراض ثانٍ مطلوب حول **الحالات**</span>
**(States)** <span dir="rtl">التي تمت زيارتها في كل تجربة من تجارب
التكييف الكلاسيكي وكيف يتم تمثيلها كمدخلات لـ **الخوارزمية التعليمية**
</span>**(Learning Algorithm)**<span dir="rtl">. هذه هي نفس المسألة التي
ناقشناها في **القسم 14.2.4**</span> <span dir="rtl">لنموذج</span>
TD<span dir="rtl">.</span> <span dir="rtl">اختار **مونتاغ وآخرون**
</span>**(Montague et al.)** <span dir="rtl">تمثيل مركب متسلسل
كامل</span> <span dir="rtl">(</span>Complete Serial Compound (CSC)
<span dir="rtl"></span>Representation<span dir="rtl">) كما هو موضح في
العمود الأيسر من **الشكل 14.1**</span> **(Figure
14.1)**<span dir="rtl">، ولكن حيث يستمر تسلسل الإشارات الداخلية قصيرة
المدى حتى بدء **المحفز غير المشروط** </span>**(Unconditioned Stimulus,
US)**<span dir="rtl">، والذي هنا هو وصول إشارة مكافأة غير صفرية. يتيح
هذا التمثيل لـ **خطأ التفاضل الزمني**</span> **(TD Error)**
<span dir="rtl">محاكاة حقيقة أن نشاط **خلية الدوبامين العصبية**
</span>**(Dopamine Neuron Activity)** <span dir="rtl">لا يتنبأ فقط
بمكافأة مستقبلية، ولكنه حساس أيضًا لموعد توقع وصول هذه المكافأة بعد إشارة
تنبؤية. يجب أن يكون هناك طريقة ما لتتبع الوقت بين **الإشارات الحسية**
</span>**(Sensory Cues)** <span dir="rtl">ووصول المكافأة. إذا بدأ
**التحفيز**</span> **(Stimulus)** <span dir="rtl">تسلسلًا من الإشارات
الداخلية التي تستمر بعد انتهاء التحفيز، وإذا كان هناك إشارة مختلفة لكل
خطوة زمنية بعد التحفيز، فإن كل خطوة زمنية بعد التحفيز تمثلها حالة مميزة.
لذلك، يمكن أن يكون **خطأ التفاضل الزمني** </span>**(TD
Error)**<span dir="rtl">، نظرًا لاعتماده على الحالة، حساسًا لتوقيت الأحداث
داخل التجربة</span>.

<span dir="rtl">في التجارب المحاكية مع هذه الافتراضات حول **معدل الإطلاق
الخلفي**</span> **<span dir="rtl">(</span>Background Firing
<span dir="rtl"></span>Rate<span dir="rtl">)
</span>**<span dir="rtl">وتمثيل المدخلات، تتشابه **أخطاء التفاضل
الزمني**</span> **(TD Errors)** <span dir="rtl">في نموذج</span> TD
<span dir="rtl">بشكل ملحوظ مع **النشاط الفازي**</span> **(Phasic
Activity)** <span dir="rtl">لـ **خلايا الدوبامين العصبية**
</span>**(Dopamine Neurons)**<span dir="rtl">. معاينة لوصفنا للتفاصيل
حول هذه التشابهات في **القسم 15.4**</span> <span dir="rtl">أدناه، تتوازى
أخطاء التفاضل الزمني مع الميزات التالية لنشاط **خلايا الدوبامين
العصبية** </span>**(Dopamine Neuron Activity)**<span dir="rtl">:</span>
1<span dir="rtl">)</span> <span dir="rtl">يحدث **الاستجابة الفازية لخلية
الدوبامين العصبية** </span>**(Phasic Response of a Dopamine Neuron)**
<span dir="rtl">فقط عندما يكون حدث المكافأة غير متوقع؛ 2) في المراحل
المبكرة من التعليم، لا تتسبب الإشارات المحايدة التي تسبق المكافأة في
استجابات فازية كبيرة للدوبامين، ولكن مع استمرار التعليم، تكتسب هذه
الإشارات **قيمة تنبؤية**</span> **(Predictive Value)**
<span dir="rtl">وتبدأ في إثارة استجابات فازية للدوبامين؛ 3) إذا كانت
إشارة سابقة تحدث بشكل موثوق قبل إشارة اكتسبت بالفعل **قيمة تنبؤية**
</span>**(Predictive Value)**<span dir="rtl">، فإن **الاستجابة
الفازية**</span> **(Phasic Dopamine Response)** <span dir="rtl">تنتقل
إلى الإشارة السابقة وتتوقف عند الإشارة اللاحقة؛ و4) إذا تم إلغاء حدث
المكافأة المتوقع بعد التعليم، ينخفض استجابة **خلية الدوبامين
العصبية**</span> **(Dopamine Neuron's Response)** <span dir="rtl">إلى ما
دون مستوى الخط الأساسي لها بعد وقت قصير من الوقت المتوقع لحدث
المكافأة</span>.

<span dir="rtl">على الرغم من أن كل **خلية دوبامين عصبية**</span>
**(Dopamine Neuron)** <span dir="rtl">تم مراقبتها في تجارب شولتز وزملائه
لم تتصرف بهذه الطرق كلها، إلا أن التوافق الملحوظ بين أنشطة معظم الخلايا
العصبية المراقبة وأخطاء **التفاضل الزمني**</span> **(TD Errors)**
<span dir="rtl">يدعم بشدة **فرضية خطأ تنبؤ المكافأة**</span>
**<span dir="rtl">(</span>Reward <span dir="rtl"></span>Prediction Error
Hypothesis<span dir="rtl">)</span>**. <span dir="rtl">ومع ذلك، هناك
مواقف لا تتطابق فيها التوقعات المستندة إلى الفرضية مع ما يتم ملاحظته في
التجارب. إن اختيار **تمثيل المدخلات** </span>**(Input Representation)**
<span dir="rtl">أمر حاسم لمدى توافق **أخطاء التفاضل الزمني**</span>
**(TD Errors)** <span dir="rtl">مع بعض التفاصيل المتعلقة بنشاط **خلايا
الدوبامين العصبية** </span>**(Dopamine Neuron
Activity)**<span dir="rtl">، خصوصًا التفاصيل المتعلقة بتوقيت استجابات
**خلايا الدوبامين العصبية** </span>**(Dopamine Neuron
Responses)**<span dir="rtl">.</span> <span dir="rtl">تم اقتراح أفكار
مختلفة، بعضها نناقشه أدناه، حول **تمثيلات المدخلات**</span> **(Input
Representations)** <span dir="rtl">وخصائص أخرى لتعلم **التفاضل
الزمني**</span> **(TD Learning)** <span dir="rtl">لجعل **أخطاء التفاضل
الزمني**</span> **(TD Errors)** <span dir="rtl">تتوافق بشكل أفضل مع
البيانات، على الرغم من أن التشابهات الرئيسية تظهر مع **تمثيل المركب
المتسلسل الكامل**</span> **(CSC Representation)** <span dir="rtl">الذي
استخدمه مونتاغ وآخرون. بشكل عام، حصلت **فرضية خطأ تنبؤ المكافأة**</span>
**(Reward Prediction Error Hypothesis)** <span dir="rtl">على قبول واسع
بين علماء الأعصاب الذين يدرسون **التعليم القائم على المكافأة**
</span>**(Reward-Based Learning)**<span dir="rtl">، وقد أثبتت أنها قوية
بشكل ملحوظ في مواجهة تراكم النتائج من تجارب علم الأعصاب</span>.

<span dir="rtl">للتحضير لوصفنا لتجارب **علم الأعصاب**</span>
**(Neuroscience Experiments)** <span dir="rtl">التي تدعم **فرضية خطأ
تنبؤ المكافأة** </span>**(Reward Prediction Error
Hypothesis)**<span dir="rtl">، ولتوفير بعض السياق حتى يمكن تقدير أهمية
الفرضية، نقدم بعد ذلك بعض المعلومات المعروفة عن **الدوبامين**
</span>**(Dopamine)**<span dir="rtl">، **الهياكل الدماغية**</span>
**(Brain Structures)** <span dir="rtl">التي يؤثر عليها، وكيف يشارك في
**التعليم القائم على المكافأة** </span>**(Reward-Based
Learning)**<span dir="rtl">.</span>

**<u>15.4 <span dir="rtl">الدوبامين</span> (Dopamine)</u>**

<span dir="rtl">يُنتج **الدوبامين**</span> **(Dopamine)**
<span dir="rtl">كـ **ناقل عصبي**</span> **(Neurotransmitter)**
<span dir="rtl">بواسطة **الخلايا العصبية** </span>**(Neurons)**
<span dir="rtl">التي تقع أجسامها الخلوية أساسًا في مجموعتين من **الخلايا
العصبية**</span> **(Neurons)** <span dir="rtl">في **الدماغ
الأوسط**</span> **(Midbrain)** <span dir="rtl">لدى الثدييات</span>:
**<span dir="rtl">المادة السوداء</span>
<span dir="rtl">(</span>Substantia Nigra Pars Compacta,
SNpc<span dir="rtl">) </span>**<span dir="rtl">و**المنطقة السقيفية
البطنية** </span>**(Ventral Tegmental Area,
VTA)**<span dir="rtl">.</span> <span dir="rtl">يلعب **الدوبامين**</span>
**(Dopamine)** <span dir="rtl">أدوارًا أساسية في العديد من العمليات في
دماغ الثدييات. من بين هذه الأدوار البارزة</span>:
**<span dir="rtl">التحفيز</span> (Motivation)**<span dir="rtl">،
**التعليم** </span>**(Learning)**<span dir="rtl">، **اختيار الإجراء**
</span>**(Action-Selection)**<span dir="rtl">، معظم أشكال **الإدمان**
</span>**(Addiction)**<span dir="rtl">، والاضطرابات مثل
**الفصام**</span> **(Schizophrenia)** <span dir="rtl">و**مرض باركنسون**
</span>**(Parkinson's Disease)**<span dir="rtl">.</span>
<span dir="rtl">يُطلق على **الدوبامين**</span> **(Dopamine)**
<span dir="rtl">مصطلح **معدل عصبي**</span> **(Neuromodulator)**
<span dir="rtl">لأنه يقوم بالعديد من الوظائف بخلاف التحفيز أو التثبيط
السريع المباشر لـ **الخلايا العصبية المستهدفة** </span>**(Targeted
Neurons)**<span dir="rtl">.</span> <span dir="rtl">على الرغم من أن
الكثير لا يزال غير معروف حول وظائف **الدوبامين**</span> **(Dopamine)**
<span dir="rtl">وتفاصيل تأثيراته الخلوية، إلا أنه من الواضح أنه أساسي في
**معالجة المكافأة**</span> **(Reward Processing)** <span dir="rtl">في
دماغ الثدييات.</span> **<span dir="rtl">الدوبامين</span> (Dopamine)**
<span dir="rtl">ليس **المعدل العصبي**</span> **(Neuromodulator)**
<span dir="rtl">الوحيد المعني بـ **معالجة المكافأة (**</span>**Reward
<span dir="rtl"></span>Processing<span dir="rtl">)</span>**<span dir="rtl">،
ودوره في المواقف المؤذية - مثل **العقاب**</span> **(Punishment)** -
<span dir="rtl">لا يزال مثيرًا للجدل. كما يمكن أن يعمل
**الدوبامين**</span> **(Dopamine)** <span dir="rtl">بشكل مختلف في
الكائنات غير الثديية. لكن لا أحد يشك في أن **الدوبامين**</span>
**(Dopamine)** <span dir="rtl">أساسي للعمليات المتعلقة بالمكافأة في
الثدييات، بما في ذلك البشر</span>.

<span dir="rtl">كان الرأي التقليدي المبكر هو أن **الخلايا العصبية
المنتجة للدوبامين**</span> **<span dir="rtl">(</span>Dopamine-Producing
<span dir="rtl"></span>Neurons<span dir="rtl">)</span>**
<span dir="rtl">تبث إشارة مكافأة إلى مناطق دماغية متعددة متورطة في
**التعليم**</span> **(Learning)** <span dir="rtl">و**التحفيز**
</span>**(Motivation)**<span dir="rtl">.</span> <span dir="rtl">نشأ هذا
الرأي من ورقة بحثية شهيرة في عام 1954 بواسطة **جيمس أولدز**</span>
**<span dir="rtl">(</span>James
<span dir="rtl"></span>Olds<span dir="rtl">)
</span>**<span dir="rtl">و**بيتر ميلنر**</span> **(Peter Milner)**
<span dir="rtl">التي وصفت تأثيرات التحفيز الكهربائي على مناطق معينة من
دماغ الفأر. وجدوا أن التحفيز الكهربائي لمناطق معينة يعمل كمكافأة قوية
جدًا في التحكم في سلوك الفأر: "… السيطرة التي تمارس على سلوك الحيوان
بوساطة هذه المكافأة شديدة للغاية، وربما تتجاوز تلك التي تمارسها أي
مكافأة أخرى تم استخدامها سابقًا في تجارب الحيوان</span>"
<span dir="rtl">(</span>Olds and Milner,
<span dir="rtl"></span>1954<span dir="rtl">)</span>.
<span dir="rtl">أظهرت الأبحاث اللاحقة أن المواقع التي كان فيها التحفيز
أكثر فعالية في إنتاج هذا التأثير المكافئ تقوم بتحفيز مسارات
**الدوبامين** </span>**(Dopamine Pathways)**<span dir="rtl">، إما بشكل
مباشر أو غير مباشر، والتي تتحفز عادةً بالمحفزات الطبيعية المكافئة. لوحظت
تأثيرات مشابهة لهذه أيضًا على البشر. أشارت هذه الملاحظات بقوة إلى أن نشاط
**الخلايا العصبية المنتجة للدوبامين**</span>
**<span dir="rtl">(</span>Dopamine <span dir="rtl"></span>Neuron
Activity<span dir="rtl">) </span>**<span dir="rtl">يُشير إلى
المكافأة</span>.

<span dir="rtl">لكن إذا كانت **فرضية خطأ تنبؤ المكافأة**</span>
**(Reward Prediction Error Hypothesis)** <span dir="rtl">صحيحة حتى لو
كانت تفسر فقط بعض ميزات نشاط **خلايا الدوبامين العصبية**</span>
**<span dir="rtl">(</span>Dopamine Neuron
<span dir="rtl"></span>Activity<span dir="rtl">)</span>** -
<span dir="rtl">فإن هذا الرأي التقليدي حول نشاط **خلايا الدوبامين
العصبية**</span> **<span dir="rtl">(</span>Dopamine Neuron
<span dir="rtl"></span>Activity<span dir="rtl">)
</span>**<span dir="rtl">ليس صحيحًا تمامًا:</span>
**<span dir="rtl">الاستجابات الفازية</span> (Phasic Responses)**
<span dir="rtl">لخلايا الدوبامين العصبية تشير إلى **أخطاء تنبؤ
المكافأة** </span>**(Reward Prediction Errors)**<span dir="rtl">، وليس
إلى المكافأة نفسها. من منظور **نظرية التعليم المعزز**</span>
**(Reinforcement Learning Theory)** <span dir="rtl">و**الخوارزميات**
</span>**(Algorithms)**<span dir="rtl">، تتوافق **الاستجابة
الفازية**</span> **(Phasic Response)** <span dir="rtl">لخلية دوبامين
عصبية عند الزمن</span> $`t`$ <span dir="rtl">مع</span>
$`\delta t - 1 = Rt + \gamma V(St) - V(St - 1)`$<span dir="rtl">، وليس
مع</span> $`Rt`$<span dir="rtl">.</span>

<span dir="rtl">تساعد **نظرية التعليم المعزز**</span> **(Reinforcement
Learning Theory)** <span dir="rtl">و**الخوارزميات**
</span>**(Algorithms)** <span dir="rtl">في التوفيق بين وجهة نظر **خطأ
تنبؤ المكافأة** </span>**(Reward Prediction Error)**
<span dir="rtl">والمفهوم التقليدي بأن **الدوبامين**</span>
**(Dopamine)** <span dir="rtl">يشير إلى المكافأة. في العديد من
**الخوارزميات** </span>**(Algorithms)** <span dir="rtl">التي نناقشها في
هذا **الكتاب** </span>**(Book)**<span dir="rtl">، تعمل</span> $`\delta`$
<span dir="rtl">كإشارة تعزيز</span>
<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Signal<span dir="rtl">)، مما يعني أنها المحرك
الرئيسي للتعليم. على سبيل المثال،</span> $`\delta`$ <span dir="rtl">هو
العامل الحاسم في نموذج **التفاضل الزمني**</span> **(TD Model)**
<span dir="rtl">لـ **التكييف الكلاسيكي** </span>**(Classical
Conditioning)**<span dir="rtl">، و</span>$`\delta`$ <span dir="rtl">هي
إشارة التعزيز لتعلم كل من **دالة القيمة**</span> **(Value Function)**
<span dir="rtl">و**السياسة**</span> **(Policy)** <span dir="rtl">في
**هيكلية الممثل–الناقد**</span> **(Actor–Critic Architecture)**
<span dir="rtl">(انظر الأقسام 13.5 و15.7). الأشكال المعتمدة على الإجراء
من</span> $`\delta`$ <span dir="rtl">هي إشارات التعزيز لـ</span>
**Q-learning** <span dir="rtl">و**سارسا**
</span>**(Sarsa)**<span dir="rtl">.</span> <span dir="rtl">إشارة
المكافأة</span> $`Rt`$ <span dir="rtl"></span>​ <span dir="rtl">هي مكون
حاسم في</span> $`\delta t - 1`$​<span dir="rtl">، لكنها ليست المحدد
الوحيد لتأثيرها التعزيزي في هذه **الخوارزميات**
</span>**(Algorithms)**<span dir="rtl">. المصطلح الإضافي</span>
$`\gamma V(St) - V(St - 1)`$ <span dir="rtl">هو الجزء المعزز من الدرجة
الأعلى في</span> $`\delta t - 1`$​<span dir="rtl">، وحتى إذا حدثت
المكافأة</span> ($`Rt \neq 0`$)<span dir="rtl">، فإن **خطأ التفاضل
الزمني**</span> **(TD Error)** <span dir="rtl">يمكن أن يكون صامتًا إذا
كانت المكافأة متوقعة بالكامل (وهو ما يتم شرحه بالتفصيل في **القسم
15.6**</span> <span dir="rtl">أدناه)</span>.

<span dir="rtl">نظرة أقرب إلى ورقة أولدز وملنر عام 1954، في الواقع، تكشف
أنها تدور بشكل رئيسي حول التأثير التعزيزي للتحفيز الكهربائي في مهمة
**التكييف الآلي** </span>**(Instrumental Conditioning
Task)**<span dir="rtl">.</span> <span dir="rtl">لم يعمل التحفيز
الكهربائي فقط على تنشيط سلوك الفئران - من خلال تأثير **الدوبامين**
</span>**(Dopamine)** <span dir="rtl">على **التحفيز**</span>
**(Motivation)** - <span dir="rtl">بل أدى أيضًا إلى تعلم الفئران بسرعة
تحفيز نفسها بالضغط على رافعة، والتي كانوا يفعلونها بشكل متكرر لفترات
طويلة من الزمن. النشاط الذي أثارته **خلايا الدوبامين العصبية**</span>
**(Dopamine Neurons)** <span dir="rtl">بسبب التحفيز الكهربائي عزز عملية
ضغط الرافعة لدى الفئران</span>.

<span dir="rtl">أظهرت تجارب أكثر حداثة باستخدام **الطرق البصرية
الوراثية**</span> **(Optogenetic Methods)** <span dir="rtl">دور
**الاستجابات الفازية**</span> **(Phasic Responses)** <span dir="rtl">لـ
**خلايا الدوبامين العصبية (**</span>**Dopamine
<span dir="rtl"></span>Neurons<span dir="rtl">)</span>**
<span dir="rtl">كإشارات تعزيز. تتيح هذه **الطرق**</span> **(Methods)**
<span dir="rtl">لعلماء الأعصاب التحكم بدقة في نشاط أنواع معينة من
**الخلايا العصبية**</span> **(Neurons)** <span dir="rtl">على نطاق زمني
بالميلي ثانية في الحيوانات المستيقظة أثناء السلوك. تقدم **الطرق البصرية
الوراثية**</span> **(Optogenetic Methods)** **<span dir="rtl">بروتينات
حساسة للضوء</span> (Light-Sensitive Proteins)** <span dir="rtl">إلى
أنواع معينة من **الخلايا العصبية**</span> **(Neurons)**
<span dir="rtl">بحيث يمكن تنشيط هذه **الخلايا العصبية**</span>
**(Neurons)** <span dir="rtl">أو إسكاتها بواسطة ومضات من **الضوء الليزري
(**</span>**Laser
<span dir="rtl"></span>Light<span dir="rtl">)</span>**.
<span dir="rtl">أظهرت أول تجربة باستخدام **الطرق البصرية
الوراثية**</span> **(Optogenetic Methods)** <span dir="rtl">لدراسة
**خلايا الدوبامين العصبية**</span> **(Dopamine Neurons)**
<span dir="rtl">أن التحفيز البصري الوراثي الذي ينتج عنه تنشيط فازي لهذه
**الخلايا العصبية**</span> **(Neurons)** <span dir="rtl">في الفئران كان
كافيًا لجعل الفئران تفضل الجانب من الغرفة حيث تلقوا هذا التحفيز بالمقارنة
مع الجانب الآخر من الغرفة حيث لم يتلقوا أي تحفيز أو تحفيز بتردد
أقل</span> (Tsai et al. 2009)<span dir="rtl">.</span> <span dir="rtl">في
مثال آخر، استخدم **شتاينبرغ وآخرون** </span>**(Steinberg et al.)**
<span dir="rtl"></span>(2013) <span dir="rtl">التنشيط البصري الوراثي لـ
**خلايا الدوبامين العصبية**</span> **(Dopamine Neurons)**
<span dir="rtl">لخلق انفجارات صناعية من **نشاط خلايا الدوبامين
العصبية**</span> **(Dopamine Neuron Activity)** <span dir="rtl">في
الفئران في الأوقات التي كانت فيها **المحفزات المكافئة**</span>
**(Rewarding Stimuli)** <span dir="rtl">متوقعة ولكن تم إلغاؤها - أوقات
يتوقف فيها عادةً **نشاط خلايا الدوبامين العصبية** </span>**(Dopamine
Neuron Activity)**<span dir="rtl">.</span> <span dir="rtl">ومع استبدال
هذه التوقفات بانفجارات صناعية، تم الحفاظ على الاستجابة عندما كان من
المفترض أن تنخفض عادةً بسبب نقص التعزيز (في تجارب الانقراض)، وتم تمكين
**التعليم**</span> **(Learning)** <span dir="rtl">عندما كان من المفترض
أن يتم حجبه عادةً بسبب توقع المكافأة بالفعل (نموذج الحجب؛ **القسم
14.2.1**)</span>.

<span dir="rtl">أدلة إضافية على الوظيفة التعزيزية **للدوبامين**</span>
**(Dopamine)** <span dir="rtl">تأتي من تجارب **البصرية الوراثية**
</span>**(Optogenetic Experiments)** <span dir="rtl">على ذباب الفاكهة،
إلا أن تأثير **الدوبامين**</span> **(Dopamine)** <span dir="rtl">في هذه
الحيوانات هو عكس تأثيره في الثدييات: الانفجارات المحفزة بصريًا من **نشاط
خلايا الدوبامين العصبية**</span> **(Dopamine Neuron Activity)**
<span dir="rtl">تعمل تمامًا مثل الصدمة الكهربائية في تعزيز سلوك التجنب،
على الأقل بالنسبة لمجموعة **الخلايا العصبية المنتجة للدوبامين**
</span>**(Dopamine Neurons)** <span dir="rtl">التي تم تنشيطها</span>
(Claridge-Chang et al. 2009)<span dir="rtl">.</span> <span dir="rtl">على
الرغم من أن أياً من هذه التجارب البصرية الوراثية لم يظهر أن **النشاط
الفازي لخلايا الدوبامين العصبية**</span>
**<span dir="rtl">(</span>Phasic Dopamine <span dir="rtl"></span>Neuron
Activity<span dir="rtl">)</span>** <span dir="rtl">يشبه تحديدًا **خطأ
التفاضل الزمني** </span>**(TD Error)**<span dir="rtl">، إلا أنها أظهرت
بشكل مقنع أن **النشاط الفازي لخلايا الدوبامين العصبية**</span> **(Phasic
Dopamine Neuron Activity)** <span dir="rtl">يعمل تمامًا كما تعمل</span> δ
<span dir="rtl"></span> <span dir="rtl">(أو ربما مثل</span> −δ
<span dir="rtl">في ذباب الفاكهة) كإشارة تعزيز في **الخوارزميات**
</span>**(Algorithms)** <span dir="rtl">لكل من **التنبؤ**</span>
**(Prediction)** <span dir="rtl">مثل **التكييف الكلاسيكي**</span>
**(Classical Conditioning)** <span dir="rtl">و**التحكم**
</span>**(Control)** <span dir="rtl">مثل **التكييف الآلي**
</span>**(Instrumental Conditioning)**<span dir="rtl">.</span>

**<span dir="rtl">الخلايا العصبية المنتجة للدوبامين</span> (Dopamine
Neurons)** <span dir="rtl">مناسبة بشكل خاص لبث إشارة تعزيز إلى مناطق
عديدة في الدماغ. هذه **الخلايا العصبية**</span> **(Neurons)**
<span dir="rtl">تمتلك **تفرعات محورية هائلة** </span>**(Huge Axonal
Arbors)**<span dir="rtl">، حيث يطلق كل منها **الدوبامين**</span>
**(Dopamine)** <span dir="rtl">في 100 إلى 1000 مرة من المواقع المشبكية
أكثر من تلك التي تصل إليها محاور **الخلايا العصبية النموذجية  
(**</span>**Typical Neurons<span dir="rtl">)</span>**.
<span dir="rtl">يظهر على اليمين **تفرع محوري**</span> **(Axonal Arbor)**
<span dir="rtl">لخلية دوبامين عصبية واحدة يوجد جسمها الخلوي في **المادة
السوداء**</span> **(SNpc)** <span dir="rtl">في دماغ فأر. يقوم كل محور
**لخلية دوبامين عصبية في**</span> **SNpc <span dir="rtl">أو</span> VTA
(Ventral Tegmental Area)** <span dir="rtl">بعمل حوالي 500,000 اتصال
مشبكي على **تغصنات**</span> **(Dendrites)** **<span dir="rtl">الخلايا
العصبية</span> (Neurons)** <span dir="rtl">في المناطق الدماغية
المستهدفة</span>.

<span dir="rtl">إذا كانت **خلايا الدوبامين العصبية**</span> **(Dopamine
Neurons)** <span dir="rtl">تبث إشارة تعزيز مثل</span> δ
<span dir="rtl">في **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">، فبما أن هذه إشارة قياسية</span> (Scalar
Signal)<span dir="rtl">، أي أنها رقم واحد، فسيُتوقع أن تُفعّل جميع **خلايا
الدوبامين العصبية**</span> **(Dopamine Neurons)** <span dir="rtl">في كل
من</span> **SNpc** <span dir="rtl">و</span>**VTA** <span dir="rtl">بشكل
شبه متطابق بحيث تعمل بتزامن شبه كامل لإرسال نفس الإشارة إلى جميع المواقع
التي تستهدفها محاورها العصبية. على الرغم من أن الاعتقاد السائد كان أن
**خلايا الدوبامين العصبية** </span>**(Dopamine Neurons)**
<span dir="rtl">تعمل معًا بهذه الطريقة، فإن الأدلة الحديثة تشير إلى صورة
أكثر تعقيدًا حيث تستجيب مجموعات فرعية مختلفة من **خلايا الدوبامين
العصبية** </span>**(Dopamine Neurons)** <span dir="rtl">للمدخلات بشكل
مختلف بناءً على **الهياكل**</span> **(Structures)** <span dir="rtl">التي
ترسل إليها إشاراتها والطرق المختلفة التي تعمل بها هذه الإشارات على
**الهياكل المستهدفة** </span>**(Target
Structures)**<span dir="rtl">.</span> **<span dir="rtl">الدوبامين</span>
(Dopamine)** <span dir="rtl">له وظائف أخرى غير **الإشارة إلى أخطاء تنبؤ
المكافأة** </span>**(Signaling RPEs)**<span dir="rtl">، وحتى بالنسبة
**لخلايا الدوبامين العصبية**</span> **(Dopamine Neurons)**
<span dir="rtl">التي تشير إلى **أخطاء تنبؤ المكافأة**
</span>**(RPEs)**<span dir="rtl">، قد يكون من المنطقي إرسال **أخطاء تنبؤ
المكافأة المختلفة**</span> **(Different RPEs)** <span dir="rtl">إلى
**هياكل مختلفة**</span> **(Different Structures)** <span dir="rtl">بناءً
على الأدوار التي تلعبها هذه **الهياكل** </span>**(Structures)**
<span dir="rtl">في إنتاج السلوك المعزز</span>.

<span dir="rtl">هذا يتجاوز ما نعالجه بتفصيل في هذا **الكتاب**
</span>**(Book)**<span dir="rtl">، ولكن إشارات **أخطاء تنبؤ المكافأة ذات
القيم المتجهة**</span> **(Vector-Valued RPE Signals)**
<span dir="rtl">منطقية من منظور **التعليم المعزز**
</span>**(Reinforcement Learning)** <span dir="rtl">عندما يمكن تقسيم
القرارات إلى **قرارات فرعية منفصلة** </span>**(Separate
Sub-Decisions)**<span dir="rtl">، أو بشكل أعم، كطريقة لمعالجة النسخة
الهيكلية من **مشكلة توزيع الائتمان** </span>**(Credit Assignment
Problem)**<span dir="rtl">:</span> <span dir="rtl">كيف توزع الائتمان
للنجاح (أو اللوم للفشل) في قرار ما بين العديد من **الهياكل
المكونة**</span> **(Component Structures)** <span dir="rtl">التي قد تكون
شاركت في إنتاجه؟ سنتحدث أكثر عن هذا في **القسم 15.10**</span>
<span dir="rtl">أدناه</span>.

<img src="./media/image177.png"
style="width:5.77814in;height:3.43399in" />

**<span dir="rtl">تفرع محوري</span> (Axonal Arbor)**
<span dir="rtl">لخلية عصبية واحدة تنتج **الدوبامين**</span>
**(Dopamine)** <span dir="rtl">كـ **ناقل عصبي**
</span>**(Neurotransmitter)**<span dir="rtl">. هذه المحاور
العصبية</span> (Axons) <span dir="rtl">تقوم بعمل اتصالات مشبكية مع عدد
هائل من **تغصنات**</span> **(Dendrites)** **<span dir="rtl">الخلايا
العصبية</span> (Neurons)** <span dir="rtl">في المناطق الدماغية
المستهدفة</span>.

<span dir="rtl">(مقتبس من</span> **The Journal of
Neuroscience**<span dir="rtl">،</span> **Matsuda<span dir="rtl">،</span>
Furuta<span dir="rtl">،</span> Nakamura<span dir="rtl">،</span>
Hioki<span dir="rtl">،</span> Fujiyama<span dir="rtl">،</span>
Arai<span dir="rtl">، و</span>Kaneko**<span dir="rtl">، المجلد 29، 2009،
الصفحة 451)</span>.

<span dir="rtl">محاور معظم **الخلايا العصبية المنتجة للدوبامين**</span>
**(Dopamine Neurons)** <span dir="rtl">تقوم بعمل اتصالات مشبكية مع
**الخلايا العصبية**</span> **(Neurons)** <span dir="rtl">في **القشرة
الأمامية**</span> **(Frontal Cortex)** <span dir="rtl">و**العقد
القاعدية** </span>**(Basal Ganglia)**<span dir="rtl">، وهي مناطق في
الدماغ تشارك في **الحركة الإرادية  
(**</span>**Voluntary
Movement<span dir="rtl">)</span>**<span dir="rtl">، **اتخاذ القرارات**
</span>**(Decision Making)**<span dir="rtl">، **التعليم**
</span>**(Learning)**<span dir="rtl">، والوظائف المعرفية مثل **التخطيط**
</span>**(Planning)**<span dir="rtl">.</span> <span dir="rtl">نظرًا لأن
معظم الأفكار التي تربط **الدوبامين** </span>**(Dopamine)**
<span dir="rtl">بـ **التعليم المعزز**</span> **(Reinforcement
Learning)** <span dir="rtl">تركز على **العقد القاعدية** </span>**(Basal
Ganglia)**<span dir="rtl">، ولأن الاتصالات من **الخلايا العصبية المنتجة
للدوبامين**</span> **<span dir="rtl">(</span>Dopamine
<span dir="rtl"></span>Neurons<span dir="rtl">)
</span>**<span dir="rtl">تكون كثيفة بشكل خاص هناك، فإننا نركز على
**العقد القاعدية** </span>**(Basal Ganglia)**
<span dir="rtl">هنا.</span> **<span dir="rtl">العقد القاعدية</span>
(Basal Ganglia)** <span dir="rtl">هي مجموعة من **مجموعات الخلايا
العصبية**</span> **<span dir="rtl">(</span>Neuron
<span dir="rtl"></span>Groups<span dir="rtl">)</span>**<span dir="rtl">،
أو **النوى** </span>**(Nuclei)**<span dir="rtl">، التي تقع في قاعدة
الدماغ الأمامي</span> (Forebrain)<span dir="rtl">.</span>
<span dir="rtl">الهيكل الأساسي للمدخلات في **العقد القاعدية**</span>
**(Basal Ganglia)** <span dir="rtl">يُسمى **المخطط**
</span>**(Striatum)**<span dir="rtl">.</span> <span dir="rtl">تقوم جميع
**القشرة الدماغية**</span> **(Cerebral Cortex)** <span dir="rtl">تقريبًا،
من بين هياكل أخرى، بتوفير المدخلات إلى **المخطط**
</span>**(Striatum)**<span dir="rtl">. ينقل **نشاط الخلايا العصبية
القشرية**</span> **(Cortical Neuron Activity)** <span dir="rtl">ثروة من
المعلومات حول المدخلات الحسية</span> (Sensory Input)<span dir="rtl">،
**الحالات الداخلية** </span>**(Internal States)**<span dir="rtl">،
و**النشاط الحركي** </span>**(Motor Activity)**<span dir="rtl">.</span>
<span dir="rtl">تقوم محاور **الخلايا العصبية القشرية**</span>
**<span dir="rtl">(</span>Cortical
<span dir="rtl"></span>Neurons<span dir="rtl">)
</span>**<span dir="rtl">بعمل اتصالات مشبكية على **تشعبات**</span>
**(Dendrites)** **<span dir="rtl">الخلايا العصبية الشوكية
المتوسطة</span> (Medium Spiny Neurons)** <span dir="rtl">التي تعتبر
الخلايا العصبية الرئيسية للمدخلات/المخرجات في **المخطط**
</span>**(Striatum)**<span dir="rtl">.</span> <span dir="rtl">تخرج
المخرجات من **المخطط**</span> **(Striatum)** <span dir="rtl">وتعود عبر
نوى أخرى من **العقد القاعدية**</span> **(Basal Ganglia)**
<span dir="rtl">ومن **المهاد**</span> **(Thalamus)** <span dir="rtl">إلى
المناطق الأمامية من **القشرة الدماغية**
</span>**(Cortex)**<span dir="rtl">، وإلى المناطق الحركية، مما يجعل من
الممكن لـ **المخطط**</span> **(Striatum)** <span dir="rtl">التأثير على
**الحركة** </span>**(Movement)**<span dir="rtl">، العمليات التجريدية
لاتخاذ القرار</span> (Abstract Decision Processes)<span dir="rtl">،
و**معالجة المكافأة** </span>**(Reward
Processing)**<span dir="rtl">.</span> <span dir="rtl">تنقسم
**المخطط**</span> **(Striatum)** <span dir="rtl">إلى قسمين رئيسيين مهمين
لـ **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">:</span> **<span dir="rtl">المخطط
الظهري</span> (Dorsal Striatum)**<span dir="rtl">، والذي يرتبط بشكل
أساسي بالتأثير على **اختيار الإجراءات** </span>**(Action
Selection)**<span dir="rtl">، و**المخطط البطني** </span>**(Ventral
Striatum)**<span dir="rtl">، الذي يُعتقد أنه حاسم للجوانب المختلفة
**لمعالجة المكافأة**</span> **<span dir="rtl">(</span>Reward
<span dir="rtl"></span>Processing<span dir="rtl">)</span>**<span dir="rtl">،
بما في ذلك تعيين **القيمة الوجدانية**</span> **(Affective Value)**
<span dir="rtl">للإحساسات</span>.

**<span dir="rtl">التشعبات</span> (Dendrites)** <span dir="rtl">لـ
**الخلايا العصبية الشوكية المتوسطة** </span>**(Medium Spiny Neurons)**
<span dir="rtl">مغطاة بالأشواك التي تقوم محاور **الخلايا العصبية
القشرية**</span> **(Cortical Neurons)** <span dir="rtl">بعمل اتصالات
مشبكية على أطرافها. أيضًا، تقوم محاور **الخلايا العصبية المنتجة
للدوبامين**</span> **<span dir="rtl">(</span>Dopamine
<span dir="rtl"></span>Neurons<span dir="rtl">)
</span>**<span dir="rtl">بعمل اتصالات مشبكية مع هذه الأشواك، وفي هذه
الحالة تتصل مع **سيقان الأشواك** </span>**(Spine Stems)**
<span dir="rtl">(كما هو موضح في الشكل 15.1). يجمع هذا الترتيب بين
**النشاط قبل التشابك** </span>**(Presynaptic Activity)**
<span dir="rtl">لـ **الخلايا العصبية القشرية** </span>**(Cortical
Neurons)**<span dir="rtl">، **النشاط بعد التشابك**</span>
**(Postsynaptic Activity)** <span dir="rtl">لـ **الخلايا العصبية الشوكية
المتوسطة**</span> **<span dir="rtl">(</span>Medium Spiny
<span dir="rtl"></span>Neurons<span dir="rtl">)
</span>**<span dir="rtl">والمدخلات من **خلايا الدوبامين العصبية**</span>
**(Dopamine Neurons)** <span dir="rtl">يحدثان في هذه الأشواك، وما يحدث
في الواقع عند هذه الأشواك معقد وغير مفهوم بالكامل. يلمح **الشكل 15.1**
إلى هذا التعقيد من خلال إظهار نوعين من **مستقبلات الدوبامين**
</span>**(Dopamine Receptors)**<span dir="rtl">، **مستقبلات
الغلوتامات**</span> **(Receptors for Glutamate)** - <span dir="rtl">وهو
**الناقل العصبي** </span>**(Neurotransmitter)** <span dir="rtl">للمدخلات
القشرية - وطرق متعددة يمكن للإشارات المختلفة أن تتفاعل بها. لكن الأدلة
تتزايد على أن التغيرات في **فعالية المشابك العصبية**</span> **(Synaptic
Efficacies)** <span dir="rtl">على المسار من **القشرة الدماغية**
</span>**(Cortex)** <span dir="rtl">إلى **المخطط**
</span>**(Striatum)**<span dir="rtl">، والتي يسميها علماء الأعصاب
**المشابك القشرية-المخططية** </span>**(Corticostriatal
Synapses)**<span dir="rtl">، تعتمد بشكل حاسم على إشارات
**الدوبامين**</span> **<span dir="rtl">(</span>Dopamine
<span dir="rtl"></span>Signals<span dir="rtl">)
</span>**<span dir="rtl">ذات التوقيت المناسب</span>.

<img src="./media/image178.png"
style="width:6.26806in;height:3.47014in" />

<span dir="rtl">**الشكل 15.1**:</span> <span dir="rtl">شوكة خلية عصبية
في **المخطط**</span> **(Striatal Neuron)** <span dir="rtl">تُظهر المدخلات
من كل من **الخلايا العصبية القشرية**</span> **(Cortical Neurons)**
<span dir="rtl">و**خلايا الدوبامين العصبية**</span>
**<span dir="rtl">(</span>Dopamine
<span dir="rtl"></span>Neurons<span dir="rtl">)</span>**.
<span dir="rtl">تؤثر محاور **الخلايا العصبية القشرية**</span>
**(Cortical Neurons)** <span dir="rtl">على **الخلايا العصبية في
المخطط**</span> **(Striatal Neurons)** <span dir="rtl">عبر **المشابك
القشرية-المخططية**</span> **<span dir="rtl">(</span>Corticostriatal
<span dir="rtl"></span>Synapses<span dir="rtl">)
</span>**<span dir="rtl">التي تطلق **الناقل العصبي**</span>
**(Neurotransmitter)** **<span dir="rtl">الغلوتامات</span> (Glutamate)**
<span dir="rtl">على أطراف الأشواك التي تغطي **التشعبات**</span>
**(Dendrites)** <span dir="rtl">لـ **الخلايا العصبية في المخطط**</span>
**<span dir="rtl">(</span>Striatal
<span dir="rtl"></span>Neurons<span dir="rtl">)</span>**.
<span dir="rtl">يظهر محور **خلية دوبامين عصبية**</span> **(Dopamine
Neuron)** <span dir="rtl">من</span> **VTA** <span dir="rtl">أو</span>
**SNpc** <span dir="rtl">يمر بجانب الشوكة (من أسفل اليمين). تُطلق
"النتوءات الدوبامينية</span> (Dopamine Varicosities)"
<span dir="rtl">على هذا المحور **الدوبامين**</span> **(Dopamine)**
<span dir="rtl">عند أو بالقرب من ساق الشوكة، في ترتيب يجمع بين
**المدخلات قبل التشابكية**</span> **(Presynaptic Input)**
<span dir="rtl">من **القشرة الدماغية**
</span>**(Cortex)**<span dir="rtl">، **النشاط بعد التشابك**</span>
**(Postsynaptic Activity)** <span dir="rtl">للخلية العصبية في المخطط،
و**الدوبامين** </span>**(Dopamine)**<span dir="rtl">، مما يجعل من الممكن
أن تحكم عدة أنواع من قواعد التعليم في **اللدونة**</span>
**(Plasticity)** <span dir="rtl">لمشابك القشرة-المخططية</span>
(Corticostriatal Synapses)<span dir="rtl">.</span> <span dir="rtl">يقوم
كل محور **لخلية دوبامين عصبية** </span>**(Dopamine Neuron)**
<span dir="rtl">بعمل اتصال مشبكي مع سيقان حوالي 500,000 شوكة. تُظهر بعض
التعقيدات التي تم حذفها من مناقشتنا هنا مسارات **ناقل عصبي**</span>
**(Neurotransmitter Pathways)** <span dir="rtl">أخرى وأنواعًا متعددة من
المستقبلات، مثل مستقبلات الدوبامين</span> **D1**
<span dir="rtl">و</span>**D2**<span dir="rtl">، التي يمكن من خلالها أن
يُحدث **الدوبامين** </span>**(Dopamine)** <span dir="rtl">تأثيرات مختلفة
في الأشواك والمواقع **بعد التشابكية** </span>**(Postsynaptic Sites)**
<span dir="rtl">الأخرى. (مقتبس من</span> **Journal of
Neurophysiology**<span dir="rtl">،</span> **W.
Schultz**<span dir="rtl">، المجلد 80، 1998، الصفحة 10)</span>.

**<u>15.5 <span dir="rtl">الدعم التجريبي لفرضية خطأ تنبؤ المكافأة</span>
<span dir="rtl">(</span>Experimental Support for the Reward Prediction
Error Hypothesis<span dir="rtl">)</span></u>**

**<span dir="rtl">الخلايا العصبية المنتجة للدوبامين</span> (Dopamine
Neurons)** <span dir="rtl">تستجيب بانفجارات من النشاط للمؤثرات
**البصرية**</span> **(Visual Stimuli)**
<span dir="rtl">و**السمعية**</span> **(Auditory Stimuli)**
<span dir="rtl">الشديدة أو الجديدة أو غير المتوقعة التي تحفز **حركات
العين**</span> **(Eye Movements)** <span dir="rtl">و**الجسم**
</span>**(Body Movements)**<span dir="rtl">، لكن القليل جدًا من نشاطها
مرتبط بالحركات نفسها. هذا الأمر مفاجئ لأن تدهور **الخلايا العصبية
المنتجة للدوبامين**</span> **(Dopamine Neurons)** <span dir="rtl">هو أحد
أسباب **مرض باركنسون**</span> **<span dir="rtl">(</span>Parkinson's
<span dir="rtl"></span>Disease<span dir="rtl">)</span>**<span dir="rtl">،
والذي تشمل أعراضه **اضطرابات حركية** </span>**(Motor
Disorders)**<span dir="rtl">، وخاصة العجز في **الحركات الذاتية**
</span>**(Self-Initiated Movement)**<span dir="rtl">.</span>
<span dir="rtl">بدافع العلاقة الضعيفة بين **نشاط خلايا الدوبامين
العصبية**</span> **(Dopamine Neuron Activity)** <span dir="rtl">و**حركات
العين والجسم المحفزة بالمؤثرات** </span>**(Stimulus-Triggered Eye and
Body Movements)**<span dir="rtl">، قام **رومو وشولتز**</span>
**<span dir="rtl">(</span>Romo and
<span dir="rtl"></span>Schultz<span dir="rtl">)</span>** (1990)
<span dir="rtl">باتخاذ الخطوات الأولى نحو **فرضية خطأ تنبؤ المكافأة  
(**</span>**Reward Prediction Error Hypothesis<span dir="rtl">)</span>**
<span dir="rtl">من خلال تسجيل **نشاط خلايا الدوبامين العصبية**</span>
**(Dopamine Neurons Activity)** <span dir="rtl">و**نشاط العضلات**</span>
**(Muscle Activity)** <span dir="rtl">بينما كانت **القردة**</span>
**(Monkeys)** <span dir="rtl">تحرك أذرعها</span>.

<span dir="rtl">قاموا بتدريب قردين على الوصول من وضعية يد استراحة إلى
صندوق يحتوي على قطعة من التفاح، أو قطعة من الكعك، أو **زبيب**
</span>**(Raisin)**<span dir="rtl">، عندما يرى **القرد**</span>
**(Monkey)** <span dir="rtl">ويسمع فتح باب **الصندوق**
</span>**(Bin)**<span dir="rtl">.</span> <span dir="rtl">بعد ذلك، يمكن
للقرد أن يمسك بالطعام ويضعه في فمه. بعد أن أصبح القرد ماهرًا في هذا، تم
تدريبه على مهمتين إضافيتين. كان الغرض من المهمة الأولى هو معرفة ما تفعله
**خلايا الدوبامين العصبية**</span> **(Dopamine Neurons)**
<span dir="rtl">عندما تكون الحركات ذاتية. تم ترك **الصندوق**</span>
**(Bin)** <span dir="rtl">مفتوحًا لكنه مغطى من الأعلى بحيث لا يمكن للقرد
أن يرى بداخله ولكن يمكنه الوصول إليه من الأسفل. لم يتم تقديم أي محفزات
محفزة، وبعد أن يصل القرد إلى **قطعة الطعام**</span> **(Food Morsel)**
<span dir="rtl">ويأكلها، يقوم المجرب عادةً (لكن ليس دائمًا) بإعادة الطعام
إلى **الصندوق**</span> **(Bin)** <span dir="rtl">بهدوء ودون أن يراه
**القرد** </span>**(Monkey)**<span dir="rtl">، عن طريق إلصاقه بسلك صلب.
هنا أيضًا، لم يكن **نشاط خلايا الدوبامين العصبية** </span>**(Dopamine
Neurons Activity)** <span dir="rtl">الذي راقبه **رومو وشولتز**</span>
**(Romo and Schultz)** <span dir="rtl">مرتبطًا بحركات القرد، لكن نسبة
كبيرة من هذه **الخلايا العصبية**</span> **(Neurons)**
<span dir="rtl">أنتجت **استجابات فازية** </span>**(Phasic Responses)**
<span dir="rtl">كلما لمس **القرد**</span> **(Monkey)**
<span dir="rtl">قطعة الطعام لأول مرة. لم تستجب هذه **الخلايا
العصبية**</span> **(Neurons)** <span dir="rtl">عندما لمس
**القرد**</span> **(Monkey)** <span dir="rtl">السلك فقط أو استكشف
**الصندوق** </span>**(Bin)** <span dir="rtl">عندما لم يكن هناك طعام. كان
هذا دليلًا جيدًا على أن **الخلايا العصبية**</span> **(Neurons)**
<span dir="rtl">كانت تستجيب للطعام وليس للجوانب الأخرى من المهمة</span>.

<span dir="rtl">كان الغرض من المهمة الثانية لـ **رومو وشولتز**</span>
**(Romo and Schultz)** <span dir="rtl">هو معرفة ما يحدث عندما يتم تحفيز
الحركات بواسطة **المؤثرات**</span> **(Stimuli)**.
<span dir="rtl">استخدمت هذه المهمة صندوقًا مختلفًا بغطاء متحرك. إن رؤية
وسماع فتح **الصندوق**</span> **(Bin)** <span dir="rtl">حفز حركات اليد
نحو **الصندوق** </span>**(Bin)**<span dir="rtl">.</span>
<span dir="rtl">في هذه الحالة، وجد **رومو وشولتز**</span> **(Romo and
Schultz)** <span dir="rtl">أنه بعد فترة من التدريب، لم تعد **الخلايا
العصبية المنتجة للدوبامين**</span> **(Dopamine Neurons)**
<span dir="rtl">تستجيب للمس الطعام، بل استجابت بدلاً من ذلك لرؤية وسماع
فتح غطاء **الصندوق** </span>**(Bin)**<span dir="rtl">.</span>
**<span dir="rtl">الاستجابات الفازية</span> (Phasic Responses)**
<span dir="rtl">لهذه **الخلايا العصبية** </span>**(Neurons)**
<span dir="rtl">تحولت من **المكافأة نفسها**</span> **(Reward Itself)**
<span dir="rtl">إلى **المؤثرات**</span> **(Stimuli)**
<span dir="rtl">التي تتنبأ بتوفر **المكافأة** </span>**(Availability of
the Reward)**<span dir="rtl">.</span> <span dir="rtl">في دراسة متابعة،
وجد **رومو وشولتز**</span> **<span dir="rtl">(</span>Romo
<span dir="rtl"></span>and Schultz<span dir="rtl">)
</span>**<span dir="rtl">أن معظم **الخلايا العصبية المنتجة
للدوبامين**</span> **(Dopamine Neurons)** <span dir="rtl">التي قاموا
بمراقبتها لم تستجب لرؤية وسماع فتح **الصندوق**</span> **(Bin)**
<span dir="rtl">خارج سياق المهمة السلوكية. أشارت هذه الملاحظات إلى أن
**الخلايا العصبية المنتجة للدوبامين**</span> **(Dopamine Neurons)**
<span dir="rtl">لم تكن تستجيب لبدء الحركة ولا للخصائص الحسية
**للمؤثرات** </span>**(Stimuli)**<span dir="rtl">، بل كانت تشير إلى
**توقع المكافأة** </span>**(Expectation of
Reward)**<span dir="rtl">.</span>

<span dir="rtl">أجرى فريق **شولتز**</span> **(Schultz's Group)**
<span dir="rtl">العديد من الدراسات الإضافية التي تضمنت **الخلايا العصبية
المنتجة للدوبامين**</span> **(Dopamine Neurons)** <span dir="rtl">في
**المادة السوداء**</span> **(SNpc)** <span dir="rtl">و**المنطقة السقيفية
البطنية** </span>**(VTA)**<span dir="rtl">.</span> <span dir="rtl">كانت
سلسلة معينة من التجارب مؤثرة في الإشارة إلى أن **الاستجابات الفازية**
</span>**(Phasic Responses)** <span dir="rtl">لـ **الخلايا العصبية
المنتجة للدوبامين**</span> **(Dopamine Neurons)** <span dir="rtl">تتوافق
مع **أخطاء التفاضل الزمني**</span> **(TD Errors)** <span dir="rtl">وليس
مع **أخطاء أبسط**</span> **(Simpler Errors)** <span dir="rtl">مثل تلك
الموجودة في نموذج **ريسورلا-فاغنر**</span> **(Rescorla–Wagner Model)**
<span dir="rtl">(انظر المعادلة 14.3). في أولى هذه التجارب</span>
(Ljungberg, Apicella, and Schultz, 1992)<span dir="rtl">، تم تدريب
**القردة** </span>**(Monkeys)** <span dir="rtl">على الضغط على رافعة بعد
إضاءة ضوء كـ "إشارة محفزة</span> (Trigger Cue)" <span dir="rtl">للحصول
على قطرة من عصير التفاح. كما لاحظ **رومو وشولتز**</span> **(Romo and
Schultz)** <span dir="rtl">في وقت سابق، استجابت العديد من **الخلايا
العصبية المنتجة للدوبامين**</span> **(Dopamine Neurons)**
<span dir="rtl">في البداية **للمكافأة** </span>**(Reward)**
<span dir="rtl">وهي قطرة العصير (انظر الشكل 15.2، اللوحة العلوية). لكن
العديد من هذه **الخلايا العصبية** </span>**(Neurons)**
<span dir="rtl">فقدت تلك الاستجابة **للمكافأة**</span> **(Reward)**
<span dir="rtl">مع استمرار التدريب وطورت استجابات بدلاً من ذلك لإضاءة
الضوء الذي يتنبأ **بالمكافأة**</span> **(Reward)** <span dir="rtl">(انظر
الشكل 15.2، اللوحة الوسطى). ومع استمرار التدريب، أصبحت عملية الضغط على
الرافعة أسرع بينما انخفض عدد **الخلايا العصبية المنتجة
للدوبامين**</span> **(Dopamine Neurons)** <span dir="rtl">التي تستجيب
**لإشارة التحفيز** </span>**(Trigger Cue)**<span dir="rtl">.</span>

<span dir="rtl">بعد هذه الدراسة، تم تدريب نفس **القردة**</span>
**(Monkeys)** <span dir="rtl">على مهمة جديدة</span>
<span dir="rtl">(</span>Schultz, Apicella, and
<span dir="rtl"></span>Ljungberg, 1993<span dir="rtl">)</span>.
<span dir="rtl">هنا، واجهت **القردة**</span> **(Monkeys)**
<span dir="rtl">رافعتين، كل واحدة منها مع ضوء فوقها. كانت إضاءة أحد هذه
الأضواء بمثابة "إشارة تعليمية</span> (Instruction Cue)"
<span dir="rtl">تشير إلى أي من الرافعتين يجب الضغط عليها</span>.

<img src="./media/image179.png"
style="width:6.29546in;height:2.56512in" />

<span dir="rtl">**الشكل 15.2**:</span> <span dir="rtl">تستجيب **الخلايا
العصبية المنتجة للدوبامين**</span> **(Dopamine Neurons)**
<span dir="rtl">في البداية **للمكافأة الأولية** </span>**(Primary
Reward)**<span dir="rtl">، ثم تنتقل استجابتها إلى **المؤثرات
التنبؤية**</span> **<span dir="rtl">(</span>Predictive
<span dir="rtl"></span>Stimuli<span dir="rtl">)
</span>**<span dir="rtl">السابقة مع مرور الوقت. تُظهر هذه الرسوم البيانية
عدد **جهود الفعل**</span> **<span dir="rtl">(</span>Action
<span dir="rtl"></span>Potentials<span dir="rtl">)
</span>**<span dir="rtl">التي أنتجتها **الخلايا العصبية المنتجة
للدوبامين**</span> **(Dopamine Neurons)** <span dir="rtl">المراقبة ضمن
فواصل زمنية صغيرة، متوسطًا على جميع **الخلايا العصبية المنتجة
للدوبامين**</span> **<span dir="rtl">(</span>Dopamine
<span dir="rtl"></span>Neurons<span dir="rtl">)
</span>**<span dir="rtl">المراقبة (التي تتراوح بين 23 إلى 44 خلية عصبية
لهذه البيانات). الجزء العلوي: تم تنشيط **الخلايا العصبية المنتجة
للدوبامين**</span> **(Dopamine Neurons)** <span dir="rtl">بواسطة توصيل
غير متوقع لقطرة من عصير التفاح. الجزء الأوسط: مع التعليم، طورت **الخلايا
العصبية المنتجة للدوبامين**</span> **<span dir="rtl">(</span>Dopamine
<span dir="rtl"></span>Neurons<span dir="rtl">)
</span>**<span dir="rtl">استجابات لإشارة التحفيز التنبؤية **للمكافأة**
</span>**(Reward-Predicting Trigger Cue)** <span dir="rtl">وفقدت
استجابتها **لتوصيل المكافأة** </span>**(Delivery of
Reward)**<span dir="rtl">.</span> <span dir="rtl">الجزء السفلي: مع إضافة
إشارة تعليمية تسبق إشارة التحفيز بفترة ثانية واحدة، نقلت **الخلايا
العصبية المنتجة للدوبامين**</span> **<span dir="rtl">(</span>Dopamine
<span dir="rtl"></span>Neurons<span dir="rtl">)</span>**
<span dir="rtl">استجابتها من إشارة التحفيز إلى الإشارة التعليمية
السابقة. مقتبس من</span> **Schultz et al.
(1995)**<span dir="rtl">،</span> MIT Press<span dir="rtl">.</span>

<span dir="rtl">في هذه المهمة، سبقت **الإشارة التعليمية**</span>
**(Instruction Cue)** **<span dir="rtl">إشارة التحفيز</span> (Trigger
Cue)** <span dir="rtl">من المهمة السابقة بفاصل زمني ثابت قدره ثانية
واحدة. تعلمت **القردة**</span> **(Monkeys)** <span dir="rtl">الامتناع عن
الوصول حتى رؤية **إشارة التحفيز** </span>**(Trigger
Cue)**<span dir="rtl">، وازداد **نشاط خلايا الدوبامين العصبية**</span>
**<span dir="rtl">(</span>Dopamine <span dir="rtl"></span>Neuron
Activity<span dir="rtl">)</span>**<span dir="rtl">، ولكن الآن أصبحت
استجابات **الخلايا العصبية المنتجة للدوبامين** </span>**(Dopamine
Neurons)** <span dir="rtl">التي تمت مراقبتها تحدث بشكل شبه حصري للإشارة
التعليمية السابقة وليس **لإشارة التحفيز**</span> **(Trigger Cue)**
<span dir="rtl">(انظر الشكل 15.2، اللوحة السفلية). هنا مرة أخرى، انخفض
عدد **الخلايا العصبية المنتجة للدوبامين**</span> **(Dopamine Neurons)**
<span dir="rtl">التي تستجيب **للإشارة التعليمية**</span> **(Instruction
Cue)** <span dir="rtl">بشكل كبير عندما تم تعلم المهمة جيدًا</span>.

<span dir="rtl">أثناء التعليم عبر هذه المهام، تحول **نشاط خلايا
الدوبامين العصبية**</span> **<span dir="rtl">(</span>Dopamine Neuron
<span dir="rtl"></span>Activity<span dir="rtl">)
</span>**<span dir="rtl">من الاستجابة في البداية **للمكافأة**</span>
**(Reward)** <span dir="rtl">إلى الاستجابة **للمؤثرات التنبؤية السابقة**
</span>**(Earlier Predictive Stimuli)**<span dir="rtl">، حيث تقدم
الاستجابة أولاً إلى **المحفز** </span>**(Trigger Stimulus)**
<span dir="rtl">ثم إلى **الإشارة التعليمية**</span> **(Instruction
Cue)** <span dir="rtl">الأسبق. كلما انتقلت الاستجابة إلى وقت أبكر في
الزمن، اختفت من **المؤثرات اللاحقة** </span>**(Later
Stimuli)**<span dir="rtl">.</span> <span dir="rtl">هذا التحول في
الاستجابات إلى **مؤشرات المكافأة السابقة** </span>**(Earlier Reward
Predictors)**<span dir="rtl">، مع فقدان الاستجابات للمؤشرات اللاحقة، هو
علامة مميزة **لتعلم التفاضل الزمني**</span> **(TD Learning)**
<span dir="rtl">(انظر، على سبيل المثال، الشكل 14.2)</span>.

<span dir="rtl">المهمة التي تم وصفها كشفت أيضًا عن خاصية أخرى **لنشاط
خلايا الدوبامين العصبية**</span> **<span dir="rtl">(</span>Dopamine
<span dir="rtl"></span>Neuron Activity<span dir="rtl">)
</span>**<span dir="rtl">تشترك مع **تعلم التفاضل الزمني** </span>**(TD
Learning)**<span dir="rtl">.</span> <span dir="rtl">في بعض الأحيان، ضغطت
**القردة**</span> **(Monkeys)** <span dir="rtl">على المفتاح الخاطئ، أي
المفتاح الآخر غير الذي تم توجيهها إليه، وبالتالي لم تتلقى أي مكافأة. في
هذه التجارب، أظهرت العديد من **الخلايا العصبية المنتجة للدوبامين**
</span>**(Dopamine Neurons)** <span dir="rtl">انخفاضًا حادًا في معدلات
إطلاقها دون المستوى الأساسي بعد وقت قصير من وقت توصيل المكافأة المعتاد،
وحدث هذا بدون توفر أي إشارة خارجية لتحديد وقت توصيل المكافأة المعتاد
(انظر الشكل 15.3). بطريقة ما، كانت **القردة**</span> **(Monkeys)**
<span dir="rtl">تتابع داخليًا توقيت المكافأة. (توقيت الاستجابة هو أحد
المجالات التي تحتاج فيها النسخة الأبسط **لتعلم التفاضل الزمني**</span>
**<span dir="rtl">(</span>TD
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">إلى تعديل لتفسير بعض التفاصيل المتعلقة بتوقيت
**استجابات خلايا الدوبامين العصبية**</span> **(Dopamine Neuron
Responses)**<span dir="rtl">.</span> <span dir="rtl">سننظر في هذه
المسألة في القسم التالي</span>.

<span dir="rtl">قادته الملاحظات من الدراسات المذكورة أعلاه
**شولتز**</span> **(Schultz)** <span dir="rtl">وفريقه إلى استنتاج أن
**خلايا الدوبامين العصبية**</span> **(Dopamine Neurons)**
<span dir="rtl">تستجيب **للمكافآت غير المتوقعة**</span>
**<span dir="rtl">(</span>Unpredicted
<span dir="rtl"></span>Rewards<span dir="rtl">)</span>**<span dir="rtl">،
ولأوائل **مؤشرات المكافأة** </span>**(Earliest Predictors of
Reward)**<span dir="rtl">، وأن **نشاط خلايا الدوبامين العصبية**</span>
**(Dopamine Neuron Activity)** <span dir="rtl">ينخفض دون المستوى الأساسي
إذا لم تحدث **المكافأة** </span>**(Reward)**<span dir="rtl">، أو **مؤشر
المكافأة** </span>**(Predictor of Reward)**<span dir="rtl">، في وقتها
المتوقع. كان الباحثون المطلعون على **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">سريعين في التعرف على أن هذه
النتائج تشبه بشكل لافت كيف يتصرف **خطأ التفاضل الزمني**</span> **(TD
Error)** <span dir="rtl">كإشارة تعزيز في **خوارزمية التفاضل الزمني**
</span>**(TD Algorithm)**<span dir="rtl">.</span> <span dir="rtl">يستكشف
القسم التالي هذا التشابه من خلال العمل على مثال محدد بالتفصيل</span>.

<img src="./media/image180.png"
style="width:6.58958in;height:6.63333in" />

<span dir="rtl">**الشكل 15.3**:</span> <span dir="rtl">ينخفض **نشاط
خلايا الدوبامين العصبية**</span> **(Dopamine Neuron Activity)**
<span dir="rtl">إلى ما دون المستوى الأساسي بعد وقت قصير من الوقت الذي
تفشل فيه المكافأة المتوقعة في الحدوث. الجزء العلوي: يتم تنشيط **خلايا
الدوبامين العصبية**</span> **(Dopamine Neurons)** <span dir="rtl">بواسطة
التوصيل غير المتوقع لقطرة من عصير التفاح. الجزء الأوسط: تستجيب **خلايا
الدوبامين العصبية**</span> **<span dir="rtl">(</span>Dopamine
<span dir="rtl"></span>Neurons<span dir="rtl">)
</span>**<span dir="rtl">لـ **المؤثر الشرطي**</span> **(Conditioned
Stimulus, CS)** <span dir="rtl">الذي يتنبأ بالمكافأة ولا تستجيب للمكافأة
نفسها. الجزء السفلي: عندما تفشل المكافأة المتنبأ بها بواسطة **المؤثر
الشرطي**</span> **(CS)** <span dir="rtl">في الحدوث، ينخفض **نشاط خلايا
الدوبامين العصبية**</span> **(Dopamine Neuron Activity)**
<span dir="rtl">إلى ما دون المستوى الأساسي بعد وقت قصير من الوقت المتوقع
لحدوث المكافأة</span>.

<span dir="rtl">في أعلى كل من هذه اللوحات يظهر العدد المتوسط من **جهود
الفعل**</span> **(Action Potentials)** <span dir="rtl">التي تنتجها
**خلايا الدوبامين العصبية المراقبة**</span> **(Monitored Dopamine
Neurons)** <span dir="rtl">ضمن فواصل زمنية صغيرة حول الأوقات المشار
إليها. تُظهر الرسوم النقطية أدناه أنماط نشاط **خلايا الدوبامين العصبية
الفردية**</span> **(Individual Dopamine Neurons)** <span dir="rtl">التي
تمت مراقبتها؛ كل نقطة تمثل **جهد فعل** </span>**(Action
Potential)**<span dir="rtl">.</span> <span dir="rtl">مقتبس من</span>
**Schultz, Dayan, and Montague, A Neural Substrate of Prediction and
Reward, Science, vol. 275, issue 5306, pages 1593-1598, March 14,
1997**. <span dir="rtl">معاد طباعة بإذن من</span>
**AAAS**<span dir="rtl">.</span>

**<span dir="rtl">خطأ</span> /TD<span dir="rtl">مطابقة الدوبامين</span>
(TD Error/Dopamine Correspondence)**

<span dir="rtl">هذه الفقرة تشرح **مطابقة الدوبامين**</span> **(Dopamine
Correspondence)** <span dir="rtl">مع **خطأ**</span> **TD**
<span dir="rtl">(والاستجابات **الفازية**</span>
**<span dir="rtl">(</span>Phasic Responses<span dir="rtl">)</span>**
**<span dir="rtl">لخلايا الدوبامين العصبية</span> (Dopamine
Neurons)**<span dir="rtl">)</span> <span dir="rtl">التي تمت ملاحظتها في
التجارب المذكورة سابقًا. سنقوم بدراسة كيف يتغير **خطأ**</span> **TD**
<span dir="rtl">على مدار عملية التعليم في مهمة تشبه المهمة التي تم وصفها
سابقًا حيث يرى القرد أولاً **إشارة تعليمية**</span> **(Instruction Cue)**
<span dir="rtl">ثم بعد فترة زمنية ثابتة يجب عليه الاستجابة بشكل صحيح
**لإشارة تحفيز**</span> **(Trigger Cue)** <span dir="rtl">من أجل الحصول
على المكافأة. سنستخدم نسخة مبسطة من هذه المهمة، لكننا سندخل في تفاصيل
أكثر من المعتاد لأننا نريد أن نؤكد على الأساس النظري للتوازي بين</span>
**TD <span dir="rtl">أخطاء</span> (TD Errors)** <span dir="rtl">و**نشاط
خلايا الدوبامين العصبية** </span>**(Dopamine Neuron
Activity)**<span dir="rtl">.</span>

<span dir="rtl">الافتراض المبسط الأول هو أن الوكيل قد تعلم بالفعل
الإجراءات المطلوبة للحصول على المكافأة. إذًا، مهمته هي فقط تعلم توقعات
دقيقة للمكافأة المستقبلية للتتابع الذي يمر به من **الحالات**
</span>**(States)**<span dir="rtl">.</span> <span dir="rtl">هذا يصبح إذًا
مهمة تنبؤ، أو من الناحية الفنية أكثر، مهمة تقييم السياسة: تعلم **دالة
القيمة**</span> **<span dir="rtl">(</span>Value
<span dir="rtl"></span>Function<span dir="rtl">)
</span>**<span dir="rtl">لسياسة ثابتة (انظر الأقسام 4.1 و6.1).</span>
**<span dir="rtl">دالة القيمة</span> (Value Function)**
<span dir="rtl">التي سيتم تعلمها تعين لكل **حالة**</span> **(State)**
<span dir="rtl">قيمة تتنبأ بالعائد الذي سيتبع تلك **الحالة**</span>
**(State)** <span dir="rtl">إذا اختار الوكيل الإجراءات وفقًا للسياسة
المعطاة، حيث يكون العائد هو مجموع المكافآت المستقبلية (المخصومة ربما).
هذا غير واقعي كنموذج لوضع القرد لأن القرد على الأرجح سيتعلم هذه التوقعات
في نفس الوقت الذي يتعلم فيه التصرف بشكل صحيح (كما ستفعل خوارزمية التعليم
المعزز التي تتعلم السياسات بالإضافة إلى **دوال القيمة** </span>**(Value
Functions)**<span dir="rtl">، مثل **خوارزمية الممثل-الناقد**</span>
**<span dir="rtl">(</span>Actor-Critic
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**<span dir="rtl">)،
لكن هذا السيناريو أبسط في الوصف من سيناريو يتم فيه تعلم السياسة و**دالة
القيمة** </span>**(Value Function)** <span dir="rtl">في نفس
الوقت</span>.

<span dir="rtl">الآن تخيل أن تجربة الوكيل تنقسم إلى عدة تجارب، في كل
منها يتكرر نفس تتابع **الحالات** </span>**(States)**<span dir="rtl">، مع
حدوث **حالة**</span> **(State)** <span dir="rtl">مميزة في كل خطوة زمنية
خلال التجربة. تخيل أيضًا أن العائد الذي يتم التنبؤ به يقتصر على العائد
على مدار التجربة، مما يجعل التجربة مشابهة **لحلقة التعليم المعزز**
</span>**(Reinforcement Learning Episode)** <span dir="rtl">كما عرفناها.
بالطبع، في الواقع، العوائد التي يتم التنبؤ بها ليست محصورة في تجارب
فردية، والفاصل الزمني بين التجارب عامل مهم في تحديد ما يتعلمه الحيوان.
هذا صحيح أيضًا بالنسبة</span> **TD <span dir="rtl">تعلم</span> (TD
Learning)**<span dir="rtl">، لكننا نفترض هنا أن العوائد لا تتراكم على
مدار تجارب متعددة. بناءً على هذا، تكون التجربة في التجارب مثل تلك التي
أجراها **شولتز وزملاؤه**</span> **(Schultz and Colleagues)**
<span dir="rtl">معادلة لحلقة **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement <span dir="rtl"></span>Learning
Episode<span dir="rtl">)</span>**. <span dir="rtl">(على الرغم من أننا في
هذه المناقشة سنستخدم مصطلح "تجربة" بدلاً من "حلقة" ليتناسب أكثر مع
التجارب)</span>.

<span dir="rtl">كالمعتاد، نحتاج أيضًا إلى افتراض حول كيفية تمثيل
**الحالات**</span> **(States)** <span dir="rtl">كمدخلات للخوارزمية
التعليمية، وهو افتراض يؤثر على مدى توافق **خطأ**</span> **TD**
<span dir="rtl">مع **نشاط خلايا الدوبامين العصبية** </span>**(Dopamine
Neuron Activity)**<span dir="rtl">. سنناقش هذه المسألة لاحقًا، ولكن في
الوقت الحالي نفترض نفس **تمثيل المركب المتسلسل الكامل**</span> **(CSC
Representation)** <span dir="rtl">الذي استخدمه **مونتاج وآخرون**
</span>**(Montague et al., 1996)** <span dir="rtl">حيث توجد **محفزات
داخلية منفصلة**</span> **<span dir="rtl">(</span>Separate Internal
<span dir="rtl"></span>Stimuli<span dir="rtl">)
</span>**<span dir="rtl">لكل **حالة**</span> **(State)**
<span dir="rtl">تتم زيارتها في كل خطوة زمنية خلال التجربة. هذا يبسط
العملية إلى الحالة الجدولية التي تم تناولها في الجزء الأول من هذا
الكتاب. أخيرًا، نفترض أن الوكيل يستخدم</span> **TD(0)**
<span dir="rtl">لتعلم **دالة القيمة** </span>**(Value
Function)**<span dir="rtl">،</span> V<span dir="rtl">، المخزنة في **جدول
بحث** </span>**(Lookup Table)** <span dir="rtl">تم تهيئته ليكون صفريًا
لجميع **الحالات** </span>**(States)**<span dir="rtl">.</span>
<span dir="rtl">نفترض أيضًا أن هذه مهمة حتمية وأن معامل الخصم</span> γ
<span dir="rtl">قريب جدًا من الواحد بحيث يمكننا تجاهله</span>.

<span dir="rtl">يظهر **الشكل 15.4** مجريات الزمن للإشارات</span>
$`R`$<span dir="rtl">، و</span>$`V`$<span dir="rtl">، و</span>$`\delta`$
<span dir="rtl">في عدة مراحل من التعليم في مهمة تقييم السياسة هذه. تمثل
محاور الزمن الفترة الزمنية التي يتم فيها زيارة تتابع **الحالات**</span>
**(States)** <span dir="rtl">خلال التجربة (حيث نغفل عن إظهار **الحالات
الفردية**</span> **(Individual States)** <span dir="rtl">من أجل الوضوح).
إشارة المكافأة تساوي صفرًا طوال التجربة إلا عندما يصل الوكيل إلى **حالة
المكافأة** </span>**(Rewarding State)**<span dir="rtl">، كما هو موضح
بالقرب من الطرف الأيمن من محور الزمن، عندما تصبح إشارة المكافأة عددًا
موجبًا، لنفترض أنها</span> R∗<span dir="rtl">.</span> <span dir="rtl">هدف
**تعلم**</span> **TD** <span dir="rtl">هو التنبؤ بالعائد لكل
**حالة**</span> **(State)** <span dir="rtl">يتم زيارتها في التجربة،
والذي في هذه الحالة غير المخصومة وبناءً على افتراضنا أن التوقعات محصورة
في التجارب الفردية، هو ببساطة</span> R∗ <span dir="rtl">لكل **حالة**
</span>**(State)**<span dir="rtl">.</span>

<img src="./media/image181.png"
style="width:6.04219in;height:4.33371in" />

<span dir="rtl">**الشكل 15.4**:</span> <span dir="rtl">سلوك
**خطأ**</span> **TD (δ)** <span dir="rtl">أثناء **تعلم**</span> **TD**
<span dir="rtl">يتماشى مع خصائص **التنشيط الفازي**</span>
**<span dir="rtl">(</span>Phasic
<span dir="rtl"></span>Activation<span dir="rtl">) لخلايا الدوبامين
العصبية</span> (Dopamine Neurons)**<span dir="rtl">.</span>
<span dir="rtl">(هنا</span> δ <span dir="rtl">هو **خطأ**</span> **TD**
<span dir="rtl">المتاح في الزمن</span> t<span dir="rtl">، أي</span>
$`\delta t - 1`$<span dir="rtl">).</span> <span dir="rtl">الجزء العلوي:
يتبع تسلسل **الحالات** </span>**(States)**<span dir="rtl">، الذي يظهر
كفاصل من **المؤشرات المنتظمة** </span>**(Regular
Predictors)**<span dir="rtl">، مكافأة غير صفرية</span>
R∗<span dir="rtl">.</span> <span dir="rtl">في بداية التعليم:</span>
**<span dir="rtl">دالة القيمة الأولية</span> (Initial Value
Function)**<span dir="rtl">،</span> $`V`$<span dir="rtl">، و</span>\*\*δ
<span dir="rtl">الأولي\*\*، الذي يكون في البداية مساويًا لـ</span>
$`R*`$<span dir="rtl">.</span> <span dir="rtl">عند اكتمال
التعليم</span>: **<span dir="rtl">دالة القيمة</span> (Value Function)**
<span dir="rtl">تتنبأ بدقة **بالمكافأة المستقبلية (**</span>**Future
Reward<span dir="rtl">)</span>**<span dir="rtl">، تكون</span> δ
<span dir="rtl">موجبة عند **الحالة التنبؤية الأقدم** </span>**(Earliest
Predictive State)**<span dir="rtl">، و</span>$`\delta = 0`$
<span dir="rtl">عند وقت **المكافأة غير الصفرية** </span>**(Non-Zero
Reward)**<span dir="rtl">.</span> <span dir="rtl">عند حذف</span>
$`R*`$<span dir="rtl">:</span> <span dir="rtl">في الوقت الذي يتم فيه حذف
**المكافأة المتوقعة** </span>**(Predicted Reward)**<span dir="rtl">،
تصبح</span> δ <span dir="rtl">سلبية. راجع النص للحصول على شرح كامل لسبب
حدوث ذلك</span>.

<span dir="rtl">يسبق **الحالة المكافئة**</span> **(Rewarding State)**
<span dir="rtl">تتابع من **الحالات التنبؤية بالمكافأة**</span>
**<span dir="rtl">(</span>Reward-Predicting
States<span dir="rtl">)</span>**<span dir="rtl">، مع عرض **أول حالة
تنبؤية بالمكافأة**</span> **<span dir="rtl">(</span>Earliest
Reward-Predicting <span dir="rtl"></span>State<span dir="rtl">)
</span>**<span dir="rtl">بالقرب من الطرف الأيسر من **الخط الزمني**
</span>**(Time Line)**<span dir="rtl">.</span> <span dir="rtl">هذه
الحالة تشبه الحالة القريبة من بداية التجربة، على سبيل المثال مثل الحالة
التي تمثلها **الإشارة التعليمية** </span>**(Instruction Cue)**
<span dir="rtl">في تجربة القردة التي وصفها **شولتز وآخرون**</span>
**(Schultz et al., 1993)** <span dir="rtl">سابقًا. إنها أول حالة في
التجربة تتنبأ بشكل موثوق بمكافأة تلك التجربة. (بالطبع، في الواقع،
**الحالات**</span> **(States)** <span dir="rtl">التي تمت زيارتها في
التجارب السابقة هي **حالات تنبؤية بالمكافأة سابقة**</span>
**<span dir="rtl">(</span>Earlier Reward-Predicting
<span dir="rtl"></span>States<span dir="rtl">)</span>**<span dir="rtl">،
ولكن نظرًا لأننا نحصر التوقعات في التجارب الفردية، فإن هذه الحالات لا
تعتبر **كمؤشرات للمكافأة** </span>**(Predictors of This Trial’s
Reward)**<span dir="rtl">).</span> <span dir="rtl">سنقدم أدناه وصفًا أكثر
رضىً، وإن كان أكثر تجريدًا، **لأول حالة تنبؤية بالمكافأة**
</span>**(Earliest Reward-Predicting State)**<span dir="rtl">.</span>
**<span dir="rtl">الحالة التنبؤية بالمكافأة الأخيرة</span> (Latest
Reward-Predicting State)** <span dir="rtl">في التجربة هي الحالة التي
تسبق مباشرة **حالة المكافأة**</span> **(Rewarding State)**
<span dir="rtl">في التجربة. هذه هي الحالة الموجودة بالقرب من الطرف
الأيمن من الخط الزمني في **الشكل 15.4**.</span> <span dir="rtl">لاحظ أن
**حالة المكافأة** </span>**(Rewarding State)** <span dir="rtl">في
التجربة لا تتنبأ بالعائد لتلك التجربة: فلو كانت قيمة هذه الحالة تتنبأ
بالعائد على جميع التجارب التالية، التي نفترض هنا أنها تساوي صفرًا في هذا
الصياغة **الحلقية** </span>**(Episodic
Formulation)**<span dir="rtl">.</span>

<span dir="rtl">يُظهر **الشكل 15.4** مجريات الزمن لـ</span>
$`\mathbf{V}`$ <span dir="rtl">و</span>$`\delta`$ <span dir="rtl">كما هو
موضح في الرسوم البيانية المسمّاة "في بداية التعليم"</span> (Early in
Learning)<span dir="rtl">.</span> <span dir="rtl">لأن **إشارة
المكافأة**</span> **(Reward Signal)** <span dir="rtl">تساوي صفرًا طوال
التجربة إلا عندما يتم الوصول إلى **حالة المكافأة**</span> **(Rewarding
State)**<span dir="rtl">، وكل قيم</span> $`\mathbf{V}`$
<span dir="rtl">تساوي صفرًا، فإن **خطأ** </span>**TD**
<span dir="rtl">يكون أيضًا صفرًا حتى يصل إلى</span> $`\mathbf{R*}`$
<span dir="rtl">عند حدوث المكافأة. هذا يتبع لأن</span>:

``` math
\delta t - 1 = Rt + Vt - Vt - 1 = Rt + 0 - 0 = Rt\ 
```

<span dir="rtl">والذي يكون صفرًا حتى يساوي</span> R∗
<span dir="rtl">عندما تحدث المكافأة. هنا</span> Vt
<span dir="rtl">و</span>Vt−1 <span dir="rtl">هما على التوالي القيم
المقدرة **للحالات**</span> **(States)** <span dir="rtl">التي تمت زيارتها
في الأوقات</span> t <span dir="rtl">و</span>t−1 <span dir="rtl">في
التجربة.</span> **<span dir="rtl">خطأ</span> TD** <span dir="rtl">في هذه
المرحلة من التعليم يشبه **استجابة خلية الدوبامين العصبية**</span>
**(Dopamine Neuron Response)** <span dir="rtl">لمكافأة غير متوقعة (مثل
قطرة عصير التفاح) في بداية التدريب</span>.

<span dir="rtl">خلال هذه التجربة الأولى وكل التجارب التالية، تحدث
تحديثات</span> **TD(0)** <span dir="rtl">عند كل **انتقال بين الحالات**
</span>**(State Transition)** <span dir="rtl">كما هو موضح في **الفصل
6**.</span> <span dir="rtl">هذا يزيد بشكل متتابع من قيم **الحالات
التنبؤية بالمكافأة** </span>**(Reward-Predicting
States)**<span dir="rtl">، مع انتشار الزيادات للخلف بدءًا من **حالة
المكافأة** </span>**(Rewarding State)**<span dir="rtl">، حتى تتقارب
القيم مع توقعات العائد الصحيحة. في هذه الحالة (لأننا نفترض عدم وجود خصم)
تكون التوقعات الصحيحة مساوية لـ</span> R∗ <span dir="rtl">لجميع
**الحالات التنبؤية بالمكافأة** </span>**(Reward-Predicting
States)**<span dir="rtl">. يمكن رؤية هذا في **الشكل 15.4** في الرسم
البياني لـ</span> **V** <span dir="rtl">المسمّى "عند اكتمال
التعليم</span> (Learning Complete) <span dir="rtl">حيث تكون قيم جميع
**الحالات**</span> **(States)** <span dir="rtl">من **الحالة التنبؤية
بالمكافأة الأقدم**</span> **(Earliest Reward-Predicting State)**
<span dir="rtl">إلى **الحالة التنبؤية بالمكافأة الأخيرة**</span>
**(Latest Reward-Predicting State)** <span dir="rtl">مساوية لـ</span>
R∗<span dir="rtl">.</span> <span dir="rtl">تبقى قيم **الحالات التي تسبق
أول حالة تنبؤية بالمكافأة** </span>**(States Preceding the Earliest
Reward-Predicting State)** <span dir="rtl">منخفضة (يظهرها **الشكل 15.4**
على أنها صفر) لأنها ليست **مؤشرات موثوقة للمكافأة**</span>
**<span dir="rtl">(</span>Reliable <span dir="rtl"></span>Predictors of
Reward<span dir="rtl">)</span>**.

<span dir="rtl">عندما يكتمل التعليم، أي عندما تصل</span> **V**
<span dir="rtl">إلى قيمها الصحيحة، تصبح</span> **TD
<span dir="rtl">أخطاء</span> (TD Errors)** <span dir="rtl">المرتبطة
**بالانتقالات من أي حالة تنبؤية بالمكافأة**</span>
**<span dir="rtl">(</span>Transitions from Any Reward-Predicting
State<span dir="rtl">) </span>**<span dir="rtl">مساوية للصفر لأن
التوقعات أصبحت دقيقة الآن. هذا لأن</span>:

``` math
\delta t - 1 = Rt + Vt - Vt - 1 = 0 + R* - R* = 0
```

<span dir="rtl">وللانتقال من **الحالة التنبؤية بالمكافأة
الأخيرة**</span> **(Latest Reward-Predicting State)**
<span dir="rtl">إلى **حالة المكافأة** </span>**(Rewarding
State)**<span dir="rtl">:</span>

``` math
\delta t - 1 = Rt + Vt - Vt - 1 = R* + 0 - R* = 0
```

<span dir="rtl">من ناحية أخرى، **خطأ**</span> **TD** <span dir="rtl">عند
الانتقال من أي حالة إلى **أول حالة تنبؤية بالمكافأة**</span>
**<span dir="rtl">(</span>Earliest
<span dir="rtl"></span>Reward-Predicting State<span dir="rtl">)
</span>**<span dir="rtl">يكون موجبًا بسبب التباين بين القيمة المنخفضة
لهذه الحالة والقيمة الأكبر للحالة التنبؤية بالمكافأة التالية. في الواقع،
إذا كانت قيمة **الحالة التي تسبق أول حالة تنبؤية بالمكافأة**</span>
**(State Preceding the Earliest Reward-Predicting State)**
<span dir="rtl">صفرًا، فبعد الانتقال إلى **أول حالة تنبؤية بالمكافأة**
</span>**(Earliest Reward-Predicting State)**<span dir="rtl">، سنحصل
على</span>:

``` math
\delta t - 1 = Rt + Vt - Vt - 1 = 0 + R* - 0 = R*
```

<span dir="rtl">يُظهر **الرسم البياني**</span> **(Learning Complete)**
<span dir="rtl">لـ</span> **δ
<span dir="rtl"></span>**<span dir="rtl">في **الشكل 15.4** هذه القيمة
الموجبة في **أول حالة تنبؤية بالمكافأة** </span>**(Earliest
Reward-Predicting State)**<span dir="rtl">، والأصفار في كل مكان
آخر</span>.

**<span dir="rtl">خطأ</span> TD** <span dir="rtl">الإيجابي عند الانتقال
إلى **أول حالة تنبؤية بالمكافأة**</span>
**<span dir="rtl">(</span>Earliest Reward-Predicting
<span dir="rtl"></span>State<span dir="rtl">)
</span>**<span dir="rtl">يماثل استمرار استجابات **الدوبامين**</span>
**(Dopamine)** <span dir="rtl">لأوائل **المؤثرات التي تتنبأ بالمكافأة**
</span>**(Stimuli Predicting Reward)**<span dir="rtl">. بنفس الطريقة،
عندما يكتمل التعليم، فإن الانتقال من **آخر حالة تنبؤية
بالمكافأة**</span> **(Latest Reward-Predicting State)**
<span dir="rtl">إلى **الحالة المكافئة**</span>
**<span dir="rtl">(</span>Rewarding
<span dir="rtl"></span>State<span dir="rtl">)
</span>**<span dir="rtl">ينتج **خطأ**</span> **TD**
<span dir="rtl">يساوي الصفر لأن قيمة **آخر حالة تنبؤية
بالمكافأة**</span> **<span dir="rtl">(</span>Latest Reward-Predicting
State<span dir="rtl">)</span>**<span dir="rtl">، كونها صحيحة، تلغي
**المكافأة** </span>**(Reward)**<span dir="rtl">.</span>
<span dir="rtl">هذا يتوازى مع الملاحظة التي تفيد بأن عددًا أقل من **خلايا
الدوبامين العصبية**</span> **(Dopamine Neurons)** <span dir="rtl">يولد
استجابة فازية **للمكافأة المتوقعة بالكامل**</span> **(Fully Predicted
Reward)** <span dir="rtl">مقارنة **بالمكافأة غير المتوقعة**</span>
**(Unpredicted Reward)**<span dir="rtl">.</span>

<span dir="rtl">بعد التعليم، إذا تم فجأة حذف **المكافأة**
</span>**(Reward)**<span dir="rtl">، فإن **خطأ**</span> **TD**
<span dir="rtl">يصبح سالبًا عند وقت **المكافأة المعتاد**</span> **(Usual
Time of Reward)** <span dir="rtl">لأن قيمة **آخر حالة تنبؤية
بالمكافأة**</span> **<span dir="rtl">(</span>Latest Reward-Predicting
State<span dir="rtl">) </span>**<span dir="rtl">تصبح مرتفعة
للغاية</span>:

``` math
\delta t - 1 = Rt + Vt - Vt - 1 = 0 + 0 - R* = - R*
```

<span dir="rtl">كما هو موضح في الطرف الأيمن من الرسم البياني "حذف
المكافأة</span> (R Omitted)" <span dir="rtl">لـ</span> δ
<span dir="rtl">في **الشكل 15.4**.</span> <span dir="rtl">هذا يشبه
انخفاض **نشاط خلايا الدوبامين العصبية**</span>
**<span dir="rtl">(</span>Dopamine Neuron Activity<span dir="rtl">)
</span>**<span dir="rtl">إلى ما دون **المستوى الأساسي**</span>
**(Baseline)** <span dir="rtl">عند حذف **المكافأة المتوقعة**</span>
**(Expected Reward)** <span dir="rtl">كما يظهر في تجربة **شولتز
وآخرين**</span> **(Schultz et al., 1993)** <span dir="rtl">الموضحة سابقًا
والمبينة في **الشكل 15.3**</span>.

<span dir="rtl">فكرة **أول حالة تنبؤية بالمكافأة**</span> **(Earliest
Reward-Predicting State)** <span dir="rtl">تستحق المزيد من الاهتمام. في
السيناريو الموضح أعلاه، نظرًا لأن التجربة مقسمة إلى تجارب، وافترضنا أن
التوقعات محصورة في تجارب فردية، فإن **أول حالة تنبؤية بالمكافأة**</span>
**<span dir="rtl">(</span>Earliest Reward-Predicting
<span dir="rtl"></span>State<span dir="rtl">)
</span>**<span dir="rtl">هي دائمًا أول حالة في التجربة. من الواضح أن هذا
غير طبيعي. طريقة أكثر عمومية للتفكير في **أول حالة تنبؤية
بالمكافأة**</span> **(Earliest Reward-Predicting State)**
<span dir="rtl">هي أنها **مؤشر غير متوقع للمكافأة**
</span>**(Unpredicted Predictor of Reward)**<span dir="rtl">، ويمكن أن
يكون هناك العديد من هذه **الحالات**
</span>**(States)**<span dir="rtl">.</span> <span dir="rtl">في حياة
الحيوان، قد تسبق العديد من **الحالات المختلفة** </span>**(Different
States)** **<span dir="rtl">أول حالة تنبؤية بالمكافأة</span> (Earliest
Reward-Predicting State)**<span dir="rtl">.</span> <span dir="rtl">ومع
ذلك، لأن هذه **الحالات**</span> **(States)** <span dir="rtl">غالبًا ما
يتبعها **حالات أخرى**</span> **(Other States)** <span dir="rtl">لا تتنبأ
**بالمكافأة** </span>**(Reward)**<span dir="rtl">، فإن قوى **التنبؤ
بالمكافأة**</span> **(Reward-Predicting Powers)** <span dir="rtl">الخاصة
بها، أي قيمها، تظل منخفضة. إذا كانت **خوارزمية**</span> **TD**
<span dir="rtl">تعمل على مدار حياة الحيوان، فستقوم بتحديث قيم هذه
**الحالات** </span>**(States)** <span dir="rtl">أيضًا، لكن التحديثات لن
تتراكم باستمرار لأنه، وفقًا للفرضية، لا تسبق أي من هذه **الحالات**
</span>**(States)** <span dir="rtl">بشكل موثوق **أول حالة تنبؤية
بالمكافأة** </span>**(Earliest Reward-Predicting
State)**<span dir="rtl">. إذا كان أي منها كذلك، فسيكونون أيضًا **حالات
تنبؤية بالمكافأة** </span>**(Reward-Predicting
States)**<span dir="rtl">. قد يفسر هذا السبب في أن استجابات
**الدوبامين**</span> **(Dopamine Responses)** <span dir="rtl">تنخفض حتى
**لأول محفز تنبؤي بالمكافأة**</span> **(Earliest Reward-Predicting
Stimulus)** <span dir="rtl">في التجربة مع **التدريب المفرط**
</span>**(Overtraining)**<span dir="rtl">.</span> <span dir="rtl">مع
**التدريب المفرط** </span>**(Overtraining)**<span dir="rtl">، نتوقع أن
تصبح حتى **الحالة التنبؤية**</span> **(Predictor State)**
<span dir="rtl">التي كانت غير متوقعة سابقًا متوقعة بواسطة **المحفزات
المرتبطة بالحالات السابقة** </span>**(Stimuli Associated with Earlier
States)**<span dir="rtl">:</span> <span dir="rtl">تفاعل الحيوان مع بيئته
داخل المهمة التجريبية وخارجها سيصبح روتينيًا. ولكن عند كسر هذا الروتين عن
طريق إدخال مهمة جديدة، ستظهر **أخطاء**</span> **TD** <span dir="rtl">مرة
أخرى، كما هو ملاحظ بالفعل في **نشاط خلايا الدوبامين العصبية**
</span>**(Dopamine Neuron Activity)**<span dir="rtl">.</span>

<span dir="rtl">المثال الموضح أعلاه يشرح لماذا يشترك **خطأ**</span>
**TD** <span dir="rtl">في ميزات رئيسية مع **النشاط الفازي لخلايا
الدوبامين العصبية**</span> **(Phasic Activity of Dopamine Neurons)**
<span dir="rtl">عندما يكون الحيوان يتعلم في مهمة مشابهة للمهمة المثالية
في مثالنا. ولكن ليست كل خاصية من **خصائص النشاط الفازي لخلايا الدوبامين
العصبية**</span> **(Phasic Activity of Dopamine Neurons)**
<span dir="rtl">تتطابق بشكل دقيق مع **خصائص**
</span>**δ**<span dir="rtl">. أحد الفوارق المزعجة يتعلق بما يحدث عندما
تحدث **المكافأة**</span> **(Reward)** <span dir="rtl">في وقت أبكر مما هو
متوقع. لقد رأينا أن حذف **المكافأة المتوقعة**</span> **(Expected
Reward)** <span dir="rtl">ينتج **خطأ تنبؤ سلبي**</span>
**<span dir="rtl">(</span>Negative <span dir="rtl"></span>Prediction
Error<span dir="rtl">) </span>**<span dir="rtl">عند **الوقت المتوقع
للمكافأة** </span>**(Expected Time of Reward)**<span dir="rtl">، والذي
يتوافق مع انخفاض **نشاط خلايا الدوبامين العصبية**</span> **(Dopamine
Neuron Activity)** <span dir="rtl">إلى ما دون **المستوى الأساسي**</span>
**(Baseline)** <span dir="rtl">عند حدوث ذلك. إذا وصلت
**المكافأة**</span> **(Reward)** <span dir="rtl">بعد الوقت المتوقع،
فإنها تصبح **مكافأة غير متوقعة**</span> **(Unexpected Reward)**
<span dir="rtl">وتولد **خطأ تنبؤ إيجابي**</span>
**<span dir="rtl">(</span>Positive <span dir="rtl"></span>Prediction
Error<span dir="rtl">)</span>**. <span dir="rtl">يحدث هذا مع كل
من</span> **TD <span dir="rtl">أخطاء</span> (TD Errors)**
<span dir="rtl">واستجابات **خلايا الدوبامين العصبية** </span>**(Dopamine
Neuron Responses)**<span dir="rtl">.</span> <span dir="rtl">ولكن عندما
تصل **المكافأة**</span> **(Reward)** <span dir="rtl">في وقت أبكر مما هو
متوقع، لا تقوم **خلايا الدوبامين العصبية**</span> **(Dopamine Neurons)**
<span dir="rtl">بما يفعله **خطأ**</span> **TD** <span dir="rtl">على
الأقل مع **تمثيل**</span> **CSC** <span dir="rtl">المستخدم من قبل
**مونتاج وآخرين**</span> **(Montague et al., 1996)**
<span dir="rtl">وأيضًا في مثالنا</span>. **<span dir="rtl">خلايا
الدوبامين العصبية</span> (Dopamine Neurons)** <span dir="rtl">تستجيب
**للمكافأة المبكرة  
(**</span>**Early Reward<span dir="rtl">)</span>**<span dir="rtl">، وهو
ما يتوافق مع **خطأ**</span> **TD <span dir="rtl">إيجابي</span> (Positive
TD Error)** <span dir="rtl">لأن **المكافأة** </span>**(Reward)**
<span dir="rtl">غير متوقعة أن تحدث في ذلك الوقت. ومع ذلك، في الوقت
اللاحق عندما يتوقع **المكافأة** </span>**(Reward)**
<span dir="rtl">ولكنها لا تحدث، فإن **خطأ**</span> **TD**
<span dir="rtl">يصبح سلبيًا بينما، على عكس هذا التوقع، لا ينخفض **نشاط
خلايا الدوبامين العصبية**</span> **(Dopamine Neuron Activity)**
<span dir="rtl">إلى ما دون **المستوى الأساسي** </span>**(Baseline)**
<span dir="rtl">كما يتوقع</span> **TD <span dir="rtl">نموذج
</span>**(Hollerman and Schultz, 1998)<span dir="rtl">.</span>
<span dir="rtl">هناك شيء أكثر تعقيدًا يحدث في دماغ الحيوان من مجرد
**تعلم**</span> **TD <span dir="rtl">مع تمثيل</span>
CSC**<span dir="rtl">.</span>

<span dir="rtl">يمكن معالجة بعض الاختلافات بين **خطأ**</span> **TD**
<span dir="rtl">و**نشاط خلايا الدوبامين العصبية  
(**</span>**Dopamine Neuron Activity<span dir="rtl">)
</span>**<span dir="rtl">من خلال اختيار قيم **بارامترات**
</span>**(Parameter Values)** <span dir="rtl">مناسبة
**لخوارزمية**</span> **TD** <span dir="rtl">واستخدام **تمثيلات
للمحفزات**</span> **(Stimulus Representations)** <span dir="rtl">غير
**تمثيل** </span>**CSC**<span dir="rtl">. على سبيل المثال، لمعالجة
التباين **للمكافأة المبكرة**</span> **(Early-Reward Mismatch)**
<span dir="rtl">الذي تم وصفه للتو، اقترح **سوري وشولتز**</span> **(Suri
and Schultz, 1999)** **<span dir="rtl">تمثيل</span> CSC**
<span dir="rtl">حيث يتم إلغاء تسلسل **الإشارات الداخلية**</span>
**(Internal Signals)** <span dir="rtl">التي يبدأها **المحفزات المبكرة**
</span>**(Earlier Stimuli)** <span dir="rtl">عند حدوث **المكافأة**
</span>**(Reward)**<span dir="rtl">.</span> <span dir="rtl">اقترح **داو،
كورفيل، وتوريتسكي**</span> **<span dir="rtl">(</span>Daw, Courville, and
<span dir="rtl"></span>Touretzky, 2006<span dir="rtl">)
</span>**<span dir="rtl">أن نظام</span> **TD <span dir="rtl">في
الدماغ</span> (Brain’s TD System)** <span dir="rtl">يستخدم **تمثيلات**
</span>**(Representations)** <span dir="rtl">تنتج عن **النمذجة
الإحصائية**</span> **(Statistical Modeling)** <span dir="rtl">التي يتم
إجراؤها في **القشرة الحسية**</span> **(Sensory Cortex)**
<span dir="rtl">بدلاً من **تمثيلات أبسط**</span> **(Simpler
Representations)** <span dir="rtl">تعتمد على **المدخلات الحسية الأولية**
</span>**(Raw Sensory Input)**<span dir="rtl">.</span>
<span dir="rtl">وجد **لودفيج، ساتون، وكيهو** </span>**(Ludvig, Sutton,
and Kehoe, 2008)** <span dir="rtl">أن **تعلم**</span> **TD
<span dir="rtl">مع تمثيل الميكرو محفز</span> (Microstimulus
Representation, MS)** <span dir="rtl">(الشكل 14.1) يتناسب مع **نشاط
خلايا الدوبامين العصبية**</span> **(Dopamine Neuron Activity)**
<span dir="rtl">في حالات **المكافأة المبكرة (**</span>**Early-Reward
<span dir="rtl"></span>Situations<span dir="rtl">)
</span>**<span dir="rtl">وحالات أخرى بشكل أفضل مقارنة باستخدام **تمثيل**
</span>**CSC**<span dir="rtl">.</span> <span dir="rtl">وجد **بان، شميدت،
ويكنز، وهيلاند**</span> **(Pan, Schmidt, Wickens, and Hyland, 2005)**
<span dir="rtl">أنه حتى مع **تمثيل**</span> **CSC**<span dir="rtl">، فإن
**آثار الأهلية الممتدة**</span> **(Prolonged Eligibility Traces)**
<span dir="rtl">تحسن من تطابق **خطأ**</span> **TD** <span dir="rtl">مع
بعض جوانب **نشاط خلايا الدوبامين العصبية** </span>**(Dopamine Neuron
Activity)**<span dir="rtl">.</span> <span dir="rtl">بشكل عام، تعتمد
العديد من التفاصيل الدقيقة لسلوك **خطأ**</span> **TD**
<span dir="rtl">على تفاعلات دقيقة بين **آثار الأهلية**
</span>**(Eligibility Traces)**<span dir="rtl">، **الخصم**
</span>**(Discounting)**<span dir="rtl">، و**تمثيلات المحفزات**
</span>**(Stimulus Representations)**<span dir="rtl">.</span>
<span dir="rtl">تقدم مثل هذه الاكتشافات **فرضية خطأ تنبؤ
المكافأة**</span> **(Reward Prediction Error Hypothesis)**
<span dir="rtl">وتوضحها دون رفض الزعم الأساسي بأن **النشاط الفازي لخلايا
الدوبامين العصبية**</span> **<span dir="rtl">(</span>Phasic Activity of
<span dir="rtl"></span>Dopamine Neurons<span dir="rtl">)</span>**
<span dir="rtl">يُوصف جيدًا بأنه **إشارات** </span>**TD
<span dir="rtl">خطأ</span> (Signaling TD
Errors)**<span dir="rtl">.</span>

<span dir="rtl">من ناحية أخرى، هناك اختلافات أخرى بين</span> **TD
<span dir="rtl">نظرية</span> (TD Theory)** <span dir="rtl">والبيانات
التجريبية التي لا يمكن استيعابها بسهولة عن طريق اختيار **قيم
البارامترات**</span> **(Parameter Values)** <span dir="rtl">و**تمثيلات
المحفزات**</span> **(Stimulus Representations)** <span dir="rtl">(نذكر
بعض هذه الاختلافات في **قسم الملاحظات الببليوغرافية والتاريخية**</span>
**(Bibliographical and Historical Remarks)** <span dir="rtl">في نهاية
هذا الفصل)، ومن المحتمل أن يتم اكتشاف المزيد من التباينات مع قيام علماء
الأعصاب بإجراء تجارب أكثر تفصيلًا. لكن **فرضية خطأ تنبؤ المكافأة**</span>
**(Reward Prediction Error Hypothesis)** <span dir="rtl">كانت تعمل
بفعالية كبيرة كعامل محفز لتحسين فهمنا لكيفية عمل **نظام المكافأة في
الدماغ**</span> **<span dir="rtl">(</span>Brain’s Reward
<span dir="rtl"></span>System<span dir="rtl">)</span>**.
<span dir="rtl">تم تصميم تجارب معقدة للتحقق من صحة أو دحض التوقعات
المستمدة من **الفرضية** </span>**(Hypothesis)**<span dir="rtl">، وأدت
النتائج التجريبية بدورها إلى تنقيح وتوضيح **فرضية خطأ**</span>
**TD/<span dir="rtl">الدوبامين  
(</span>TD Error/Dopamine Hypothesis<span dir="rtl">)</span>**.

<span dir="rtl">جانب مذهل من هذه التطورات هو أن **خوارزميات ونظرية
التعليم المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning Algorithms and Theory<span dir="rtl">)
</span>**<span dir="rtl">التي تتصل بشكل جيد بخصائص **نظام الدوبامين**
</span>**(Dopamine System)** <span dir="rtl">تم تطويرها من منظور حاسوبي
تمامًا دون أي معرفة مسبقة بالخصائص ذات الصلة **بخلايا الدوبامين
العصبية**</span> **(Dopamine Neurons)** - <span dir="rtl">تذكر أن
**تعلم** </span>**TD<span dir="rtl">  
(</span>TD Learning<span dir="rtl">) </span>**<span dir="rtl">وارتباطاته
**بالتحكم الأمثل**</span> **(Optimal Control)**
<span dir="rtl">و**البرمجة الديناميكية** </span>**(Dynamic
Programming)** <span dir="rtl">تم تطويرها قبل سنوات عديدة من إجراء أي من
التجارب التي كشفت عن الطبيعة الشبيهة **بـ**</span> **TD
<span dir="rtl">لنشاط خلايا الدوبامين العصبية</span>
<span dir="rtl">(</span>TD-Like Nature of Dopamine
<span dir="rtl"></span>Neuron Activity<span dir="rtl">)</span>**.
<span dir="rtl">هذا التوافق غير المخطط له، على الرغم من أنه ليس مثاليًا،
يوحي بأن **التوازي بين خطأ**</span> **TD
<span dir="rtl">والدوبامين</span> (TD Error/Dopamine Parallel)**
<span dir="rtl">يعكس شيئًا مهمًا حول عمليات **المكافأة في الدماغ**
</span>**(Brain Reward Processes)**<span dir="rtl">.</span>

<span dir="rtl">بالإضافة إلى تفسير العديد من ميزات **النشاط الفازي
لخلايا الدوبامين العصبية**</span> **<span dir="rtl">(</span>Phasic
Activity <span dir="rtl"></span>of Dopamine
Neurons<span dir="rtl">)</span>**<span dir="rtl">، تربط **فرضية خطأ تنبؤ
المكافأة**</span> **<span dir="rtl">(</span>Reward Prediction Error
<span dir="rtl"></span>Hypothesis<span dir="rtl">)
</span>**<span dir="rtl">علم الأعصاب بجوانب أخرى من **التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">، خاصة **الخوارزميات
التعليمية**</span> **(Learning Algorithms)** <span dir="rtl">التي
تستخدم</span> **TD <span dir="rtl">أخطاء</span> (TD Errors)**
<span dir="rtl">كإشارات تعزيز. لا يزال علم الأعصاب بعيدًا عن الوصول إلى
فهم كامل **للدارات العصبية** </span>**(Circuits)**<span dir="rtl">،
**الآليات الجزيئية** </span>**(Molecular Mechanisms)**<span dir="rtl">،
ووظائف **النشاط الفازي لخلايا الدوبامين العصبية** </span>**(Phasic
Activity of Dopamine Neurons)**<span dir="rtl">، لكن الأدلة التي تدعم
**فرضية خطأ تنبؤ المكافأة** </span>**(Reward Prediction Error
Hypothesis)**<span dir="rtl">، جنبًا إلى جنب مع الأدلة على أن استجابات
الدوبامين الفازية هي **إشارات تعزيز للتعليم** </span>**(Reinforcement
Signals for Learning)**<span dir="rtl">، تشير إلى أن الدماغ قد ينفذ شيئًا
مشابهًا **لخوارزمية الممثل-الناقد**</span> **(Actor–Critic Algorithm)**
<span dir="rtl">حيث تلعب</span> **TD <span dir="rtl">أخطاء</span> (TD
Errors)** <span dir="rtl">أدوارًا حاسمة. هناك **خوارزميات تعلم معزز
أخرى  
(**</span>**Other Reinforcement Learning Algorithms<span dir="rtl">)
</span>**<span dir="rtl">تعتبر مرشحة معقولة أيضًا، ولكن **خوارزميات
الممثل-الناقد**</span> **(Actor–Critic Algorithms)**
<span dir="rtl">تتناسب بشكل خاص مع **تشريح وفسيولوجيا الدماغ الثديي**
</span>**(Anatomy and Physiology of the Mammalian
Brain)**<span dir="rtl">، كما نصف في القسمين التاليين</span>.

**<u>15.7 <span dir="rtl">الممثل-الناقد العصبي</span> (Neural
Actor–Critic)</u>**

**<span dir="rtl">خوارزميات الممثل-الناقد</span> (Actor–Critic
Algorithms)** <span dir="rtl">تتعلم **السياسات**</span> **(Policies)**
<span dir="rtl">و**دوال القيمة**</span> **(Value Functions)**
<span dir="rtl">معًا.</span> **<span dir="rtl">الممثل</span> (Actor)**
<span dir="rtl">هو المكون الذي يتعلم **السياسات**
</span>**(Policies)**<span dir="rtl">، بينما **الناقد**</span>
**(Critic)** <span dir="rtl">هو المكون الذي يتعلم عن **السياسة**</span>
**(Policy)** <span dir="rtl">المتبعة حاليًا من قبل الممثل من أجل "نقد"
اختيارات الممثل للإجراءات. يستخدم الناقد</span> **TD
<span dir="rtl">خوارزمية</span> (TD Algorithm)** <span dir="rtl">لتعلم
**دالة قيمة الحالة**</span> **(State-Value Function)**
<span dir="rtl">للسياسة الحالية للممثل. تسمح **دالة القيمة**</span>
**<span dir="rtl">(</span>Value
<span dir="rtl"></span>Function<span dir="rtl">)
</span>**<span dir="rtl">للناقد بانتقاد اختيارات الممثل للإجراءات من
خلال إرسال **أخطاء**</span> **TD (δ)** <span dir="rtl">إلى الممثل. إذا
كان</span> **δ <span dir="rtl"></span>**<span dir="rtl">إيجابيًا، فهذا
يعني أن **الإجراء**</span> **(Action)** <span dir="rtl">كان "جيدًا" لأنه
أدى إلى **حالة** </span>**(State)** <span dir="rtl">ذات **قيمة**
</span>**(Value)** <span dir="rtl">أفضل مما كان متوقعًا؛ وإذا كان</span>
**δ <span dir="rtl"></span>**<span dir="rtl">سلبيًا، فهذا يعني أن
**الإجراء**</span> **(Action)** <span dir="rtl">كان "سيئًا" لأنه أدى إلى
**حالة**</span> **(State)** <span dir="rtl">ذات **قيمة**</span>
**(Value)** <span dir="rtl">أسوأ مما كان متوقعًا. بناءً على هذه
الانتقادات، يقوم **الممثل**</span> **(Actor)** <span dir="rtl">بتحديث
**سياساته**</span> **(Policies)** <span dir="rtl">باستمرار</span>.

<span dir="rtl">هناك ميزتان مميزتان **لخوارزميات الممثل-الناقد**</span>
**(Actor–Critic Algorithms)** <span dir="rtl">تجعلان من المحتمل أن
الدماغ قد ينفذ **خوارزمية مشابهة لهذه** </span>**(Similar
Algorithm)**<span dir="rtl">.</span> <span dir="rtl">أولاً، مكونا
**خوارزمية الممثل-الناقد**</span> **(Actor–Critic Algorithm)**
—**<span dir="rtl">الممثل</span> (Actor)** <span dir="rtl">و**الناقد**
</span>**(Critic)** <span dir="rtl">يشيران إلى أن جزئين من **المخطط
العصبي** </span>**(Striatum)** <span dir="rtl">التقسيمات **الظهرية
والبطنية**</span> **<span dir="rtl">(</span>Dorsal and Ventral
<span dir="rtl"></span>Subdivisions<span dir="rtl">)</span>**
<span dir="rtl">(راجع القسم 15.4)، كلاهما حاسم **للتعليم القائم على
المكافأة  
(**</span>**Reward-Based Learning<span dir="rtl">)</span>**
<span dir="rtl">قد يعملان بطريقة مشابهة **للممثل**</span> **(Actor)**
<span dir="rtl">و**الناقد** </span>**(Critic)** <span dir="rtl">على
التوالي. الخاصية الثانية التي تشير إلى إمكانية تنفيذ الدماغ **لخوارزمية
الممثل-الناقد**</span> **<span dir="rtl">(</span>Actor–Critic
Algorithm<span dir="rtl">) </span>**<span dir="rtl">هي أن **خطأ**</span>
**TD (δ)** <span dir="rtl">له دور مزدوج باعتباره **إشارة تعزيز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Signal<span dir="rtl">)
</span>**<span dir="rtl">لكل من **الممثل**</span> **(Actor)**
<span dir="rtl">و**الناقد** </span>**(Critic)**<span dir="rtl">، على
الرغم من أن له تأثيرًا مختلفًا على **التعليم** </span>**(Learning)**
<span dir="rtl">في كل من هذه المكونات. يتناسب هذا جيدًا مع العديد من
خصائص **الدارات العصبية** </span>**(Neural
Circuitry)**<span dir="rtl">.</span>

<img src="./media/image182.png"
style="width:6.26806in;height:3.58194in" />

<span dir="rtl">**الشكل 15.5**:</span> **<span dir="rtl">الشبكة العصبية
الاصطناعية</span> (ANN) <span dir="rtl">للممثل-الناقد</span>
(Actor–Critic ANN)** <span dir="rtl">وتنفيذ عصبي افتراضي</span>.

**(a<span dir="rtl">خوارزمية الممثل-الناقد</span> (Actor–Critic
Algorithm)** <span dir="rtl">كشبكة عصبية اصطناعية</span>
(ANN)<span dir="rtl">.</span> <span dir="rtl">يقوم **الممثل**</span>
**(Actor)** <span dir="rtl">بتعديل **السياسة**</span> **(Policy)**
<span dir="rtl">بناءً على **خطأ**</span> **TD (δ)** <span dir="rtl">الذي
يتلقاه من **الناقد** </span>**(Critic)**<span dir="rtl">؛ يقوم الناقد
بتعديل **بارامترات قيمة الحالة**</span> **(State-Value Parameters)**
<span dir="rtl">باستخدام نفس</span> **δ**<span dir="rtl">.</span>
<span dir="rtl">ينتج الناقد **خطأ**</span> **TD** <span dir="rtl">من
**إشارة المكافأة**</span> **(Reward Signal, R)** <span dir="rtl">ومن
التغيير الحالي في تقديره **لقيم الحالات** </span>**(State
Values)**<span dir="rtl">.</span> <span dir="rtl">لا يملك
**الممثل**</span> **(Actor)** <span dir="rtl">وصولاً مباشرًا **لإشارة
المكافأة** </span>**(Reward Signal)**<span dir="rtl">، ولا يملك
**الناقد**</span> **(Critic)** <span dir="rtl">وصولاً مباشرًا **للإجراء**
</span>**(Action)**<span dir="rtl">.</span>

(b <span dir="rtl">تنفيذ عصبي افتراضي **لخوارزمية الممثل-الناقد**</span>
**<span dir="rtl">(</span>Neural Implementation of an Actor–Critic
Algorithm<span dir="rtl">)</span>**. <span dir="rtl">يتم وضع
**الممثل**</span> **(Actor)** <span dir="rtl">والجزء المتعلم للقيمة **في
الناقد**</span> **<span dir="rtl">(</span>Value-Learning Part of the
Critic<span dir="rtl">) </span>**<span dir="rtl">في التقسيمات **الظهرية
والبطنية**</span> **<span dir="rtl">(</span>Dorsal and Ventral
<span dir="rtl"></span>Subdivisions<span dir="rtl">) للمخطط
العصبي</span> (Striatum)** <span dir="rtl">على التوالي. يتم نقل
**خطأ**</span> **TD** <span dir="rtl">بواسطة **خلايا الدوبامين
العصبية**</span> **(Dopamine Neurons)** <span dir="rtl">الموجودة في
**المنطقة السقيفية البطنية**</span> **(VTA)** <span dir="rtl">و**المادة
السوداء**</span> **(SNpc)** <span dir="rtl">لتعديل التغيرات **في فعالية
المشابك العصبية**</span> **(Synaptic Efficacies)** <span dir="rtl">في
المدخلات من المناطق القشرية **إلى المخطط العصبي الظهري والبطني
(**</span>**Dorsal and Ventral
<span dir="rtl"></span>Striatum<span dir="rtl">)</span>**.

<span dir="rtl">كل من شبكتي **الناقد**</span> **(Critic)**
<span dir="rtl">و**الممثل**</span> **(Actor)** <span dir="rtl">تتلقى
مدخلات تتكون من ميزات متعددة تمثل **حالة بيئة الوكيل** </span>**(State
of the Agent’s Environment)**<span dir="rtl">.</span>
<span dir="rtl">(تذكر من **الفصل 1**</span> <span dir="rtl">أن **بيئة
وكيل التعليم المعزز**</span> **(Environment of a Reinforcement Learning
Agent)** <span dir="rtl">تتضمن مكونات داخلية وخارجية بالنسبة
**للكائن**</span> **(Organism)** <span dir="rtl">الذي يحتوي على الوكيل).
تُظهر **الشكل**</span> **(Figure)** <span dir="rtl">هذه الميزات على أنها
**دوائر تحمل
تسميات**</span>$`\mathbf{\ x1،\ x2،...،\ xn}`$<span dir="rtl">، وقد تم
عرضها مرتين فقط للحفاظ على بساطة الشكل. يرتبط **وزن**</span>
**(Weight)** <span dir="rtl">يمثل **فعالية مشبك عصبي**
</span>**(Efficacy of a Synapse)** <span dir="rtl">بكل اتصال من كل
**ميزة**</span> $`\mathbf{xi}`$ **(Feature xi)**
<span dir="rtl">مع</span> $`\mathbf{V}`$ **<span dir="rtl">وحدة
الناقد</span> (Critic Unit, V)**<span dir="rtl">، ومع كل وحدة من
**وحدات** </span>**Ai <span dir="rtl">الممثل</span> (Actor Units,
Ai)**<span dir="rtl">.</span> <span dir="rtl">تقوم الأوزان في شبكة
الناقد **بتحديد بارامترات دالة القيمة** </span>**(Parameterizing the
Value Function)**<span dir="rtl">، بينما تقوم الأوزان في شبكة الممثل
**بتحديد بارامترات السياسة** </span>**(Parameterizing the
Policy)**<span dir="rtl">.</span> <span dir="rtl">تتعلم الشبكات كلما
تغيرت هذه الأوزان وفقًا **لقواعد تعلم الناقد والممثل**</span> **(Critic
and Actor Learning Rules)** <span dir="rtl">التي سنصفها في القسم
التالي</span>.

<span dir="rtl">يُعد</span> **TD <span dir="rtl">خطأ</span> (TD Error)**
<span dir="rtl">الناتج عن الدارات في **الناقد**</span> **(Critic)**
<span dir="rtl">هو **إشارة التعزيز** </span>**(Reinforcement Signal)**
<span dir="rtl">لتغيير الأوزان في كل من شبكتي **الناقد**</span>
**(Critic)** <span dir="rtl">و**الممثل**
</span>**(Actor)**<span dir="rtl">. يُظهر **الشكل 15.5**</span>**(a**
<span dir="rtl">هذا من خلال الخط المسمى **"**</span>
**TD<span dir="rtl">خطأ</span> (δ)** <span dir="rtl">الممتد عبر جميع
الاتصالات في شبكتي **الناقد**</span> **(Critic)**
<span dir="rtl">و**الممثل** </span>**(Actor)**<span dir="rtl">.</span>
<span dir="rtl">هذا الجانب من تنفيذ الشبكة، مع **فرضية خطأ تنبؤ
المكافأة**</span> **(Reward Prediction Error Hypothesis)**
<span dir="rtl">وحقيقة أن **نشاط خلايا الدوبامين العصبية**</span>
**(Activity of Dopamine Neurons)** <span dir="rtl">يتم توزيعه على نطاق
واسع من خلال **التفرعات الواسعة للمحاور العصبية لهذه الخلايا**
</span>**(Extensive Axonal Arbors of These Neurons)**<span dir="rtl">،
يقترح أن شبكة **ممثل-ناقد**</span> **(Actor–Critic Network)**
<span dir="rtl">مشابهة لهذا قد لا تكون بعيدة عن الواقع كفرضية حول كيفية
حدوث **التعليم المرتبط بالمكافأة في الدماغ**</span>
**<span dir="rtl">(</span>Reward-Related Learning
<span dir="rtl"></span>in the Brain<span dir="rtl">)</span>**.

<span dir="rtl">يقترح **الشكل 15.5**</span>**(b
<span dir="rtl"></span>**<span dir="rtl">بشكل مبسط للغاية - كيف يمكن
**للشبكة العصبية الاصطناعية**</span> **(ANN)** <span dir="rtl">على
الجانب الأيسر من الشكل أن تنطبق على الهياكل في **الدماغ**</span>
**(Brain)** <span dir="rtl">وفقًا **لفرضية تاكاهاشي وآخرين**
</span>**(Hypothesis of Takahashi et al.,
2008)**<span dir="rtl">.</span> <span dir="rtl">تضع الفرضية
**الممثل**</span> **(Actor)** <span dir="rtl">وجزء **تعلم القيمة في
الناقد**</span> **(Value-Learning Part of the Critic)**
<span dir="rtl">على التوالي في **التقسيمات الظهرية والبطنية للمخطط
العصبي** </span>**(Dorsal and Ventral Subdivisions of the
Striatum)**<span dir="rtl">، وهو الهيكل المدخلي **للجسم المخطط**
</span>**(Basal Ganglia)**<span dir="rtl">.</span> <span dir="rtl">تذكر
من **القسم 15.4**</span> <span dir="rtl">أن **المخطط الظهري**
</span>**(Dorsal Striatum)** <span dir="rtl">متورط أساسًا في **تأثير
اختيار الإجراء**</span> **<span dir="rtl">(</span>Influencing Action
<span dir="rtl"></span>Selection<span dir="rtl">)</span>**<span dir="rtl">،
و**المخطط البطني**</span> **(Ventral Striatum)** <span dir="rtl">يُعتقد
أنه حاسم للجوانب المختلفة من **معالجة المكافأة** </span>**(Reward
Processing)**<span dir="rtl">، بما في ذلك **تحديد القيمة العاطفية
للأحاسيس** </span>**(Assignment of Affective Value to
Sensations)**<span dir="rtl">.</span> <span dir="rtl">يقوم **القشرة
المخية**</span> **<span dir="rtl">(</span>Cerebral
<span dir="rtl"></span>Cortex<span dir="rtl">)</span>**<span dir="rtl">،
إلى جانب هياكل أخرى، بإرسال مدخلات إلى **المخطط العصبي**
</span>**(Striatum)**<span dir="rtl">، تنقل المعلومات حول **المحفزات**
</span>**(Stimuli)**<span dir="rtl">، **الحالات الداخلية**
</span>**(Internal States)**<span dir="rtl">، و**النشاط الحركي**</span>
**(Motor Activity)**<span dir="rtl">.</span>

<span dir="rtl">في هذا التنفيذ العصبي الافتراضي **لخوارزمية
الممثل-الناقد**</span> **<span dir="rtl">(</span>Actor–Critic Brain
<span dir="rtl"></span>Implementation<span dir="rtl">)</span>**<span dir="rtl">،
يرسل **المخطط البطني**</span> **(Ventral Striatum)**
<span dir="rtl">معلومات **القيمة**</span>
**<span dir="rtl">(</span>Value
<span dir="rtl"></span>Information<span dir="rtl">)
</span>**<span dir="rtl">إلى **المنطقة السقيفية البطنية**</span>
**(VTA)** <span dir="rtl">و**المادة السوداء**
</span>**(SNpc)**<span dir="rtl">، حيث تقوم **خلايا الدوبامين
العصبية**</span> **(Dopamine Neurons)** <span dir="rtl">في هذه النوى
بدمجها مع **معلومات المكافأة** </span>**(Reward Information)**
<span dir="rtl">لتوليد نشاط يتوافق مع</span> **TD
<span dir="rtl">أخطاء</span> (TD Errors)** <span dir="rtl">(على الرغم من
أن كيفية حساب خلايا الدوبامين العصبية لهذه الأخطاء لم تُفهم بعد). يصبح
الخط المسمى</span> **"<span dir="rtl">خطأ</span> TD
(δ)<span dir="rtl">"</span>** <span dir="rtl">في **الشكل 15.5**
</span>**a<span dir="rtl">)</span>** <span dir="rtl">هو الخط
المسمى</span> **"<span dir="rtl">دوبامين</span> (Dopamine)"**
<span dir="rtl">في **الشكل 15.5** </span>**(b**<span dir="rtl">، والذي
يمثل **المحاور العصبية الواسعة التفرع لخلايا الدوبامين العصبية**</span>
**<span dir="rtl">(</span>Widely Branching Axons
<span dir="rtl"></span>of Dopamine Neurons<span dir="rtl">)</span>**
<span dir="rtl">التي توجد أجسامها الخلوية في **المنطقة السقيفية
البطنية** </span>**(VTA)** <span dir="rtl">و**المادة السوداء**
</span>**(SNpc)**<span dir="rtl">.</span> <span dir="rtl">بالعودة إلى
**الشكل 15.1** </span>**(Figure)**<span dir="rtl">، تقوم هذه المحاور
العصبية بتكوين **مشابك عصبية**</span> **(Synaptic Contacts)**
<span dir="rtl">مع **التغصنات الشوكية للخلايا العصبية المتوسطة**
</span>**(Spines on the Dendrites of Medium Spiny
Neurons)**<span dir="rtl">، وهي **الخلايا العصبية المدخلية/المخرجة
الرئيسية في التقسيمين الظهري والبطني للمخطط العصبي**</span>
**<span dir="rtl">(</span>Main <span dir="rtl"></span>Input/Output
Neurons of Both the Dorsal and Ventral Divisions of the
Striatum<span dir="rtl">)</span>**. <span dir="rtl">تقوم **المحاور
العصبية للخلايا القشرية**</span> **(Axons of Cortical Neurons)**
<span dir="rtl">التي ترسل المدخلات إلى **المخطط العصبي**</span>
**(Striatum)** <span dir="rtl">بتكوين **مشابك عصبية**</span>
**<span dir="rtl">(</span>Synaptic
<span dir="rtl"></span>Contacts<span dir="rtl">)
</span>**<span dir="rtl">على **نصائح هذه التغصنات الشوكية**
</span>**(Tips of These Spines)**<span dir="rtl">.</span>
<span dir="rtl">وفقًا **للفرضية**
</span>**(Hypothesis)**<span dir="rtl">، يحدث **التغير في فعالية المشابك
العصبية**</span> **<span dir="rtl">(</span>Changes in the Efficacies
<span dir="rtl"></span>of Synapses<span dir="rtl">)</span>**
<span dir="rtl">من المناطق القشرية إلى **المخطط العصبي**</span>
**(Striatum)** <span dir="rtl">عند هذه التغصنات وفقًا **لقواعد
التعليم**</span> **(Learning Rules)** <span dir="rtl">التي تعتمد بشكل
حاسم على **إشارة التعزيز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Signal<span dir="rtl">)
</span>**<span dir="rtl">التي يوفرها **الدوبامين**
</span>**(Dopamine)**<span dir="rtl">.</span>

<span dir="rtl">تشير **الفرضية**</span> **(Hypothesis)**
<span dir="rtl">الموضحة في **الشكل 15.5** </span>**b<span dir="rtl">)
</span>**<span dir="rtl">إلى أن **إشارة الدوبامين** </span>**(Dopamine
Signal)** <span dir="rtl">ليست</span> **"<span dir="rtl">إشارة المكافأة
الرئيسية</span> (Master Reward Signal)"** <span dir="rtl">مثل **الإشارة
العددية**</span> $`\mathbf{Rt}`$ **(Scalar Signal** $`\mathbf{Rt}`$**)**
<span dir="rtl">في **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">.</span> <span dir="rtl">في الواقع، تشير
الفرضية إلى أنه لا ينبغي بالضرورة أن تكون قادرًا على فحص
**الدماغ**</span> **(Brain)** <span dir="rtl">وتسجيل أي إشارة
تشبه</span> $`\mathbf{Rt}`$ <span dir="rtl">في نشاط أي خلية عصبية واحدة.
تقوم العديد من الأنظمة العصبية المترابطة **بتوليد معلومات متعلقة
بالمكافأة** </span>**(Generating Reward-Related
Information)**<span dir="rtl">، مع تجنيد هياكل مختلفة حسب أنواع المكافآت
المختلفة. تستقبل **خلايا الدوبامين العصبية** </span>**(Dopamine
Neurons)** <span dir="rtl">معلومات من العديد من مناطق الدماغ المختلفة،
لذا فإن المدخلات **إلى المادة السوداء والمنطقة السقيفية البطنية**</span>
**(Input to the SNpc and VTA)** <span dir="rtl">المسمى</span>
**"<span dir="rtl">مكافأة</span> (Reward)"** <span dir="rtl">في **الشكل
15.5** </span>**b<span dir="rtl">)</span>** <span dir="rtl">يجب أن تُفهم
على أنها **متجه من المعلومات المتعلقة بالمكافأة**</span>
**<span dir="rtl">(</span>Vector of Reward-Related
Information<span dir="rtl">) </span>**<span dir="rtl">الذي يصل إلى
الخلايا العصبية في هذه النوى عبر قنوات إدخال متعددة. ما قد يتوافق مع
**إشارة المكافأة العددية**</span> $`\mathbf{Rt}`$ **(Scalar Reward
Signal** $`\mathbf{Rt}`$**)** <span dir="rtl">هو المساهمة الصافية لجميع
**المعلومات المتعلقة بالمكافأة**</span> **(Reward-Related Information)**
<span dir="rtl">في **نشاط خلايا الدوبامين العصبية** </span>**(Dopamine
Neuron Activity)**<span dir="rtl">.</span> <span dir="rtl">إنها نتيجة
**نمط نشاط عبر العديد من الخلايا العصبية في مناطق مختلفة من
الدماغ**</span> **<span dir="rtl">(</span>Pattern of Activity Across
Many <span dir="rtl"></span>Neurons in Different Areas of the
Brain<span dir="rtl">)</span>**.

<span dir="rtl">على الرغم من أن التنفيذ العصبي **لخوارزمية
الممثل-الناقد**</span> **<span dir="rtl">(</span>Actor–Critic Neural
<span dir="rtl"></span>Implementation<span dir="rtl">)
</span>**<span dir="rtl">الموضح في **الشكل 15**</span>**.5b**
<span dir="rtl">قد يكون صحيحًا في بعض النواحي، فمن الواضح أنه يحتاج إلى
**تنقيح وتوسيع وتعديل**</span> **(Refined, Extended, and Modified)**
<span dir="rtl">ليكون نموذجًا كاملاً **لوظيفة النشاط الفازي لخلايا
الدوبامين العصبية**</span> **<span dir="rtl">(</span>Function of the
Phasic Activity <span dir="rtl"></span>of Dopamine
Neurons<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">يذكر **قسم الملاحظات التاريخية والببليوغرافية**</span>
**<span dir="rtl">(</span>Historical and
<span dir="rtl"></span>Bibliographic Remarks<span dir="rtl">)
</span>**<span dir="rtl">في نهاية هذا الفصل المنشورات التي تناقش بتفصيل
أكبر كلاً من **الدعم التجريبي لهذه الفرضية**</span> **(Empirical Support
for This Hypothesis)** <span dir="rtl">وأماكن فشلها. الآن سنلقي نظرة
مفصلة على ما تقترحه **خوارزميات تعلم الممثل والناقد**</span>
**<span dir="rtl">(</span>Actor and Critic
<span dir="rtl"></span>Learning Algorithms<span dir="rtl">)
</span>**<span dir="rtl">حول **القواعد التي تحكم تغيرات فعالية المشابك
العصبية  
(**</span>**Rules <span dir="rtl"></span>Governing Changes in Synaptic
Efficacies<span dir="rtl">) </span>**<span dir="rtl">للمشابك العصبية
**القشرية-المخططية** </span>**(Corticostriatal
Synapses)**<span dir="rtl">.</span>

**<u>15.8 <span dir="rtl">قواعد تعلم الممثل والناقد</span> (Actor and
Critic Learning Rules)</u>**

<span dir="rtl">إذا كان الدماغ ينفذ شيئًا مشابهًا **لخوارزمية
الممثل-الناقد**</span> **(Actor–Critic Algorithm)** -
<span dir="rtl">وبافتراض أن مجموعات **خلايا الدوبامين العصبية**</span>
**(Dopamine Neurons)** <span dir="rtl">تقوم ببث **إشارة تعزيز مشتركة**
</span>**(Common Reinforcement Signal)** <span dir="rtl">إلى **المشابك
العصبية القشرية-المخططية** </span>**(Corticostriatal Synapses)**
<span dir="rtl">في كل من **المخطط الظهري والبطني**</span>
**<span dir="rtl">(</span>Dorsal and Ventral
<span dir="rtl"></span>Striatum<span dir="rtl">)
</span>**<span dir="rtl">كما هو موضح في **الشكل 15.5**</span>**b**
<span dir="rtl">) وهو احتمال مبسط كما ذكرنا سابقًا - فإن هذه **إشارة
التعزيز**</span> **(Reinforcement Signal)** <span dir="rtl">تؤثر على
**المشابك العصبية**</span> **(Synapses)** <span dir="rtl">لهذه الهياكل
بطريقتين مختلفتين.</span> **<span dir="rtl">قواعد تعلم الناقد
والممثل</span> (Learning Rules for the Critic and Actor)**
<span dir="rtl">تستخدم نفس **إشارة التعزيز** </span>**(Reinforcement
Signal)**<span dir="rtl">، وهي **خطأ** </span>**TD
(δ)**<span dir="rtl">، ولكن تأثيرها على التعليم يختلف بين هذين
المكونين</span>.

**<span dir="rtl">خطأ</span> TD** <span dir="rtl">(المشترك مع **آثار
الأهلية** </span>**(Eligibility Traces)**<span dir="rtl">)</span>
<span dir="rtl">يخبر **الممثل**</span> **(Actor)**
<span dir="rtl">بكيفية تحديث **احتمالات الإجراء**</span> **(Action
Probabilities)** <span dir="rtl">من أجل الوصول إلى **الحالات ذات القيم
الأعلى** </span>**(Higher-Valued States)**<span dir="rtl">.</span>
<span dir="rtl">التعليم من قبل **الممثل**</span> **(Actor)**
<span dir="rtl">يشبه **الشرط الآلي** </span>**(Instrumental
Conditioning)** <span dir="rtl">باستخدام قاعدة تعلم من نوع **قانون
التأثير**</span> **<span dir="rtl">(</span>Law-of-Effect-Type Learning
Rule<span dir="rtl">)</span>** (<span dir="rtl">راجع **القسم
1.7**)</span>: <span dir="rtl">يعمل **الممثل**</span> **(Actor)**
<span dir="rtl">على إبقاء</span> **δ
<span dir="rtl"></span>**<span dir="rtl">إيجابيًا قدر الإمكان. من ناحية
أخرى، يخبر **خطأ**</span> **TD** (<span dir="rtl">المشترك مع **آثار
الأهلية** </span>**(Eligibility Traces)**)
**<span dir="rtl">الناقد</span> (Critic)** <span dir="rtl">بالاتجاه
والحجم اللذين يجب فيهما تغيير **بارامترات دالة القيمة**</span>
**<span dir="rtl">(</span>Parameters of <span dir="rtl"></span>the Value
Function<span dir="rtl">)</span>** <span dir="rtl">من أجل تحسين دقة
**التنبؤ** </span>**(Predictive Accuracy)**<span dir="rtl">.</span>
<span dir="rtl">يعمل **الناقد** </span>**(Critic)** <span dir="rtl">على
تقليل حجم</span> **δ <span dir="rtl"></span>**<span dir="rtl">ليكون أقرب
ما يمكن إلى الصفر باستخدام قاعدة تعلم مثل **نموذج** </span>**TD
<span dir="rtl">للتعليم الشرطي الكلاسيكي</span> (TD Model of Classical
Conditioning)** <span dir="rtl">(راجع **القسم 14.2**)</span>.
<span dir="rtl">الفرق بين **قواعد تعلم الناقد والممثل**</span> **(Critic
and Actor Learning Rules)** <span dir="rtl">بسيط نسبيًا، ولكن هذا
الاختلاف له تأثير عميق على **التعليم**</span> **(Learning)**
<span dir="rtl">وهو أساسي لكيفية عمل **خوارزمية الممثل الناقد**
</span>**(Actor–Critic Algorithm)**<span dir="rtl">.</span>
<span dir="rtl">يكمن الاختلاف فقط في **آثار الأهلية**</span>
**<span dir="rtl">(</span>Eligibility
<span dir="rtl"></span>Traces<span dir="rtl">)
</span>**<span dir="rtl">التي يستخدمها كل نوع من **قواعد التعليم**
</span>**(Learning Rules)**<span dir="rtl">.</span>

<span dir="rtl">يمكن استخدام أكثر من مجموعة من **قواعد التعليم**</span>
**(Learning Rules)** <span dir="rtl">في **الشبكات العصبية
للممثل-الناقد**</span> **(Actor–Critic Neural Networks)**
<span dir="rtl">مثل تلك الموجودة في **الشكل 15.5  **
</span>**b** <span dir="rtl">ولكن، لتوضيح محدد، سنركز هنا على **خوارزمية
الممثل-الناقد** </span>**(Actor–Critic Algorithm)**
<span dir="rtl">للمشاكل المستمرة مع **آثار الأهلية**</span>
**(Eligibility Traces)** <span dir="rtl">التي تم تقديمها في **القسم
13.6**.</span> <span dir="rtl">عند كل **انتقال من حالة**</span> **St
<span dir="rtl">إلى حالة</span>** $`S_{t + 1}`$**<span dir="rtl">، مع
تنفيذ الإجراء</span> At <span dir="rtl">واستلام المكافأة</span>**
$`R_{t + 1}`$<span dir="rtl">، تقوم هذه **الخوارزمية**</span>
**(Algorithm)** <span dir="rtl">بحساب **خطأ**</span> **TD (δ)**
<span dir="rtl">ثم تحديث **متجهات آثار الأهلية**</span>
**<span dir="rtl">(</span>Eligibility <span dir="rtl"></span>Trace
Vectors<span dir="rtl">)</span> <span dir="rtl">(</span>zwt
<span dir="rtl">و</span>zθt<span dir="rtl">)</span>**
<span dir="rtl">و**بارامترات الناقد والممثل**</span>
**<span dir="rtl">(</span>Parameters for the Critic and
<span dir="rtl"></span>Actor)) <span dir="rtl"></span>w
<span dir="rtl">و</span>θ<span dir="rtl">)</span>**<span dir="rtl">،
وفقًا لـ</span>...

``` math
\delta_{t} = R_{t + 1} + \gamma\,\widehat{v}\left( S_{t + 1},w \right) - \widehat{v}\left( S_{t},w \right)
```

``` math
z_{t}^{w} = \lambda\, z_{t - 1}^{w} + \nabla_{w}\widehat{v}\left( S_{t},w \right)
```

``` math
z_{t}^{\theta} = \lambda\, z_{t - 1}^{\theta} + \nabla_{\theta}\ln\pi\left( A_{t} \middle| S_{t},\theta \right)
```

``` math
w \leftarrow w + \alpha_{w}\,\delta_{t}\, z_{t}^{w}
```

``` math
\theta \leftarrow \theta + \alpha_{\theta}\,\delta_{t}\, z_{t}^{\theta}
```

<span dir="rtl">حيث أن</span> γ <span dir="rtl">تقع في النطاق \[0, 1)
وهي **معامل الخصم** </span>**(Discount-Rate
Parameter)**<span dir="rtl">، و</span>λw <span dir="rtl">تقع في النطاق
\[0, 1\] و</span>λθ <span dir="rtl">تقع في النطاق \[0, 1\] وهما **معاملا
التمهيد**</span> **<span dir="rtl">(</span>Bootstrapping
<span dir="rtl"></span>Parameters<span dir="rtl">)
</span>**<span dir="rtl">للناقد والممثل على التوالي، و</span>αw
<span dir="rtl">أكبر من 0 و</span>αθ <span dir="rtl">أكبر من 0 وهما
**معاملا حجم الخطوة** </span>**(Step-Size
Parameters)**<span dir="rtl">.</span>

<span dir="rtl">يمكنك التفكير في **دالة القيمة التقريبية**
</span>$`\mathbf{v\hat{}}`$ **<span dir="rtl">(</span>Approximate Value
Function** $`\mathbf{v\hat{}}`$<span dir="rtl">**)** على أنها مخرجات
وحدة عصبية خطية واحدة، تُسمى **وحدة الناقد**</span> **(Critic Unit)**
<span dir="rtl">ويُرمز لها</span> **V
<span dir="rtl"></span>**<span dir="rtl">في **الشكل
15.5**</span>**a**<span dir="rtl">.</span> <span dir="rtl">إذًا، تكون
**دالة القيمة**</span> **(Value Function)** <span dir="rtl">عبارة عن
**دالة خطية تعتمد على تمثيل المتجه المميز للحالة**
</span>$`\mathbf{s}`$**<span dir="rtl">،</span>**
$`\mathbf{x(s)\  = \ (x\_ 1(s))\hat{}T}`$<span dir="rtl">، معبرًا عنها
باستخدام **متجه الوزن  **
</span>$`\mathbf{\ (w\  = \ (w\_ 1,,\ w\_ n)\hat{}T)}`$<span dir="rtl">.</span>

``` math
\widehat{v}(s,w) = w^{\top}x(s)
```

<span dir="rtl">كل</span> $`\mathbf{xi(s)}`$ <span dir="rtl">يشبه
**الإشارة قبل المشبكية**</span> **(Presynaptic Signal)**
<span dir="rtl">إلى **المشبك العصبي** </span>**(Synapse)**
<span dir="rtl">الخاص بالخلايا العصبية والذي تكون فعاليته هي</span>
$`\mathbf{wi}`$<span dir="rtl">.</span> <span dir="rtl">يتم زيادة أوزان
**الناقد** </span>**(Critic)** <span dir="rtl">وفقًا للقاعدة المذكورة
أعلاه باستخدام</span>
$`\mathbf{\alpha w\,\delta t\, ztw}`$**​**<span dir="rtl">، حيث تتوافق
**إشارة التعزيز**</span> **(**$`\mathbf{\delta t}`$**)**
<span dir="rtl">مع **إشارة الدوبامين** </span>**(Dopamine Signal)**
<span dir="rtl">التي يتم بثها إلى جميع مشابك وحدة الناقد</span>.
**<span dir="rtl">متجه أثر الأهلية (</span>Eligibility
<span dir="rtl"></span>Trace Vector,**
$`\mathbf{z\hat{}w\_ t}`$**<span dir="rtl">)</span>**
<span dir="rtl">لوحدة الناقد هو أثر (متوسط القيم الحديثة) لـ</span>
$`\mathbf{\nabla wv\hat{}(St,w)}`$<span dir="rtl">.</span>
<span dir="rtl">نظرًا لأن</span> $`\mathbf{v\hat{}(s,w)}`$
<span dir="rtl">خطي في الأوزان، فإن</span>
$`\mathbf{\nabla wv\hat{}(St,w) = x(St)}`$<span dir="rtl">.</span>

<span dir="rtl">بالمصطلحات العصبية، يعني هذا أن كل مشبك لديه **أثر أهلية
خاص به** </span>**(Eligibility Trace)**<span dir="rtl">، وهو مكون من
**متجه أثر الأهلية** </span>**(Eligibility Trace Vector,**
$`\mathbf{z\hat{}w\_ t}`$**)**<span dir="rtl">.</span>
<span dir="rtl">يتراكم أثر الأهلية الخاص بمشبك معين وفقًا لمستوى النشاط
الذي يصل إلى هذا المشبك، أي مستوى النشاط **قبل المشبكي**
</span>**(Presynaptic Activity)**<span dir="rtl">، الممثل هنا بواسطة
**مكون متجه الميزات** </span>$`\mathbf{x(St)}`$ <span dir="rtl">**القادم
إلى هذا المشبك**.</span> <span dir="rtl">يتلاشى الأثر بخلاف ذلك نحو
الصفر بمعدل تحكمه **الجزء**
</span>$`\mathbf{\lambda w}`$**​**<span dir="rtl">.</span>
<span dir="rtl">يكون المشبك مؤهلاً للتعديل طالما أن أثر الأهلية الخاص به
ليس صفراً. كيف يتم تعديل فعالية المشبك يعتمد بالفعل على إشارات التعزيز
التي تصل بينما يكون المشبك مؤهلاً. نطلق على آثار الأهلية غير المشروطة
الخاصة بمشابك وحدة الناقد **آثار أهلية غير مشروطة**</span>
**(Non-Contingent Eligibility Traces)** <span dir="rtl">لأنها تعتمد فقط
على النشاط قبل المشبكي وليست مشروطة بأي شكل من الأشكال على النشاط **بعد
المشبكي** </span>**(Postsynaptic Activity)**<span dir="rtl">.</span>

<span dir="rtl">تعني آثار الأهلية غير المشروطة الخاصة بمشابك وحدة الناقد
أن قاعدة تعلم وحدة الناقد هي في الأساس **نموذج**</span> **TD
<span dir="rtl">للتعليم الشرطي الكلاسيكي</span> (TD Model of Classical
Conditioning)** <span dir="rtl">الموصوف في **القسم 14.2**.</span>
<span dir="rtl">مع التعريف الذي قدمناه أعلاه لوحدة الناقد وقاعدة تعلمها،
فإن الناقد في **الشكل 15.5**</span>**a** <span dir="rtl">هو نفسه الناقد
في **الشبكة العصبية الاصطناعية الممثل-الناقد**</span>
**<span dir="rtl">(</span>ANN Actor–Critic<span dir="rtl">)
</span>**<span dir="rtl">لـ **بارتو وآخرين** </span>**(Barto et al.,
1983)**<span dir="rtl">.</span> <span dir="rtl">من الواضح أن ناقدًا كهذا
يتألف من وحدة عصبية خطية واحدة هو أبسط نقطة انطلاق؛ وحدة الناقد هذه هي
وكيل لشبكة عصبية أكثر تعقيدًا قادرة على تعلم **دوال القيمة**</span>
**(Value Functions)** <span dir="rtl">ذات تعقيد أكبر</span>.

<span dir="rtl">الممثل في **الشكل 15.5** </span>**a
<span dir="rtl"></span>**<span dir="rtl">هو شبكة ذات طبقة واحدة تتكون
من</span> $`k`$ <span dir="rtl">وحدات ممثل عصبية، كل منها يتلقى في
الوقت</span> t <span dir="rtl">نفس **متجه الميزات**
</span>$`\mathbf{x(St)}`$ <span dir="rtl">الذي تتلقاه وحدة الناقد. كل
وحدة ممثل</span> $`j`$<span dir="rtl">، حيث</span>
$`j = 1,\ldots,kj\  = \ 1`$,<span dir="rtl">، لديها **متجه وزن خاص بها**
</span>$`\mathbf{\theta j}`$**​**<span dir="rtl">، ولكن نظرًا لأن وحدات
الممثل كلها متطابقة، سنصف وحدة واحدة فقط ونحذف المؤشر السفلي. إحدى الطرق
التي يمكن أن تتبع بها هذه الوحدات **خوارزمية الممثل-الناقد**</span>
**(Actor–Critic Algorithm)** <span dir="rtl">المعطاة في المعادلات أعلاه
هي أن تكون كل منها وحدة **بيرنولي لوجستية** </span>**(Bernoulli-Logistic
Unit)**<span dir="rtl">.</span> <span dir="rtl">هذا يعني أن مخرجات كل
وحدة ممثل في كل وقت هي **متغير عشوائي** </span>**(Random
Variable)**<span dir="rtl">،</span> **At​**<span dir="rtl">، تأخذ القيمة
0 أو 1. فكر في القيمة 1 على أنها إطلاق العصبون، أي إصدار **جهد فعل**
</span>**(Action Potential)**<span dir="rtl">.</span>
<span dir="rtl">يتم تحديد **الاحتمالات الإجرائية**</span> **(Action
Probabilities)** <span dir="rtl">للوحدة من خلال **المجموع الوزني
المدخلات** </span>$`\mathbf{\theta\top x(St)}`$ <span dir="rtl">عبر
**توزيع سوفت ماكس الأسي**</span> **(Exponential Soft-Max Distribution)**
<span dir="rtl">(راجع **المعادلة 13.2**)، والذي بالنسبة للإجراءات
الثنائية هو **دالة لوجستية** </span>**(Logistic
Function)**<span dir="rtl">.</span>

``` math
\pi\left( 1 \middle| s,\theta \right) = 1 - \pi\left( 0 \middle| s,\theta \right) = \frac{1}{1 + \exp\left( - \theta^{\top}x(s) \right)}
```

<span dir="rtl">تتم زيادة **أوزان كل وحدة ممثل**</span> **(Weights of
Each Actor Unit)**<span dir="rtl">، كما هو مذكور أعلاه، باستخدام</span>:
θ←θ+αθ δt ztθ\theta \leftarrow \theta + \alpha\_{\theta} \\ \delta_t \\
z^{\theta}\_tθ←θ+αθ​δt​ztθ​<span dir="rtl">، حيث تتوافق</span> **δ
<span dir="rtl"></span>** <span dir="rtl">مرة أخرى مع **إشارة
الدوبامين**</span> **<span dir="rtl">(</span>Dopamine
<span dir="rtl"></span>Signal<span dir="rtl">)</span>**:
<span dir="rtl">وهي نفس **إشارة التعزيز**</span> **(Reinforcement
Signal)** <span dir="rtl">التي يتم إرسالها إلى جميع **المشابك العصبية
لوحدة الناقد** </span>**(Synapses of the Critic
Unit)**<span dir="rtl">.</span> <span dir="rtl">يوضح **الشكل
15.5**</span>**a** <span dir="rtl">أنه يتم بث</span> δt\delta_tδt​
<span dir="rtl">إلى جميع **المشابك العصبية**</span> **(Synapses)**
<span dir="rtl">الخاصة بجميع **وحدات الممثل** </span>**(Actor Units)**
<span dir="rtl">(مما يجعل **شبكة الممثل**</span> **(Actor Network)**
<span dir="rtl">فريقًا من **وكلاء التعليم المعزز**
</span>**(Reinforcement Learning Agents)**<span dir="rtl">، وهو ما
نناقشه في **القسم 15.10** أدناه)</span>. **<span dir="rtl">متجه أثر
الأهلية للممثل</span> (Actor Eligibility Trace Vector, z^{\theta}\_t)**
<span dir="rtl">هو أثر (متوسط القيم الحديثة) لـ</span>
$`\nabla\theta\ln\pi(At \mid St,\theta)`$<span dir="rtl">.</span>
<span dir="rtl">لفهم هذا **أثر الأهلية** </span>**(Eligibility
Trace)**<span dir="rtl">، ارجع إلى **التمرين 13.5** </span>**(Exercise
13.5)**<span dir="rtl">، الذي يعرف هذا النوع من الوحدات ويطلب منك إعطاء
**قاعدة تعلم**</span> **<span dir="rtl">(</span>Learning
<span dir="rtl"></span>Rule<span dir="rtl">)
</span>**<span dir="rtl">لها. يطلب منك هذا التمرين التعبير عن</span>
$`s`$, $`\nabla\theta\ln\pi(a \mid s,\theta)`$ <span dir="rtl">من
حيث</span> $`\mathbf{a}`$ **<span dir="rtl">الإجراء (</span>Action
<span dir="rtl"></span>**$`\mathbf{a}`$<span dir="rtl">**)**،</span>
$`\mathbf{x(s)}`$ **<span dir="rtl">متجه الميزات</span> (Feature
Vector** $`\mathbf{x(s)}`$**)**<span dir="rtl">،
و</span>$`\pi(a \mid s,\theta)`$ <span dir="rtl">لحالة عشوائية</span>
$`s`$ <span dir="rtl">وإجراء عشوائي</span> $`a`$<span dir="rtl">)</span>
<span dir="rtl">عن طريق حساب **التدرج**
</span>**(Gradient)**<span dir="rtl">.</span> <span dir="rtl">بالنسبة
**للإجراء**</span> **(Action)** <span dir="rtl">و**الحالة**
</span>**(State)** <span dir="rtl">الفعليين اللذين يحدثان في
الوقت</span> $`t`$<span dir="rtl">، الإجابة هي</span>:

``` math
\nabla_{\theta}\ln\pi\left( A_{t} \middle| S_{t},\theta \right) = \left( A_{t} - \pi\left( A_{t} \middle| S_{t},\theta \right) \right)x\left( S_{t} \right)
```

<span dir="rtl">على عكس **أثر الأهلية غير المشروط**</span>
**(Non-Contingent Eligibility Trace)** <span dir="rtl">في **مشبك الناقد
العصبي**</span> **(Critic Synapse)** <span dir="rtl">الذي يراكم فقط
**النشاط قبل المشبكي** </span>**(Presynaptic Activity)**
<span dir="rtl">المتمثل في</span> $`\mathbf{x(St)}`$<span dir="rtl">،
فإن **أثر الأهلية**</span> **(Eligibility Trace)** <span dir="rtl">في
**مشبك وحدة الممثل (**</span>**Actor <span dir="rtl"></span>Unit’s
Synapse<span dir="rtl">)</span>** <span dir="rtl">يعتمد بالإضافة إلى ذلك
على **نشاط وحدة الممثل (**</span>**Activity of the Actor
<span dir="rtl"></span>Unit<span dir="rtl">)</span>**
<span dir="rtl">نفسها. نطلق على هذا **أثر الأهلية المشروط**</span>
**<span dir="rtl">(</span>Contingent Eligibility
<span dir="rtl"></span>Trace<span dir="rtl">)
</span>**<span dir="rtl">لأنه يعتمد على هذا **النشاط بعد المشبكي**
</span>**(Postsynaptic Activity)**<span dir="rtl">.</span>
<span dir="rtl">يتلاشى أثر الأهلية في كل مشبك باستمرار، ولكن يتم زيادته
أو تقليله اعتمادًا على **نشاط الخلية العصبية قبل المشبكية
(**</span>**Presynaptic <span dir="rtl"></span>Neuron
Activity<span dir="rtl">)</span>** <span dir="rtl">وما إذا كانت **الخلية
العصبية بعد المشبكية** </span>**(Postsynaptic Neuron)**
<span dir="rtl">قد أطلقت إشارة أم لا. العامل</span>
$`\mathbf{At - \pi(At \mid St,\theta)}`$ <span dir="rtl">في المعادلة
(15.3) يكون إيجابيًا عندما يكون</span> $`\mathbf{At = 1}`$
<span dir="rtl">وسالبًا في الحالات الأخرى. الاعتماد على **النشاط بعد
المشبكي** </span>**(Postsynaptic Activity)** <span dir="rtl">في **آثار
الأهلية لوحدات الممثل**</span> **<span dir="rtl">(</span>Eligibility
Traces of Actor <span dir="rtl"></span>Units<span dir="rtl">)
</span>**<span dir="rtl">هو الفرق الوحيد بين **قواعد تعلم الناقد
والممثل**</span> **<span dir="rtl">(</span>Critic and Actor Learning
<span dir="rtl"></span>Rules<span dir="rtl">)</span>**.
<span dir="rtl">من خلال الاحتفاظ بمعلومات حول **الإجراءات**</span>
**(Actions)** <span dir="rtl">التي تم اتخاذها في **حالات معينة**
</span>**(States)**<span dir="rtl">، تتيح **آثار الأهلية المشروطة**
</span>**(Contingent Eligibility Traces)** <span dir="rtl">توزيع الفضل
عن **المكافأة**</span> **(Reward)** <span dir="rtl">(القيمة الإيجابية
لـ</span> δ<span dir="rtl">)، أو **اللوم عن العقوبة** </span>**(Blame
for Punishment)** <span dir="rtl">(القيمة السلبية لـ</span>
δ)<span dir="rtl">، بين **بارامترات السياسة**</span> **(Policy
Parameters)** (<span dir="rtl">أي **فعالية المشابك العصبية لوحدات
الممثل** </span>**(Synaptic Efficacies of the Actor Units’
Synapses)**<span dir="rtl">)</span> <span dir="rtl">وفقًا للمساهمات التي
قدمتها هذه البارامترات في **مخرجات الوحدات**</span> **(Units’ Outputs)**
<span dir="rtl">التي كان من الممكن أن تؤثر على القيم اللاحقة لـ</span>
δ<span dir="rtl">. **آثار الأهلية المشروطة**</span>
**<span dir="rtl">(</span>Contingent Eligibility
<span dir="rtl"></span>Traces<span dir="rtl">)
</span>**<span dir="rtl">تحدد **المشابك العصبية** </span>**(Synapses)**
<span dir="rtl">حول كيفية تعديلها لتغيير **استجابات الوحدات في المستقبل
(**</span>**Future Responses <span dir="rtl"></span>of
Units<span dir="rtl">)</span>**<span dir="rtl">لتفضيل القيم الإيجابية
لـ</span> δ<span dir="rtl">.</span>

<span dir="rtl">ماذا تقترح **قواعد تعلم الناقد والممثل**</span>
**(Critic and Actor Learning Rules)** <span dir="rtl">حول كيفية تغيير
فعالية **المشابك العصبية القشرية-المخططية** </span>**(Corticostriatal
Synapses)**<span dir="rtl">؟ كلا **القاعدتين التعليميتين**</span>
**(Learning Rules)** <span dir="rtl">مرتبطتان بالاقتراح الكلاسيكي لـ
**دونالد هيب** </span>**(Donald Hebb)** <span dir="rtl">الذي ينص على أنه
كلما شاركت **إشارة قبل المشبكية**</span> **(Presynaptic Signal)**
<span dir="rtl">في تنشيط **الخلية العصبية بعد المشبكية**
</span>**(Postsynaptic Neuron)**<span dir="rtl">، تزداد **فعالية
المشبك**</span> **<span dir="rtl">(</span>Synapse’s
<span dir="rtl"></span>Efficacy<span dir="rtl">)</span>**
<span dir="rtl"></span>(Hebb, 1949)<span dir="rtl">. تشترك **قواعد تعلم
الناقد والممثل**</span> **<span dir="rtl">(</span>Critic and Actor
Learning <span dir="rtl"></span>Rules<span dir="rtl">)
</span>**<span dir="rtl">مع اقتراح هيب في فكرة أن تغييرات **فعالية
المشبك**</span> **(Synapse’s Efficacy)** <span dir="rtl">تعتمد على تفاعل
عدة عوامل. في **قاعدة تعلم الناقد** </span>**(Critic Learning
Rule)**<span dir="rtl">، يكون التفاعل بين **إشارة التعزيز**</span> **(δ)
(Reinforcement Signal δ)** <span dir="rtl">و**آثار الأهلية**</span>
**(Eligibility Traces)** <span dir="rtl">التي تعتمد فقط على **الإشارات
قبل المشبكية** </span>**(Presynaptic Signals)**<span dir="rtl">.</span>
<span dir="rtl">يطلق علماء الأعصاب على هذه القاعدة **قاعدة تعلم ثنائية
العامل**</span> **(Two-Factor Learning Rule)** <span dir="rtl">لأن
التفاعل يحدث بين إشارتين أو كميتين. من ناحية أخرى، **قاعدة تعلم
الممثل**</span> **(Actor Learning Rule)** <span dir="rtl">هي **قاعدة
تعلم ثلاثية العامل**</span> **(Three-Factor Learning Rule)**
<span dir="rtl">لأنه بالإضافة إلى اعتمادها على</span> δ<span dir="rtl">،
تعتمد **آثار الأهلية**</span> **(Eligibility Traces)**
<span dir="rtl">على كل من **النشاط قبل المشبكي** </span>**(Presynaptic
Activity)** <span dir="rtl">و**النشاط بعد المشبكي**
</span>**(Postsynaptic Activity)**<span dir="rtl">.</span>
<span dir="rtl">على عكس اقتراح هيب، فإن التوقيت النسبي للعوامل يكون
حاسمًا في كيفية تغيير **فعالية المشابك** </span>**(Synaptic
Efficacies)**<span dir="rtl">، مع تدخل **آثار الأهلية**</span>
**(Eligibility Traces)** <span dir="rtl">للسماح لإشارة التعزيز بالتأثير
على **المشابك**</span> **(Synapses)** <span dir="rtl">التي كانت نشطة في
الماضي القريب</span>.

<span dir="rtl">تستحق بعض التعقيدات المتعلقة بتوقيت **الإشارات**</span>
**(Signals)** <span dir="rtl">في **قواعد تعلم الناقد والممثل**</span>
**<span dir="rtl">(</span>Critic <span dir="rtl"></span>and Actor
Learning Rules<span dir="rtl">) </span>**<span dir="rtl">اهتمامًا أقرب.
عند تعريف **الوحدات العصبية الشبيهة بالخلايا العصبية**</span>
**(Neuron-Like Units)** <span dir="rtl">في الناقد والممثل، تجاهلنا مقدار
الوقت الصغير الذي يستغرقه **المدخل المشبكي**</span> **(Synaptic Input)**
<span dir="rtl">للتأثير على إطلاق **الخلية العصبية الحقيقية**</span>
**<span dir="rtl">(</span>Real
<span dir="rtl"></span>Neuron<span dir="rtl">)</span>**.
<span dir="rtl">عندما يصل **جهد الفعل**</span> **(Action Potential)**
<span dir="rtl">من **الخلية العصبية قبل المشبكية** </span>**(Presynaptic
Neuron)** <span dir="rtl">إلى **المشبك العصبي**
</span>**(Synapse)**<span dir="rtl">، يتم إطلاق **جزيئات الناقل العصبي**
</span>**(Neurotransmitter Molecules)** <span dir="rtl">التي تنتشر عبر
**الفجوة المشبكية**</span> **(Synaptic Cleft)** <span dir="rtl">إلى
**الخلية العصبية بعد المشبكية** </span>**(Postsynaptic
Neuron)**<span dir="rtl">، حيث ترتبط **بالمستقبلات**</span>
**(Receptors)** <span dir="rtl">على سطح **الخلية العصبية بعد المشبكية**
</span>**(Postsynaptic Neuron)**<span dir="rtl">؛ هذا ينشط **الآليات
الجزيئية**</span> **(Molecular Machinery)** <span dir="rtl">التي تسبب
إطلاق **الخلية العصبية بعد المشبكية** </span>**(Postsynaptic Neuron)**
<span dir="rtl">(أو تثبيط إطلاقها في حالة **المدخل المشبكي
المثبط**</span> **<span dir="rtl">)</span>Inhibitory Synaptic
Input)**<span dir="rtl">)</span>. <span dir="rtl">قد تستغرق هذه العملية
عدة عشرات من الميلي ثانية. وفقًا للمعادلتين (15.1) و (15.2)، فإن **المدخل
إلى وحدة الناقد والممثل**</span> **(Input to a Critic and Actor Unit)**
<span dir="rtl">ينتج **مخرجات الوحدة**</span> **(Unit’s Output)**
<span dir="rtl">على الفور. تجاهل وقت التفعيل بهذا الشكل شائع في النماذج
المجردة لتغير **البلاستيك العصبي على غرار هيب** </span>**(Hebbian-Style
Plasticity)**<span dir="rtl">، حيث تتغير **فعالية المشابك**</span>
**(Synaptic Efficacies)** <span dir="rtl">وفقًا لمنتج بسيط **للنشاط
المتزامن قبل المشبكي وبعد المشبكي** </span>**(Simultaneous Pre- and
Postsynaptic Activity)**<span dir="rtl">.</span> <span dir="rtl">يجب على
النماذج الأكثر واقعية أن تأخذ في الاعتبار **وقت التفعيل**
</span>**(Activation Time)**<span dir="rtl">.</span>

**<span dir="rtl">وقت التفعيل</span> (Activation Time)**
<span dir="rtl">مهم بشكل خاص لوحدة ممثل أكثر واقعية لأنه يؤثر على كيفية
عمل **آثار الأهلية المشروطة**</span> **(Contingent Eligibility Traces)**
<span dir="rtl">بشكل صحيح لتوزيع الفضل عن **التعزيز**</span>
**(Reinforcement)** <span dir="rtl">على **المشابك العصبية المناسبة**
</span>**(Appropriate Synapses)**<span dir="rtl">. التعبير</span>
**At​−π(At​∣St​,θ)<span dir="rtl">،</span> x(St)** <span dir="rtl">الذي
يحدد **آثار الأهلية المشروطة**</span>
**<span dir="rtl">(</span>Contingent Eligibility
<span dir="rtl"></span>Traces<span dir="rtl">)
</span>**<span dir="rtl">لقاعدة تعلم وحدة الممثل المعطاة أعلاه يشمل
العامل **بعد المشبكي**</span> **<span dir="rtl">(</span>Postsynaptic
<span dir="rtl"></span>Factor<span dir="rtl">)</span>**
<span dir="rtl"></span>$`\mathbf{At - \pi(At \mid St,\theta)}`$
<span dir="rtl">والعامل **قبل المشبكي** </span>**(Presynaptic Factor)**
$`\mathbf{x(St)}`$<span dir="rtl">.</span> <span dir="rtl">يعمل هذا
لأننا بتجاهل **وقت التفعيل**</span> **(Activation
Time)**<span dir="rtl">، يشارك **النشاط قبل المشبكي**</span>
**<span dir="rtl">(</span>Presynaptic
<span dir="rtl"></span>Activity<span dir="rtl">)</span>**
$`\mathbf{x(St)}`$ <span dir="rtl">في التسبب في **النشاط بعد
المشبكي**</span> **(Postsynaptic Activity)** <span dir="rtl">الذي يظهر
في</span>
$`\mathbf{At - \pi(At \mid St,\theta)}`$<span dir="rtl">.</span>
<span dir="rtl">لتوزيع الفضل عن **التعزيز**</span> **(Reinforcement)**
<span dir="rtl">بشكل صحيح، يجب أن يكون **العامل قبل المشبكي**</span>
**(Presynaptic Factor)** <span dir="rtl">الذي يحدد **أثر
الأهلية**</span> **<span dir="rtl">(</span>Eligibility
<span dir="rtl"></span>Trace<span dir="rtl">)
</span>**<span dir="rtl">سببًا في **العامل بعد المشبكي**</span>
**(Postsynaptic Factor)** <span dir="rtl">الذي يحدد أيضًا الأثر. يجب أن
تأخذ **آثار الأهلية المشروطة**</span> **(Contingent Eligibility
Traces)** <span dir="rtl">لوحدة الممثل الأكثر واقعية في الاعتبار **وقت
التفعيل** </span>**(Activation Time)**<span dir="rtl">.</span>
<span dir="rtl">يجب عدم الخلط بين **وقت التفعيل** </span>**(Activation
Time)** <span dir="rtl">و**الوقت المطلوب لتلقي الخلية العصبية إشارة
تعزيز تتأثر بنشاط تلك الخلية العصبية**</span>
**<span dir="rtl">(</span>Time <span dir="rtl"></span>Required for a
Neuron to Receive a Reinforcement Signal Influenced by That Neuron’s
Activity<span dir="rtl">)</span>**. <span dir="rtl">وظيفة **آثار
الأهلية**</span> **(Eligibility Traces)** <span dir="rtl">هي تغطية هذه
الفترة الزمنية التي تكون عادةً أطول بكثير من **وقت التفعيل**
</span>**(Activation Time)**<span dir="rtl">.</span> <span dir="rtl">نحن
نناقش هذا بشكل أكبر في القسم التالي</span>.

<span dir="rtl">هناك إشارات من **علم الأعصاب**</span> **(Neuroscience)**
<span dir="rtl">حول كيفية عمل هذه العملية في **الدماغ**
</span>**(Brain)**<span dir="rtl">. اكتشف **علماء الأعصاب**</span>
**(Neuroscientists)** <span dir="rtl">شكلًا من أشكال **التغير البلاستيكي
العصبي على غرار هيب**</span> **(Hebbian-Style Plasticity)**
<span dir="rtl">يسمى **التغير البلاستيكي المعتمد على توقيت
النبضات**</span> **(Spike-Timing-Dependent Plasticity, STDP)**
<span dir="rtl">الذي يعزز من احتمال وجود **تغيرات بلاستيكية في المشابك
الشبيهة بوحدات الممثل**</span> **<span dir="rtl">(</span>Synaptic
Plasticity in Actor-Like Synapses<span dir="rtl">)
</span>**<span dir="rtl">في **الدماغ**
</span>**(Brain)**<span dir="rtl">.</span> **<span dir="rtl">التغير
البلاستيكي المعتمد على توقيت النبضات</span> (STDP)** <span dir="rtl">هو
**تغير بلاستيكي على غرار هيب** </span>**(Hebbian-Style
Plasticity)**<span dir="rtl">، لكن التغيرات في **فعالية المشبك
العصبي**</span> **(Synapse’s Efficacy)** <span dir="rtl">تعتمد على
التوقيت النسبي **لجهود الفعل قبل المشبكي وبعد المشبكي**</span>
**<span dir="rtl">(</span>Relative Timing of Presynaptic and
Postsynaptic Action
<span dir="rtl"></span>Potentials<span dir="rtl">)</span>**.
<span dir="rtl">يمكن أن يأخذ الاعتماد أشكالًا مختلفة، ولكن في الشكل
الأكثر دراسة، يزداد **قوة المشبك العصبي**</span> **(Synaptic Strength)**
<span dir="rtl">إذا وصلت النبضات عبر هذا **المشبك العصبي**
</span>**(Synapse)** <span dir="rtl">قبل وقت قصير من إطلاق **الخلية
العصبية بعد المشبكية**</span> **<span dir="rtl">(</span>Postsynaptic
<span dir="rtl"></span>Neuron<span dir="rtl">)</span>**.
<span dir="rtl">إذا تم عكس العلاقة الزمنية، مع وصول **النبضة قبل
المشبكية** </span>**(Presynaptic Spike)** <span dir="rtl">بعد وقت قصير
من إطلاق **الخلية العصبية بعد المشبكية** </span>**(Postsynaptic
Neuron)**<span dir="rtl">، فإن **قوة المشبك العصبي**</span> **(Synaptic
Strength)** <span dir="rtl">تنخفض</span>. **<span dir="rtl">التغير
البلاستيكي المعتمد على توقيت النبضات</span> (STDP)** <span dir="rtl">هو
نوع من **التغير البلاستيكي على غرار هيب**</span> **(Hebbian
Plasticity)** <span dir="rtl">الذي يأخذ في الاعتبار **وقت
التفعيل**</span> **(Activation Time)** <span dir="rtl">للخلية العصبية،
وهو أحد المكونات الضرورية **لتعلم وحدات الممثل** </span>**(Actor-Like
Learning)**<span dir="rtl">.</span>

<span dir="rtl">اكتشاف **التغير البلاستيكي المعتمد على توقيت
النبضات**</span> **<span dir="rtl">(</span>Spike-Timing-Dependent
<span dir="rtl"></span>Plasticity, STDP<span dir="rtl">)
</span>**<span dir="rtl">قاد **علماء الأعصاب**</span>
**(Neuroscientists)** <span dir="rtl">إلى التحقيق في إمكانية وجود شكل
من</span> **STDP <span dir="rtl">بثلاثة عوامل</span> (Three-Factor Form
of STDP)** <span dir="rtl">حيث يجب أن يتبع المدخل التنظيمي العصبي</span>
(Neuromodulatory Input) <span dir="rtl">النبضات **قبل المشبكية**</span>
**<span dir="rtl">(</span>Presynaptic
<span dir="rtl"></span>Spikes<span dir="rtl">)
</span>**<span dir="rtl">و**بعد المشبكية**</span> **(Postsynaptic
Spikes)** <span dir="rtl">في التوقيت المناسب. هذا الشكل من **التغير
البلاستيكي المشبكي**</span> **(Synaptic Plasticity)**<span dir="rtl">،
الذي يُطلق عليه</span> **STDP <span dir="rtl">المعدل بواسطة
المكافأة</span> (Reward-Modulated STDP)**<span dir="rtl">، يشبه إلى حد
كبير **قاعدة تعلم الممثل**</span> **<span dir="rtl">(</span>Actor
Learning <span dir="rtl"></span>Rule<span dir="rtl">)
</span>**<span dir="rtl">التي نوقشت هنا. التغيرات المشبكية التي قد تنتج
عن</span> **STDP <span dir="rtl">العادي</span> (Regular STDP)**
<span dir="rtl">تحدث فقط إذا كان هناك مدخل تنظيمي عصبي</span>
(Neuromodulatory Input) <span dir="rtl">ضمن نافذة زمنية بعد أن يتبع نبضة
**قبل المشبكية**</span> **(Presynaptic Spike)** <span dir="rtl">نبضة
**بعد المشبكية**</span> **<span dir="rtl">(</span>Postsynaptic
<span dir="rtl"></span>Spike<span dir="rtl">)
</span>**<span dir="rtl">بفترة قصيرة. تتراكم الأدلة على أن</span> **STDP
<span dir="rtl">المعدل بواسطة المكافأة</span>
<span dir="rtl">(</span>Reward-Modulated STDP<span dir="rtl">)
</span>**<span dir="rtl">يحدث في **التشعبات الشوكية للخلايا العصبية
المتوسطة الشوكية**</span> **<span dir="rtl">(</span>Spines
<span dir="rtl"></span>of Medium Spiny Neurons<span dir="rtl">)
</span>**<span dir="rtl">في **المخطط الظهري** </span>**(Dorsal
Striatum)**<span dir="rtl">، حيث يقدم **الدوبامين**</span>
**(Dopamine)** <span dir="rtl">العامل التنظيمي العصبي</span>
(Neuromodulatory Factor) - <span dir="rtl">المواقع التي يحدث فيها تعلم
الممثل</span> (Actor Learning) <span dir="rtl">في **التطبيق العصبي
الافتراضي لخوارزمية الممثل-الناقد** </span>**(Hypothetical Neural
Implementation of an Actor–Critic Algorithm)** <span dir="rtl">الموضحة
في **الشكل** </span>**(Figure 15.5b)**<span dir="rtl">.</span>
<span dir="rtl">أظهرت التجارب</span> **STDP <span dir="rtl">المعدل
بواسطة المكافأة</span> <span dir="rtl">(</span>Reward-Modulated
STDP<span dir="rtl">) </span>**<span dir="rtl">حيث تحدث تغييرات دائمة في
**فعالية المشابك القشرية-المخططية** </span>**(Efficacies of
Corticostriatal Synapses)** <span dir="rtl">فقط إذا وصل **نبضة تنظيم
عصبي** </span>**(Neuromodulatory Pulse)** <span dir="rtl">ضمن نافذة
زمنية يمكن أن تستمر حتى 10 ثوانٍ بعد أن يتبع نبضة **قبل المشبكية**</span>
**(Presynaptic Spike)** <span dir="rtl">نبضة **بعد المشبكية**</span>
**(Postsynaptic Spike)** <span dir="rtl">بفترة قصيرة</span> (Yagishita
et al. 2014)<span dir="rtl">.</span> <span dir="rtl">على الرغم من أن
الأدلة غير مباشرة، فإن هذه التجارب تشير إلى وجود **آثار أهلية
مشروطة**</span> **(Contingent Eligibility Traces)** <span dir="rtl">ذات
مسارات زمنية طويلة. الآليات الجزيئية التي تنتج هذه الآثار، وكذلك الآثار
الأقصر بكثير التي من المحتمل أن تكون وراء</span>
**STDP**<span dir="rtl">، لم يتم فهمها بعد، ولكن الأبحاث التي تركز على
**التغير البلاستيكي المشبكي المعتمد على الزمن والمنظم العصبي**</span>
**<span dir="rtl">(</span>Time-Dependent and Neuromodulator-Dependent
Synaptic <span dir="rtl"></span>Plasticity<span dir="rtl">)
</span>**<span dir="rtl">لا تزال مستمرة</span>.

**<span dir="rtl">وحدة الممثل الشبيهة بالخلايا العصبية</span>
(Neuron-Like Actor Unit)** <span dir="rtl">التي وصفناها هنا، مع **قاعدة
تعلم على غرار قانون الأثر** </span>**(Law-of-Effect-Style Learning
Rule)**<span dir="rtl">، ظهرت في شكل أبسط بعض الشيء في **شبكة
الممثل-الناقد**</span> **(Actor–Critic Network)**
<span dir="rtl">لـ</span> **Barto et al.
(1983)**<span dir="rtl">.</span> <span dir="rtl">كانت تلك الشبكة مستوحاة
من **فرضية الخلايا العصبية اللذة (**</span>**Hedonistic Neuron
Hypothesis<span dir="rtl">) </span>**<span dir="rtl">التي اقترحها
**الفيزيولوجي** </span>**A. H. Klopf (1972,
1982)**<span dir="rtl">.</span> <span dir="rtl">ليست جميع تفاصيل
فرضية</span> **Klopf** <span dir="rtl">متسقة مع ما تم تعلمه عن **التغير
البلاستيكي المشبكي** </span>**(Synaptic Plasticity)**<span dir="rtl">،
ولكن اكتشاف</span> **STDP** <span dir="rtl">والأدلة المتزايدة على وجود
شكل من</span> **STDP <span dir="rtl">المعدل بواسطة المكافأة</span>
(Reward-Modulated STDP)** <span dir="rtl">يشير إلى أن أفكار</span>
**Klopf** <span dir="rtl">قد لا تكون بعيدة عن الصواب. نحن نناقش فرضية
**الخلايا العصبية اللذة**</span> **(Hedonistic Neuron Hypothesis)**
<span dir="rtl">لـ</span> **Klopf** <span dir="rtl">بعد ذلك</span>.

**<u>15.9 <span dir="rtl">الخلايا العصبية المتعة</span> (Hedonistic
Neurons)</u>**

<span dir="rtl">في **فرضية الخلايا العصبية المتعة**</span> **(Hedonistic
Neuron Hypothesis)** <span dir="rtl">التي اقترحها</span> **Klopf (1972,
1982)**<span dir="rtl">، افترض أن **الخلايا العصبية الفردية**</span>
**(Individual Neurons)** <span dir="rtl">تسعى لتعظيم الفرق بين المدخلات
المشبكية</span> (Synaptic Input) <span dir="rtl">التي تُعامل على أنها
مكافأة والمدخلات المشبكية التي تُعامل على أنها عقاب من خلال تعديل فعالية
مشابكها</span> (Synaptic Efficacies) <span dir="rtl">بناءً على النتائج
المكافئة أو العقابية لجهود الفعل الخاصة بها. بمعنى آخر، يمكن تدريب
**الخلايا العصبية الفردية** </span>**(Individual Neurons)**
<span dir="rtl">باستخدام التعزيز المرتبط بالاستجابة</span>
<span dir="rtl">(</span>Response-Contingent
<span dir="rtl"></span>Reinforcement<span dir="rtl">) كما يمكن تدريب
الحيوان في مهمة **التكييف الآلي**</span>
**<span dir="rtl">(</span>Instrumental
<span dir="rtl"></span>Conditioning Task<span dir="rtl">)</span>**.
<span dir="rtl">تضمنت فرضيته الفكرة القائلة بأن **المكافآت**</span>
**(Rewards)** <span dir="rtl">و**العقوبات** </span>**(Punishments)**
<span dir="rtl">يتم نقلها إلى الخلية العصبية عبر نفس المدخلات المشبكية
التي تحفز أو تثبط نشاط توليد النبضات العصبية</span> (Spike-Generating
Activity) <span dir="rtl">للخلية. (لو كان</span> **Klopf**
<span dir="rtl">يعرف ما نعرفه اليوم عن **أنظمة التعديل العصبي**
</span>**(Neuromodulatory Systems)**<span dir="rtl">، ربما كان قد خصص
الدور المعزز للمدخلات التنظيمية العصبية</span> (Neuromodulatory
Input)<span dir="rtl">، لكنه أراد تجنب أي مصدر مركزي لمعلومات التدريب).
كان للآثار المحلية للمشتركة **قبل المشبكية**</span> **(Presynaptic)**
<span dir="rtl">و**بعد المشبكية**</span> **(Postsynaptic)**
<span dir="rtl">دورًا رئيسيًا في فرضية</span> **Klopf** <span dir="rtl">في
جعل المشابك مؤهلة (التي قدمها المصطلح) للتعديل بواسطة المكافأة أو العقاب
اللاحق. وقد افترض أن هذه الآثار يتم تنفيذها بواسطة آليات جزيئية محلية
لكل مشبك وبالتالي تختلف عن النشاط الكهربائي لكل من الخلايا العصبية قبل
المشبكية وبعد المشبكية</span>.

<span dir="rtl">افترض</span> **Klopf** <span dir="rtl">بشكل محدد أن
**فعالية المشابك العصبية**</span> **(Synaptic Efficacies)**
<span dir="rtl">تتغير بالطريقة التالية: عندما تطلق خلية عصبية **جهد
فعل** </span>**(Action Potential)**<span dir="rtl">، تصبح جميع مشابكها
التي كانت نشطة في المساهمة في هذا الجهد مؤهلة للخضوع لتغييرات في
فعاليتها. إذا تبع جهد الفعل زيادة في **المكافأة**</span> **(Reward)**
<span dir="rtl">خلال فترة زمنية مناسبة، فإن فعالية جميع المشابك المؤهلة
تزداد. وبالمثل، إذا تبع جهد الفعل زيادة في **العقوبة**</span>
**(Punishment)** <span dir="rtl">خلال فترة زمنية مناسبة، فإن فعالية
المشابك المؤهلة تنخفض. يتم تنفيذ ذلك عن طريق تحفيز أثر أهلية في المشبك
عند تزامن النشاط **قبل المشبكي** </span>**(Presynaptic Activity)**
<span dir="rtl">والنشاط **بعد المشبكي**</span> **(Postsynaptic
Activity)** <span dir="rtl">(أو بشكل أكثر دقة، عند اقتران النشاط قبل
المشبكي مع النشاط بعد المشبكي الذي يشارك النشاط قبل المشبكي في التسبب
فيه) - ما نسميه أثر الأهلية المشروط</span> (Contingent Eligibility
Trace)<span dir="rtl">.</span> <span dir="rtl">هذه في الأساس هي **قاعدة
التعليم ثلاثية العوامل**</span> **(Three-Factor Learning Rule)**
<span dir="rtl">الخاصة بوحدة الممثل</span> (Actor Unit)
<span dir="rtl">التي تم وصفها في القسم السابق</span>.

<span dir="rtl">يعكس شكل ومسار الوقت لأثر الأهلية في نظرية</span>
**Klopf** <span dir="rtl">الفترات الزمنية للعديد من حلقات التغذية
الراجعة التي تكون الخلية العصبية جزءًا منها، بعضها يقع بالكامل داخل
**الدماغ والجسم**</span> **<span dir="rtl">(</span>Brain and
<span dir="rtl"></span>Body<span dir="rtl">)
</span>**<span dir="rtl">للكائن الحي، في حين يمتد البعض الآخر عبر البيئة
الخارجية للكائن الحي كما يتم توسيطه عبر أنظمة **الحركة والحسية**</span>
**(Motor and Sensory Systems)** <span dir="rtl">الخاصة به. كانت فكرته أن
شكل أثر الأهلية المشبكية</span> (Synaptic Eligibility Trace)
<span dir="rtl">يشبه **مخططًا زمنيًا** </span>**(Histogram)**
<span dir="rtl">للفترات الزمنية لحلقات التغذية الراجعة التي تشارك فيها
تلك الخلية العصبية. وبالتالي، سيكون قمة أثر الأهلية عند فترة الزمن
الأكثر شيوعًا لحلقات التغذية الراجعة التي تشارك فيها تلك الخلية
العصبية</span>.

<span dir="rtl">آثار الأهلية المستخدمة في **الخوارزميات**</span>
**(Algorithms)** <span dir="rtl">الموصوفة في هذا الكتاب هي إصدارات مبسطة
من فكرة</span> **Klopf** <span dir="rtl">الأصلية، حيث تكون دوال متناقصة
بشكل أسي (أو هندسي) يتم التحكم فيها بواسطة **البارامترات**</span>
**(Parameters)** <span dir="rtl">مثل</span> "λ"
<span dir="rtl">و</span>"γ"<span dir="rtl">.</span> <span dir="rtl">هذا
يبسط المحاكاة وكذلك النظرية، لكننا نعتبر هذه الآثار البسيطة كبديل لآثار
أقرب إلى مفهوم</span> **Klopf** <span dir="rtl">الأصلي، الذي سيكون له
مزايا حسابية في الأنظمة المعقدة للتعليم المعزز</span> (Reinforcement
Learning Systems) <span dir="rtl">من خلال تحسين عملية تخصيص
الائتمان</span>.

<span dir="rtl">فرضية **الخلايا العصبية المتعة**</span> **(Hedonistic
Neuron Hypothesis)** <span dir="rtl">لـ</span> **Klopf**
<span dir="rtl">ليست غير محتملة كما قد تبدو في البداية. مثال مدروس جيدًا
لخلايا فردية تسعى لبعض **المنبهات** </span>**(Stimuli)**
<span dir="rtl">وتتجنب أخرى هو **البكتيريا الإشريكية القولونية**
</span>**(Escherichia coli Bacterium)**<span dir="rtl">.</span>
<span dir="rtl">يتأثر **حركة** </span>**(Movement)** <span dir="rtl">هذا
الكائن وحيد الخلية بالمنبهات الكيميائية في بيئته، وهي **سلوك يُعرف باسم
الانجذاب الكيميائي** </span>**(Chemotaxis)**<span dir="rtl">.</span>
<span dir="rtl">تسبح **الإشريكية القولونية**</span> **(E. coli)**
<span dir="rtl">في بيئتها السائلة عن طريق تدوير **هياكل تشبه الشعيرات
تسمى الأسواط**</span> **(Flagella)** <span dir="rtl">المثبتة على سطحها.
(نعم، تقوم بتدويرها!) ترتبط **الجزيئات**</span> **(Molecules)**
<span dir="rtl">في بيئة البكتيريا بمستقبلات على سطحها. تؤثر هذه
**الأحداث الربطية**</span> **(Binding Events)** <span dir="rtl">على
تواتر انقلاب اتجاه دوران الأسواط. كل انقلاب يتسبب في أن تتخبط
**البكتيريا**</span> **(Bacterium)** <span dir="rtl">في مكانها ثم تنطلق
في اتجاه جديد عشوائي. تؤدي ذاكرة كيميائية بسيطة وحسابات إلى تقليل تواتر
انقلاب الأسواط عندما تسبح **البكتيريا**</span> **(Bacterium)**
<span dir="rtl">نحو تركيزات أعلى من الجزيئات التي تحتاجها للبقاء
(المنجذبات)، وتزيد من تواتر الانقلاب عندما تسبح نحو تركيزات أعلى من
الجزيئات الضارة (المواد الطاردة). النتيجة هي أن **البكتيريا**</span>
**(Bacterium)** <span dir="rtl">تميل إلى الاستمرار في السباحة باتجاه
**منحدرات المنجذبات**</span> **(Attractant Gradients)**
<span dir="rtl">وتميل إلى تجنب السباحة باتجاه **منحدرات المواد الطاردة**
</span>**(Repellant Gradients)**<span dir="rtl">.</span>

**<span dir="rtl">السلوك الكيميائي التكتيكي</span> (Chemotactic
Behavior)** <span dir="rtl">الذي تم وصفه للتو يُسمى **الحركة الكلينوكية**
</span>**(Klinokinesis)**<span dir="rtl">.</span> <span dir="rtl">وهو
نوع من السلوك التجريبي **التجربة والخطأ**</span>
**<span dir="rtl">(</span>Trial-and-Error
<span dir="rtl"></span>Behavior<span dir="rtl">)</span>**<span dir="rtl">،
على الرغم من أنه من غير المحتمل أن يكون **التعليم**</span>
**(Learning)** <span dir="rtl">مشمولًا فيه</span>:
**<span dir="rtl">البكتيريا</span> (Bacterium)** <span dir="rtl">تحتاج
إلى قدر ضئيل من الذاكرة قصيرة الأمد لاكتشاف تدرجات تركيز الجزيئات، لكنها
على الأرجح لا تحتفظ بذكريات طويلة الأمد. أطلق رائد **الذكاء الاصطناعي**
</span>**(Artificial Intelligence)** **<span dir="rtl">أوليفر
سيلفريدج</span> (Oliver Selfridge)** <span dir="rtl">على هذه
الاستراتيجية اسم "الركض والتميل</span>" <span dir="rtl"></span>(Run and
Twiddle)<span dir="rtl">، مشيرًا إلى فائدتها كاستراتيجية تكيفية أساسية:
"استمر في نفس الاتجاه إذا كانت الأمور تتحسن، وإلا فانتقل إلى مكان
آخر</span> (Selfridge, 1978, 1984)<span dir="rtl">.</span>
<span dir="rtl">وبالمثل، يمكن للمرء أن يتخيل **الخلية العصبية**</span>
**(Neuron)** "<span dir="rtl">تسبح" (وليس حرفياً بالطبع) في وسط يتكون من
مجموعة معقدة من حلقات التغذية الراجعة التي تكون جزءًا منها، وتعمل للحصول
على نوع واحد من إشارات المدخلات وتجنب الأخرى. على عكس **البكتيريا**
</span>**(Bacterium)**<span dir="rtl">، ومع ذلك، تحتفظ **الخلية
العصبية**</span> **(Neuron)** <span dir="rtl">بقوة التشابك العصبي
بمعلومات حول سلوكها السابق في التجربة والخطأ. إذا كانت هذه النظرة إلى
سلوك الخلية العصبية (أو نوع واحد فقط من الخلايا العصبية) معقولة، فإن
الطبيعة المغلقة لحلقة التغذية الراجعة التي تتفاعل بها الخلية العصبية مع
بيئتها مهمة لفهم سلوكها، حيث تتكون بيئة الخلية العصبية من باقي الكائن
الحي مع البيئة التي يتفاعل معها الكائن الحي ككل</span>.

<span dir="rtl">امتدت فرضية **الخلية العصبية المتعة**</span>
**(Hedonistic Neuron Hypothesis)** <span dir="rtl">لـ</span> **Klopf**
<span dir="rtl">إلى ما هو أبعد من فكرة أن **الخلايا العصبية
الفردية**</span> **(Individual Neurons)** <span dir="rtl">هي عوامل
**التعليم المعزز** </span>**(Reinforcement Learning
Agents)**<span dir="rtl">. جادل بأن العديد من جوانب السلوك الذكي يمكن
فهمها كنتيجة للسلوك الجماعي لمجموعة من **الخلايا العصبية المتعة**</span>
**(Hedonistic Neurons)** <span dir="rtl">المهتمة بمصلحتها الذاتية، والتي
تتفاعل مع بعضها البعض في مجتمع هائل أو نظام اقتصادي يشكل **الجهاز
العصبي**</span> **(Nervous System)** <span dir="rtl">للكائن الحي. سواء
أكانت هذه النظرة إلى **الجهاز العصبي** </span>**(Nervous System)**
<span dir="rtl">مفيدة أم لا، فإن السلوك الجماعي لعوامل **التعليم
المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning Agents<span dir="rtl">)</span>**
<span dir="rtl">له آثار على **علم الأعصاب**
</span>**(Neuroscience)**<span dir="rtl">.</span>
<span dir="rtl">سنتناول هذا الموضوع في القسم التالي</span>.

**<u>15.10 <span dir="rtl">التعليم المعزز الجماعي</span> (Collective
Reinforcement Learning)</u>**

<span dir="rtl">سلوك مجموعات من **عوامل التعليم المعزز**</span>
**(Reinforcement Learning Agents)** <span dir="rtl">ذو صلة وثيقة بدراسة
الأنظمة الاجتماعية والاقتصادية، وإذا كان هناك أي صحة لفرضية **الخلية
العصبية المتعة**</span> **(Hedonistic Neuron Hypothesis)**
<span dir="rtl">لـ</span> **Klopf**<span dir="rtl">، فهو يتعلق أيضًا بعلم
الأعصاب</span> (Neuroscience)<span dir="rtl">.</span>
<span dir="rtl">الفرضية التي وصفت أعلاه حول كيفية تنفيذ **خوارزمية
الممثل-الناقد**</span> **<span dir="rtl">(</span>Actor–Critic
Algorithm<span dir="rtl">) </span>**<span dir="rtl">في الدماغ تتناول
بشكل ضيق آثار حقيقة أن التقسيمات الظهرانية والبطنية للجسم المخطط</span>
(Striatum)<span dir="rtl">، المواقع المفترضة للممثل والناقد وفقًا
للفرضية، تحتوي كل منها على ملايين من **الخلايا العصبية الشوكية
المتوسطة**</span> **(Medium Spiny Neurons)** <span dir="rtl">التي تخضع
مشابكها العصبية للتغيير بتأثير الانفجارات الفازية لنشاط **خلايا
الدوبامين العصبية**</span> **<span dir="rtl">(</span>Dopamine
<span dir="rtl"></span>Neurons<span dir="rtl">)</span>**.

**<span dir="rtl">الممثل</span> (Actor)** <span dir="rtl">في **الشكل
15.5**</span>**a** <span dir="rtl">هو شبكة ذات طبقة واحدة من وحدات
ممثلين</span> $`k`$<span dir="rtl">.</span> <span dir="rtl">الإجراءات
التي تنتجها هذه الشبكة هي متجهات</span>
($`A1,\ A2,\  \cdot \  \cdot \  \cdot \ ,\ Ak`$) <span dir="rtl">يفترض
أنها تدفع سلوك الحيوان. التغييرات في فعالية المشابك العصبية لكل هذه
الوحدات تعتمد على **إشارة التعزيز** </span>**(Reinforcement Signal)
<span dir="rtl">(</span>,**. <span dir="rtl">لأن وحدات الممثلين تحاول
جعل) كبيرة قدر الإمكان، فإن (تعمل فعليًا كإشارة مكافأة لها (في هذه
الحالة، التعزيز هو نفسه المكافأة). لذلك، كل وحدة ممثل هي في حد ذاتها
**عامل تعلم معزز** </span>**(Reinforcement Learning Agent)**
<span dir="rtl">أو **خلية عصبية متعة**</span> **(Hedonistic Neuron)**
<span dir="rtl">إذا صح التعبير. الآن، لجعل الوضع بسيطًا قدر الإمكان،
افترض أن كل من هذه الوحدات تتلقى نفس **إشارة المكافأة**</span> **(Reward
Signal)** <span dir="rtl">في نفس الوقت (على الرغم من أن الافتراض بأن
الدوبامين يُطلق في جميع المشابك القشرية تحت نفس الظروف وفي نفس الأوقات
ربما يكون مبسطًا بشكل مفرط)</span>.

<span dir="rtl">ما الذي يمكن أن تخبرنا به نظرية **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">حول ما يحدث
عندما يتعلم جميع أعضاء مجموعة من عوامل **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement Learning
<span dir="rtl"></span>Agents<span dir="rtl">)</span>**
<span dir="rtl">وفقًا لإشارة مكافأة مشتركة؟ مجال **التعليم المعزز متعدد
الوكلاء**</span> **<span dir="rtl">(</span>Multi-Agent
<span dir="rtl"></span>Reinforcement Learning<span dir="rtl">)</span>**
<span dir="rtl">يتناول العديد من الجوانب المتعلقة بالتعليم من خلال
مجموعات من عوامل **التعليم المعزز** </span>**(Reinforcement Learning
Agents)**<span dir="rtl">.</span> <span dir="rtl">على الرغم من أن هذا
المجال يتجاوز نطاق هذا الكتاب، إلا أننا نعتقد أن بعض مفاهيمه الأساسية
ونتائجه ذات صلة بالتفكير في أنظمة **التنظيم العصبي المنتشرة**</span>
**(Diffuse Neuromodulatory Systems)** <span dir="rtl">في الدماغ. في
**التعليم المعزز متعدد العوامل**</span> **(Multi-Agent Reinforcement
Learning)** (<span dir="rtl">وفي **نظرية الألعاب** </span>**(Game
Theory)**<span dir="rtl">)، يُعرف السيناريو الذي يحاول فيه جميع العوامل
تعظيم **إشارة مكافأة مشتركة** </span>**(Common Reward Signal)**
<span dir="rtl">يتلقونها في الوقت نفسه باسم **لعبة تعاونية**</span>
**<span dir="rtl">(</span>Cooperative
<span dir="rtl"></span>Game<span dir="rtl">) </span>**<span dir="rtl">أو
**مشكلة فريق** </span>**(Team Problem)**<span dir="rtl">.</span>

<span dir="rtl">ما يجعل **مشكلة الفريق**</span> **(Team Problem)**
<span dir="rtl">مثيرة للاهتمام وتحديًا هو أن **إشارة المكافأة المشتركة**
</span>**(Common Reward Signal)** <span dir="rtl">المرسلة إلى كل عامل
تُقيِّم نمط النشاط الناتج عن المجموعة الكاملة، أي أنها تُقيِّم **الفعل
الجماعي**</span> **(Collective Action)** <span dir="rtl">لأعضاء الفريق.
وهذا يعني أن أي عامل فردي لديه قدرة محدودة على التأثير على **إشارة
المكافأة**</span> **(Reward Signal)** <span dir="rtl">لأن كل عامل يساهم
فقط في مكون واحد من **الفعل الجماعي**</span> **(Collective Action)**
<span dir="rtl">الذي يتم تقييمه بواسطة **إشارة المكافأة المشتركة**
</span>**(Common Reward Signal)**<span dir="rtl">.</span>
<span dir="rtl">التعليم الفعال في هذا السيناريو يتطلب معالجة مشكلة
**توزيع الفضل البنيوي** </span>**(Structural Credit Assignment
Problem)<span dir="rtl">:</span>** <span dir="rtl">من يستحق الفضل في
الفريق على **إشارة المكافأة الإيجابية** </span>**(Positive Reward
Signal)**<span dir="rtl">، أو اللوم على **إشارة المكافأة السلبية**
</span>**(Negative Reward Signal)**<span dir="rtl">؟ إنها **لعبة
تعاونية**</span> **(Cooperative Game)** <span dir="rtl">أو **مشكلة
فريق**</span> **(Team Problem)** <span dir="rtl">لأن العوامل موحدة في
السعي لزيادة **إشارة المكافأة المشتركة** </span>**(Common Reward
Signal)<span dir="rtl">:</span>** <span dir="rtl">لا توجد تضاربات في
المصالح بين العوامل. سيكون السيناريو **لعبة تنافسية**</span>
**(Competitive Game)** <span dir="rtl">إذا تلقت العوامل المختلفة
**إشارات مكافأة مختلفة** </span>**(Different Reward
Signals)**<span dir="rtl">، حيث تقوم كل **إشارة مكافأة**</span>
**(Reward Signal)** <span dir="rtl">مرة أخرى بتقييم **الفعل
الجماعي**</span> **(Collective Action)** <span dir="rtl">للسكان، والهدف
لكل عامل هو زيادة **إشارة مكافأته الخاصة** </span>**(Own Reward
Signal)**<span dir="rtl">.</span> <span dir="rtl">في هذه الحالة، قد تكون
هناك تضاربات في المصالح بين العوامل، مما يعني أن الأفعال التي تكون جيدة
لبعض العوامل قد تكون سيئة للآخرين. حتى تحديد ما يجب أن يكون **الفعل
الجماعي الأمثل**</span> **(Best Collective Action)** <span dir="rtl">ليس
بالأمر البسيط وفقًا لـ **نظرية الألعاب** </span>**(Game
Theory)**<span dir="rtl">.</span> <span dir="rtl">قد يكون هذا الإعداد
التنافسي ذو صلة أيضًا بـ **علم الأعصاب** </span>**(Neuroscience)**
<span dir="rtl">(على سبيل المثال، لتفسير تغاير نشاط **خلايا الدوبامين
العصبية (**</span>**Dopamine <span dir="rtl"></span>Neuron
Activity<span dir="rtl">)</span>**<span dir="rtl">)، لكننا هنا نركز فقط
على الحالة التعاونية أو الجماعية</span>.

<span dir="rtl">كيف يمكن لكل **عامل تعلم معزز**</span> **(Reinforcement
Learning Agent)** <span dir="rtl">في فريق أن يتعلم "القيام بالشيء
الصحيح" بحيث يتم مكافأة **الفعل الجماعي**</span> **(Collective Action)**
<span dir="rtl">للفريق بشكل كبير؟ نتيجة مثيرة للاهتمام هي أنه إذا كان
بإمكان كل عامل أن يتعلم بشكل فعال على الرغم من أن **إشارة مكافأته**
</span>**(Reward Signal)** <span dir="rtl">تتعرض للتشويش بشكل كبير، وعلى
الرغم من عدم وصوله إلى معلومات الحالة الكاملة، فإن المجموعة ككل ستتعلم
إنتاج **أفعال جماعية**</span> **(Collective Actions)**
<span dir="rtl">تتحسن بمرور الوقت كما تم تقييمها بواسطة **إشارة المكافأة
المشتركة** </span>**(Common Reward Signal)**<span dir="rtl">، حتى عندما
لا يستطيع العوامل التواصل مع بعضهم البعض. يواجه كل عامل **مهمة تعلم
معزز** </span>**(Reinforcement Learning Task)** <span dir="rtl">حيث
يتعمق تأثيره على **إشارة المكافأة (**</span>**Reward
<span dir="rtl"></span>Signal<span dir="rtl">)
</span>**<span dir="rtl">في الضوضاء التي تُنتجها تأثيرات العوامل الأخرى.
في الواقع، بالنسبة لأي عامل، جميع العوامل الأخرى هي جزء من بيئته لأن
مدخلاته، سواء كانت جزءًا ناقلًا لمعلومات الحالة أو جزء المكافأة، تعتمد على
كيفية تصرف جميع العوامل الأخرى. علاوة على ذلك، لعدم وجود الوصول إلى
أفعال العوامل الأخرى، بل عدم وجود الوصول إلى **البارامترات**</span>
**(Parameters)** <span dir="rtl">التي تحدد سياساتهم، يمكن لكل عامل
مراقبة جزء فقط من حالة بيئته. هذا يجعل مهمة التعليم لكل عضو في الفريق
صعبة للغاية، لكن إذا استخدم كل عامل خوارزمية **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement Learning
<span dir="rtl"></span>Algorithm<span dir="rtl">)
</span>**<span dir="rtl">القادرة على زيادة **إشارة المكافأة**</span>
**(Reward Signal)** <span dir="rtl">حتى في ظل هذه الظروف الصعبة، يمكن
لمجموعات من **عوامل التعليم المعزز**</span> **(Reinforcement Learning
Agents)** <span dir="rtl">أن تتعلم إنتاج **أفعال جماعية**</span>
**(Collective Actions)** <span dir="rtl">تتحسن بمرور الوقت كما يتم
تقييمها بواسطة **إشارة المكافأة المشتركة** </span>**(Common Reward
Signal)**<span dir="rtl">.</span>

<span dir="rtl">إذا كان أعضاء الفريق هم وحدات تشبه الخلايا
العصبية</span> **(Neuron-like Units)**<span dir="rtl">، فيجب على كل وحدة
أن يكون هدفها زيادة مقدار المكافأة</span> **(Reward)**
<span dir="rtl">التي تتلقاها بمرور الوقت، كما تفعل وحدة الممثل</span>
**(Actor Unit)** <span dir="rtl">التي وصفناها في القسم 15.8. يجب أن
تحتوي خوارزمية التعليم لكل وحدة على ميزتين أساسيتين. أولاً، يجب أن تستخدم
آثار الأهلية المشروطة **(**</span>**Contingent Eligibility
<span dir="rtl"></span>Traces<span dir="rtl">)</span>**.
<span dir="rtl">تذكر أن أثر الأهلية المشروط، من حيث المصطلحات
العصبية</span> **(Neural Terms)**<span dir="rtl">، يتم تفعيله (أو
زيادته) عند المشبك</span> **(Synapse)** <span dir="rtl">عندما يشارك
المدخل قبل المشبكي **(**</span>**Presynaptic
<span dir="rtl"></span>Input<span dir="rtl">)
</span>**<span dir="rtl">في جعل الخلية العصبية بعد المشبكية</span>
**(Postsynaptic Neuron)** <span dir="rtl">تطلق نبضة. على النقيض من ذلك،
يتم تفعيل أثر الأهلية غير المشروط</span> **(Non-Contingent Eligibility
Trace)** <span dir="rtl">أو زيادته بواسطة المدخل قبل المشبكي بشكل مستقل
عن نشاط الخلية العصبية بعد المشبكية</span>.

<span dir="rtl">كما تم شرحه في القسم 15.8، من خلال الاحتفاظ بمعلومات حول
الإجراءات</span> **(Actions)** <span dir="rtl">التي تم اتخاذها في حالات
معينة</span> **(States)**<span dir="rtl">، تتيح آثار الأهلية المشروطة
**(**</span>**Contingent Eligibility
<span dir="rtl"></span>Traces<span dir="rtl">)
</span>**<span dir="rtl">توزيع الفضل على المكافأة</span> **(Reward)**
<span dir="rtl">أو اللوم على العقاب</span> **(Punishment)**
<span dir="rtl">على **بارامترات**</span> **(Parameters)**
<span dir="rtl">سياسة الوكيل وفقًا للمساهمة التي قدمتها قيم هذه
**البارامترات** </span>**(Parameters)** <span dir="rtl">في تحديد إجراء
الوكيل. بناءً على نفس المنطق، يجب على عضو الفريق تذكر الإجراء
الأخير</span> **(Recent Action)** <span dir="rtl">حتى يتمكن من زيادة أو
تقليل احتمالية إنتاج ذلك الإجراء وفقًا لإشارة المكافأة</span> **(Reward
Signal)** <span dir="rtl">التي تم تلقيها لاحقًا. يقوم مكون الإجراء في أثر
الأهلية المشروطة بتنفيذ هذه الذاكرة الإجرائية</span> **(Action
Memory)**<span dir="rtl">.</span> <span dir="rtl">ومع ذلك، نظرًا لتعقيد
مهمة التعليم، فإن الأهلية المشروطة</span> **(Contingent Eligibility)**
<span dir="rtl">ليست سوى خطوة أولية في عملية توزيع الفضل
**(**</span>**Credit <span dir="rtl"></span>Assignment
Process<span dir="rtl">)</span>**: <span dir="rtl">العلاقة بين إجراء عضو
الفريق الفردي والتغييرات في إشارة المكافأة</span> **(Reward Signal)**
<span dir="rtl">للفريق هي ارتباط إحصائي يجب تقديره عبر العديد من
التجارب. تعتبر الأهلية المشروطة خطوة أساسية ولكنها أولية في هذه
العملية</span>.

<span dir="rtl">التعليم باستخدام آثار الأهلية غير المشروطة</span>
**(Non-Contingent Eligibility Traces)** <span dir="rtl">لا يعمل على
الإطلاق في إعداد الفريق لأنه لا يوفر طريقة لربط الإجراءات بالتغييرات
الناتجة في إشارة المكافأة</span> **(Reward
Signal)**<span dir="rtl">.</span> <span dir="rtl">آثار الأهلية غير
المشروطة كافية للتعليم من أجل التنبؤ</span>
**(Prediction)**<span dir="rtl">، كما يفعل مكون الناقد</span> **(Critic
Component)** <span dir="rtl">في خوارزمية الممثل-الناقد
**(**</span>**Actor-Critic
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**<span dir="rtl">،
لكنها لا تدعم التعليم للتحكم</span> **(Control)**<span dir="rtl">، كما
يجب أن يفعل مكون الممثل **(**</span>**Actor
<span dir="rtl"></span>Component<span dir="rtl">)</span>**.
<span dir="rtl">يمكن لأعضاء مجموعة من الوكلاء الشبيهين بالناقد</span>
**(Critic-like Agents)** <span dir="rtl">أن يتلقوا إشارة تعزيز
مشتركة</span> **(Common Reinforcement Signal)**<span dir="rtl">، لكنهم
جميعًا سيتعلمون التنبؤ بنفس الكمية</span> **(Expected Return)**
<span dir="rtl">(والتي في حالة طريقة الممثل-الناقد
**(**</span>**Actor-Critic
<span dir="rtl"></span>Method<span dir="rtl">)</span>**<span dir="rtl">،
ستكون العائد المتوقع للسياسة الحالية</span> **(Current
Policy)**<span dir="rtl">).</span> <span dir="rtl">يعتمد مدى نجاح كل عضو
في المجموعة في التعليم على التنبؤ بالعائد المتوقع على المعلومات</span>
**(Information)** <span dir="rtl">التي يتلقاها، والتي يمكن أن تكون
مختلفة تمامًا لأعضاء مختلفين في المجموعة. لن تكون هناك حاجة لإنتاج
المجموعة لأنماط نشاط متمايزة. هذا ليس مشكلة فريق</span> **(Team
Problem)** <span dir="rtl">كما تم تعريفها هنا</span>.

<span dir="rtl">المطلب الثاني للتعليم الجماعي في مشكلة الفريق</span>
**(Team Problem)** <span dir="rtl">هو أنه يجب أن يكون هناك تباين في
الإجراءات</span> **(Actions)** <span dir="rtl">التي يتخذها أعضاء الفريق
حتى يتمكن الفريق من استكشاف مساحة الأفعال الجماعية</span> **(Collective
Actions)**<span dir="rtl">.</span> <span dir="rtl">أبسط طريقة لفريق من
عوامل التعليم المعزز</span> **(Reinforcement Learning Agents)**
<span dir="rtl">لتحقيق ذلك هي أن يستكشف كل عضو مساحته الإجرائية
الخاصة</span> **(Action Space)** <span dir="rtl">من خلال تباين مستمر في
ناتجه</span> **(Output)**<span dir="rtl">.</span> <span dir="rtl">سيؤدي
ذلك إلى جعل الفريق ككل يغير أفعاله الجماعية</span> **(Collective
Actions)**<span dir="rtl">.</span> <span dir="rtl">على سبيل المثال،
يستكشف فريق من وحدات الممثل</span> **(Actor Units)**
<span dir="rtl">الموصوفة في القسم 15.8 مساحة الأفعال الجماعية</span>
**(Collective Actions)** <span dir="rtl">لأن ناتج كل وحدة</span>
**(Unit)**<span dir="rtl">، كونها وحدة</span>
**Bernoulli-Logistic**<span dir="rtl">، يعتمد بشكل احتمالي على مجموع
الأوزان</span> **(Weighted Sum)** <span dir="rtl">لمكونات متجه المدخلات
**(**</span>**Input
<span dir="rtl"></span>Vector<span dir="rtl">)</span>**.
<span dir="rtl">يقوم مجموع الأوزان بتحيز احتمال الإطلاق إما لأعلى أو
لأسفل، ولكن هناك دائمًا تباين</span> **(Variability)**<span dir="rtl">.
نظرًا لأن كل وحدة تستخدم خوارزمية تعزيز السياسة **(**</span>**REINFORCE
Policy <span dir="rtl"></span>Gradient Algorithm<span dir="rtl">)
</span>**<span dir="rtl">(الفصل 13)، فإن كل وحدة تقوم بتعديل
أوزانها</span> **(Weights)** <span dir="rtl">بهدف زيادة معدل المكافأة
المتوسط</span> **(Average Reward Rate)** <span dir="rtl">الذي تتلقاه
أثناء استكشاف مساحتها الإجرائية</span> **(Action Space)**
<span dir="rtl">بشكل عشوائي. يمكن إثبات، كما فعل</span> **Williams
(1992)**<span dir="rtl">، أن فريقًا من وحدات</span> **Bernoulli-Logistic
REINFORCE** <span dir="rtl">ينفذ خوارزمية تعزيز سياسة
**(**</span>**Policy Gradient
<span dir="rtl"></span>Algorithm<span dir="rtl">)
</span>**<span dir="rtl">ككل فيما يتعلق بمعدل متوسط إشارة المكافأة
المشتركة للفريق **(**</span>**Team's Common
<span dir="rtl"></span>Reward
Signal<span dir="rtl">)</span>**<span dir="rtl">، حيث تكون
الأفعال</span> **(Actions)** <span dir="rtl">هي الأفعال الجماعية
**(**</span>**Collective <span dir="rtl"></span>Actions<span dir="rtl">)
</span>**<span dir="rtl">للفريق</span>.

<span dir="rtl">علاوة على ذلك، أظهر **ويليامز**</span> **(Williams,
1992)** <span dir="rtl">أن فريقًا من الوحدات</span> **Bernoulli-logistic
Units** <span dir="rtl">التي تستخدم</span> **REINFORCE**
<span dir="rtl">يصعد في متوسط تدرج المكافأة **(**</span>**Average Reward
<span dir="rtl"></span>Gradient<span dir="rtl">)
</span>**<span dir="rtl">عندما تكون الوحدات في الفريق مترابطة لتشكيل
شبكة عصبية اصطناعية متعددة الطبقات</span> **(Multilayer
ANN)**<span dir="rtl">.</span> <span dir="rtl">في هذه الحالة، يتم بث
إشارة المكافأة</span> **(Reward Signal)** <span dir="rtl">إلى جميع
الوحدات في الشبكة، على الرغم من أن المكافأة قد تعتمد فقط على الأفعال
الجماعية</span> **(Collective Actions)** <span dir="rtl">للوحدات
الموجودة في مخرجات الشبكة</span> **(Output
Units)**<span dir="rtl">.</span> <span dir="rtl">هذا يعني أن فريقًا متعدد
الطبقات من وحدات</span> **Bernoulli-logistic REINFORCE**
<span dir="rtl">يتعلم مثل شبكة متعددة الطبقات يتم تدريبها بواسطة طريقة
الانتشار العكسي للخطأ</span> **(Error Backpropagation Method)**
<span dir="rtl">الشائعة الاستخدام، ولكن في هذه الحالة يتم استبدال عملية
الانتشار العكسي ببث إشارة المكافأة **(**</span>**Broadcasted Reward
<span dir="rtl"></span>Signal<span dir="rtl">)</span>**.
<span dir="rtl">في الممارسة العملية، تكون طريقة الانتشار العكسي للخطأ
أسرع بكثير، ولكن طريقة التعليم بواسطة فريق التعزيز</span>
**(Reinforcement Learning Team Method)** <span dir="rtl">هي أكثر احتمالًا
كآلية عصبية</span> **(Neural Mechanism)**<span dir="rtl">، خاصة في ضوء
ما يتم تعلمه عن</span> **STDP <span dir="rtl">المعدلة بالمكافأة</span>
(Reward-Modulated STDP)** <span dir="rtl">كما تم مناقشته في القسم
15.8</span>.

<span dir="rtl">الاستكشاف من خلال الاستكشاف المستقل</span>
**(Independent Exploration)** <span dir="rtl">من قبل أعضاء الفريق هو فقط
أبسط طريقة لاستكشاف الفريق؛ هناك طرق أكثر تطورًا ممكنة إذا نسق أعضاء
الفريق أفعالهم</span> **(Actions)** <span dir="rtl">للتركيز على أجزاء
معينة من مساحة الأفعال الجماعية **(**</span>**Collective Action
<span dir="rtl"></span>Space<span dir="rtl">)</span>**<span dir="rtl">،
إما عن طريق التواصل مع بعضهم البعض أو عن طريق الاستجابة لمدخلات
مشتركة</span> **(Common Inputs)**<span dir="rtl">.</span>
<span dir="rtl">هناك أيضًا آليات أكثر تطورًا من آثار الأهلية المشروطة
**(**</span>**Contingent <span dir="rtl"></span>Eligibility
Traces<span dir="rtl">) </span>**<span dir="rtl">لمعالجة توزيع الفضل
الهيكلي</span> **(Structural Credit Assignment)**<span dir="rtl">، وهو
أسهل في مشكلة الفريق</span> **(Team Problem)** <span dir="rtl">عندما
تكون مجموعة الأفعال الجماعية الممكنة</span> **(Set of Possible
Collective Actions)** <span dir="rtl">مقيدة بطريقة ما. إحدى الحالات
القصوى هي ترتيب الفائز يأخذ كل شيء</span> **(Winner-Take-All
Arrangement)** <span dir="rtl">(على سبيل المثال، نتيجة التثبيط
الجانبي</span> **(Lateral Inhibition)** <span dir="rtl">في الدماغ) الذي
يقيد الأفعال الجماعية</span> **(Collective Actions)**
<span dir="rtl">لتلك التي يساهم فيها عضو واحد فقط أو عدد قليل من أعضاء
الفريق. في هذه الحالة، يحصل الفائزون على الفضل أو اللوم على المكافأة أو
العقاب الناتج</span>.

<span dir="rtl">تفاصيل التعليم في الألعاب التعاونية</span>
**(Cooperative Games)** <span dir="rtl">(أو مشاكل الفريق</span> **Team
<span dir="rtl"></span>Problems**<span dir="rtl">) ومشاكل الألعاب غير
التعاونية</span> **(Non-Cooperative Game Problems)**
<span dir="rtl">تتجاوز نطاق هذا الكتاب. قسم الملاحظات الببليوغرافية
والتاريخية **(**</span>**Bibliographical and Historical
<span dir="rtl"></span>Remarks Section<span dir="rtl">)
</span>**<span dir="rtl">في نهاية هذا الفصل يستشهد بمجموعة من المنشورات
ذات الصلة، بما في ذلك مراجع واسعة للبحث حول تأثيرات التعليم المعزز
الجماعي **(**</span>**Collective Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">على علم الأعصاب</span>
**(Neuroscience)**<span dir="rtl">.</span>

**<u>15.11 <span dir="rtl">الطرق المعتمدة على النموذج في الدماغ</span>
(Model-based Methods in the Brain)</u>**

<span dir="rtl">يُعتبر التمييز بين الخوارزميات غير المعتمدة على
النموذج</span> (Model-free Algorithms) <span dir="rtl">والخوارزميات
المعتمدة على النموذج</span> (Model-based Algorithms) <span dir="rtl">في
التعليم المعزز</span> <span dir="rtl">(</span>Reinforcement
Learning<span dir="rtl">) مفيدًا في التفكير في عمليات التعليم واتخاذ
القرارات لدى الحيوانات. يتناول القسم 14.6 كيفية توافق هذا التمييز مع
التمييز بين السلوك الحيواني المعتاد</span> (Habitual Behavior)
<span dir="rtl">والسلوك الموجه نحو الهدف</span> (Goal-directed
Behavior)<span dir="rtl">.</span> <span dir="rtl">الفرضية التي نوقشت
أعلاه حول كيفية تنفيذ الدماغ لخوارزمية الممثل-الناقد</span>
(Actor–Critic Algorithm) <span dir="rtl">ترتبط فقط بوضع السلوك المعتاد
للحيوان، لأن خوارزمية الممثل-الناقد</span> (Actor–Critic Algorithm)
<span dir="rtl">الأساسية هي غير معتمدة على النموذج</span>
(Model-free)<span dir="rtl">.</span> <span dir="rtl">فما هي الآليات
العصبية المسؤولة عن إنتاج السلوك الموجه نحو الهدف</span> (Goal-directed
Behavior)<span dir="rtl">، وكيف تتفاعل مع تلك التي تكمن وراء السلوك
المعتاد</span> <span dir="rtl">(</span>Habitual
<span dir="rtl"></span>Behavior<span dir="rtl">)؟</span>

<span dir="rtl">إحدى الطرق للتحقيق في الأسئلة المتعلقة بالبنى
الدماغية</span> (Brain Structures) <span dir="rtl">المشاركة في هذه
الأنماط من السلوك هي تعطيل منطقة من دماغ الفأر ثم ملاحظة ما يفعله الفأر
في تجربة خفض قيمة النتائج</span> (Outcome-Devaluation Experiment)
<span dir="rtl">(القسم 14.6). تشير نتائج التجارب مثل هذه إلى أن فرضية
الممثل-الناقد</span> (Actor–Critic Hypothesis) <span dir="rtl">التي تم
وصفها أعلاه هي بسيطة للغاية عندما تضع الممثل</span> (Actor)
<span dir="rtl">في الجسم المخطط الظهري</span> (Dorsal
Striatum)<span dir="rtl">.</span> <span dir="rtl">تعطيل جزء من الجسم
المخطط الظهري</span> (Dorsal Striatum)<span dir="rtl">، وهو الجسم المخطط
الجانبي الظهري</span> <span dir="rtl">(</span>Dorsolateral
<span dir="rtl"></span>Striatum – DLS<span dir="rtl">)، يضعف تعلم
العادات</span> (Habit Learning)<span dir="rtl">، مما يجعل الحيوان يعتمد
أكثر على العمليات الموجهة نحو الهدف</span> (Goal-Directed
Processes)<span dir="rtl">.</span> <span dir="rtl">من ناحية أخرى، فإن
تعطيل الجسم المخطط الوسطي الظهري</span> (Dorsomedial Striatum - DMS)
<span dir="rtl">يضعف العمليات الموجهة نحو الهدف</span> (Goal-Directed
Processes)<span dir="rtl">، مما يجبر الحيوان على الاعتماد أكثر على تعلم
العادات</span> (Habit Learning)<span dir="rtl">.</span>
<span dir="rtl">تدعم نتائج كهذه الرؤية التي تفيد بأن الجسم المخطط
الجانبي الظهري</span> (DLS) <span dir="rtl">في القوارض</span> (Rodents)
<span dir="rtl">يكون أكثر تورطًا في العمليات غير المعتمدة على النموذج
(</span>Model-Free <span dir="rtl"></span>Processes<span dir="rtl">)،
بينما يكون الجسم المخطط الوسطي الظهري</span> (DMS) <span dir="rtl">أكثر
تورطًا في العمليات المعتمدة على النموذج</span> (Model-Based
Processes)<span dir="rtl">.</span> <span dir="rtl">تدعم نتائج الدراسات
التي أجريت على البشر باستخدام تصوير الأعصاب الوظيفي</span> (Functional
Neuroimaging)<span dir="rtl">، وكذلك على الرئيسيات غير البشرية</span>
(Non-Human Primates)<span dir="rtl">، الرؤية التي تشير إلى أن البنى
المماثلة في دماغ الرئيسيات</span> <span dir="rtl">(</span>Primate
<span dir="rtl"></span>Brain<span dir="rtl">)</span>
<span dir="rtl">تشارك بشكل تفاضلي في الأنماط السلوكية المعتادة</span>
(Habitual) <span dir="rtl">والموجهة نحو الهدف</span>
(Goal-Directed)<span dir="rtl">.</span>

<span dir="rtl">تشير دراسات أخرى إلى نشاط مرتبط بالعمليات المعتمدة على
النموذج</span> <span dir="rtl">(</span>Model-Based
<span dir="rtl"></span>Processes<span dir="rtl">) في القشرة الجبهية
الأمامية</span> (Prefrontal Cortex) <span dir="rtl">من دماغ الإنسان، وهي
الجزء الأمامي من القشرة الجبهية</span> (Frontal Cortex)
<span dir="rtl">الذي يرتبط بالوظيفة التنفيذية</span>
<span dir="rtl">(</span>Executive
<span dir="rtl"></span>Function<span dir="rtl">)، بما في ذلك التخطيط
واتخاذ القرار</span> (Decision-Making)<span dir="rtl">.</span>
<span dir="rtl">تشارك بشكل خاص القشرة الجبهية الحجاجية</span>
(Orbitofrontal Cortex - OFC)<span dir="rtl">، وهي جزء من القشرة الجبهية
الأمامية</span> (Prefrontal Cortex) <span dir="rtl">الموجودة فوق العينين
مباشرة. يكشف تصوير الأعصاب الوظيفي</span> (Functional Neuroimaging)
<span dir="rtl">لدى البشر، وأيضًا تسجيل نشاط الخلايا العصبية الفردية في
القرود، عن نشاط قوي في القشرة الجبهية الحجاجية</span> (OFC)
<span dir="rtl">يرتبط بالقيمة الذاتية للمكافأة</span>
<span dir="rtl">(</span>Subjective <span dir="rtl"></span>Reward
Value<span dir="rtl">) للمحفزات ذات الأهمية البيولوجية، وكذلك بالنشاط
المرتبط بالمكافأة المتوقعة كنتاج للأفعال. على الرغم من عدم خلو هذه
النتائج من الجدل، فإنها تشير إلى تورط كبير للقشرة الجبهية
الحجاجية</span> (OFC) <span dir="rtl">في الاختيار الموجه نحو
الهدف</span> (Goal-Directed Choice)<span dir="rtl">.</span>
<span dir="rtl">قد تكون حاسمة لجزء المكافأة من نموذج بيئة الحيوان</span>
(Environment Model)<span dir="rtl">.</span>

<span dir="rtl">هيكل آخر متورط في السلوك المعتمد على النموذج</span>
(Model-Based Behavior) <span dir="rtl">هو الحُصين</span>
(Hippocampus)<span dir="rtl">، وهو هيكل حاسم للذاكرة</span> (Memory)
<span dir="rtl">والملاحة المكانية</span> <span dir="rtl">(</span>Spatial
Navigation<span dir="rtl">)</span>. <span dir="rtl">يلعب الحُصين</span>
(Hippocampus) <span dir="rtl">لدى الفئران دورًا حاسمًا في قدرة الفئران على
التنقل في متاهة بطريقة موجهة نحو الهدف</span> (Goal-Directed Manner)
<span dir="rtl">التي قادت تولمان</span> (Tolman) <span dir="rtl">إلى
فكرة أن الحيوانات تستخدم نماذج، أو خرائط معرفية</span> (Cognitive
Maps)<span dir="rtl">، في اختيار الأفعال (القسم 14.5). قد يكون
الحُصين</span> (Hippocampus) <span dir="rtl">أيضًا مكونًا حاسمًا في قدرتنا
نحن البشر على تخيل تجارب جديدة</span> <span dir="rtl">(</span>Hassabis
and Maguire, 2007; ´Olafsd´ottir, Barry, Saleem, Hassabis, and
<span dir="rtl"></span>Spiers, 2015<span dir="rtl">)</span>.

<span dir="rtl">تشير النتائج التي تتعلق بشكل مباشر بارتباط الحُصين</span>
(Hippocampus) <span dir="rtl">بالتخطيط</span> (Planning) -
<span dir="rtl">العملية التي يحتاج فيها النموذج البيئي</span>
(Environment Model) <span dir="rtl">للتجنيد في اتخاذ القرارات - إلى أن
تمثيل النشاط العصبي</span> (Neuronal Activity) <span dir="rtl">في
الحُصين</span> (Hippocampus) <span dir="rtl">يتم تحليله لتحديد الجزء من
الفضاء الذي يمثل نشاط الحُصين</span> (Hippocampal Activity)
<span dir="rtl">في لحظة معينة. عندما يتوقف الفأر عند نقطة اختيار في
المتاهة، ينتقل تمثيل الفضاء في الحُصين</span> (Hippocampus)
<span dir="rtl">إلى الأمام (وليس إلى الخلف) على طول المسارات الممكنة
التي يمكن أن يسلكها الحيوان من تلك النقطة</span> (Johnson and Redish,
2007)<span dir="rtl">. علاوة على ذلك، تتوافق المسارات المكانية التي
يمثلها هذا الانتقال عن كثب مع سلوك الفأر التنقلي اللاحق</span> (Pfei↵er
and Foster, 2013)<span dir="rtl">.</span> <span dir="rtl">تشير هذه
النتائج إلى أن الحُصين</span> (Hippocampus) <span dir="rtl">ضروري لجزء
الانتقال بين الحالات في نموذج بيئة الحيوان</span> (State-Transition Part
of an Animal’s Environment Model)<span dir="rtl">، وأنه جزء من نظام
يستخدم النموذج لمحاكاة التسلسلات الزمنية المستقبلية الممكنة لتقييم عواقب
المسارات السلوكية المحتملة: وهو شكل من أشكال التخطيط</span>
(Planning)<span dir="rtl">.</span>

<span dir="rtl">النتائج المذكورة أعلاه تضاف إلى الأدبيات الغزيرة حول
الآليات العصبية التي تكمن وراء التعليم الموجه نحو الهدف</span>
(Goal-Directed Learning) <span dir="rtl">أو المعتمد على النموذج</span>
(Model-Based Learning) <span dir="rtl">واتخاذ القرارات</span>
(Decision-Making)<span dir="rtl">، لكن تبقى العديد من الأسئلة دون إجابة.
على سبيل المثال، كيف يمكن لمناطق متشابهة بنيويًا مثل الجسم المخطط الجانبي
الظهري</span> <span dir="rtl">(</span>Dorsolateral Striatum
<span dir="rtl"></span>– DLS<span dir="rtl">) والجسم المخطط الوسطي
الظهري</span> (Dorsomedial Striatum - DMS) <span dir="rtl">أن تكون
مكونات أساسية لأنماط من التعليم والسلوك تختلف بقدر كبير مثل الخوارزميات
غير المعتمدة على النموذج</span> (Model-Free Algorithms)
<span dir="rtl">والخوارزميات المعتمدة على النموذج</span>
<span dir="rtl">(</span>Model-Based
<span dir="rtl"></span>Algorithms<span dir="rtl">)؟ هل هناك هياكل منفصلة
مسؤولة عن مكونات الانتقال (</span>Transition Components<span dir="rtl">)
والمكافأة</span> (Reward Components) <span dir="rtl">في نموذج
البيئة</span> (Environment Model)<span dir="rtl">؟ هل يتم إجراء كل
التخطيط في وقت اتخاذ القرار عبر محاكاة المسارات المستقبلية المحتملة كما
يشير النشاط الانتقالي إلى الأمام في الحُصين</span>
(Hippocampus)<span dir="rtl">؟ بعبارة أخرى، هل كل التخطيط يشبه خوارزمية
التمرير</span> (Rollout Algorithm) <span dir="rtl">(الفصل 8.10)؟ أم يتم
أحيانًا استخدام النماذج في الخلفية لتحسين أو إعادة حساب معلومات القيمة
كما هو موضح في بنية</span> Dyna <span dir="rtl">(الفصل 8.2)؟ كيف يقوم
الدماغ بالتحكيم بين استخدام النظام المعتاد</span> (Habitual System)
<span dir="rtl">والنظام الموجه نحو الهدف</span>
<span dir="rtl">(</span>Goal-Directed
<span dir="rtl"></span>System<span dir="rtl">)؟ وهل هناك فصل واضح بين
الركائز العصبية</span> (Neural Substrates) <span dir="rtl">لهذه
الأنظمة؟</span>

<span dir="rtl">الأدلة لا تشير إلى إجابة إيجابية لهذا السؤال الأخير. وفي
تلخيص لهذا الوضع، كتب دول، سيمون، وداو</span> (Doll, Simon, and Daw,
2012) <span dir="rtl">أن "التأثيرات المعتمدة على النموذج
(</span>Model-Based <span dir="rtl"></span>Influences<span dir="rtl">)
تظهر بشكل واسع في كل مكان تقريبًا يعالج فيه الدماغ معلومات المكافأة"،
وهذا يشمل حتى المناطق التي يُعتقد أنها حاسمة للتعليم غير المعتمد على
النموذج</span> (Model-Free Learning)<span dir="rtl">. ويشمل ذلك إشارات
الدوبامين</span> (Dopamine Signals) <span dir="rtl">نفسها، والتي يمكن أن
تُظهر تأثير المعلومات المعتمدة على النموذج</span> (Model-Based
Information) <span dir="rtl">بالإضافة إلى أخطاء توقع المكافأة</span>
(Reward Prediction Errors) <span dir="rtl">التي يُعتقد أنها أساس العمليات
غير المعتمدة على النموذج</span> (Model-Free Processes)<span dir="rtl">.
إن استمرار الأبحاث في علم الأعصاب المستندة إلى التمييز بين التعليم غير
المعتمد على النموذج</span> (Model-Free) <span dir="rtl">والمعتمد على
النموذج</span> (Model-Based) <span dir="rtl">في التعليم المعزز</span>
(Reinforcement Learning) <span dir="rtl">لديه القدرة على تعزيز فهمنا
للعمليات المعتادة</span> (Habitual Processes) <span dir="rtl">والموجهة
نحو الهدف</span> (Goal-Directed Processes) <span dir="rtl">في الدماغ.
ويمكن أن يؤدي الفهم الأفضل لهذه الآليات العصبية إلى تطوير خوارزميات تجمع
بين الأساليب غير المعتمدة على النموذج</span> (Model-Free)
<span dir="rtl">والمعتمدة على النموذج</span> (Model-Based)
<span dir="rtl">بطرق لم يتم استكشافها بعد في التعليم المعزز
الحسابي</span> (Computational Reinforcement
Learning)<span dir="rtl">.</span>

**<u>15.12 <span dir="rtl">الإدمان</span> (Addiction)</u>**

<span dir="rtl">فهم الأساس العصبي لتعاطي المخدرات هو هدف ذو أولوية عالية
في علم الأعصاب مع إمكانية تطوير علاجات جديدة لهذه المشكلة الصحية العامة
الخطيرة. إحدى النظريات تشير إلى أن الرغبة الشديدة في المخدرات</span>
(Drug Craving) <span dir="rtl">هي نتيجة لنفس عمليات التحفيز والتعليم
التي تدفعنا للبحث عن تجارب مجزية طبيعية تلبي احتياجاتنا البيولوجية.
فالمواد المسببة للإدمان، من خلال كونها ذات تأثير تعزيز قوي، تستحوذ
بفعالية على آلياتنا الطبيعية للتعليم واتخاذ القرار. هذا محتمل نظرًا لأن
العديد من المخدرات - رغم أن ليس جميعها - تزيد من مستويات
الدوبامين</span> (Dopamine) <span dir="rtl">إما بشكل مباشر أو غير مباشر
في المناطق المحيطة بالنهايات العصبية لمحاور خلايا الدوبامين
العصبية</span> <span dir="rtl">(</span>Dopamine
<span dir="rtl"></span>Neuron Axons<span dir="rtl">)</span>
<span dir="rtl">في الجسم المخطط</span> (Striatum)<span dir="rtl">، وهو
جزء من الدماغ مرتبط بشكل كبير بالتعليم القائم على المكافأة</span>
(Reward-Based Learning)<span dir="rtl">.</span>

<span dir="rtl">لكن السلوك المدمر للذات المرتبط بالإدمان ليس سمة من سمات
التعليم الطبيعي. ما المختلف في التعليم المعتمد على الدوبامين عندما تكون
المكافأة نتيجة لمادة مسببة للإدمان؟ هل الإدمان هو نتيجة لتعلم طبيعي
استجابة لمواد لم تكن متاحة بشكل كبير طوال تاريخنا التطوري، بحيث لم يكن
بإمكان التطور اختيار خصائص ضد تأثيراتها الضارة؟ أم أن المواد المسببة
للإدمان تتداخل بطريقة ما مع التعليم الطبيعي المعتمد على
الدوبامين؟</span>

<span dir="rtl">فرضية خطأ التنبؤ بالمكافأة</span> (Reward Prediction
Error Hypothesis) <span dir="rtl">لنشاط خلايا الدوبامين العصبية</span>
(Dopamine Neuron Activity) <span dir="rtl">وارتباطها بتعلم</span> TD
<span dir="rtl">هي أساس نموذج اقترحه ريديش</span> (Redish)
<span dir="rtl">عام 2004 لبعض - لكن ليس كل - خصائص الإدمان. يعتمد
النموذج على ملاحظة أن تعاطي الكوكايين وبعض المخدرات الأخرى المسببة
للإدمان ينتج زيادة مؤقتة في الدوبامين. في النموذج، يُفترض أن هذه الزيادة
في الدوبامين تزيد من خطأ</span> TD<span dir="rtl">،</span>
(δ)<span dir="rtl">، بطريقة لا يمكن إلغاؤها بواسطة التغيرات في دالة
القيمة</span> (Value Function). <span dir="rtl">بمعنى آخر، في حين يتم
تقليل</span> (δ) <span dir="rtl">بالقدر الذي يتم فيه التنبؤ بمكافأة
طبيعية بواسطة الأحداث السابقة، فإن المساهمة في</span> (δ)
<span dir="rtl">بسبب منبه إدماني لا تقل مع ازدياد التنبؤ بالإشارة إلى
المكافأة: لا يمكن "إزالة التنبؤ" بمكافآت المخدرات. يقوم النموذج بهذا من
خلال منع</span> (δ) <span dir="rtl">من أن يصبح سلبياً عندما تكون إشارة
المكافأة نتيجة لمادة مسببة للإدمان، مما يلغي ميزة تصحيح الأخطاء في
تعلم</span> TD <span dir="rtl">للحالات المرتبطة بتعاطي المخدرات.
والنتيجة هي أن قيم هذه الحالات تزداد بلا حدود، مما يجعل الأفعال التي
تؤدي إلى هذه الحالات مفضلة فوق جميع الأفعال الأخرى</span>.

**<span dir="rtl">السلوك الإدماني</span> (Addictive Behavior)**
<span dir="rtl">أكثر تعقيدًا بكثير من هذه النتيجة المستخلصة من **نموذج
ريديشي** </span>**(Redish’s Model)**<span dir="rtl">، ولكن الفكرة
الرئيسية للنموذج قد تكون جزءًا من "الحل". أو قد يكون النموذج مضللًا. يبدو
أن **الدوبامين**</span> **(Dopamine)** <span dir="rtl">لا يلعب دورًا
حاسمًا في جميع أشكال **الإدمان** </span>**(Addiction)**<span dir="rtl">،
كما أن ليس الجميع عرضة بنفس القدر لتطوير **السلوك الإدماني**</span>
**<span dir="rtl">(</span>Addictive
<span dir="rtl"></span>Behavior<span dir="rtl">)</span>**.
<span dir="rtl">علاوة على ذلك، فإن النموذج لا يشمل التغيرات التي تحدث في
العديد من الدوائر والمناطق الدماغية التي ترافق تناول المخدرات بشكل مزمن،
على سبيل المثال، التغيرات التي تؤدي إلى تقليل تأثير المخدر مع الاستخدام
المتكرر. من المرجح أيضًا أن **الإدمان**</span> **(Addiction)**
<span dir="rtl">يتضمن عمليات تعتمد على النموذج. ومع ذلك، يوضح **نموذج
ريديشي**</span> **(Redish’s Model)** <span dir="rtl">كيف يمكن استخدام
نظرية **التعليم المعزز**</span> **(Reinforcement Learning Theory)**
<span dir="rtl">في الجهد لفهم مشكلة صحية كبيرة. وبطريقة مماثلة، كانت
**نظرية التعليم المعزز**</span> **<span dir="rtl">(</span>Reinforcement
Learning <span dir="rtl"></span>Theory<span dir="rtl">)</span>**
<span dir="rtl">مؤثرة في تطوير مجال **الطب النفسي الحسابي**
</span>**(Computational Psychiatry)** <span dir="rtl">الجديد، والذي يهدف
إلى تحسين فهم **الاضطرابات العقلية**</span> **(Mental Disorders)**
<span dir="rtl">من خلال الأساليب الرياضية والحسابية</span>.

**<u>15.13 <span dir="rtl">الخلاصة</span> (Summary)</u>**

**<span dir="rtl">قواعد التعليم</span> (Learning Rules)**
<span dir="rtl">لنظام الممثل في شبكة **الممثل-الناقد**</span>
**<span dir="rtl">(</span>Actor-Critic
<span dir="rtl"></span>System<span dir="rtl">)
</span>**<span dir="rtl">تتوافق بشكل وثيق مع **اللدونة المعتمدة على
توقيت النبضات والمكافأة**</span>
**<span dir="rtl">(</span>Reward-Modulated Spike-Timing-Dependent
Plasticity<span dir="rtl">)</span>**. <span dir="rtl">في **اللدونة
المعتمدة على توقيت النبضات** </span>**(Spike-Timing-Dependent
Plasticity)**<span dir="rtl">، يحدد التوقيت النسبي لنشاط **قبل
التشابك**</span> **(Presynaptic)** <span dir="rtl">و**بعد
التشابك**</span> **(Postsynaptic)** <span dir="rtl">اتجاه التغيير في
**المشابك العصبية** </span>**(Synapses)**<span dir="rtl">.</span>
<span dir="rtl">في **اللدونة المعتمدة على توقيت النبضات
والمكافأة**</span> **<span dir="rtl">(</span>Reward-Modulated
<span dir="rtl"></span>STDP<span dir="rtl">)</span>**<span dir="rtl">،
تعتمد التغييرات في **المشابك العصبية القشرية المخططية**</span>
**<span dir="rtl">(</span>Corticostriatal
<span dir="rtl"></span>Synapses<span dir="rtl">)
</span>**<span dir="rtl">أيضًا على **ناقل عصبي**</span>
**(Neuromodulator)** <span dir="rtl">مثل **الدوبامين**
</span>**(Dopamine)**<span dir="rtl">، الذي يصل ضمن نافذة زمنية يمكن أن
تستمر حتى 10 ثوانٍ بعد تحقق شروط **اللدونة المعتمدة على توقيت النبضات**
</span>**(STDP)**<span dir="rtl">.</span> <span dir="rtl">تتزايد الأدلة
على أن **اللدونة المعتمدة على توقيت النبضات والمكافأة**</span>
**<span dir="rtl">(</span>Reward-Modulated STDP<span dir="rtl">)
</span>**<span dir="rtl">تحدث في **المشابك العصبية القشرية
المخططية**</span> **<span dir="rtl">(</span>Corticostriatal
<span dir="rtl"></span>Synapses<span dir="rtl">)</span>**<span dir="rtl">،
حيث يحدث التعليم لنظام **الممثل**</span> **(Actor)** <span dir="rtl">في
التنفيذ الافتراضي لنظام **الممثل-الناقد في الدماغ**
</span>**(Actor-Critic System in the Brain)**<span dir="rtl">، مما يضيف
إلى احتمال أن شيئًا مشابهًا لنظام **الممثل-الناقد**</span> **(Actor-Critic
System)** <span dir="rtl">يوجد في أدمغة بعض الحيوانات</span>.

<span dir="rtl">فكرة **التأهيل المشبكي**</span> **(Synaptic
Eligibility)** <span dir="rtl">والميزات الأساسية لقاعدة التعليم لنظام
**الممثل** </span>**(Actor)** <span dir="rtl">تستمد من فرضية **الخلايا
العصبية الهادونية** </span>**(Hedonistic Neuron Hypothesis)**
<span dir="rtl">لكولف</span> (Klopf, 1972, 1981)<span dir="rtl">.</span>
<span dir="rtl">لقد افترض أن **الخلايا العصبية**</span> **(Neurons)**
<span dir="rtl">تسعى للحصول على المكافأة وتجنب العقاب من خلال تعديل
فعالية **المشابك العصبية**</span> **(Synapses)** <span dir="rtl">بناءً
على العواقب المكافئة أو العقابية لنشاطها الكهربائي. يمكن أن يؤثر نشاط
**الخلايا العصبية** </span>**(Neuron Activity)** <span dir="rtl">على
مدخلاتها اللاحقة لأنها مدمجة في العديد من حلقات التغذية الراجعة، بعضها
داخل **الجهاز العصبي والجسم**</span> **(Nervous System and Body)**
<span dir="rtl">للحيوان والبعض الآخر يمر عبر البيئة الخارجية للحيوان.
فكرة التأهيل لكولف هي أن **المشابك العصبية**</span> **(Synapses)**
<span dir="rtl">يتم وضع علامة عليها مؤقتًا على أنها مؤهلة للتعديل إذا
شاركت في إطلاق **الخلايا العصبية**</span> **(Neuron Firing)**
<span dir="rtl">(مما يجعل هذا الشكل من التأهيل مرتبطًا). يتم تعديل فعالية
**المشابك العصبية**</span> **(Synaptic Efficacy)** <span dir="rtl">إذا
وصلت إشارة تعزيز أثناء تأهيل المشبك. لقد أشرنا إلى السلوك الكيميائي
لبكتيريا كمثال على خلية واحدة توجه حركاتها لالتماس بعض الجزيئات وتجنب
أخرى</span>.

**<span dir="rtl">خاصية بارزة في نظام الدوبامين</span> (Dopamine
System)** <span dir="rtl">هي أن الألياف التي تطلق **الدوبامين**
</span>**(Dopamine)** <span dir="rtl">تتفرع على نطاق واسع إلى أجزاء
متعددة من الدماغ. على الرغم من أنه من المحتمل أن تكون بعض مجموعات
**الخلايا العصبية**</span> **(Dopamine Neurons)** <span dir="rtl">فقط هي
التي تبث نفس إشارة التعزيز، إلا أنه إذا وصلت هذه الإشارة إلى **المشابك
العصبية**</span> **(Synapses)** <span dir="rtl">للخلايا العصبية التي
تشارك في التعليم من نوع **الممثل** </span>**(Actor-Type
Learning)**<span dir="rtl">، فإن الوضع يمكن أن يُنمذج على أنه مشكلة فريق.
في هذا النوع من المشاكل، يتلقى كل **وكيل**</span> **(Agent)**
<span dir="rtl">في مجموعة من وكلاء **التعليم المعزز**</span>
**(Reinforcement Learning Agents)** <span dir="rtl">نفس إشارة التعزيز،
حيث تعتمد تلك الإشارة على أنشطة جميع أعضاء المجموعة أو الفريق. إذا
استخدم كل عضو في الفريق خوارزمية تعلم كافية القدرة، يمكن للفريق أن يتعلم
بشكل جماعي لتحسين أداء الفريق بأكمله كما يتم تقييمه بواسطة إشارة التعزيز
المذاعة على المستوى العالمي، حتى إذا لم يتواصل أعضاء الفريق بشكل مباشر
مع بعضهم البعض. هذا يتماشى مع التوزيع الواسع لإشارات الدوبامين في الدماغ
ويوفر بديلاً معقولاً على مستوى الأعصاب لطريقة **الانتشار العكسي
للخطأ**</span> **(Error-Backpropagation Method)**
<span dir="rtl">الشائعة لتدريب الشبكات متعددة الطبقات</span>.

<span dir="rtl">التمييز بين **التعليم المعزز الخالي من النموذج**
</span>**(Model-Free Reinforcement Learning)**
<span dir="rtl">و**التعليم المعزز المعتمد على النموذج**</span>
**(Model-Based Reinforcement Learning)** <span dir="rtl">يساعد علماء
الأعصاب في التحقيق في الأسس العصبية لكل من التعليم الاعتيادي</span>
(Habitual Learning) <span dir="rtl">والتعليم الموجه نحو الهدف</span>
(Goal-Directed Learning) <span dir="rtl">واتخاذ القرارات. تشير الأبحاث
حتى الآن إلى أن هناك بعض مناطق الدماغ التي تشارك بشكل أكبر في نوع واحد
من العمليات مقارنةً بالآخر، لكن الصورة لا تزال غير واضحة لأن العمليات
المعتمدة على النموذج والعمليات غير المعتمدة على النموذج لا تبدو مفصولة
بشكل دقيق في الدماغ. هناك العديد من الأسئلة التي لا تزال بدون إجابة.
ربما الأكثر إثارة للاهتمام هو الأدلة التي تشير إلى أن **الحُصين**
</span>**(Hippocampus)**<span dir="rtl">، وهو هيكل مرتبط تقليديًا بالتنقل
المكاني والذاكرة، يبدو أنه يشارك في محاكاة المسارات المحتملة للعمل كجزء
من عملية اتخاذ القرار لدى الحيوان. هذا يشير إلى أنه جزء من نظام يستخدم
نموذج البيئة لأغراض التخطيط</span>.

<span dir="rtl">تؤثر **نظرية التعليم المعزز**</span> **(Reinforcement
Learning Theory)** <span dir="rtl">أيضًا على التفكير حول العمليات العصبية
الكامنة وراء تعاطي المخدرات. نموذج لبعض خصائص إدمان المخدرات يعتمد على
**فرضية خطأ التنبؤ بالمكافأة** </span>**(Reward Prediction Error
Hypothesis)**<span dir="rtl">.</span> <span dir="rtl">يقترح أن المنبهات
المسببة للإدمان، مثل الكوكايين، تزعزع استقرار **تعلم التفاضل
الزمني**</span> **(TD Learning)** <span dir="rtl">لتنتج نموًا غير محدود
في قيم الأفعال المرتبطة بتناول المخدرات. هذا بعيد عن كونه نموذجًا كاملاً
للإدمان، ولكنه يوضح كيف يمكن لمنظور حسابي أن يقترح نظريات يمكن اختبارها
بمزيد من البحث. يركز المجال الجديد **الطب النفسي الحسابي**</span>
**(Computational Psychiatry)** <span dir="rtl">بالمثل على استخدام
النماذج الحسابية، بعضها مشتق من **التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">، لفهم أفضل
للاضطرابات العقلية</span>.

<span dir="rtl">لم يتناول هذا الفصل إلا سطح كيفية تأثير علم الأعصاب
المتعلق بـ **التعليم المعزز** </span>**(Reinforcement Learning)**
<span dir="rtl">وتطور **التعليم المعزز**</span> **(Reinforcement
Learning)** <span dir="rtl">في علوم الحاسوب والهندسة على بعضهما البعض.
معظم ميزات **خوارزميات التعليم المعزز** </span>**(Reinforcement Learning
Algorithms)** <span dir="rtl">تستمد تصميمها من اعتبارات حسابية بحتة، لكن
بعضها تأثر بفرضيات حول آليات التعليم العصبي. من اللافت للنظر أنه مع
تراكم البيانات التجريبية حول عمليات المكافأة في الدماغ، أصبحت العديد من
ميزات **خوارزميات التعليم المعزز** </span>**(Reinforcement Learning
Algorithms)** <span dir="rtl">التي تحفزها الحسابات البحتة متسقة مع
بيانات علم الأعصاب. ميزات أخرى للتعليم المعزز الحسابي، مثل **آثار
التأهيل**</span> **(Eligibility Traces)** <span dir="rtl">وقدرة فرق من
وكلاء **التعليم المعزز**</span> **(Reinforcement Learning Agents)**
<span dir="rtl">على التعليم للعمل بشكل جماعي تحت تأثير إشارة التعزيز
المذاعة على المستوى العالمي، قد تتوازى أيضًا مع البيانات التجريبية مع
استمرار علماء الأعصاب في كشف الأساس العصبي لتعلم وسلوك الحيوانات القائم
على المكافأة</span>.

<span dir="rtl">الفصل السادس عشر:</span>  
<span dir="rtl">التطبيقات ودراسات الحالة</span>
<span dir="rtl">(</span>Applications and Case
<span dir="rtl"></span>Studies<span dir="rtl">)</span>

<span dir="rtl">في هذا الفصل، نقدم بعض دراسات الحالة المتعلقة بالتعليم
المعزز</span> (Reinforcement Learning)<span dir="rtl">. تشمل هذه
الدراسات بعض التطبيقات المهمة التي قد تكون لها أهمية اقتصادية. واحدة من
هذه الدراسات هي برنامج سامويل للعب الداما</span> (Samuel’s Checkers
Player)<span dir="rtl">، الذي يعتبر ذا أهمية تاريخية بالدرجة الأولى.
تهدف عروضنا إلى توضيح بعض التنازلات والقضايا التي تنشأ في التطبيقات
الواقعية. على سبيل المثال، نركز على كيفية دمج المعرفة الخاصة
بالمجال</span> (Domain Knowledge) <span dir="rtl">في صياغة وحل المشكلة.
كما نسلط الضوء على قضايا التمثيل</span> (Representation Issues)
<span dir="rtl">التي غالبًا ما تكون حاسمة لنجاح التطبيقات. الخوارزميات
المستخدمة في بعض هذه الدراسات أكثر تعقيدًا بشكل كبير من تلك التي قدمناها
في بقية الكتاب. تطبيقات التعليم المعزز</span> (Reinforcement Learning)
<span dir="rtl">لا تزال بعيدة عن الروتينية وتتطلب في العادة الكثير من
الفن إلى جانب العلم. تسهيل التطبيقات وجعلها أكثر وضوحًا هو أحد أهداف
البحث الحالي في مجال التعليم المعزز</span> (Reinforcement
Learning)<span dir="rtl">.</span>

**<u>TD 16.1 <span dir="rtl">\_جامون</span> (TD-Gammon)</u>**

<span dir="rtl">إحدى أكثر تطبيقات **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">إثارة للإعجاب حتى الآن هو
ما قام به **جيرالد تسورو**</span> **(Gerald Tesauro)**
<span dir="rtl">في لعبة **الطاولة**</span> **(Backgammon)**
<span dir="rtl">(تسورو، 1992، 1994، 1995، 2002). برنامج تسورو،</span>
**TD <span dir="rtl">جامون</span> (TD-Gammon)**<span dir="rtl">، لم
يتطلب معرفة كبيرة بلعبة الطاولة، ولكنه تعلم اللعب بشكل ممتاز، وقريب من
مستوى أقوى أساتذة اللعبة في العالم</span>.
**<span dir="rtl">الخوارزمية</span> (Algorithm)**
<span dir="rtl">التعليمية في</span> **TD <span dir="rtl">جامون</span>
(TD-Gammon)** <span dir="rtl">كانت عبارة عن مزيج بسيط من
**خوارزمية**</span> **TD(λ)** <span dir="rtl">وتقريب الدوال غير الخطية
باستخدام **شبكة عصبية اصطناعية متعددة الطبقات** </span>**(Multilayer
Artificial Neural Network - ANN)** <span dir="rtl">تم تدريبها بواسطة
**الانتشار العكسي لأخطاء** </span>**TD
<span dir="rtl"></span>(Backpropagating TD
Errors)**<span dir="rtl">.</span>

<img src="./media/image183.png"
style="width:3.04306in;height:2.09306in" /><span dir="rtl">تعتبر لعبة
الطاولة لعبة مهمة نظرًا لأنها تُلعب في جميع أنحاء العالم، ولها العديد من
البطولات ومباريات **بطولة العالم** </span>**(World Championship)**
<span dir="rtl">المنتظمة. هي جزء من ألعاب الحظ، وتعتبر وسيلة شائعة
للمراهنة بمبالغ كبيرة من المال. ربما يكون هناك عدد من لاعبي الطاولة
المحترفين أكثر من لاعبي الشطرنج المحترفين. تُلعب اللعبة بـ 15 قطعة بيضاء
و15 قطعة سوداء على لوحة بها 24 موقعًا تُسمى **النقاط**
</span>**(Points)**<span dir="rtl">.</span> <span dir="rtl">يظهر على
اليمين في الصفحة التالية وضع نموذجي في بداية اللعبة، من منظور اللاعب
الأبيض. هنا، قام الأبيض للتو برمي النرد وحصل على 5 و2. هذا يعني أنه
يمكنه تحريك إحدى قطعه 5 خطوات وواحدة أخرى (قد تكون نفس القطعة) خطوتين.
على سبيل المثال، يمكنه تحريك قطعتين من النقطة 12، واحدة إلى النقطة 17
وواحدة إلى النقطة 14. هدف الأبيض هو التقدم بجميع قطعه إلى الربع الأخير
(النقاط 19-24) ثم إخراجها من اللوحة. اللاعب الأول الذي يزيل جميع قطعه
يفوز. إحدى التعقيدات هي أن القطع تتفاعل أثناء مرورها ببعضها البعض في
اتجاهات مختلفة. على سبيل المثال، إذا كانت الحركة للاعب الأسود، يمكنه
استخدام رمية النرد 2 لتحريك قطعة من النقطة 24 إلى النقطة 22، وضرب القطعة
البيضاء هناك. القطع التي تم ضربها تُوضع على</span>
**"<span dir="rtl">الشريط</span> (Bar)"** <span dir="rtl">في منتصف
اللوحة (حيث نرى بالفعل قطعة سوداء تم ضربها سابقًا)، ومن هناك تعود إلى
السباق من البداية. ومع ذلك، إذا كانت هناك قطعتان على نقطة معينة، فلا
يمكن للخصم التحرك إلى تلك النقطة؛ القطع تكون محمية من الضرب. وبالتالي،
لا يمكن للأبيض استخدام رمية النرد 5-2 لتحريك أي من قطعه على النقطة 1،
لأن النقاط المحتملة الناتجة عنها مشغولة بمجموعات من قطع الأسود. تكوين
كتل متجاورة من النقاط المحتلة لمنع الخصم هو إحدى الاستراتيجيات الأساسية
في اللعبة</span>.

<span dir="rtl">تتضمن لعبة **الطاولة**</span> **(Backgammon)**
<span dir="rtl">العديد من التعقيدات الإضافية، ولكن الوصف السابق يعطي
فكرة أساسية عن اللعبة. مع وجود 30 قطعة و24 موقعًا محتملاً (26 إذا أضفنا
**الشريط**</span> **(Bar)** <span dir="rtl">وخارج اللوحة)، يجب أن يكون
واضحًا أن عدد المواقف المحتملة في لعبة الطاولة هائل جدًا، أكبر بكثير من
عدد عناصر الذاكرة التي يمكن أن يحتويها أي حاسوب ممكن تحقيقه فعليًا. كما
أن عدد الحركات الممكنة من كل موقف كبير أيضًا. بالنسبة لرمي نرد تقليدي، قد
يكون هناك 20 طريقة مختلفة للعب. عند النظر في الحركات المستقبلية، مثل
استجابة الخصم، يجب أيضًا النظر في احتمالات رمي النرد. والنتيجة هي أن
**شجرة اللعبة**</span> **(Game Tree)** <span dir="rtl">تحتوي على عامل
تشعب فعال يقارب 400. هذا الرقم كبير جدًا بحيث لا يسمح بالاستخدام الفعال
لأساليب البحث **الاستكشافية**</span> **(Heuristic)**
<span dir="rtl">التقليدية التي أثبتت فعاليتها في ألعاب مثل الشطرنج
والداما</span>.

<img src="./media/image184.png"
style="width:2.61042in;height:2.09306in" /><span dir="rtl">من ناحية
أخرى، تعتبر اللعبة مناسبة تمامًا لإمكانيات طرق</span> **TD
<span dir="rtl">التعليم</span> (TD Learning)**<span dir="rtl">.</span>
<span dir="rtl">على الرغم من أن اللعبة عشوائية بشكل كبير، إلا أن وصفًا
كاملاً لحالة اللعبة يكون متاحًا في جميع الأوقات. تتطور اللعبة عبر سلسلة من
الحركات والمواقف حتى تنتهي أخيرًا بفوز أحد اللاعبين، مما ينهي اللعبة.
يمكن تفسير النتيجة على أنها مكافأة نهائية يجب التنبؤ بها. من ناحية أخرى،
لا يمكن تطبيق النتائج النظرية التي وصفناها حتى الآن بشكل مفيد على هذه
المهمة. عدد **الحالات** </span>**(States)** <span dir="rtl">كبير جدًا
لدرجة أنه لا يمكن استخدام **جدول بحث** </span>**(Lookup
Table)**<span dir="rtl">، ويعتبر الخصم مصدرًا لعدم اليقين وتغير
الزمن</span>.

<span dir="rtl">استخدم برنامج</span> **TD <span dir="rtl">جامون</span>
(TD-Gammon)** <span dir="rtl">شكلًا غير خطي من</span>
**TD(λ)**<span dir="rtl">.</span> <span dir="rtl">كان الهدف من **القيمة
المقدرة** </span>**(Estimated Value)**<span dir="rtl">،</span>
$`v\hat{}(s,w)`$<span dir="rtl">، لأي حالة (موضع على اللوحة)</span>
$`s`$ <span dir="rtl">هو تقدير احتمال الفوز بدءًا من الحالة</span>
$`s`$<span dir="rtl">.</span> <span dir="rtl">لتحقيق ذلك، تم تعريف
**المكافآت**</span> **(Rewards)** <span dir="rtl">على أنها صفر لجميع
الخطوات الزمنية باستثناء تلك التي يتم فيها الفوز باللعبة. لتنفيذ **دالة
القيمة** </span>**(Value Function)**<span dir="rtl">، استخدم</span> **TD
<span dir="rtl">جامون</span> (TD-Gammon)** **<span dir="rtl">شبكة عصبية
اصطناعية متعددة الطبقات</span> <span dir="rtl">(</span>Multilayer
Artificial <span dir="rtl"></span>Neural Network –
ANN<span dir="rtl">)</span>**<span dir="rtl">، مشابهة لتلك التي تظهر على
اليمين في الصفحة التالية. (الشبكة الحقيقية كان لديها وحدتان إضافيتان في
الطبقة النهائية لتقدير احتمال فوز كل لاعب بطريقة خاصة تسمى "جامون" أو
"باك جامون"). كانت الشبكة تتألف من طبقة **وحدات إدخال** </span>**(Input
Units)**<span dir="rtl">، طبقة **وحدات مخفية** </span>**(Hidden
Units)**<span dir="rtl">، ووحدة إخراج نهائية. كان الإدخال إلى الشبكة
تمثيلاً لموضع في لعبة الطاولة، وكان الإخراج تقديرًا لقيمة ذلك
الموضع</span>.

<span dir="rtl">في النسخة الأولى من</span> **TD
<span dir="rtl">جامون</span> (TD-Gammon)**<span dir="rtl">،</span>
**TD-Gammon 0.0**<span dir="rtl">، تم تمثيل مواضع الطاولة للشبكة بطريقة
مباشرة نسبيًا تضمنت معرفة قليلة بلعبة الطاولة. ومع ذلك، تضمنت معرفة كبيرة
بكيفية عمل الشبكات العصبية الاصطناعية وكيفية تقديم المعلومات لها بشكل
أمثل. من المفيد ملاحظة التمثيل الدقيق الذي اختاره تسورو. كان هناك ما
مجموعه 198 وحدة إدخال في الشبكة. بالنسبة لكل نقطة على لوحة الطاولة، كانت
أربع وحدات تشير إلى عدد القطع البيضاء على النقطة. إذا لم يكن هناك قطع
بيضاء، فإن جميع الوحدات الأربع تأخذ القيمة صفر. إذا كانت هناك قطعة
واحدة، فإن الوحدة الأولى تأخذ القيمة 1. هذا يمثل المفهوم الأساسي لـ
"بلاط"، أي قطعة يمكن للخصم ضربها. إذا كان هناك قطعتان أو أكثر، فإن
الوحدة الثانية يتم ضبطها على 1. هذا يمثل المفهوم الأساسي لـ "النقطة
المحققة" التي لا يمكن للخصم النزول عليها. إذا كانت هناك ثلاث قطع بالضبط
على النقطة، فإن الوحدة الثالثة يتم ضبطها على 1. هذا يمثل المفهوم الأساسي
لـ "الاحتياطي الفردي"، أي قطعة إضافية بالإضافة إلى القطعتين اللتين صنعتا
النقطة. أخيرًا، إذا كان هناك أكثر من ثلاث قطع، فإن الوحدة الرابعة تأخذ
قيمة تتناسب مع عدد القطع الإضافية بعد الثلاث. إذا دعونا</span> $`n`$
<span dir="rtl">تمثل العدد الإجمالي للقطع على النقطة، إذا كان</span>
$`n > 3`$<span dir="rtl">، فإن الوحدة الرابعة تأخذ القيمة</span>
$`(n - 3)\ /2`$<span dir="rtl">.</span> <span dir="rtl">هذا يمثل تمثيلًا
خطيًا لـ "الاحتياطيات المتعددة" في النقطة المحددة</span>.

<span dir="rtl">مع وجود أربع وحدات للقطع البيضاء وأربع للقطع السوداء في
كل من النقاط الـ 24، فإن ذلك يجعل المجموع 192 وحدة. تم ترميز وحدتين
إضافيتين لعدد القطع البيضاء والسوداء على **الشريط** </span>**(Bar)**
<span dir="rtl">(حيث تأخذ كل واحدة القيمة</span>
$`n/2`$<span dir="rtl">، حيث</span> $`n`$ <span dir="rtl">هو عدد القطع
على الشريط)، ووحدتين إضافيتين لعدد القطع السوداء والبيضاء التي تم
إزالتها بنجاح من اللوحة (حيث تأخذ كل واحدة القيمة</span>
$`n/15`$<span dir="rtl">، حيث</span> $`n`$ <span dir="rtl">هو عدد القطع
التي تم إزالتها بالفعل). وأخيرًا، تم استخدام وحدتين للإشارة بشكل ثنائي ما
إذا كانت الحركة للأبيض أو للأسود. يجب أن تكون المنطقية العامة وراء هذه
الاختيارات واضحة. حاول **تسورو** </span>**(Tesauro)**
<span dir="rtl">تمثيل **الموضع**</span> **(Position)**
<span dir="rtl">بطريقة مباشرة، مع الحفاظ على عدد الوحدات صغيرًا نسبيًا.
وقد وفر وحدة لكل احتمال متميز من الناحية المفاهيمية الذي بدا أنه من
المحتمل أن يكون ذا صلة، وقام بتوسيعها لتتناسب تقريبًا في نفس النطاق، في
هذه الحالة بين 0 و1</span>.

<span dir="rtl">بعد تمثيل **موضع**</span> **(Position)**
<span dir="rtl">في لعبة **الطاولة**
</span>**(Backgammon)**<span dir="rtl">، قامت الشبكة بحساب **القيمة
المقدرة**</span> **(Estimated Value)** <span dir="rtl">بالطريقة
القياسية. كان لكل اتصال من **وحدة إدخال** </span>**(Input Unit)**
<span dir="rtl">إلى **وحدة مخفية**</span> **(Hidden Unit)**
**<span dir="rtl">وزن ذو قيمة حقيقية</span> (Real-Valued
Weight)**<span dir="rtl">.</span> <span dir="rtl">تم ضرب
**الإشارات**</span> **(Signals)** <span dir="rtl">من كل **وحدة
إدخال**</span> **(Input Unit)** <span dir="rtl">في **أوزانها
المقابلة**</span> **<span dir="rtl">(</span>Corresponding
<span dir="rtl"></span>Weights<span dir="rtl">)
</span>**<span dir="rtl">وتم تجميعها في **الوحدة المخفية**
</span>**(Hidden Unit)**<span dir="rtl">.</span> <span dir="rtl">وكان
**الإخراج** </span>**(Output)**<span dir="rtl">،</span>
$`h(j)`$<span dir="rtl">، للوحدة المخفية</span> $`j`$ <span dir="rtl">هو
**دالة سيجمويد غير خطية**</span> **(Nonlinear Sigmoid Function)**
<span dir="rtl">للمجموع الموزون</span>:

``` math
h(j) = \left( \sum_{i}^{}{w_{ji}x_{i}} \right) = \frac{1}{1 + e^{- \left( \sum_{i}^{}{w_{ji}x_{i}} \right)}}
```

<span dir="rtl">حيث</span> $`xi`$ <span dir="rtl">هو قيمة وحدة
الإدخال</span> $`i`$ <span dir="rtl">و</span>$`wji`$ <span dir="rtl">هو
وزن الاتصال بوحدة المخفية</span> j <span dir="rtl">(جميع الأوزان في
الشبكة تشكل معًا **متجه المعاملات** </span>**(Parameter Vector)**
w<span dir="rtl">)</span>. <span dir="rtl">يكون **إخراج دالة
السيجمويد**</span> **<span dir="rtl">(</span>Sigmoid
<span dir="rtl"></span>Output<span dir="rtl">)</span>**
<span dir="rtl">دائمًا بين 0 و1، وله تفسير طبيعي كاحتمال بناءً على **تجميع
الأدلة**</span> **<span dir="rtl">(</span>Summation of
<span dir="rtl"></span>Evidence<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">كان الحساب من **الوحدات المخفية**</span> **(Hidden
Units)** <span dir="rtl">إلى **وحدة الإخراج**</span>
**<span dir="rtl">(</span>Output
<span dir="rtl"></span>Unit<span dir="rtl">)
</span>**<span dir="rtl">مشابهًا تمامًا. كان لكل اتصال من **وحدة
مخفية**</span> **(Hidden Unit)** <span dir="rtl">إلى **وحدة الإخراج**
</span>**(Output Unit)** <span dir="rtl">وزن منفصل. كانت **وحدة
الإخراج**</span> **(Output Unit)** <span dir="rtl">تشكل المجموع الموزون
ثم تمرره عبر نفس **اللامنطقيات السيجمويدية** </span>**(Sigmoid
Nonlinearity)**<span dir="rtl">.</span>

<span dir="rtl">استخدم</span> **TD <span dir="rtl">جامون</span>
(TD-Gammon)** <span dir="rtl">شكل **التدرج الجزئي**</span>
**(Semi-Gradient)** <span dir="rtl">من خوارزمية</span> **TD(λ)**
<span dir="rtl">الموضحة في القسم 12.2، مع حساب التدرجات بواسطة خوارزمية
**انتشار الأخطاء**</span> **<span dir="rtl">(</span>Error
<span dir="rtl"></span>Backpropagation
Algorithm<span dir="rtl">)</span>** <span dir="rtl"></span>(Rumelhart,
Hinton, and Williams, 1986)<span dir="rtl">.</span> <span dir="rtl">تذكر
أن قاعدة التحديث العامة في هذه الحالة هي</span>:

``` math
w_{t + 1} = w_{t} + \alpha\left\lbrack R_{t + 1} + \lambda\widehat{v}\left( S_{t + 1},w_{t} \right) - \widehat{v}\left( S_{t},w_{t} \right) \right\rbrack z_{t}
```

<span dir="rtl">حيث</span> $`wt`$ <span dir="rtl"></span>​
<span dir="rtl">هو **متجه جميع المعاملات القابلة للتعديل**</span>
**<span dir="rtl">(</span>Vector of All Modifiable
<span dir="rtl"></span>Parameters<span dir="rtl">)</span>**
<span dir="rtl">(في هذه الحالة، أوزان الشبكة) و</span>$`zt`$
<span dir="rtl">هو **متجه تتبع التأهيل**</span>
**<span dir="rtl">(</span>Eligibility
<span dir="rtl"></span>Traces<span dir="rtl">)</span>**<span dir="rtl">،
واحد لكل مكون من</span> $`wt`$​<span dir="rtl">، ويتم تحديثه
بواسطة</span>:

``` math
z_{t} = \gamma\lambda z_{t - 1} + \nabla\widehat{v}\left( S_{t},w_{t} \right)
```

<span dir="rtl">مع</span> $`z0 = 0`$<span dir="rtl">.</span>
<span dir="rtl">يمكن حساب التدرج في هذه المعادلة بكفاءة بواسطة **إجراء
الانتشار العكسي** </span>**(Backpropagation
Procedure)**<span dir="rtl">.</span> <span dir="rtl">بالنسبة لتطبيق
**الطاولة** </span>**(Backgammon)**<span dir="rtl">، حيث</span>
$`\lambda = 1`$ <span dir="rtl">والمكافأة دائمًا صفر باستثناء الفوز، فإن
جزء</span> **TD <span dir="rtl">خطأ</span> (TD Error)**
<span dir="rtl">من قاعدة التعليم هو عادةً</span>
$`v\hat{}(S_{t + 1},w) - v\hat{}(St,w)`$<span dir="rtl">، كما هو موضح في
الشكل 16.1</span>.

<span dir="rtl">لتطبيق قاعدة التعليم، نحتاج إلى مصدر لألعاب **الطاولة**
</span>**(Backgammon)**<span dir="rtl">.</span> <span dir="rtl">حصل
**تسورو** </span>**(Tesauro)** <span dir="rtl">على سلسلة لا تنتهي من
الألعاب عن طريق لعب برنامجه التعليمي للطاولة ضد نفسه. لاختيار حركاته،
اعتبر</span> **TD-Gammon** <span dir="rtl">كل واحدة من حوالي 20 طريقة
يمكنه اللعب بها بالاستناد إلى نتائج الرمية. كانت المواقع الناتجة هي
**حالات بعدية**</span> **(Afterstates)** <span dir="rtl">كما نوقش في
القسم 6.8. تم استشارة الشبكة لتقدير قيمتها. ثم تم اختيار الحركة التي
تؤدي إلى الموقع بأعلى قيمة مقدرة. بالاستمرار بهذه الطريقة، مع</span>
**TD-Gammon** <span dir="rtl">يقوم بالحركات لكلا الجانبين، كان من الممكن
بسهولة توليد أعداد كبيرة من ألعاب الطاولة. تم اعتبار كل لعبة كحلقة، مع
تسلسل المواقع كحالات،</span> $`S0,S1,S2,\ldots S\_ 0`$,
<span dir="rtl">طبق **تسورو**</span> **(Tesauro)**
<span dir="rtl">قاعدة</span> **TD <span dir="rtl">غير الخطية</span>
(Nonlinear TD Rule)** (16.1) <span dir="rtl">بشكل تزايدي بالكامل، أي بعد
كل حركة فردية</span>.

<span dir="rtl">تم تعيين أوزان الشبكة في البداية إلى قيم عشوائية صغيرة.
كانت التقييمات الأولية بالتالي عشوائية تمامًا. نظرًا لأن الحركات تم
اختيارها بناءً على هذه التقييمات، كانت الحركات الأولية غير جيدة بشكل
حتمي، وغالبًا ما استمرت الألعاب الأولية مئات أو آلاف الحركات قبل أن يفوز
أحد الجانبين تقريبًا عن طريق الصدفة. بعد بضع عشرات من الألعاب، تحسنت
الأداء بسرعة</span>.

<span dir="rtl">بعد لعب حوالي 300,000 لعبة ضد نفسه، تعلم</span>
**TD-Gammon 0.0** <span dir="rtl">كما هو موصوف أعلاه أن يلعب تقريبًا بنفس
مستوى أفضل برامج **الطاولة**</span> **(Backgammon)**
<span dir="rtl">السابقة. كان هذا نتيجة لافتة لأن جميع برامج الكمبيوتر
عالية الأداء السابقة استخدمت معرفة واسعة بـ **الطاولة**
</span>**(Backgammon)**<span dir="rtl">. على سبيل المثال، كان البرنامج
البطل في ذلك الوقت، والذي يمكن القول إنه</span>
**Neurogammon**<span dir="rtl">، برنامجًا آخر كتبه **تسورو**</span>
**(Tesauro)** <span dir="rtl">والذي استخدم **شبكة عصبية صناعية**</span>
**(ANN)** <span dir="rtl">ولكن دون</span> **TD**
<span dir="rtl">تعلم</span> **(TD Learning)**<span dir="rtl">.</span>
<span dir="rtl">تم تدريب شبكة</span> **Neurogammon** <span dir="rtl">على
مجموعة تدريبية كبيرة من الحركات النموذجية المقدمة من خبراء **الطاولة**
</span>**(Backgammon)**<span dir="rtl">، بالإضافة إلى أنها بدأت بمجموعة
من الميزات المصممة خصيصًا لـ **الطاولة**
</span>**(Backgammon)**<span dir="rtl">.</span>
<span dir="rtl">كان</span> **Neurogammon** <span dir="rtl">برنامجًا
مضبوطًا بشكل كبير وفعال للغاية في **الطاولة**</span> **(Backgammon)**
<span dir="rtl">وقد فاز بشكل حاسم في **أولمبياد الطاولة العالمي**</span>
**(World Backgammon Olympiad)** <span dir="rtl">في عام 1989. من ناحية
أخرى، تم بناء  
</span>**TD-Gammon 0.0** <span dir="rtl">بمستوى ضئيل من معرفة
**الطاولة** </span>**(Backgammon)**<span dir="rtl">.</span>
<span dir="rtl">أن يكون قادرًا على تحقيق نفس مستوى</span> **Neurogammon**
<span dir="rtl">وجميع الطرق الأخرى هو شهادة واضحة على إمكانات طرق
التعليم الذاتي</span>.

<span dir="rtl">نجاح</span> **TD-Gammon 0.0** <span dir="rtl">في البطولة
بدون معرفة خبراء **الطاولة**</span> **(Backgammon)**
<span dir="rtl">أشار إلى تعديل واضح: إضافة الميزات المتخصصة في
**الطاولة**</span> **(Backgammon)** <span dir="rtl">مع الحفاظ على طريقة
تعلم</span> **TD** <span dir="rtl">الذاتية. أدى ذلك إلى إنتاج</span>
**TD-Gammon 1.0**<span dir="rtl">.</span> <span dir="rtl">كان</span>
**TD-Gammon 1.0** <span dir="rtl">واضحًا أنه أفضل بكثير من جميع برامج
**الطاولة**</span> **(Backgammon)** <span dir="rtl">السابقة ووجد منافسة
جدية فقط بين الخبراء البشريين. الإصدارات اللاحقة من البرنامج،</span>
**TD-Gammon 2.0** <span dir="rtl">(40 وحدة خفية)  
و</span>**TD-Gammon 2.1** <span dir="rtl">(80 وحدة خفية)، تم تعزيزها
بإجراء بحث ثنائي الطبقة انتقائي. لاختيار الحركات، نظرت هذه البرامج ليس
فقط إلى المواقع التي ستنتج على الفور، ولكن أيضًا إلى رميات النرد الممكنة
وحركات الخصم. بافتراض أن الخصم دائمًا ما يتخذ الحركة التي تبدو الأفضل له
على الفور، تم حساب القيمة المتوقعة لكل حركة مرشحة وتم اختيار الأفضل.
لتوفير وقت الكمبيوتر، تم إجراء الطبقة الثانية من البحث فقط للحركات
المرشحة التي تم تصنيفها بشكل عالٍ بعد الطبقة الأولى، حوالي أربع أو خمس
حركات في المتوسط. أثر البحث الثنائي الطبقة فقط على الحركات المختارة؛
كانت عملية التعليم تسير تمامًا كما كانت من قبل. الإصدارات النهائية من
البرنامج،</span> **TD-Gammon 3.0** <span dir="rtl">  
و</span>**TD-Gammon 3.1**<span dir="rtl">، استخدمت 160 وحدة خفية وبحث
ثلاثي الطبقة انتقائي. يُظهر  
</span>**TD-Gammon** <span dir="rtl">دمج وظائف القيمة المتعلمة وبحث وقت
القرار كما في **البحث الاستدلالي** </span>**(Heuristic Search)**
<span dir="rtl">و**طرق مونت كارلو للشجرات**
</span>**(MCTS)**<span dir="rtl">.</span> <span dir="rtl">في العمل
اللاحق، استكشف **تسورو**</span> **(Tesauro)**
<span dir="rtl">و**جالبرين**</span> **(Galperin)** (1997)
<span dir="rtl">طرق **عينات المسار**</span>
**<span dir="rtl">(</span>Trajectory
<span dir="rtl"></span>Sampling<span dir="rtl">)
</span>**<span dir="rtl">كبديل للبحث الكامل العرض، والذي قلل من معدل
الخطأ في اللعب الحي بمقدار عوامل عددية كبيرة</span> (4x–6x)
<span dir="rtl">مع الحفاظ على وقت التفكير معقولًا عند حوالي 5–10 ثوانٍ لكل
حركة</span>.

<span dir="rtl">خلال التسعينات، تمكن **تسارو**</span> **(Tesauro)**
<span dir="rtl">من اختبار برامجه في عدد كبير من المباريات ضد لاعبين
بشريين من الطراز العالمي. ملخص النتائج موجود في الجدول 16.1</span>.

<img src="./media/image185.png"
style="width:6.26806in;height:1.89028in" />

<span dir="rtl">أثناء التسعينيات، تمكن **تسارو**</span> **(Tesauro)**
<span dir="rtl">من تشغيل برامجه في عدد كبير من الألعاب ضد لاعبين عالميين
بارزين. ملخص النتائج موجود في **الجدول 16.1**.</span>
<span dir="rtl">بناءً على هذه النتائج والتحليلات من قبل أساتذة
الباكمان</span> <span dir="rtl">(**روبرت**
</span>**(Robertie)**<span dir="rtl">، 1992؛ انظر **تسارو**
</span>**(Tesauro)**<span dir="rtl">، 1995)، بدا أن  
</span>**TD-Gammon 3.0** <span dir="rtl">يلعب تقريبًا بمستوى القوة ذاته
أو ربما أفضل من أفضل اللاعبين البشريين في العالم. ذكر **تسارو**</span>
**(Tesauro)** <span dir="rtl">في مقال لاحق (تسارو</span>
**(Tesauro)**<span dir="rtl">، 2002)</span> <span dir="rtl">نتائج تحليل
شامل لقرارات الحركة وقرارات التدوير في</span> **TD-Gammon**
<span dir="rtl">مقارنة بأفضل اللاعبين البشريين. الاستنتاج كان أن</span>
**TD-Gammon 3.1** <span dir="rtl">كان لديه "ميزة كبيرة" في قرارات حركة
القطع، و"ميزة طفيفة" في قرارات التدوير، مقارنة بأفضل البشر</span>.

<span dir="rtl">كان لـ</span> **TD-Gammon** <span dir="rtl">تأثير كبير
على الطريقة التي يلعب بها أفضل اللاعبين البشر. على سبيل المثال، تعلم أن
يلعب بعض الوضعيات الافتتاحية بشكل مختلف عن المعتاد بين أفضل اللاعبين
البشريين. بناءً على نجاح</span> **TD-Gammon** <span dir="rtl">وتحليل
إضافي، بدأ أفضل اللاعبين البشر الآن في لعب هذه الوضعيات كما يفعل</span>
**TD-Gammon** <span dir="rtl">(**تسارو**
</span>**(Tesauro)**<span dir="rtl">، 1995)</span>.
<span dir="rtl">تسارع تأثير  
</span>**TD-Gammon** <span dir="rtl">على اللعب البشري بشكل كبير عندما
أصبحت عدة برامج أخرى لتعليم الذات بواسطة الشبكات العصبية الاصطناعية
مستوحاة من</span> **TD-Gammon**<span dir="rtl">، مثل</span>
**Jellyfish** <span dir="rtl">و</span>**Snowie**
<span dir="rtl">و</span>**GNUBackgammon**<span dir="rtl">، متاحة على
نطاق واسع. هذه البرامج سهلت نشر المعرفة الجديدة التي تولدها الشبكات
العصبية، مما أدى إلى تحسين كبير في مستوى اللعب في البطولات
البشرية</span> <span dir="rtl">(**تسارو**
</span>**(Tesauro)**<span dir="rtl">، 2002)</span>.

**<u>16.2 <span dir="rtl">لاعب الداما لِـ سامويل</span> (Samuel’s
Checkers Player)</u>**

<span dir="rtl">مقدمة مهمة لبرنامج</span> TD-Gammon <span dir="rtl">الذي
ابتكره تيساورو كانت العمل الرائد لأرثر سامويل (1959، 1967) في بناء برامج
لتعلم لعب الداما. كان سامويل من أوائل الأشخاص الذين استخدموا بفعالية طرق
البحث الاستدلالي وما نطلق عليه الآن التعليم بالفرق الزمني. تعتبر برامج
الداما التي طورها سامويل دراسات حالة تعليمية بالإضافة إلى كونها ذات
أهمية تاريخية. نركز على العلاقة بين طرق سامويل وطرق التعليم المعزز
الحديثة ونحاول نقل بعض من دوافع سامويل لاستخدامها</span>.

<span dir="rtl">كتب سامويل أول برنامج للعب الداما على جهاز</span> IBM
701 <span dir="rtl">في عام 1952. تم الانتهاء من برنامجه الأول للتعليم في
عام 1955 وعرضه على التلفاز في عام 1956. حققت الإصدارات اللاحقة من
البرنامج مستوى جيدًا في اللعب، على الرغم من أنها لم تكن خبرة. جذب سامويل
إلى دراسة الألعاب كمنطقة لدراسة تعلم الآلة لأن الألعاب أقل تعقيدًا من
المشكلات "المأخوذة من الحياة" بينما لا تزال تسمح بدراسة مثمرة لكيفية
استخدام الإجراءات الاستدلالية والتعليم معًا. اختار دراسة الداما بدلاً من
الشطرنج لأن بساطته النسبية جعلت من الممكن التركيز بشكل أقوى على
التعليم</span>.

<span dir="rtl">لعبت برامج سامويل عن طريق إجراء بحث نظري</span>
(lookahead search) <span dir="rtl">من كل وضعية حالية. استخدمت طرق البحث
الهيوريستي</span> (heuristic search methods) <span dir="rtl">التي نطلق
عليها الآن لتحديد كيفية توسيع شجرة البحث ومتى يجب إيقاف البحث. تم تقييم
أو "تقييم" وضعيات اللوحة النهائية لكل بحث بواسطة دالة قيمة</span> (value
function) <span dir="rtl">أو "متعددات الحدود التقييمية</span>"
<span dir="rtl"></span>(scoring polynomial) <span dir="rtl">باستخدام
تقريب الدالة الخطية</span> (linear function
approximation)<span dir="rtl">.</span> <span dir="rtl">في هذا وفي جوانب
أخرى، يبدو أن عمل سامويل قد استوحى من اقتراحات شانن</span> (Shannon)
(1950)<span dir="rtl">.</span> <span dir="rtl">على وجه الخصوص، كان
برنامج سامويل يعتمد على إجراء مينيمكس</span> (minimax)
<span dir="rtl">لشانون لإيجاد أفضل حركة من الوضعية الحالية. من خلال
العمل بشكل عكسي عبر شجرة البحث من الوضعيات النهائية المقيّمة، تم إعطاء كل
وضعية تقييم الوضعية التي ستنتج من أفضل حركة، على افتراض أن الآلة ستحاول
دائمًا تعظيم التقييم، بينما سيحاول الخصم دائمًا تقليله. أطلق سامويل على
هذا اسم "التقييم المدعوم</span>" <span dir="rtl">(</span>backed-up
<span dir="rtl"></span>score<span dir="rtl">) للوضعية. عندما وصل إجراء
مينيمكس إلى جذر شجرة البحث - الوضعية الحالية – أعطى أفضل حركة بناءً على
افتراض أن الخصم سيستخدم نفس معيار التقييم، محولًا إلى وجهة نظره. استخدمت
بعض نسخ برامج سامويل طرق تحكم بحث متقدمة مماثلة لما يُعرف بقطع
ألفا-بيتا  
(</span>alpha-beta cuts<span dir="rtl">)</span>
<span dir="rtl">(انظر</span> Pearl<span dir="rtl">، 1984)</span>.

<span dir="rtl">استخدم سامويل طريقتين رئيسيتين للتعليم، أبسطهما أطلق
عليه اسم "التعليم التقليدي</span>" <span dir="rtl">(</span>rote
<span dir="rtl"></span>learning<span dir="rtl">)</span>.
<span dir="rtl">كان يتكون ببساطة من حفظ وصف لكل وضعية لوحية يتم مواجهتها
أثناء اللعب، إلى جانب قيمتها المدعومة التي تحددها عملية مينيمكس</span>
(minimax)<span dir="rtl">.</span> <span dir="rtl">والنتيجة هي أنه إذا
ظهرت وضعية سبق مواجهتها مرة أخرى كوضعية نهائية في شجرة البحث، فإن عمق
البحث يتضخم فعليًا لأن قيمة هذه الوضعية المخزنة قد خزنت نتائج بحث أو أكثر
أجريت في وقت سابق. كانت إحدى المشكلات الأولية هي أن البرنامج لم يكن
مشجعًا على التحرك على المسار الأكثر مباشرة نحو الفوز. أعطى سامويل
البرنامج "حسًا بالاتجاه" عن طريق تقليل قيمة الوضعية بمقدار صغير كلما تم
دعمها مستوى</span> <span dir="rtl">(يسمى</span> "ply"<span dir="rtl">)
خلال تحليل مينيمكس. "إذا كان البرنامج الآن أمام خيار بين وضعيات لوحية
تختلف نتائجها فقط بعدد</span> ply<span dir="rtl">، فسوف يقوم تلقائيًا
باختيار الأكثر ملاءمة، مختارًا خيارًا منخفض</span> ply <span dir="rtl">إذا
كان الفوز وبديلًا عالي</span> ply <span dir="rtl">إذا كان الخسارة"
(سامويل، 1959، ص. 80). وجد سامويل أن هذه التقنية الشبيهة بالخصم كانت
أساسية للتعليم الناجح. أنتج التعليم التقليدي تحسينًا بطيئًا ولكنه مستمر
كان أكثر فعالية في اللعب الافتتاحي ونهاية اللعبة. أصبح برنامجه "أفضل من
مبتدئ متوسط" بعد تعلمه من العديد من الألعاب ضد نفسه، ومن مجموعة متنوعة
من الخصوم البشريين، ومن ألعاب الكتب في وضع التعليم المراقب</span>.

<span dir="rtl">يُشير التعليم التقليدي وجوانب أخرى من عمل سامويل بقوة إلى
الفكرة الأساسية لتعلم الفروق الزمنية</span> (temporal-difference
learning) <span dir="rtl">أن قيمة حالة يجب أن تساوي قيمة الحالات التالية
المحتملة. اقترب سامويل من هذه الفكرة في طريقته الثانية للتعليم، وهي
"التعليم بالتعميم</span>" <span dir="rtl">(</span>learning by
<span dir="rtl"></span>generalization<span dir="rtl">) لتعديل معلمات
دالة القيمة. كانت طريقة سامويل مماثلة من حيث المفهوم لتلك المستخدمة
لاحقًا بكثير من قبل تسوراو في</span> TD-Gammon<span dir="rtl">.</span>
<span dir="rtl">لعب برنامجه العديد من الألعاب ضد نسخة أخرى من نفسه وأجرى
تحديثًا بعد كل حركة. الفكرة في تحديث سامويل مقترحة من خلال الرسم البياني
للتدعيم في الشكل 16.2. يمثل كل دائرة مفتوحة وضعية حيث يتحرك البرنامج
التالي، وهي وضعية حركة، ويمثل كل دائرة صلبة وضعية حيث يتحرك الخصم
التالي. تم إجراء تحديث لقيمة كل وضعية حركة بعد حركة من كل جانب، مما أدى
إلى وضعية حركة ثانية. كان التحديث نحو قيمة مينيمكس لبحث أطلق من الوضعية
الثانية. وبالتالي، كان التأثير العام هو التأثير المشابه للدعم عبر حركة
كاملة من الأحداث الحقيقية ثم البحث عبر الأحداث المحتملة، كما اقترح الشكل
16.2. كانت خوارزمية سامويل الفعلية أكثر تعقيدًا بشكل كبير من ذلك لأسباب
حسابية، ولكن هذه كانت الفكرة الأساسية</span>.

<span dir="rtl">لم يتضمن سامويل مكافآت صريحة. بدلاً من ذلك، قام بتثبيت
وزن الخاصية الأكثر أهمية، وهي خاصية ميزة القطع</span> (Piece Advantage
Feature)<span dir="rtl">، التي قيست بعدد القطع التي يمتلكها البرنامج
مقارنةً بعدد القطع التي يمتلكها خصمه، مع إعطاء وزن أكبر للملوك، وضم
تحسينات تجعل من الأفضل تبادل القطع عند الفوز مقارنةً عند الخسارة.
وبالتالي، كان هدف برنامج سامويل هو تحسين ميزة القطع، التي ترتبط ارتباطًا
وثيقًا بالفوز في لعبة الداما</span> (Checkers)<span dir="rtl">.</span>

<img src="./media/image186.png"
style="width:6.26806in;height:3.59306in" />

<span dir="rtl">الشكل 16.2: رسم تدعيم لاعب الداما</span> (Checkers)
<span dir="rtl">لسامويل</span>.

<span dir="rtl">ومع ذلك، قد يكون هناك جزء أساسي مفقود في طريقة التعليم
الخاصة بـ "صامويل</span> (Samuel)" <span dir="rtl">من أجل أن تكون
خوارزمية **التفاضل الزمني**</span> **(Temporal-Difference)**
<span dir="rtl">سليمة. يمكن اعتبار **التعليم التفاضلي الزمني**</span>
**(Temporal-Difference Learning)** <span dir="rtl">طريقة لجعل **دالة
القيمة**</span> **<span dir="rtl">(</span>Value
<span dir="rtl"></span>Function<span dir="rtl">)
</span>**<span dir="rtl">متوافقة مع نفسها، ويمكننا أن نرى هذا بوضوح في
طريقة "صامويل</span> (Samuel)"<span dir="rtl">.</span>
<span dir="rtl">لكن الأمر يتطلب أيضًا طريقة لربط **دالة القيمة**</span>
**(Value Function)** <span dir="rtl">بالقيمة الحقيقية **للحالات**
</span>**(States)**<span dir="rtl">. لقد قمنا بفرض هذا عبر
**المكافآت**</span> **(Rewards)** <span dir="rtl">وعن طريق خصم أو إعطاء
قيمة ثابتة **للحالة النهائية** </span>**(Terminal
State)**<span dir="rtl">.</span> <span dir="rtl">لكن طريقة
"صامويل</span> (Samuel)" <span dir="rtl">لم تتضمن أي **مكافآت**
</span>**(Rewards)** <span dir="rtl">ولا معاملة خاصة **للمواقع النهائية
للألعاب** </span>**(Terminal Positions of Games)**<span dir="rtl">. وكما
أشار "صامويل</span> (Samuel)" <span dir="rtl">بنفسه، كان من الممكن أن
تصبح **دالة القيمة (**</span>**Value
<span dir="rtl"></span>Function<span dir="rtl">)
</span>**<span dir="rtl">متسقة فقط من خلال إعطاء قيمة ثابتة لجميع
المواقع. كان يأمل في تثبيط مثل هذه الحلول من خلال إعطاء مصطلح **ميزة
القطعة**</span> **(Piece-Advantage)** <span dir="rtl">وزناً كبيرًا وغير
قابل للتعديل. ولكن على الرغم من أن هذا قد يقلل من احتمالية إيجاد **دوال
التقييم**</span> **(Evaluation Functions)** <span dir="rtl">غير المفيدة،
فإنه لا يمنعها تمامًا. على سبيل المثال، كان لا يزال من الممكن الوصول إلى
**دالة ثابتة**</span> **<span dir="rtl">(</span>Constant
<span dir="rtl"></span>Function<span dir="rtl">)
</span>**<span dir="rtl">من خلال ضبط الأوزان القابلة للتعديل بطريقة تلغي
تأثير الوزن غير القابل للتعديل</span>.

<span dir="rtl">لأن إجراء التعليم الخاص بـ "صامويل</span> (Samuel)"
<span dir="rtl">لم يكن مقيدًا بإيجاد **دوال التقييم**</span>
**<span dir="rtl">(</span>Evaluation
<span dir="rtl"></span>Functions<span dir="rtl">)</span>**
<span dir="rtl">المفيدة، فقد كان من الممكن أن تصبح **دالة
التقييم**</span> **(Evaluation Function)** <span dir="rtl">أسوأ مع
التجربة. في الواقع، أشار "صامويل</span> (Samuel)" <span dir="rtl">إلى
ملاحظة ذلك خلال جلسات تدريب مكثفة للعب الذاتي. لإعادة تحسين البرنامج،
اضطر "صامويل</span> (Samuel)" <span dir="rtl">إلى التدخل وضبط الوزن الذي
كان له أكبر قيمة مطلقة إلى الصفر. كان تفسيره أن هذا التدخل الجذري أخرج
البرنامج من **الأمثلة المحلية** </span>**(Local
Optima)**<span dir="rtl">، ولكن من المحتمل أيضًا أن يكون قد أخرج البرنامج
من **دوال التقييم** </span>**(Evaluation Functions)**
<span dir="rtl">التي كانت متسقة لكنها لم تكن مرتبطة بالفوز أو الخسارة في
اللعبة</span>.

<span dir="rtl">على الرغم من هذه المشاكل المحتملة، اقترب لاعب
"صامويل</span> (Samuel)" <span dir="rtl">للشطرنج باستخدام طريقة التعليم
بالتعميم من مستوى لعب "أفضل من المتوسط". وصفه منافسون هواة بارعون بأنه
"ماكر لكنه قابل للهزيمة" (صامويل، 1959). على عكس النسخة التي تعتمد على
التعليم التلقائي، كانت هذه النسخة قادرة على تطوير **اللعبة
الوسطى**</span> **(Middle Game)** <span dir="rtl">بشكل جيد لكنها بقيت
ضعيفة في **اللعب الافتتاحي**</span> **(Opening Play)**
<span dir="rtl">و**اللعب النهائي** </span>**(Endgame
Play)**<span dir="rtl">.</span> <span dir="rtl">تضمن هذا البرنامج أيضًا
القدرة على البحث من خلال مجموعات من **الميزات**</span> **(Features)**
<span dir="rtl">للعثور على تلك التي كانت الأكثر فائدة في تشكيل **دالة
القيمة** </span>**(Value Function)**<span dir="rtl">.</span>
<span dir="rtl">تضمن إصدار لاحق</span> (Samuel, 1967)
<span dir="rtl">تحسينات في إجراء البحث الخاص به، مثل **التقليم
ألفا-بيتا** </span>**(Alpha-Beta Pruning)**<span dir="rtl">، والاستخدام
المكثف لوضع **التعليم الخاضع للإشراف**</span> **(Supervised Learning)**
<span dir="rtl">الذي يسمى "التعليم من الكتاب</span> (Book
Learning)<span dir="rtl">، وجداول بحث هرمية تسمى **جداول التوقيع**
</span>**(Signature Tables)** <span dir="rtl">  
(</span>Griffith, 1966<span dir="rtl">) لتمثيل **دالة القيمة**</span>
**(Value Function)** <span dir="rtl">بدلاً من **تقريب الدوال الخطية
(**</span>**Linear <span dir="rtl"></span>Function
Approximation<span dir="rtl">)</span>**. <span dir="rtl">تعلم هذا
الإصدار اللعب بشكل أفضل بكثير من برنامج 1959، على الرغم من أنه لم يصل
إلى مستوى المعلم. تم الاعتراف على نطاق واسع ببرنامج "صامويل</span>
(Samuel) <span dir="rtl">للعب الشطرنج كإنجاز كبير في **الذكاء
الاصطناعي** </span>**(Artificial Intelligence)**
<span dir="rtl">و**التعليم الآلي** </span>**(Machine
Learning)**<span dir="rtl">.</span>

**<u>16.3 <span dir="rtl">المراهنة في جولة "الرهان المضاعف اليومي" لـ
"واتسون</span>" <span dir="rtl">(</span>Watson’s Daily-Double
<span dir="rtl"></span>Wagering<span dir="rtl">)</span></u>**

<span dir="rtl">نظام</span> **IBM Watson** <span dir="rtl">هو النظام
الذي تم تطويره من قبل فريق من الباحثين في</span> IBM
<span dir="rtl">للعب برنامج المسابقات التلفزيوني الشهير</span>
**"Jeopardy!"**<span dir="rtl">.</span> <span dir="rtl">اكتسب شهرة في
عام 2011 بفوزه بالجائزة الأولى في مباراة استعراضية ضد أبطال بشر. على
الرغم من أن الإنجاز التقني الرئيسي الذي أظهره **واتسون**
</span>**(Watson)** <span dir="rtl">كان قدرته على الإجابة بسرعة ودقة على
الأسئلة المطروحة باللغة الطبيعية عبر مجالات واسعة من المعرفة العامة، فإن
أداءه الفائز في</span> **Jeopardy!** <span dir="rtl">اعتمد أيضًا على
استراتيجيات اتخاذ القرار المعقدة لأجزاء حاسمة من اللعبة. قام</span>
**Tesauro, Gondek, Lechner, Fan, and Prager** (2012, 2013)
<span dir="rtl">بتكييف نظام</span> **Tesauro’s TD-Gammon**
<span dir="rtl">الموضح أعلاه لإنشاء الاستراتيجية التي استخدمها
**واتسون**</span> **(Watson)** <span dir="rtl">في المراهنة في جولة
"الرهان المضاعف اليومي  
(</span>**Daily-Double (DD)**<span dir="rtl">)</span> <span dir="rtl">في
أدائه الشهير ضد الأبطال البشريين. يشير هؤلاء المؤلفون إلى أن فعالية هذه
الاستراتيجية تجاوزت بكثير ما يمكن للاعبين البشر تحقيقه في اللعب المباشر،
وأنها، جنبًا إلى جنب مع استراتيجيات متقدمة أخرى، كانت مساهمًا مهمًا في أداء
**واتسون**</span> **(Watson)** <span dir="rtl">المذهل. هنا نركز فقط على
المراهنة في جولة</span> **DD** <span dir="rtl">لأنها المكون الذي يعتمد
فيه **واتسون**</span> **(Watson)** <span dir="rtl">بشكل أكبر على
**التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">.</span>

**Jeopardy!** <span dir="rtl">تُلعب بواسطة ثلاثة متسابقين يواجهون لوحة
تعرض 30 مربعًا، يخفي كل منها لغزًا وله قيمة بالدولار. تُرتب المربعات في ستة
أعمدة، كل منها يوافق فئة مختلفة. يختار المتسابق مربعًا، ويقرأ المضيف لغز
المربع، ويمكن لكل متسابق اختيار الرد على اللغز عن طريق الضغط على الزر
("الضغط للدخول"). يحصل أول متسابق يضغط على الزر على فرصة محاولة الرد على
اللغز. إذا كانت إجابة هذا المتسابق صحيحة، تزيد درجاته بمقدار قيمة
المربع؛ وإذا لم تكن إجابته صحيحة، أو إذا لم يرد خلال خمس ثوانٍ، تنقص
درجاته بنفس المقدار، ويحصل المتسابقون الآخرون على فرصة للضغط والرد على
نفس اللغز. مربع أو مربعان (حسب الجولة الحالية من اللعبة) هما
مربعات</span> **DD** <span dir="rtl">خاصة. يحصل المتسابق الذي يختار أحد
هذه المربعات على فرصة حصرية للرد على لغز المربع ويجب عليه أن يقرر—قبل
الكشف عن اللغز—مقدار المراهنة أو الرهان. يجب أن يكون الرهان أكثر من خمسة
دولارات ولكن ليس أكبر من درجة المتسابق الحالية. إذا أجاب المتسابق بشكل
صحيح على لغز</span> **DD**<span dir="rtl">، تزيد درجاته بمقدار الرهان؛
وإلا تنقص درجاته بنفس المقدار. في نهاية كل لعبة، توجد جولة
"نهائية</span> **Jeopardy!** (**Final Jeopardy!**) <span dir="rtl">يكتب
فيها كل متسابق رهانًا مختومًا ثم يكتب إجابة بعد قراءة اللغز. المتسابق الذي
يحصل على أعلى درجة بعد ثلاث جولات من اللعب (حيث تتكون الجولة من كشف جميع
الألغاز الثلاثين) هو الفائز. تحتوي اللعبة على العديد من التفاصيل الأخرى،
ولكن هذه كافية لتقدير أهمية المراهنة في جولة</span>
**DD**<span dir="rtl">.</span> <span dir="rtl">يعتمد الفوز أو الخسارة
غالبًا على استراتيجية مراهنة المتسابق في جولة</span>
**DD**<span dir="rtl">.</span>

<span dir="rtl">كلما اختار **واتسون**</span> **(Watson)**
<span dir="rtl">مربع</span> **DD**<span dir="rtl">، كان يختار رهانه
بمقارنة **قيم الإجراءات**</span> **<span dir="rtl">(</span>Action
<span dir="rtl"></span>Values<span dir="rtl">)</span>**<span dir="rtl">،</span>
q^(s,bet)<span dir="rtl">، التي قدرت احتمال الفوز من حالة اللعبة
الحالية،</span> s<span dir="rtl">، لكل رهان قانوني بالدولار في الجولة.
باستثناء بعض التدابير لتقليل المخاطر الموصوفة أدناه، اختار
**واتسون**</span> **(Watson)** <span dir="rtl">الرهان مع أقصى **قيمة
للإجراء** </span>**(Action Value)**<span dir="rtl">.</span>
<span dir="rtl">تم حساب **قيم الإجراءات**</span> **(Action Values)**
<span dir="rtl">كلما كانت هناك حاجة لقرار مراهنة باستخدام نوعين من
التقديرات التي تم تعلمها قبل بدء أي لعبة مباشرة. كانت التقديرات الأولى
هي تقديرات **قيم الحالات**</span> **(Afterstate Values)**
<span dir="rtl">الناتجة عن اختيار كل رهان قانوني. تم الحصول على هذه
التقديرات من **دالة قيمة الحالة** </span>**(State-Value
Function)**<span dir="rtl">،</span> v^(⋅,w)<span dir="rtl">، المعرف
بواسطة **البارامترات** </span>**(Parameters)** w<span dir="rtl">، التي
أعطت تقديرات لاحتمال فوز **واتسون** </span>**(Watson)**
<span dir="rtl">من أي حالة لعبة. التقديرات الثانية المستخدمة لحساب **قيم
الإجراءات (**</span>**Action
<span dir="rtl"></span>Values<span dir="rtl">)
</span>**<span dir="rtl">قدمت "ثقة</span> **DD** <span dir="rtl">داخل
الفئة</span>"<span dir="rtl">،</span> pD​<span dir="rtl">، التي قدرت
احتمال أن يستجيب **واتسون** </span>**(Watson)** <span dir="rtl">بشكل
صحيح على لغز</span> **DD** <span dir="rtl">الذي لم يُكشف بعد</span>.

<span dir="rtl">استخدم</span> **Tesauro** <span dir="rtl">وزملاؤه نهج
**التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">الخاص بـ  
</span>**TD-Gammon** <span dir="rtl">الموصوف أعلاه لتعلم</span>
v^(⋅,w)<span dir="rtl">:</span> <span dir="rtl">وهو **دمج مباشر لـ**
</span>**TD(λ)** <span dir="rtl">غير الخطي باستخدام **شبكة عصبية
اصطناعية متعددة الطبقات**</span> **(Multilayer ANN)** <span dir="rtl">مع
أوزان</span> w <span dir="rtl">تم تدريبها بواسطة **الانتشار العكسي
لأخطاء**</span> **TD (Backpropagating TD Errors)** <span dir="rtl">خلال
العديد من الألعاب المحاكاة. تم تمثيل **الحالات**</span> **(States)**
<span dir="rtl">للشبكة بواسطة **متجهات ميزات**</span> **(Feature
Vectors)** <span dir="rtl">تم تصميمها خصيصًا للعبة</span>
**Jeopardy<span dir="rtl">.</span>** <span dir="rtl">تضمنت
**الميزات**</span> **(Features)** <span dir="rtl">الدرجات الحالية
للثلاثة لاعبين، وعدد **مربعات**</span> **DD** <span dir="rtl">المتبقية،
وإجمالي قيمة الدولار للألغاز المتبقية، ومعلومات أخرى تتعلق بكمية اللعب
المتبقية في اللعبة. على عكس</span> **TD-Gammon** <span dir="rtl">الذي
تعلم من اللعب الذاتي، تم تعلم</span> v <span dir="rtl">الخاص  
بـ **واتسون**</span> **(Watson)** <span dir="rtl">عبر ملايين الألعاب
المحاكاة ضد نماذج مدروسة بعناية للاعبين البشر. تم تكييف تقديرات الثقة
داخل الفئة بناءً على عدد **الإجابات الصحيحة** </span>**r
<span dir="rtl">والإجابات الخاطئة</span> w
<span dir="rtl"></span>**<span dir="rtl">التي قدمها **واتسون**</span>
**(Watson)** <span dir="rtl">في الألغاز السابقة التي لعبت في الفئة
الحالية. تم تقدير التبعية على</span> r <span dir="rtl">و</span>w
<span dir="rtl">من دقة **واتسون**</span> **(Watson)**
<span dir="rtl">الفعلية عبر آلاف الفئات التاريخية</span>.

<span dir="rtl">مع **دالة القيمة**</span> **(Value Function)**
$`\widehat{v}`$ <span dir="rtl">التي تم تعلمها سابقًا و**ثقة**</span>
**DD <span dir="rtl">داخل الفئة  
(</span>In-Category DD Confidence<span dir="rtl">)
</span>**$`p_{DD}`$**​**<span dir="rtl">، قام **واتسون**</span>
**(Watson)** <span dir="rtl">بحساب</span> $`\widehat{q}(s,bet)`$
<span dir="rtl">لكل رهان قانوني بالدولار في الجولة كما يلي</span>:

``` math
\widehat{q}(s,bet) = p_{DD} \times \widehat{v}(SW + bet,\ldots) + \left( 1 - p_{DD} \right) \times \widehat{v}(SW - bet,\ldots)
```

<span dir="rtl">حيث أن</span> $`SW`$ <span dir="rtl">هو **النتيجة
الحالية لـ واتسون** </span>**(Watson's Current Score)**<span dir="rtl">،
و</span>$`\widehat{v}`$ <span dir="rtl">يعطي **القيمة المقدرة**</span>
**(Estimated Value)** **<span dir="rtl">لحالة اللعبة</span> (Game
State)** <span dir="rtl">بعد استجابة **واتسون** </span>**(Watson)**
<span dir="rtl">للغز</span> **DD**<span dir="rtl">، والذي قد يكون صحيحًا
أو غير صحيح. حساب قيمة الإجراء بهذه الطريقة يتوافق مع الفكرة المذكورة في
**التمرين 3.19** بأن **قيمة الإجراء**</span> **(Action Value)**
<span dir="rtl">هي القيمة المتوقعة **للحالة التالية** </span>**(Next
State Value)** <span dir="rtl">عند إعطاء الإجراء (باستثناء أن هنا القيمة
المتوقعة هي **للحالة التالية** </span>**(Next Afterstate Value)**
<span dir="rtl">لأن الحالة الكاملة التالية للعبة تعتمد على اختيار المربع
التالي)</span>.

<span dir="rtl">وجد</span> **Tesauro** <span dir="rtl">وزملاؤه أن اختيار
الرهانات بزيادة **قيم الإجراءات**</span> **(Action Values)**
<span dir="rtl">كان يحمل "كمية مخيفة من المخاطر"، مما يعني أنه إذا كانت
استجابة **واتسون**</span> **(Watson)** <span dir="rtl">للغز خاطئة، فإن
الخسارة قد تكون كارثية على فرصه في الفوز. لتقليل **مخاطر الهبوط**</span>
**(Downside Risk)** <span dir="rtl">في حالة الإجابة الخاطئة، قام</span>
**Tesauro** <span dir="rtl">وزملاؤه بتعديل المعادلة (16.2) بطرح جزء صغير
من **الانحراف المعياري**</span> **(Standard Deviation)**
<span dir="rtl">على تقييمات **الحالة بعد الإجابة**</span>
**<span dir="rtl">(</span>Afterstate
<span dir="rtl"></span>Evaluations<span dir="rtl">)
</span>**<span dir="rtl">الصحيحة/الخاطئة لـ **واتسون**
</span>**(Watson)**<span dir="rtl">.</span> <span dir="rtl">كما قاموا
بتقليل المخاطر بشكل أكبر عن طريق حظر الرهانات التي قد تؤدي إلى تقليل
قيمة الحالة بعد الإجابة الخاطئة إلى ما دون حد معين. على الرغم من أن هذه
التدابير قللت قليلاً من توقع **واتسون**</span> **(Watson)**
<span dir="rtl">للفوز، إلا أنها قللت بشكل كبير من **مخاطر الهبوط**
</span>**(Downside Risk)**<span dir="rtl">، ليس فقط من حيث **المخاطر
المتوسطة لكل رهان** </span>**DD**<span dir="rtl">، ولكن بشكل أكبر في
سيناريوهات المخاطر القصوى حيث يمكن لـ **واتسون**</span> **(Watson)**
<span dir="rtl">المحايد تجاه المخاطر أن يراهن بمعظم أو كل رصيده</span>.

<span dir="rtl">لماذا لم يتم استخدام طريقة</span> **TD-Gammon**
<span dir="rtl">للتعليم من اللعب الذاتي لتعلم **دالة القيمة  
(**</span>**Value
<span dir="rtl"></span>Function<span dir="rtl">)</span>**
$`\widehat{v\ }`$<span dir="rtl">الحاسمة؟ التعليم من اللعب الذاتي
في</span> **Jeopardy!** <span dir="rtl">لم يكن لينجح بشكل جيد لأن
**واتسون**</span> **(Watson)** <span dir="rtl">كان مختلفًا جدًا عن أي
متسابق بشري. كان اللعب الذاتي سيؤدي إلى استكشاف مناطق في **فضاء
الحالة**</span> **(State Space)** <span dir="rtl">غير نمطية للعب ضد
الخصوم البشريين، خاصةً الأبطال البشريين. بالإضافة إلى ذلك، على عكس
**الباكامون** </span>**(Backgammon)**<span dir="rtl">، فإن</span>
**Jeopardy!** <span dir="rtl">هي لعبة **معلومات غير كاملة**</span>
**(Imperfect Information)** <span dir="rtl">لأن المتسابقين لا يمتلكون
الوصول إلى جميع المعلومات التي تؤثر على لعب خصومهم. على وجه الخصوص، لا
يعرف متسابقو</span> **Jeopardy!** <span dir="rtl">مقدار الثقة التي
يمتلكها خصومهم للرد على الألغاز في الفئات المختلفة. كان اللعب الذاتي
سيكون شيئًا مشابهًا للعب **البوكر**</span> **(Poker)** <span dir="rtl">مع
شخص يحمل نفس الأوراق التي تحملها</span>.

<span dir="rtl">نتيجة لهذه التعقيدات، تم تكريس جزء كبير من الجهد في
تطوير **استراتيجية المراهنة في**</span> **DD <span dir="rtl">الخاصة بـ
واتسون</span> (Watson’s DD-Wagering Strategy)** <span dir="rtl">لإنشاء
نماذج جيدة للخصوم البشريين. لم تتناول النماذج جانب **اللغة
الطبيعية**</span> **(Natural Language)** <span dir="rtl">من اللعبة، بل
كانت بدلاً من ذلك نماذج عمليات **عشوائية**</span> **(Stochastic Process
Models)** <span dir="rtl">للأحداث التي يمكن أن تحدث أثناء اللعب. تم
استخراج الإحصائيات من أرشيف واسع تم إنشاؤه من قبل المعجبين يحتوي على
معلومات اللعبة منذ بداية العرض حتى يومنا هذا. يتضمن الأرشيف معلومات مثل
ترتيب الألغاز، إجابات المتسابقين الصحيحة والخاطئة، مواقع</span>
**DD**<span dir="rtl">، ورهانات</span> **DD <span dir="rtl">و</span>FJ**
<span dir="rtl">لأكثر من 300,000 لغز</span>.

<span dir="rtl">تم بناء ثلاثة نماذج:</span> **<span dir="rtl">نموذج
المتسابق المتوسط</span> (Average Contestant Model)**
<span dir="rtl">(بناءً على جميع البيانات)، **نموذج البطل**</span>
**(Champion Model)** <span dir="rtl">(بناءً على إحصائيات من الألعاب مع
أفضل 100 لاعب)، و**نموذج البطل الكبير**</span> **(Grand Champion
Model)** <span dir="rtl">(بناءً على إحصائيات من الألعاب مع أفضل 10
لاعبين). بالإضافة إلى استخدامها كخصوم أثناء التعليم، تم استخدام النماذج
لتقييم الفوائد التي أنتجتها **استراتيجية المراهنة في**</span> **DD
<span dir="rtl">المتعلمة</span> <span dir="rtl">(</span>Learned
DD-Wagering <span dir="rtl"></span>Strategy<span dir="rtl">)</span>**.
<span dir="rtl">كانت نسبة فوز **واتسون**</span> **(Watson)**
<span dir="rtl">في المحاكاة عندما استخدم استراتيجية **المراهنة
في**</span> **DD** <span dir="rtl">التي تعتمد على</span> **Heuristic**
<span dir="rtl">الأساسي 61%؛ وعندما استخدم القيم المتعلمة وقيمة الثقة
الافتراضية، ارتفعت نسبة فوزه إلى 64%؛ ومع الثقة الحية داخل الفئة، كانت
النسبة 67%. اعتبر</span> **Tesauro** <span dir="rtl">وزملاؤه هذا تحسينًا
كبيرًا، بالنظر إلى أن المراهنة في</span> **DD** <span dir="rtl">كانت
مطلوبة فقط حوالي 1.5 إلى 2 مرة في كل لعبة</span>.

<span dir="rtl">لأن **واتسون**</span> **(Watson)** <span dir="rtl">كان
لديه بضع ثوانٍ فقط للمراهنة، وكذلك لاختيار المربعات وتحديد ما إذا كان
عليه الضغط على الزر أم لا، كان وقت الحساب اللازم لاتخاذ هذه القرارات
عاملاً حاسمًا. سمحت **تنفيذ الشبكة العصبية الاصطناعية**</span> **(ANN
Implementation) <span dir="rtl">لـ </span>**$`\widehat{v}`$
<span dir="rtl">بالمراهنة في</span> **DD** <span dir="rtl">بسرعة كافية
لتلبية قيود الوقت الخاصة باللعب الحي. ومع ذلك، بمجرد أن أصبحت الألعاب
قابلة للمحاكاة بسرعة كافية من خلال تحسينات في برنامج المحاكاة، في نهاية
اللعبة كان من الممكن تقدير قيمة الرهانات من خلال المتوسط على العديد من
التجارب **مونت كارلو**</span> **(Monte-Carlo Trials)**
<span dir="rtl">التي تم فيها تحديد نتيجة كل رهان عن طريق محاكاة اللعب
حتى نهاية اللعبة. أدى اختيار رهانات</span> **DD** <span dir="rtl">في
نهاية اللعبة أثناء اللعب الحي بناءً على **تجارب مونت كارلو**</span>
**(Monte-Carlo Trials)** <span dir="rtl">بدلاً من **الشبكة العصبية
الاصطناعية**</span> **(ANN)** <span dir="rtl">إلى تحسين أداء
**واتسون**</span> **(Watson)** <span dir="rtl">بشكل كبير لأن الأخطاء في
تقدير القيم في نهاية اللعبة يمكن أن تؤثر بشكل خطير على فرصه في الفوز. قد
يكون اتخاذ جميع القرارات من خلال **تجارب مونت كارلو**</span>
**(Monte-Carlo Trials)** <span dir="rtl">أدى إلى قرارات مراهنة أفضل،
ولكن هذا كان ببساطة مستحيلاً نظرًا لتعقيد اللعبة وقيود الوقت الخاصة باللعب
الحي</span>.

<span dir="rtl">على الرغم من أن قدرته على الإجابة بسرعة ودقة على الأسئلة
المطروحة باللغة الطبيعية تبرز كإنجاز رئيسي لـ **واتسون**
</span>**(Watson)**<span dir="rtl">، إلا أن جميع استراتيجياته المعقدة
لاتخاذ القرار ساهمت في هزيمته المثيرة للإعجاب للأبطال البشريين. وفقًا
لـ</span> **Tesauro** <span dir="rtl">وزملائه</span>
(2012)<span dir="rtl">:</span>

"... <span dir="rtl">من الواضح أن خوارزميات استراتيجيتنا تحقق مستوى من
الدقة الكمية والأداء في الوقت الحقيقي يتجاوز القدرات البشرية</span>."

<span dir="rtl">هذا ينطبق بشكل خاص في حالات **المراهنة في**</span>
**DD** <span dir="rtl">و**الضغط على الزر في نهاية اللعبة**</span>
**(Endgame Buzzing)**<span dir="rtl">، حيث لا يمكن للبشر ببساطة الاقتراب
من مطابقة تقديرات **الأسهم**</span> **(Equity)**
<span dir="rtl">و**الثقة**</span> **(Confidence)**
<span dir="rtl">الدقيقة وحسابات القرارات المعقدة التي يقوم بها
**واتسون**</span> **(Watson)**<span dir="rtl">.</span>

**<u>16.4 <span dir="rtl">تحسين التحكم في الذاكرة</span> (Optimizing
Memory Control)</u>**

<span dir="rtl">تستخدم معظم أجهزة الحاسوب **ذاكرة الوصول العشوائي
الديناميكية**</span> **<span dir="rtl">(</span>Dynamic Random
<span dir="rtl"></span>Access Memory – DRAM<span dir="rtl">)
</span>**<span dir="rtl">كذاكرة رئيسية لها بسبب تكلفتها المنخفضة وقدرتها
العالية. وظيفة **متحكم ذاكرة**</span> **DRAM** <span dir="rtl">هي
استخدام الواجهة بين **شريحة المعالج**</span> **(Processor Chip)**
<span dir="rtl">ونظام</span> **DRAM** <span dir="rtl">خارج الشريحة
بكفاءة لتوفير النطاق الترددي العالي وزمن الانتقال المنخفض الضروري لنقل
البيانات اللازم لتنفيذ البرامج بسرعة عالية. يحتاج **متحكم
الذاكرة**</span> **(Memory Controller)** <span dir="rtl">إلى التعامل مع
أنماط متغيرة ديناميكيًا من طلبات القراءة/الكتابة مع الالتزام بعدد كبير من
القيود الزمنية وقيود الموارد التي يفرضها العتاد. هذه مشكلة جدولة هائلة،
خاصة مع المعالجات الحديثة متعددة النوى التي تشترك في نفس **ذاكرة**
</span>**DRAM**<span dir="rtl">.</span>

<span dir="rtl">قام **إبك** </span>**(İpek)<span dir="rtl">،
موتلو</span> (Mutlu)<span dir="rtl">، مارتينيز</span>
(Martínez)<span dir="rtl">، وكاروانا</span> (Caruana)** (2008)
<span dir="rtl">(وأيضًا **مارتينيز**</span> **(Martínez)
<span dir="rtl">وإبك</span> (İpek)**<span dir="rtl">، 2009)</span>
<span dir="rtl">بتصميم **متحكم ذاكرة يعتمد على التعليم المعزز**
</span>**(Reinforcement Learning Memory Controller)**
<span dir="rtl">وأظهروا أنه يمكن أن يحسن بشكل كبير سرعة تنفيذ البرامج
مقارنة بما كان ممكنًا مع المتحكمات التقليدية في وقت بحثهم. كانت دافعهم
القيود التي كانت موجودة في المتحكمات المتطورة حينها والتي كانت تستخدم
سياسات لا تستفيد من تجارب الجدولة السابقة ولا تأخذ بعين الاعتبار العواقب
طويلة الأجل لقرارات الجدولة. تم تنفيذ مشروع **إبك وزملائه**</span>
**(İpek et al.)** <span dir="rtl">من خلال المحاكاة، ولكنهم صمموا المتحكم
على مستوى تفصيلي يتضمن العتاد اللازم لتنفيذه</span>—including the
learning algorithm—<span dir="rtl">مباشرةً على شريحة المعالج</span>.

<span dir="rtl">يتضمن الوصول إلى **ذاكرة**</span> **DRAM**
<span dir="rtl">عددًا من الخطوات التي يجب القيام بها وفقًا لقيود زمنية
صارمة. تتكون **أنظمة**</span> **DRAM** <span dir="rtl">من العديد من
**شرائح** </span>**DRAM**<span dir="rtl">، كل منها يحتوي على مصفوفات
مستطيلة متعددة من خلايا التخزين المرتبة في صفوف وأعمدة. تخزن كل خلية بتًا
واحدًا كالشحنة على مكثف. بسبب انخفاض الشحنة بمرور الوقت، تحتاج كل
خلية</span> **DRAM** <span dir="rtl">إلى إعادة الشحن—التحديث—كل بضعة
ميلي ثانية لمنع فقدان محتوى الذاكرة. هذه الحاجة إلى تحديث الخلايا هي
السبب في أن</span> **DRAM** <span dir="rtl">تسمى "ديناميكية". تحتوي كل
مصفوفة خلايا على **مخزن مؤقت للصفوف**</span> **(Row Buffer)**
<span dir="rtl">الذي يحتفظ بصف من البتات التي يمكن نقلها من أو إلى أحد
صفوف المصفوفة. يقوم **أمر التفعيل**</span>
**<span dir="rtl">(</span>Activate
<span dir="rtl"></span>Command<span dir="rtl">)</span>**
"<span dir="rtl">بفتح صف"، مما يعني نقل محتويات الصف الذي يشير إليه
العنوان في الأمر إلى **مخزن مؤقت للصفوف** </span>**(Row
Buffer)**<span dir="rtl">.</span> <span dir="rtl">عندما يكون الصف
مفتوحًا، يمكن للمتحكم إصدار أوامر القراءة والكتابة إلى مصفوفة الخلايا.
يقوم كل **أمر قراءة**</span> **(Read Command)** <span dir="rtl">بنقل
كلمة (تسلسل قصير من البتات المتتالية) في **مخزن مؤقت للصفوف**</span>
**(Row Buffer)** <span dir="rtl">إلى **ناقل البيانات الخارجي**
</span>**(External Data Bus)**<span dir="rtl">، وكل **أمر كتابة**</span>
**(Write Command)** <span dir="rtl">ينقل كلمة في **ناقل البيانات
الخارجي**</span> **(External Data Bus)** <span dir="rtl">إلى **مخزن مؤقت
للصفوف** </span>**(Row Buffer)**<span dir="rtl">.</span>
<span dir="rtl">قبل أن يمكن فتح صف آخر، يجب إصدار **أمر الشحن
المسبق**</span> **(Precharge Command)** <span dir="rtl">الذي ينقل
البيانات (التي قد تكون محدثة) في **مخزن مؤقت للصفوف**</span> **(Row
Buffer)** <span dir="rtl">مرة أخرى إلى الصف الذي تم تحديد عنوانه في
**مصفوفة الخلايا** </span>**(Cell Array)**<span dir="rtl">.</span>
<span dir="rtl">بعد ذلك، يمكن لـ **أمر التفعيل**</span>
**<span dir="rtl">(</span>Activate Command<span dir="rtl">)
</span>**<span dir="rtl">آخر فتح صف جديد للوصول إليه. أوامر القراءة
والكتابة هي **أوامر أعمدة (**</span>**Column
Commands<span dir="rtl">)</span>** <span dir="rtl">لأنها تنقل بتات إلى
أو من أعمدة **مخزن مؤقت للصفوف**</span> **(Row Buffer)**
<span dir="rtl">بشكل متسلسل؛ يمكن نقل عدة بتات دون إعادة فتح الصف. يمكن
تنفيذ **أوامر القراءة والكتابة**</span> **<span dir="rtl">(</span>Read
and Write <span dir="rtl"></span>Commands<span dir="rtl">)
</span>**<span dir="rtl">للصف المفتوح حاليًا بسرعة أكبر من الوصول إلى صف
آخر، مما قد يتطلب أوامر صف إضافية:</span> **<span dir="rtl">الشحن المسبق
والتفعيل</span> (Precharge and Activate)**<span dir="rtl">؛ هذا ما يُعرف
أحيانًا  
بـ "محلية الصف</span> (Row Locality)"<span dir="rtl">.</span>
<span dir="rtl">يحتفظ **متحكم الذاكرة**</span> **(Memory Controller)**
<span dir="rtl">بقائمة انتظار معاملات الذاكرة التي تخزن طلبات الوصول إلى
الذاكرة من **المعالجات التي تشترك في نظام الذاكرة** </span>**(Processors
Sharing the Memory System)**<span dir="rtl">. يجب على المتحكم معالجة
الطلبات عن طريق إصدار أوامر إلى نظام الذاكرة مع الالتزام بعدد كبير من
القيود الزمنية</span>.

<span dir="rtl">يمكن أن يكون لسياسة المتحكم في جدولة طلبات الوصول تأثير
كبير على أداء نظام الذاكرة، مثل زمن الانتقال المتوسط الذي يمكن تلبية
الطلبات به ومعدل الإنتاجية الذي يمكن للنظام تحقيقه. أبسط استراتيجية
جدولة تتعامل مع طلبات الوصول بالترتيب الذي تصل فيه عن طريق إصدار جميع
الأوامر المطلوبة من قبل الطلب قبل البدء في خدمة الطلب التالي. ولكن إذا
لم يكن النظام جاهزًا لأحد هذه الأوامر، أو إذا كان تنفيذ الأمر سيؤدي إلى
نقص في استخدام الموارد (مثل القيود الزمنية الناتجة عن خدمة ذلك الأمر
الواحد)، فإنه من المنطقي البدء في خدمة طلب أحدث قبل الانتهاء من الطلب
الأقدم. يمكن للسياسات زيادة الكفاءة عن طريق إعادة ترتيب الطلبات، على
سبيل المثال، عن طريق إعطاء الأولوية لطلبات القراءة على طلبات الكتابة، أو
بإعطاء الأولوية لأوامر القراءة/الكتابة للصفوف المفتوحة بالفعل. السياسة
المسماة</span> **First-Ready, First-Come-First-Serve (FR-FCFS)**
<span dir="rtl">تعطي الأولوية لأوامر الأعمدة (القراءة والكتابة) على
أوامر الصفوف (التفعيل والشحن المسبق)، وفي حالة التعادل تعطي الأولوية
لأقدم أمر. تم إثبات أن</span> **FR-FCFS** <span dir="rtl">تتفوق على
سياسات الجدولة الأخرى من حيث متوسط زمن الوصول للذاكرة في الظروف
الشائعة</span> <span dir="rtl">(</span>Rixner<span dir="rtl">،
2004)</span>.

<span dir="rtl">**الشكل 16.3** هو عرض عالي المستوى لـ **متحكم الذاكرة
المعتمد على التعليم المعزز** </span>**(Reinforcement Learning Memory
Controller)** <span dir="rtl">الذي طوره **إبك وزملاؤه**</span>
**<span dir="rtl">(</span>İpek et
<span dir="rtl"></span>al.<span dir="rtl">)</span>**.
<span dir="rtl">لقد قاموا بنمذجة عملية الوصول إلى</span> **DRAM**
<span dir="rtl">كعملية **قرار ماركوفية**</span> **(MDP)**
<span dir="rtl">حيث تكون الحالات هي محتويات قائمة انتظار المعاملات
والإجراءات هي الأوامر إلى نظام</span> **DRAM**<span dir="rtl">:</span>
**<span dir="rtl">الشحن المسبق</span> (Precharge)**<span dir="rtl">،
**التفعيل** </span>**(Activate)**<span dir="rtl">، **القراءة**
</span>**(Read)**<span dir="rtl">، **الكتابة**
</span>**(Write)**<span dir="rtl">، و**لا إجراء**
</span>**(NoOp)**<span dir="rtl">. تكون إشارة المكافأة هي 1 كلما كان
الإجراء قراءة أو كتابة، وإلا تكون 0. تم اعتبار انتقالات الحالة عشوائية
لأن الحالة التالية للنظام تعتمد ليس فقط على أمر المجدول، ولكن أيضًا على
جوانب من سلوك النظام التي لا يمكن للمجدول التحكم فيها، مثل أعباء العمل
الخاصة بنوى المعالج التي تصل إلى نظام</span>
**DRAM**<span dir="rtl">.</span>

<img src="./media/image187.png"
style="width:6.26806in;height:3.29306in" />

**<span dir="rtl">الشكل 16.3</span>**: <span dir="rtl">عرض عالي المستوى
**لمتحكم**</span> **DRAM <span dir="rtl">المعتمد على التعليم
المعزز</span> (Reinforcement Learning DRAM
Controller)**<span dir="rtl">. **المجدول**</span> **(Scheduler)**
<span dir="rtl">هو **العميل** </span>**(Agent)** <span dir="rtl">في
التعليم المعزز. بيئته ممثلة من خلال **ميزات قائمة انتظار
المعاملات**</span> **<span dir="rtl">(</span>Features of
<span dir="rtl"></span>the Transaction
Queue<span dir="rtl">)</span>**<span dir="rtl">، وإجراءاته هي
**الأوامر**</span> **(Commands)** <span dir="rtl">لنظام</span>
**DRAM**<span dir="rtl">.</span> © 2009 IEEE<span dir="rtl">. معاد
طباعته بإذن من</span> J. F. Martínez <span dir="rtl">و</span> E.
İpek<span dir="rtl">، إدارة الموارد الديناميكية في المعالجات متعددة
النوى: مقاربة التعليم الآلي،</span> **Micro, IEEE<span dir="rtl">،
29</span>(5)<span dir="rtl">، ص. 12</span>.**

<span dir="rtl">تعتبر القيود المفروضة على **الإجراءات**</span>
**(Actions)** <span dir="rtl">المتاحة في كل حالة جزءًا حاسمًا من **عملية
قرار ماركوف**</span> **(MDP)** <span dir="rtl">هذه. تذكر من **الفصل 3**
أن مجموعة الإجراءات المتاحة يمكن أن تعتمد على **الحالة**
</span>**(State)**<span dir="rtl">: حيث يكون</span>
$`At \in A(St)`$<span dir="rtl">، حيث</span> $`At`$ <span dir="rtl">هو
**الإجراء**</span> **(Action)** <span dir="rtl">في الخطوة الزمنية</span>
$`t`$ <span dir="rtl">و</span>A(St) <span dir="rtl">هي مجموعة الإجراءات
المتاحة في الحالة</span> $`St`$​<span dir="rtl">.</span>
<span dir="rtl">في هذا التطبيق، تم ضمان سلامة نظام</span> **DRAM**
<span dir="rtl">بعدم السماح بالإجراءات التي من شأنها انتهاك القيود
الزمنية أو قيود الموارد. على الرغم من أن **إبك وزملاؤه** </span>**(İpek
et al.)** <span dir="rtl">لم يوضحوا ذلك بشكل صريح، إلا أنهم حققوا ذلك
بفعالية من خلال تعريف مسبق لمجموعات</span> $`A(St)`$
<span dir="rtl">لجميع الحالات الممكنة</span>
$`St`$​<span dir="rtl">.</span>

<span dir="rtl">تفسر هذه القيود سبب وجود إجراء</span> **NoOp**
<span dir="rtl">في **عملية قرار ماركوف**</span> **(MDP)**
<span dir="rtl">ولماذا تكون إشارة المكافأة 0 باستثناء عندما يتم إصدار
**أمر قراءة**</span> **(Read Command)** <span dir="rtl">أو **أمر
كتابة**</span> **<span dir="rtl">(</span>Write
<span dir="rtl"></span>Command<span dir="rtl">)</span>**.
<span dir="rtl">يتم إصدار</span> **NoOp** <span dir="rtl">عندما يكون هو
الإجراء القانوني الوحيد في حالة معينة. لتحقيق أقصى استفادة من نظام
الذاكرة، تكون مهمة المتحكم هي قيادة النظام إلى الحالات التي يمكن فيها
اختيار **إجراء قراءة أو كتابة** </span>**(Read or Write
Action)**<span dir="rtl">:</span> <span dir="rtl">فقط هذه الإجراءات تؤدي
إلى إرسال البيانات عبر **ناقل البيانات الخارجي** </span>**(External Data
Bus)**<span dir="rtl">، وبالتالي فهي الوحيدة التي تساهم في إنتاجية
النظام. على الرغم من أن **التفعيل والشحن المسبق**</span> **(Activate and
Precharge)** <span dir="rtl">لا ينتجان مكافأة فورية، إلا أن العميل يحتاج
إلى اختيار هذه الإجراءات لجعل من الممكن لاحقًا اختيار **إجراءات القراءة
والكتابة** </span>**(Read and Write Actions)** <span dir="rtl">التي
تكافأ</span>.

<span dir="rtl">استخدم **وكيل الجدولة**</span> **(Scheduling Agent)**
<span dir="rtl">خوارزمية **سارسا**</span> **(Sarsa)**
<span dir="rtl">(انظر **الفصل 6.4**) لتعلم **دالة قيمة الإجراءات**
</span>**(Action-Value Function)**<span dir="rtl">.</span>
<span dir="rtl">تم تمثيل **الحالات**</span> **(States)**
<span dir="rtl">بواسطة ست ميزات ذات قيم صحيحة. لتقريب **دالة قيمة
الإجراءات** </span>**(Action-Value Function)**<span dir="rtl">، استخدمت
الخوارزمية **تقريب الدوال الخطية**</span> **(Linear Function
Approximation)** <span dir="rtl">المنفذة باستخدام **ترميز التجانب مع
التجزئة**</span> **(Tile Coding with Hashing)** <span dir="rtl">(انظر
**الفصل 9.5.4**).</span> <span dir="rtl">كان لدى **ترميز
التجانب**</span> **(Tile Coding)** 32 <span dir="rtl">تجانبًا، كل منها
يخزن 256 قيمة للإجراءات كأرقام ثابتة النقطة ذات 16 بت. كان
**الاستكشاف**</span> **(ϵ-greedy)** <span dir="rtl">مع</span>
$`\epsilon = 0.05`$<span dir="rtl">.</span>

<span dir="rtl">تضمنت ميزات **الحالة**</span> **(State Features)**
<span dir="rtl">عدد طلبات القراءة في **قائمة انتظار المعاملات**
</span>**(Transaction Queue)**<span dir="rtl">، وعدد طلبات الكتابة في
**قائمة انتظار المعاملات**</span> **<span dir="rtl">(</span>Transaction
<span dir="rtl"></span>Queue<span dir="rtl">)</span>**<span dir="rtl">،
وعدد طلبات الكتابة في **قائمة انتظار المعاملات**</span> **(Transaction
Queue)** <span dir="rtl">التي تنتظر فتح صفوفها، وعدد طلبات القراءة في
**قائمة انتظار المعاملات**</span> **(Transaction Queue)**
<span dir="rtl">التي تنتظر فتح صفوفها والتي تم إصدارها من قبل **معالجات
الطلب** </span>**(Requesting Processors)**<span dir="rtl">.</span>
<span dir="rtl">(تعتمد الميزات الأخرى على كيفية تفاعل</span> **DRAM**
<span dir="rtl">مع ذاكرة التخزين المؤقت، وهي تفاصيل نغفلها هنا). كان
اختيار ميزات الحالة قائمًا على فهم **إبك وزملاؤه**</span> **(İpek et
al.)** <span dir="rtl">للعوامل التي تؤثر على أداء</span>
**DRAM**<span dir="rtl">. على سبيل المثال، يمكن أن يساعد موازنة معدل
خدمة القراءة والكتابة بناءً على عدد كل منها في **قائمة انتظار
المعاملات**</span> **(Transaction Queue)** <span dir="rtl">على تجنب توقف
تفاعل **نظام**</span> **DRAM** <span dir="rtl">مع **ذاكرة التخزين
المؤقت** </span>**(Cache Memory)**<span dir="rtl">.</span>
<span dir="rtl">قام المؤلفون في الواقع بإنشاء قائمة طويلة نسبيًا من
**الميزات المحتملة** </span>**(Potential Features)**<span dir="rtl">، ثم
قاموا بتقليصها إلى عدد قليل باستخدام المحاكاة الموجهة بواسطة اختيار
الميزات خطوة بخطوة</span>.

<span dir="rtl">جانب مثير للاهتمام في هذا التشكيل لمشكلة الجدولة كـ
**عملية قرار ماركوف**</span> **(MDP)** <span dir="rtl">هو أن الميزات
المدخلة إلى **ترميز التجانب**</span> **(Tile Coding)**
<span dir="rtl">لتعريف **دالة قيمة الإجراءات**</span>
**<span dir="rtl">(</span>Action-Value
<span dir="rtl"></span>Function<span dir="rtl">)
</span>**<span dir="rtl">كانت مختلفة عن الميزات المستخدمة لتحديد
**مجموعات قيود الإجراءات** </span>**A(St)**<span dir="rtl">.</span>
<span dir="rtl">في حين أن مدخلات **ترميز التجانب**</span> **(Tile Coding
Input)** <span dir="rtl">كانت مشتقة من محتويات **قائمة انتظار
المعاملات** </span>**(Transaction Queue)**<span dir="rtl">، كانت
**مجموعات القيود**</span> **(Constraint Sets)** <span dir="rtl">تعتمد
على مجموعة من الميزات الأخرى المتعلقة بالقيود الزمنية وقيود الموارد التي
يجب تلبيتها بواسطة التنفيذ المادي للنظام بأكمله. بهذه الطريقة، ضمنت
**قيود الإجراءات**</span> **(Action Constraints)** <span dir="rtl">أن
استكشاف خوارزمية التعليم لن يعرض سلامة النظام المادي للخطر، بينما كان
التعليم محدودًا بشكل فعال بمنطقة "آمنة" من **الفضاء الحيزي الأكبر للنظام
المادي**</span> **<span dir="rtl">(</span>Larger State Space of the
Hardware
<span dir="rtl"></span>Implementation<span dir="rtl">)</span>**.

<span dir="rtl">نظرًا لأن الهدف من هذا العمل هو أن يكون **متحكم
التعليم**</span> **(Learning Controller)** <span dir="rtl">قابلاً للتنفيذ
على شريحة حتى يتمكن التعليم من الحدوث عبر الإنترنت بينما يكون الحاسوب
قيد التشغيل، كانت تفاصيل التنفيذ المادي اعتبارات مهمة. شمل التصميم
**أنبوبتي معالجة من خمس مراحل**</span> **<span dir="rtl">(</span>Two
Five-Stage Pipelines<span dir="rtl">) </span>**<span dir="rtl">لحساب
ومقارنة قيمتين للإجراءات في كل دورة من دورات المعالج، وتحديث قيمة
الإجراء المناسبة. شمل هذا الوصول إلى **ترميز التجانب**</span> **(Tile
Coding)** <span dir="rtl">الذي تم تخزينه على الشريحة في **ذاكرة الوصول
العشوائي الثابتة** </span>**(Static RAM)**<span dir="rtl">.</span>
<span dir="rtl">بالنسبة للتكوين الذي تمت محاكاته بواسطة **إبك وزملاؤه**
</span>**(İpek et al.)**<span dir="rtl">، والذي كان عبارة عن شريحة بأربع
نوى بسرعة 4 جيجاهرتز نموذجية لمحطات العمل المتطورة في وقت بحثهم، كان
هناك 10 دورات معالج لكل دورة</span> **DRAM**<span dir="rtl">.</span>
<span dir="rtl">بالنظر إلى الدورات اللازمة لملء **الأنابيب**
</span>**(Pipelines)**<span dir="rtl">، كان من الممكن تقييم ما يصل إلى
12 إجراءً في كل دورة</span> **DRAM**<span dir="rtl">.</span>
<span dir="rtl">وجد **إبك وزملاؤه**</span> **(İpek et al.)**
<span dir="rtl">أن عدد **الأوامر القانونية** </span>**(Legal Commands)**
<span dir="rtl">لأي حالة نادرًا ما كان أكبر من هذا، وأن فقدان الأداء كان
ضئيلاً إذا لم يكن هناك دائمًا وقت كافٍ للنظر في جميع **الأوامر القانونية**
</span>**(Legal Commands)**<span dir="rtl">.</span> <span dir="rtl">جعلت
هذه التفاصيل الذكية وغيرها من الممكن تنفيذ **المتحكم الكامل وخوارزمية
التعليم على شريحة متعددة المعالجات**</span>
**<span dir="rtl">(</span>Multi-Processor
Chip<span dir="rtl">)</span>**.

<span dir="rtl">قام **إبك وزملاؤه**</span> **(İpek et al.)**
<span dir="rtl">بتقييم **متحكم التعليم**</span> **(Learning
Controller)** <span dir="rtl">في المحاكاة بمقارنته مع ثلاثة
**متحكمات**</span> **(Controllers)** <span dir="rtl">أخرى: 1</span>)
**<span dir="rtl">متحكم</span> FR-FCFS** <span dir="rtl">المذكور أعلاه
الذي يقدم أفضل أداء متوسط، 2)</span> **<span dir="rtl">متحكم
تقليدي</span> (Conventional Controller)** <span dir="rtl">يعالج كل طلب
بالترتيب، و3</span>) **<span dir="rtl">متحكم مثالي غير قابل
للتحقيق</span> (Unrealizable Ideal Controller)**<span dir="rtl">، يسمى
**المتحكم المتفائل** </span>**(Optimistic Controller)**<span dir="rtl">،
القادر على تحقيق 100% من **معدل إنتاجية** </span>**DRAM
<span dir="rtl"></span>(DRAM Throughput)** <span dir="rtl">إذا تم تلبية
الطلب الكافي بتجاهل جميع القيود الزمنية وقيود الموارد، ولكن مع ذلك يقوم
بنمذجة</span> **DRAM <span dir="rtl">زمن استجابة</span> (DRAM Latency)**
<span dir="rtl">(كضربات **مخزن الصفوف** </span>**(Row Buffer
Hits)**<span dir="rtl">) و**عرض النطاق الترددي**
</span>**(Bandwidth)**<span dir="rtl">.</span> <span dir="rtl">قاموا
بمحاكاة تسعة أحمال عمل موازية مكثفة الذاكرة تتكون من تطبيقات علمية
واستخراج البيانات. يظهر **الشكل 16.4** الأداء</span>
<span dir="rtl">(معكوس زمن التنفيذ مقارنةً بأداء</span>
FR-FCFS<span dir="rtl">)</span> <span dir="rtl">لكل **متحكم**</span>
**(Controller)** <span dir="rtl">للتطبيقات التسعة، إلى جانب المتوسط
الهندسي لأدائهم عبر التطبيقات.</span> **<span dir="rtl">متحكم
التعليم</span> (Learning Controller)**<span dir="rtl">، المسمى</span>
**RL** <span dir="rtl">في الشكل، حقق تحسينًا يتراوح بين 7% إلى 33% على
مدار التطبيقات التسعة، مع متوسط تحسين قدره 19%. بالطبع، لا يمكن لأي
متحكم قابل للتحقيق مطابقة أداء **المتحكم المتفائل**</span>
**<span dir="rtl">(</span>Optimistic
<span dir="rtl"></span>Controller<span dir="rtl">)</span>**<span dir="rtl">،
الذي يتجاهل جميع القيود الزمنية وقيود الموارد، لكن أداء **متحكم
التعليم**</span> **<span dir="rtl">(</span>Learning
<span dir="rtl"></span>Controller<span dir="rtl">)
</span>**<span dir="rtl">قلص الفجوة مع الحد الأعلى **للمتحكم المتفائل**
</span>**(Optimistic’s Upper Bound)** <span dir="rtl">بشكل مثير للإعجاب
بنسبة 27</span>%.

<span dir="rtl">نظرًا لأن المبرر لتنفيذ خوارزمية التعليم على الشريحة كان
السماح لسياسة الجدولة بالتكيف عبر الإنترنت مع أحمال العمل المتغيرة، قام
**إبك وزملاؤه**</span> **(İpek et al.)** <span dir="rtl">بتحليل تأثير
التعليم عبر الإنترنت مقارنةً بسياسة ثابتة تم تعلمها مسبقًا</span>.

<img src="./media/image188.png"
style="width:6.26806in;height:1.69375in" />

<span dir="rtl">**الشكل 16.4**:</span> <span dir="rtl">أداء أربع متحكمات
عبر مجموعة من 9 تطبيقات مرجعية محاكاة. المتحكمات هي</span>:
**<span dir="rtl">المتحكم الأبسط 'داخل الترتيب</span>
(In-Order)<span dir="rtl">،</span>**
<span dir="rtl"></span>**FR-FCFS**<span dir="rtl">، **متحكم
التعليم**</span> **<span dir="rtl">(</span>Learning
<span dir="rtl"></span>Controller –
RL<span dir="rtl">)</span>**<span dir="rtl">، و**المتحكم المتفائل غير
القابل للتحقيق**</span> **<span dir="rtl">(</span>Unrealizable
Optimistic <span dir="rtl"></span>Controller<span dir="rtl">)
</span>**<span dir="rtl">الذي يتجاهل جميع القيود الزمنية وقيود الموارد
لتقديم الحد الأعلى للأداء. الأداء، الذي يتم تطبيعه مع</span>
**FR-FCFS**<span dir="rtl">، هو معكوس زمن التنفيذ. في أقصى اليمين يظهر
المتوسط الهندسي للأداءات عبر التطبيقات المرجعية التسعة لكل متحكم</span>.
**<span dir="rtl">متحكم</span> RL** <span dir="rtl">يأتي الأقرب إلى
الأداء المثالي.  
© 2009</span> IEEE<span dir="rtl">.</span> <span dir="rtl">معاد طباعته
بإذن من</span> J. F. Martínez <span dir="rtl">و</span> E.
İpek<span dir="rtl">، إدارة الموارد الديناميكية في المعالجات متعددة
النوى: مقاربة التعليم الآلي،</span> **Micro, IEEE,
29(5)<span dir="rtl">، ص. 13</span>**.

<span dir="rtl">قاموا بتدريب **متحكمهم**</span> **(Controller)**
<span dir="rtl">باستخدام بيانات من جميع التطبيقات المرجعية التسعة ثم
أبقوا على **قيم الإجراءات**</span> **(Action Values)**
<span dir="rtl">الناتجة ثابتة طوال فترة تنفيذ التطبيقات المحاكية. وجدوا
أن الأداء المتوسط **للمتحكم**</span> **(Controller)**
<span dir="rtl">الذي تعلم عبر الإنترنت كان أفضل بنسبة 8% من **المتحكم**
</span>**(Controller)** <span dir="rtl">الذي يستخدم السياسة الثابتة، مما
دفعهم إلى الاستنتاج بأن التعليم عبر الإنترنت هو ميزة مهمة في
نهجهم</span>.

<span dir="rtl">لم يتم تنفيذ **متحكم الذاكرة المتعلم**</span>
**(Learning Memory Controller)** <span dir="rtl">هذا في **عتاد مادي**
</span>**(Physical Hardware)** <span dir="rtl">أبدًا بسبب التكلفة الكبيرة
للتصنيع. ومع ذلك، استطاع **إبك وزملاؤه  
(**</span>**İpek <span dir="rtl"></span>et al.<span dir="rtl">)
</span>**<span dir="rtl">أن يجادلوا بشكل مقنع بناءً على نتائج محاكاتهم
بأن **متحكم ذاكرة يتعلم عبر الإنترنت من خلال التعليم المعزز**</span>
**<span dir="rtl">(</span>Memory Controller That Learns Online via
<span dir="rtl"></span>Reinforcement Learning<span dir="rtl">)
</span>**<span dir="rtl">لديه القدرة على تحسين الأداء إلى مستويات تتطلب
عادةً أنظمة ذاكرة أكثر تعقيدًا وأغلى تكلفة، بينما يخفف من بعض العبء الذي
يتحمله المصممون البشريون لتصميم سياسات جدولة فعالة يدويًا. قام
**موكوندان**</span> **(Mukundan)** <span dir="rtl">و**مارتينيز**</span>
**(Martínez)** (2012) <span dir="rtl">بدفع هذا المشروع قدمًا من خلال
التحقيق في **متحكمات التعليم**</span> **(Learning Controllers)**
<span dir="rtl">التي تحتوي على إجراءات إضافية، معايير أداء أخرى، ووظائف
مكافأة أكثر تعقيدًا مستمدة باستخدام **الخوارزميات الجينية**
</span>**(Genetic Algorithms)**<span dir="rtl">.</span>
<span dir="rtl">لقد أخذوا في الاعتبار معايير أداء إضافية تتعلق بكفاءة
الطاقة. تجاوزت نتائج هذه الدراسات النتائج السابقة بشكل كبير وتجاوزت بشكل
كبير **المستوى المتقدم لعام 2012** </span>**(2012 State-of-the-Art)**
<span dir="rtl">لجميع معايير الأداء التي اعتبروها. النهج يعد واعدًا بشكل
خاص لتطوير **واجهات**</span> **DRAM <span dir="rtl">متطورة مدركة
للطاقة</span> <span dir="rtl">(</span>Sophisticated Power-Aware DRAM
<span dir="rtl"></span>Interfaces<span dir="rtl">)</span>**.

**<u>16.5 <span dir="rtl">لعب ألعاب الفيديو بمستوى بشري</span>
(Human-level Video Game Play)</u>**

<span dir="rtl">واحدة من أكبر التحديات في تطبيق **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">على المشاكل
الحقيقية هي اتخاذ قرار حول كيفية تمثيل وتخزين **دوال القيمة**</span>
**(Value Functions)** <span dir="rtl">و/أو **السياسات**
</span>**(Policies)**<span dir="rtl">.</span> <span dir="rtl">ما لم يكن
**مجموعة الحالات**</span> **(State Set)** <span dir="rtl">محدودة وصغيرة
بما يكفي للسماح بالتمثيل الشامل باستخدام **جدول بحث**</span> **(Lookup
Table)** —<span dir="rtl">كما هو الحال في العديد من أمثلتنا
التوضيحية—فإنه يجب استخدام **مخطط تقريب دالة بارامتري**</span>
**<span dir="rtl">(</span>Parameterized Function Approximation
<span dir="rtl"></span>Scheme<span dir="rtl">)</span>**.
<span dir="rtl">سواء كان التقريب خطيًا أو غير خطي، يعتمد على ميزات يجب أن
تكون متاحة بشكل سهل للنظام التعليمي وقادرة على نقل المعلومات الضرورية
للأداء الماهر. تعتمد معظم تطبيقات **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">الناجحة بشكل كبير على
مجموعات من الميزات التي تم تصميمها يدويًا بعناية استنادًا إلى المعرفة
البشرية والحدس حول المشكلة المحددة التي يتم معالجتها</span>.

<span dir="rtl">طور فريق من الباحثين في</span> **Google DeepMind**
<span dir="rtl">عرضًا مثيرًا للإعجاب يظهر أن **شبكة عصبية اصطناعية عميقة
متعددة الطبقات**</span> **(Deep Multi-layer ANN)**
<span dir="rtl">يمكنها أتمتة عملية تصميم الميزات</span> (Mnih et al.,
2013, 2015). <span dir="rtl">لقد تم استخدام **الشبكات العصبية الاصطناعية
متعددة الطبقات**</span> **(Multi-layer ANNs)** <span dir="rtl">لتقريب
الدوال في **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**
<span dir="rtl">منذ عام 1986 عندما تم تعميم خوارزمية **الانتشار العكسي**
</span>**(Backpropagation)** <span dir="rtl">كطريقة لتعلم **التمثيلات
الداخلية** </span>**(Internal Representations)**
<span dir="rtl">(</span>Rumelhart, Hinton, and Williams,
1986<span dir="rtl">؛ انظر **الفصل 9.6**)</span>. <span dir="rtl">تم
الحصول على نتائج مذهلة بدمج **التعليم المعزز** </span>**(Reinforcement
Learning)** <span dir="rtl">مع **الانتشار العكسي**
</span>**(Backpropagation)**<span dir="rtl">.</span>
<span dir="rtl">النتائج التي حصل عليها</span> **Tesauro**
<span dir="rtl">وزملاؤه مع</span> **TD-Gammon**
<span dir="rtl">و**واتسون**</span> **(Watson)** <span dir="rtl">التي
نوقشت أعلاه هي أمثلة ملحوظة. استفادت هذه التطبيقات وغيرها من قدرة
**الشبكات العصبية الاصطناعية متعددة الطبقات** </span>**(Multi-layer
ANNs)** <span dir="rtl">على تعلم الميزات ذات الصلة بالمهمة. ومع ذلك، في
جميع الأمثلة التي نحن على علم بها، كانت العروض الأكثر إثارة للإعجاب
تتطلب أن يتم تمثيل مدخلات الشبكة من حيث الميزات المتخصصة المصممة يدويًا
للمشكلة المعينة</span>.

<span dir="rtl">يظهر هذا بوضوح في نتائج</span> **TD-Gammon**.
**TD-Gammon 0.0**<span dir="rtl">، الذي كان مدخل الشبكة الخاص به في
الأساس تمثيلًا "خامًا" للوحة **الباكامون**
</span>**(Backgammon)**<span dir="rtl">، بمعنى أنه تضمن معرفة قليلة جدًا
عن اللعبة، تعلم اللعب بشكل مماثل تقريبًا لأفضل برامج **الباكامون**
</span>**(Backgammon)** <span dir="rtl">السابقة. إضافة الميزات المتخصصة
في **الباكامون**</span> **(Backgammon)** <span dir="rtl">أنتجت</span>
**TD-Gammon <span dir="rtl"></span>1.0** <span dir="rtl">الذي كان أفضل
بشكل كبير من جميع برامج **الباكامون**</span> **(Backgammon)**
<span dir="rtl">السابقة وتنافس جيدًا مع الخبراء البشريين</span>.

<span dir="rtl">قام</span> **Mnih** <span dir="rtl">وزملاؤه بتطوير وكيل
تعلم معزز يسمى **شبكة**</span> **Q <span dir="rtl">العميقة  
(</span>(Deep Q-network – DQN <span dir="rtl">التي</span>**
<span dir="rtl">جمعت بين</span> **Q-learning** <span dir="rtl">و**شبكة
عصبية اصطناعية التفافية عميقة** </span>**(Deep Convolutional
ANN)**<span dir="rtl">، وهي شبكة عصبية اصطناعية ذات طبقات متعددة، أو
**عميقة** </span>**(Deep)**<span dir="rtl">، متخصصة في معالجة مصفوفات
البيانات المكانية مثل الصور. نصف **الشبكات العصبية الالتفافية
العميقة**</span> **(Deep Convolutional ANNs)** <span dir="rtl">في
**الفصل 9.6**</span>. <span dir="rtl">بحلول وقت عمل</span> **Mnih**
<span dir="rtl">وزملائه مع</span> **DQN**<span dir="rtl">، كانت
**الشبكات العصبية العميقة** </span>**(Deep ANNs)**<span dir="rtl">، بما
في ذلك **الشبكات العصبية الالتفافية العميقة** </span>**(Deep
Convolutional ANNs)**<span dir="rtl">، قد حققت نتائج مثيرة للإعجاب في
العديد من التطبيقات، ولكنها لم تكن مستخدمة على نطاق واسع في **التعليم
المعزز** </span>**(Reinforcement Learning)**<span dir="rtl">.</span>

<span dir="rtl">استخدم</span> **Mnih** <span dir="rtl">وزملاؤه</span>
**DQN** <span dir="rtl">لإظهار كيف يمكن لوكيل **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">تحقيق مستوى عالٍ من الأداء على أي مجموعة من
المشاكل المختلفة دون الحاجة إلى استخدام مجموعات ميزات مختلفة خاصة بكل
مشكلة. لإثبات ذلك، سمحوا لـ</span> **DQN** <span dir="rtl">بتعلم لعب 49
لعبة فيديو مختلفة على **أتاري 2600**</span> **(Atari 2600)**
<span dir="rtl">عن طريق التفاعل مع **محاكي اللعبة**</span>
**<span dir="rtl">(</span>Game
<span dir="rtl"></span>Emulator<span dir="rtl">)</span>**.
<span dir="rtl">تعلمت</span> **DQN** <span dir="rtl">سياسة مختلفة لكل
واحدة من الألعاب الـ 49 (لأن أوزان الشبكة العصبية الاصطناعية الخاصة به
كانت تعيد ضبطها إلى قيم عشوائية قبل التعليم على كل لعبة)، لكنها استخدمت
نفس المدخلات الخام، بنية الشبكة، وقيم **البارامترات**</span>
**(Parameters)** (<span dir="rtl">مثل حجم الخطوة، معدل الخصم،
**بارامترات الاستكشاف** </span>**(Exploration
Parameters)**<span dir="rtl">، وأكثر من ذلك بكثير يتعلق بالتنفيذ) لجميع
الألعاب. حققت</span> **DQN** <span dir="rtl">مستويات من اللعب على مستوى
أو تتجاوز المستوى البشري في جزء كبير من هذه الألعاب. على الرغم من أن
الألعاب كانت متشابهة من حيث أنها تُلعب من خلال مشاهدة تدفقات من **الصور
الفيديوية** </span>**(Video Images)**<span dir="rtl">، إلا أنها اختلفت
بشكل كبير في جوانب أخرى. كانت **إجراءاتها**</span> **(Actions)**
<span dir="rtl">لها تأثيرات مختلفة، وكانت لها ديناميكيات انتقال حالة
مختلفة، وكانت بحاجة إلى سياسات مختلفة لتحقيق درجات عالية. تعلمت **الشبكة
العصبية الاصطناعية الالتفافية العميقة**</span> **(Deep Convolutional
ANN)** <span dir="rtl">تحويل المدخلات الخام المشتركة بين جميع الألعاب
إلى ميزات متخصصة لتمثيل **قيم الإجراءات**</span> **(Action Values)**
<span dir="rtl">اللازمة للعب بالمستوى العالي الذي حققته</span> **DQN**
<span dir="rtl">في معظم الألعاب</span>.

**<span dir="rtl">أتاري 2600</span> (Atari 2600)** <span dir="rtl">هو
جهاز لعبة فيديو منزلي تم بيعه في إصدارات مختلفة من قبل **أتاري
إنك**</span> **(Atari Inc.)** <span dir="rtl">من عام 1977 إلى عام 1992.
قدم أو شجع العديد من ألعاب الفيديو الآركيد التي تعتبر الآن كلاسيكيات،
مثل</span> **Pong**<span dir="rtl">،</span>
**Breakout**<span dir="rtl">،</span> **Space Invaders**<span dir="rtl">،
و</span>**Asteroids**<span dir="rtl">.</span> <span dir="rtl">على الرغم
من أنها أبسط بكثير من ألعاب الفيديو الحديثة، إلا أن ألعاب **أتاري
2600**</span> **(Atari 2600)** <span dir="rtl">لا تزال ممتعة وتحديًا
للاعبين البشريين، وكانت جذابة كمنصات اختبار لتطوير وتقييم طرق **التعليم
المعزز (**</span>**Reinforcement Learning<span dir="rtl">)</span>**
<span dir="rtl">(</span>Diuk, Cohen, Littman,
2008<span dir="rtl">؛</span> Naddaf, 2010<span dir="rtl">؛</span> Cobo,
<span dir="rtl"></span>Zang, Isbell, and Thomaz,
2011<span dir="rtl">؛</span> Bellemare, Veness, and Bowling,
2013<span dir="rtl">)</span>. <span dir="rtl">طور</span> **Bellemare,
Naddaf, Veness, and Bowling** (2012) **<span dir="rtl">بيئة التعليم
الآركيدية</span> <span dir="rtl">(</span>Arcade
<span dir="rtl"></span>Learning Environment – ALE<span dir="rtl">)
</span>**<span dir="rtl">المتاحة للجمهور لتشجيع وتبسيط استخدام ألعاب
**أتاري 2600**</span> **(Atari 2600)** <span dir="rtl">لدراسة
**خوارزميات التعليم والتخطيط**</span> **<span dir="rtl">(</span>Learning
and Planning
<span dir="rtl"></span>Algorithms<span dir="rtl">)</span>**.

<span dir="rtl">جعلت الدراسات السابقة وتوفر **بيئة التعليم
الآركيدية**</span> **(ALE)** <span dir="rtl">مجموعة ألعاب **أتاري
2600**</span> **<span dir="rtl">(</span>Atari
<span dir="rtl"></span>2600<span dir="rtl">)
</span>**<span dir="rtl">خيارًا جيدًا لعرض</span> **Mnih**
<span dir="rtl">وزملائه، الذي تأثر أيضًا بالأداء المثير للإعجاب على
المستوى البشري الذي استطاع</span> **TD-Gammon** <span dir="rtl">تحقيقه
في **الباكامون** </span>**(Backgammon)**<span dir="rtl">.</span>
<span dir="rtl">يشبه</span> **DQN** <span dir="rtl">نظام</span>
**TD-Gammon** <span dir="rtl">في استخدام **شبكة عصبية اصطناعية متعددة
الطبقات** </span>**(Multi-layer ANN)** <span dir="rtl">كطريقة **لتقريب
الدالة**</span> **(Function Approximation)** <span dir="rtl">لشكل **شبه
التدرج**</span> **(Semi-gradient)** <span dir="rtl">من خوارزمية</span>
**TD**<span dir="rtl">، مع حساب التدرجات باستخدام **خوارزمية الانتشار
العكسي**</span> **<span dir="rtl">(</span>Backpropagation
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**.
<span dir="rtl">ومع ذلك، بدلاً من استخدام</span> **TD(λ)**
<span dir="rtl">كما فعل</span> **TD-Gammon**<span dir="rtl">،
استخدم</span> **DQN** <span dir="rtl">الشكل **شبه التدرج**</span>
**(Semi-gradient)** <span dir="rtl">من</span>
**Q-learning**<span dir="rtl">.</span> <span dir="rtl">قام</span>
**TD-Gammon** <span dir="rtl">بتقدير قيم **الحالات اللاحقة**
</span>**(Afterstates)**<span dir="rtl">، والتي تم الحصول عليها بسهولة
من القواعد الخاصة بتحركات **الباكامون**
</span>**(Backgammon)**<span dir="rtl">.</span> <span dir="rtl">لاستخدام
نفس الخوارزمية مع ألعاب **أتاري** </span>**(Atari)**<span dir="rtl">،
كان سيتطلب توليد **الحالات التالية**</span> **(Next States)**
<span dir="rtl">لكل إجراء ممكن (والتي لن تكون حالات لاحقة في هذه
الحالة). كان من الممكن القيام بذلك باستخدام **محاكي اللعبة**</span>
**(Game Emulator)** <span dir="rtl">لتشغيل محاكاة بخطوة واحدة لجميع
الإجراءات الممكنة (التي تجعلها</span> **ALE** <span dir="rtl">ممكنة). أو
كان من الممكن تعلم نموذج لوظيفة **انتقال الحالة**
</span>**(State-Transition Function)** <span dir="rtl">لكل لعبة
واستخدامه للتنبؤ بالحالات التالية  
(</span>Oh, Guo, Lee, Lewis, and Singh, 2015<span dir="rtl">)</span>.
<span dir="rtl">على الرغم من أن هذه الطرق قد تكون قد أنتجت نتائج مشابهة
لنتائج</span> **DQN**<span dir="rtl">، إلا أنها كانت ستكون أكثر تعقيدًا
في التنفيذ وستزيد بشكل كبير من الوقت اللازم للتعليم. دافع آخر
لاستخدام</span> **Q-learning** <span dir="rtl">هو أن</span> **DQN**
<span dir="rtl">استخدم طريقة **إعادة تشغيل التجربة**
</span>**(Experience Replay)**<span dir="rtl">، الموضحة أدناه، والتي
تتطلب خوارزمية **خارج السياسة**</span>
**<span dir="rtl">(</span>Off-Policy
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**.
<span dir="rtl">كونه **خارج السياسة**</span> **(Off-Policy)**
<span dir="rtl">وبدون نموذج جعل</span> **Q-learning**
<span dir="rtl">خيارًا طبيعيًا</span>.

<span dir="rtl">قبل وصف تفاصيل</span> **DQN** <span dir="rtl">وكيفية
إجراء التجارب، نلقي نظرة على مستويات المهارة التي استطاع</span> **DQN**
<span dir="rtl">تحقيقها. قارن</span> **Mnih** <span dir="rtl">وزملاؤه
درجات</span> **DQN** <span dir="rtl">مع درجات أفضل نظام تعلم تم نشره في
الأدبيات في ذلك الوقت، ومع درجات مختبر محترف لألعاب الفيديو، ومع درجات
وكيل يقوم باختيار الإجراءات بشكل عشوائي. استخدم أفضل نظام من الأدبيات
**تقريب الدوال الخطية**</span> **<span dir="rtl">(</span>Linear Function
<span dir="rtl"></span>Approximation<span dir="rtl">)</span>**
<span dir="rtl">مع ميزات مصممة باستخدام بعض المعرفة حول ألعاب **أتاري
2600**</span> **<span dir="rtl">(</span>Atari
<span dir="rtl"></span>2600)** (Bellemare, Naddaf, Veness, and Bowling,
2013<span dir="rtl">)</span>. <span dir="rtl">تعلم</span> **DQN**
<span dir="rtl">في كل لعبة من خلال التفاعل مع **محاكي اللعبة**</span>
**(Game Emulator)** <span dir="rtl">لـ 50 مليون إطار، وهو ما يعادل حوالي
38 يومًا من الخبرة مع اللعبة. في بداية التعليم في كل لعبة، تم إعادة تعيين
أوزان شبكة</span> **DQN** <span dir="rtl">إلى قيم عشوائية. لتقييم مستوى
مهارة</span> **DQN** <span dir="rtl">بعد التعليم، تم حساب متوسط درجاته
على 30 جلسة في كل لعبة، كل منها استمرت حتى 5 دقائق وبدأت بحالة لعبة
عشوائية. لعب المختبر البشري المحترف باستخدام نفس **المحاكي**</span>
**(Emulator)** <span dir="rtl">(مع إيقاف الصوت لإزالة أي ميزة محتملة
على</span> **DQN** <span dir="rtl">الذي لم يعالج الصوت). بعد ساعتين من
التدريب، لعب البشري حوالي 20 حلقة من كل لعبة لمدة تصل إلى 5 دقائق لكل
منها ولم يُسمح له بأخذ أي استراحة خلال هذا الوقت. تعلم</span> **DQN**
<span dir="rtl">اللعب بشكل أفضل من أفضل أنظمة **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">السابقة في جميع الألعاب
باستثناء 6 منها، ولعب بشكل أفضل من اللاعب البشري في 22 من الألعاب.
باعتبار أن أي أداء سجل بنسبة 75% أو أكثر من درجة الإنسان يعتبر مماثلاً أو
أفضل من مستوى اللعب البشري، خلص</span> **Mnih** <span dir="rtl">وزملاؤه
إلى أن مستويات اللعب التي تعلمها</span> **DQN** <span dir="rtl">وصلت إلى
أو تجاوزت المستوى البشري في 29 من الألعاب الـ 46. لمزيد من التفاصيل حول
هذه النتائج، راجع</span> **Mnih et al. (2015)**<span dir="rtl">.</span>

<span dir="rtl">إن تحقيق نظام تعلم اصطناعي لهذه المستويات من اللعب سيكون
مثيرًا للإعجاب بما فيه الكفاية، لكن ما يجعل هذه النتائج ملحوظة—وما اعتبره
الكثيرون في ذلك الوقت اختراقًا في مجال **الذكاء الاصطناعي**
</span>**(Artificial Intelligence)** <span dir="rtl">هو أن نفس نظام
التعليم حقق هذه المستويات من اللعب في ألعاب متباينة بشكل كبير دون
الاعتماد على أي تعديلات خاصة بكل لعبة</span>.

<span dir="rtl">عند لعب أي من هذه الألعاب الـ 49 على **أتاري**
</span>**(Atari)**<span dir="rtl">، يرى الإنسان إطارات صور</span>
**210×160 <span dir="rtl">بكسل</span>** <span dir="rtl">مع 128 لونًا
بمعدل</span> **60 <span dir="rtl">هرتز</span>
(60Hz)**<span dir="rtl">.</span> <span dir="rtl">من حيث المبدأ، يمكن أن
تكون هذه الصور بالضبط قد شكلت المدخلات الخام لـ</span>
**DQN**<span dir="rtl">، ولكن لتقليل متطلبات الذاكرة والمعالجة،
قام</span> **Mnih** <span dir="rtl">وزملاؤه بمعالجة مسبقة لكل إطار
لإنتاج مصفوفة</span> **84×84** <span dir="rtl">من قيم الإضاءة. نظرًا لأن
الحالات الكاملة للعديد من ألعاب **أتاري**</span> **(Atari)**
<span dir="rtl">ليست قابلة للملاحظة بشكل كامل من إطارات الصور،
قام</span> **Mnih** <span dir="rtl">وزملاؤه "بتكديس</span>"
**<span dir="rtl">أربعة من الإطارات الأكثر حداثة</span> (Stacked the
Four Most Recent Frames)** <span dir="rtl">بحيث تكون المدخلات إلى الشبكة
ذات أبعاد</span> **84×84×4**<span dir="rtl">.</span> <span dir="rtl">لم
يقضِ هذا على القابلية الجزئية للملاحظة لجميع الألعاب، ولكنه كان مفيدًا في
جعل العديد منها أكثر ماركوفية</span>.

<span dir="rtl">نقطة أساسية هنا هي أن هذه الخطوات المسبقة كانت هي نفسها
بالضبط لجميع الألعاب الـ 46. لم يتم استخدام أي معرفة مسبقة خاصة بلعبة
معينة باستثناء الفهم العام بأنه يجب أن يكون من الممكن تعلم سياسات جيدة
باستخدام هذا البعد المخفض، وأن **تكديس الإطارات المجاورة**</span>
**<span dir="rtl">(</span>Stacking Adjacent
<span dir="rtl"></span>Frames<span dir="rtl">)
</span>**<span dir="rtl">يجب أن يساعد في **القابلية الجزئية
للملاحظة**</span> **(Partial Observability)** <span dir="rtl">لبعض
الألعاب. نظرًا لأنه لم يتم استخدام أي معرفة مسبقة خاصة باللعبة بخلاف هذا
القدر الأدنى في معالجة إطارات الصور مسبقًا، يمكننا اعتبار متجهات الإدخال
بحجم</span> **84×84×4** <span dir="rtl">على أنها مدخلات "خام"
لشبكة</span> **DQN**<span dir="rtl">.</span>

<span dir="rtl">البنية الأساسية لـ</span> **DQN** <span dir="rtl">مشابهة
لـ **الشبكة العصبية الاصطناعية الالتفافية العميقة**</span>
**<span dir="rtl">(</span>Deep <span dir="rtl"></span>Convolutional
Artificial Neural Network – ANN<span dir="rtl">)
</span>**<span dir="rtl">الموضحة في **الشكل 9.15** على الرغم من أنه على
عكس تلك الشبكة، يتم التعامل مع **التحت العينة**</span> **(Subsampling)**
<span dir="rtl">في</span> **DQN** <span dir="rtl">كجزء من كل **طبقة
التفافية** </span>**(Convolutional Layer)**<span dir="rtl">، مع **خرائط
الميزات** </span>**(Feature Maps)** <span dir="rtl">المكونة من وحدات لها
فقط اختيار من **الحقول الاستقبالية الممكنة**</span>
**<span dir="rtl">(</span>Possible Receptive
<span dir="rtl"></span>Fields**<span dir="rtl">)</span>.
<span dir="rtl">يحتوي</span> **DQN** <span dir="rtl">على ثلاث **طبقات
التفافية مخفية**</span> **<span dir="rtl">(</span>Hidden Convolutional
<span dir="rtl"></span>Layers<span dir="rtl">)</span>**<span dir="rtl">،
تليها **طبقة مخفية مترابطة بالكامل** </span>**(Fully Connected Hidden
Layer)**<span dir="rtl">، ثم **طبقة الإخراج** </span>**(Output
Layer)**<span dir="rtl">.</span> <span dir="rtl">تنتج الطبقات الالتفافية
المخفية الثلاث المتعاقبة لـ</span> **DQN** **32 <span dir="rtl">خريطة
ميزات بحجم 20×20</span> (20×20 Feature Maps)**<span dir="rtl">،</span>
**64 <span dir="rtl">خريطة ميزات بحجم 9×9</span>
<span dir="rtl">(</span>9×9 <span dir="rtl"></span>Feature
Maps<span dir="rtl">)</span>**<span dir="rtl">، و</span>**64
<span dir="rtl">خريطة ميزات بحجم 7×7</span> (7×7 Feature
Maps)**<span dir="rtl">.</span> **<span dir="rtl">وظيفة التفعيل</span>
(Activation Function)** <span dir="rtl">لوحدات كل **خريطة ميزات**</span>
**(Feature Map)** <span dir="rtl">هي **اللاخطية المعدلة**
</span>**(Rectifier Nonlinearity)** (max(0,x))<span dir="rtl">.</span>
<span dir="rtl">تتصل جميع **الوحدات الـ 3,136**</span> **(64×7×7)**
<span dir="rtl">في هذه **الطبقة الالتفافية الثالثة**</span> **(Third
Convolutional Layer)** <span dir="rtl">بكل واحدة من</span> **512
<span dir="rtl">وحدة</span> <span dir="rtl">(</span>512
Units<span dir="rtl">) </span>**<span dir="rtl">في **الطبقة المخفية
المترابطة بالكامل** </span>**(Fully Connected Hidden
Layer)**<span dir="rtl">، والتي تتصل بعد ذلك بكل</span> **18
<span dir="rtl">وحدة في طبقة الإخراج</span> (18 Units in the Output
Layer)**<span dir="rtl">، واحدة لكل **إجراء ممكن**</span> **(Possible
Action)** <span dir="rtl">في لعبة **أتاري**
</span>**(Atari)**<span dir="rtl">.</span> <span dir="rtl">كانت
**مستويات التفعيل**</span> **<span dir="rtl">(</span>Activation
<span dir="rtl"></span>Levels<span dir="rtl">)
</span>**<span dir="rtl">لوحدات الإخراج في</span> **DQN**
<span dir="rtl">هي **قيم الإجراءات المثلى المقدرة**</span>
**<span dir="rtl">(</span>Estimated Optimal
<span dir="rtl"></span>Action Values<span dir="rtl">)
</span>**<span dir="rtl">لأزواج **الحالة-الإجراء**</span>
**(State-Action Pairs)** <span dir="rtl">المقابلة، للحالة التي تمثلها
مدخلات الشبكة. كان **تخصيص وحدات الإخراج**</span> **(Assignment of
Output Units)** <span dir="rtl">لإجراءات اللعبة يختلف من لعبة إلى أخرى،
ولأن عدد **الإجراءات الصالحة**</span> **(Valid Actions)**
<span dir="rtl">يتراوح بين 4 و18 للألعاب، لم يكن لجميع وحدات الإخراج
أدوار وظيفية في جميع الألعاب. من المفيد التفكير في الشبكة كما لو
كانت</span> **18 <span dir="rtl">شبكة منفصلة</span> (18 Separate
Networks)**<span dir="rtl">، واحدة لتقدير **القيمة المثلى
للإجراء**</span> **(Optimal Action Value)** <span dir="rtl">لكل إجراء
ممكن. في الواقع، كانت هذه **الشبكات** </span>**(Networks)**
<span dir="rtl">تشترك في **طبقاتها الأولية** </span>**(Initial
Layers)**<span dir="rtl">، ولكن وحدات الإخراج تعلمت استخدام **الميزات
المستخرجة**</span> **(Extracted Features)** <span dir="rtl">بواسطة هذه
**الطبقات**</span> **(Layers)** <span dir="rtl">بطرق مختلفة</span>.

<span dir="rtl">كانت إشارة المكافأة في</span> **DQN**
<span dir="rtl">تشير إلى كيفية تغير درجة اللعبة من خطوة زمنية إلى
أخرى</span>: **+1** <span dir="rtl">كلما زادت،</span> **-1**
<span dir="rtl">كلما انخفضت، و</span>**0** <span dir="rtl">في الحالات
الأخرى. هذا أدى إلى **توحيد إشارة المكافأة** </span>**(Standardizing the
Reward Signal)** <span dir="rtl">عبر الألعاب وجعل **معامل حجم الخطوة
الفردي** </span>**(Single Step-size Parameter)** <span dir="rtl">يعمل
بشكل جيد لجميع الألعاب على الرغم من اختلاف نطاقات درجاتها. استخدم</span>
**DQN** <span dir="rtl">سياسة</span> **ϵ-greedy**<span dir="rtl">، مع
تقليل</span> $`\epsilon`$ <span dir="rtl">بشكل خطي خلال المليون إطار
الأولى وظل عند قيمة منخفضة لبقية جلسة التعليم. تم اختيار قيم **المعاملات
المختلفة**</span> **<span dir="rtl">(</span>Various
<span dir="rtl"></span>Other
Parameters<span dir="rtl">)</span>**<span dir="rtl">، مثل **حجم خطوة
التعليم** </span>**(Learning Step Size)**<span dir="rtl">، **معدل
الخصم** </span>**(Discount Rate)**<span dir="rtl">، وغيرها من القيم
الخاصة بالتنفيذ، من خلال إجراء بحث غير رسمي لرؤية أي القيم تعمل بشكل
أفضل لمجموعة صغيرة من الألعاب. ثم تم تثبيت هذه القيم لجميع
الألعاب</span>.

<span dir="rtl">بعد أن اختار</span> **DQN** <span dir="rtl">إجراءً، تم
تنفيذ الإجراء بواسطة **محاكي اللعبة** </span>**(Game
Emulator)**<span dir="rtl">، الذي أعاد **مكافأة**</span> **(Reward)**
<span dir="rtl">والإطار الفيديوي التالي. تم **معالجة الإطار
مسبقًا**</span> **(Preprocessed)** <span dir="rtl">وأضيف إلى **تكديس
الإطارات الأربعة**</span> **(Four-Frame Stack)** <span dir="rtl">التي
أصبحت المدخلات التالية للشبكة. باستخدام شكل **شبه التدرج**</span>
**(Semi-gradient Form)** <span dir="rtl">من</span> **Q-learning**
<span dir="rtl">لتحديث أوزان الشبكة، استخدم</span> **DQN**
<span dir="rtl">المعادلة التالية</span>:

``` math
w_{t + 1} = w_{t} + \alpha\left\lbrack R_{t + 1} + \gamma\max_{a}\widehat{q}\left( S_{t + 1},a,w_{t} \right) - \widehat{q}\left( S_{t},A_{t},w_{t} \right) \right\rbrack\nabla\widehat{q}\left( S_{t},A_{t},w_{t} \right)
```

<span dir="rtl">حيث أن</span> wt <span dir="rtl">هو **متجه أوزان
الشبكة** </span>**(Vector of the Network’s Weights)**<span dir="rtl">،
و</span>At <span dir="rtl">هو **الإجراء المختار في** </span>**t
<span dir="rtl">الخطوة الزمنية</span> (Action Selected at Time Step
t)**<span dir="rtl">، و</span>St <span dir="rtl"></span>​
<span dir="rtl">و</span>$`S_{t + 1}`$ <span dir="rtl">هما **مكدسات الصور
المعالجة مسبقًا**</span> **(Preprocessed Image Stacks)**
<span dir="rtl">المدخلة إلى الشبكة في الخطوتين الزمنيتين</span> t
<span dir="rtl">و</span>t+1 <span dir="rtl">على التوالي</span>.

<span dir="rtl">تم حساب **التدرج**</span> **(Gradient)**
<span dir="rtl">في المعادلة (16.3) باستخدام **خوارزمية الانتشار العكسي**
</span>**(Backpropagation Algorithm)**<span dir="rtl">. وبالنظر إلى أنه
كان هناك شبكة منفصلة لكل إجراء، في التحديث عند الخطوة الزمنية</span>
t<span dir="rtl">، تم تطبيق **الانتشار العكسي**</span>
**(Backpropagation)** <span dir="rtl">فقط على الشبكة المقابلة لـ</span>
At<span dir="rtl">. استفاد</span> **Mnih** <span dir="rtl">وزملاؤه من
تقنيات أثبتت أنها تحسن الخوارزمية الأساسية **للانتشار العكسي**</span>
**(Backpropagation Algorithm)** <span dir="rtl">عند تطبيقها على
**الشبكات الكبيرة**</span> **<span dir="rtl">(</span>Large
<span dir="rtl"></span>Networks<span dir="rtl">)</span>**.
<span dir="rtl">استخدموا طريقة **الدُفعة الصغيرة**</span> **(Mini-Batch
Method)** <span dir="rtl">التي تحدث الأوزان فقط بعد تراكم معلومات التدرج
على دفعة صغيرة من الصور (هنا بعد 32 صورة). أدى ذلك إلى **تدرجات عينية
أكثر سلاسة**</span> **(Smoother Sample Gradients)**
<span dir="rtl">مقارنة بالإجراء المعتاد الذي يحدث الأوزان بعد كل إجراء.
كما استخدموا خوارزمية **صعود التدرج** </span>**(Gradient-Ascent
Algorithm)** <span dir="rtl">تسمى</span> **RMSProp** (Tieleman and
Hinton, 2012) <span dir="rtl">التي تسرع عملية التعليم من خلال ضبط
**معامل حجم الخطوة**</span> **(Step-Size Parameter)**
<span dir="rtl">لكل وزن بناءً على **متوسط متحرك**</span>
**<span dir="rtl">(</span>Running
<span dir="rtl"></span>Average<span dir="rtl">)</span>**
<span dir="rtl">لأحجام التدرجات الأخيرة لذلك الوزن</span>.

<span dir="rtl">قام</span> **Mnih** <span dir="rtl">وزملاؤه بتعديل
الإجراء الأساسي **لـ**</span> **Q-learning** <span dir="rtl">بثلاث طرق.
أولاً، استخدموا طريقة تسمى **إعادة تشغيل التجربة**</span> **(Experience
Replay)** <span dir="rtl">التي تمت دراستها لأول مرة بواسطة</span> **Lin
(1992)**<span dir="rtl">. تخزن هذه الطريقة **تجربة الوكيل**</span>
**(Agent's Experience)** <span dir="rtl">في كل خطوة زمنية في **ذاكرة
إعادة التشغيل**</span> **(Replay Memory)** <span dir="rtl">التي يتم
الوصول إليها لأداء تحديثات الأوزان. عملت هذه الطريقة في</span> **DQN**
<span dir="rtl">كما يلي: بعد أن نفذ **محاكي اللعبة**</span> **(Game
Emulator)** <span dir="rtl">الإجراء</span> At <span dir="rtl">في حالة
ممثلة بـ</span> **St​ <span dir="rtl">مكدس الصور</span> (Image Stack
St​)**<span dir="rtl">، وأعاد **المكافأة** </span>$`R_{t + 1}`$
**<span dir="rtl">(</span>Reward​<span dir="rtl">)</span>**
<span dir="rtl">و**مكدس الصور** </span>$`S_{t + 1}`$
<span dir="rtl"></span>**(Image Stack** $`S_{t + 1}`$
**​)**<span dir="rtl">، أضاف الزوج</span> St​,$`A_{t}`$,Rt+1​,​
<span dir="rtl">إلى **ذاكرة إعادة التشغيل**</span>
**<span dir="rtl">(</span>Memory<span dir="rtl">)</span>**.
<span dir="rtl">هذه الذاكرة تراكمت فيها التجارب على مدار العديد من مرات
اللعب لنفس اللعبة. في كل خطوة زمنية، تم تنفيذ تحديثات متعددة **لـ**
</span>**Q-learning** <span dir="rtl">كـ **دفعة صغيرة**
</span>**(Mini-Batch)** <span dir="rtl">بناءً على تجارب تم أخذ عينات منها
بشكل عشوائي موحد من **ذاكرة إعادة التشغيل** </span>**(Replay
Memory)**<span dir="rtl">.</span> <span dir="rtl">بدلاً من أن تصبح</span>
$`S_{t + 1}`$ <span dir="rtl">هي</span> St <span dir="rtl"></span>​
<span dir="rtl">الجديدة للتحديث التالي كما هو الحال في الشكل المعتاد
لـ</span> **Q-learning**<span dir="rtl">، تم سحب تجربة جديدة غير متصلة
من **ذاكرة إعادة التشغيل**</span> **(Replay Memory)**
<span dir="rtl">لتوفير البيانات للتحديث التالي. نظرًا لأن</span>
**Q-learning** <span dir="rtl">هو **خوارزمية خارج السياسة**
</span>**(Off-Policy Algorithm)**<span dir="rtl">، فإنه لا يحتاج إلى أن
يُطبق على طول المسارات المتصلة</span>.

<span dir="rtl">قدمت</span> **Q-learning** <span dir="rtl">مع **إعادة
تشغيل التجربة**</span> **(Experience Replay)** <span dir="rtl">عدة مزايا
على الشكل المعتاد لـ</span> **Q-learning**<span dir="rtl">.</span>
<span dir="rtl">القدرة على استخدام كل تجربة مخزنة للعديد من التحديثات
سمحت لـ</span> **DQN** <span dir="rtl">بالتعليم بكفاءة أكبر من تجاربه.
قللت **إعادة تشغيل التجربة**</span> **(Experience Replay)**
<span dir="rtl">من **تباين التحديثات**</span> **(Variance of the
Updates)** <span dir="rtl">لأن التحديثات المتتالية لم تكن مترابطة مع
بعضها البعض كما كانت ستكون مع</span> **Q-learning**
<span dir="rtl">القياسية. وعن طريق إزالة اعتماد التجارب المتتالية على
الأوزان الحالية، أزالت **إعادة تشغيل التجربة**</span> **(Experience
Replay)** <span dir="rtl">أحد مصادر عدم الاستقرار</span>.

<span dir="rtl">قام</span> **Mnih** <span dir="rtl">وزملاؤه
بتعديل</span> **Q-learning** <span dir="rtl">القياسية بطريقة ثانية
لتحسين استقرارها. كما هو الحال في طرق أخرى تعتمد على</span>
**bootstrap**<span dir="rtl">، يعتمد الهدف لتحديث</span> **Q-learning**
<span dir="rtl">على التقدير الحالي **لدالة قيمة الإجراء**
</span>**(Action-Value Function Estimate)**<span dir="rtl">.</span>
<span dir="rtl">عند استخدام طريقة **تقريب دالة بارامترية**
</span>**(Parameterized Function Approximation Method)**
<span dir="rtl">لتمثيل **قيم الإجراءات**</span>
**<span dir="rtl">(</span>Action
<span dir="rtl"></span>Values<span dir="rtl">)</span>**<span dir="rtl">،
يكون الهدف دالة للبارامترات نفسها التي يتم تحديثها. على سبيل المثال،
الهدف في التحديث المعطى بواسطة المعادلة (16.3) هو</span>
max⁡aq^($`S_{t + 1}`$,a,wt)<span dir="rtl">،</span> a,.
<span dir="rtl">يعتمد هذا الهدف على</span> $`wt`$ <span dir="rtl">مما
يعقد العملية مقارنة بحالة **التعليم الخاضع للإشراف الأبسط**</span>
**<span dir="rtl">(</span>Simpler Supervised-Learning
<span dir="rtl"></span>Situation<span dir="rtl">)</span>**
<span dir="rtl">حيث الأهداف لا تعتمد على البارامترات التي يتم تحديثها.
كما نوقش في **الفصل 11**، يمكن أن يؤدي ذلك إلى **تذبذبات و/أو تباعد**
</span>**(Oscillations and/or Divergence)**<span dir="rtl">.</span>
<span dir="rtl">لمعالجة هذه المشكلة، استخدم</span> **Mnih**
<span dir="rtl">وزملاؤه تقنية تقرب</span> **Q-learning**
<span dir="rtl">أكثر إلى حالة **التعليم الخاضع للإشراف الأبسط**</span>
**(Simpler Supervised-Learning Case)** <span dir="rtl">مع السماح لها
بالاعتماد على</span> **bootstrap**<span dir="rtl">. كلما تم إجراء عدد
معين،</span> C<span dir="rtl">، من التحديثات على أوزان</span> $`w`$
<span dir="rtl">الخاصة بشبكة **قيمة الإجراء** </span>**(Action-Value
Network)**<span dir="rtl">، قاموا بإدراج الأوزان الحالية للشبكة في شبكة
أخرى وثبتوا هذه الأوزان المكررة للتحديثات الـ</span> C
<span dir="rtl">التالية لـ</span> $`w`$<span dir="rtl">.</span>
<span dir="rtl">تم استخدام مخرجات هذه الشبكة المكررة كأهداف</span>
**Q-learning** <span dir="rtl">على مدار التحديثات الـ</span> C
<span dir="rtl">التالية لـ</span> $`w`$<span dir="rtl">.</span>
<span dir="rtl">لنفترض أن</span> $`q\sim`$ <span dir="rtl">ترمز إلى
مخرجات هذه الشبكة المكررة، عندئذٍ بدلاً من (16.3) كانت قاعدة التحديث كما
يلي</span>:

``` math
w_{t + 1} = w_{t} + \alpha\left\lbrack R_{t + 1} + \gamma\max_{a}\widetilde{q}\left( S_{t + 1},a,w_{t} \right) - \widehat{q}\left( S_{t},A_{t},w_{t} \right) \right\rbrack\nabla\widehat{q}\left( S_{t},A_{t},w_{t} \right)
```

<span dir="rtl">تم العثور على تعديل أخير في</span> **Q-learning**
<span dir="rtl">القياسية ليحسن الاستقرار أيضًا. لقد قاموا بقص **مصطلح
الخطأ** </span>**(Error Term)** Rt+1+γmax⁡aq~($`S_{t + 1}`$,a,wt)
−q^(St,At,wt) <span dir="rtl"></span>​, <span dir="rtl">بحيث يبقى ضمن
النطاق</span> \[−1,1\]<span dir="rtl">.</span>

<span dir="rtl">قام</span> **Mnih** <span dir="rtl">وزملاؤه بإجراء عدد
كبير من عمليات التعليم على 5 من الألعاب لاكتساب نظرة ثاقبة على تأثير
مختلف ميزات تصميم</span> **DQN** <span dir="rtl">على أدائها. قاموا
بتشغيل</span> **DQN** <span dir="rtl">مع التوليفات الأربعة من **إعادة
تشغيل التجربة**</span> **(Experience Replay)** <span dir="rtl">و**الشبكة
الهدف المكررة**</span> **<span dir="rtl">(</span>Duplicate Target
<span dir="rtl"></span>Network<span dir="rtl">)
</span>**<span dir="rtl">سواء تم تضمينها أم لا. على الرغم من أن النتائج
اختلفت من لعبة إلى أخرى، إلا أن كل واحدة من هذه الميزات وحدها حسنت
الأداء بشكل كبير، وحسنت الأداء بشكل درامي عندما تم استخدامها معًا.
قام</span> **Mnih** <span dir="rtl">وزملاؤه أيضًا بدراسة الدور الذي تلعبه
**الشبكة العصبية الاصطناعية الالتفافية العميقة**</span> **(Deep
Convolutional ANN)** <span dir="rtl">في قدرة</span> **DQN**
<span dir="rtl">على التعليم من خلال مقارنة الإصدار الالتفافي العميق
من</span> **DQN** <span dir="rtl">مع إصدار يحتوي على **شبكة ذات طبقة
خطية واحدة**</span> **<span dir="rtl">(</span>Network with
<span dir="rtl"></span>Just One Linear
Layer<span dir="rtl">)</span>**<span dir="rtl">، وكلاهما يتلقى نفس
**إطارات الفيديو المكدسة المعالجة مسبقًا** </span>**(Stacked Preprocessed
Video Frames)**<span dir="rtl">. هنا، كان التحسن في الإصدار الالتفافي
العميق مقارنةً بالإصدار الخطي ملحوظًا بشكل خاص عبر جميع الألعاب الخمس التي
تم اختبارها</span>.

<span dir="rtl">إن إنشاء وكلاء اصطناعيين يتفوقون على مجموعة متنوعة من
المهام التحديّة كان هدفًا دائمًا للذكاء الاصطناعي. وقد أحبط الوعد بتعلم
الآلة كوسيلة لتحقيق هذا الهدف بسبب الحاجة إلى تصميم تمثيلات خاصة
بالمشكلة.</span> **DQN** <span dir="rtl">من</span> **DeepMind**
<span dir="rtl">يمثل خطوة كبيرة إلى الأمام من خلال إظهار أن وكيلًا واحدًا
يمكنه تعلم **ميزات خاصة بالمشكلة**</span> **(Problem-Specific
Features)** <span dir="rtl">تمكنه من اكتساب مهارات تنافسية بشرية عبر
مجموعة من المهام. لم تنتج هذه التجربة وكيلًا يتفوق في جميع المهام في نفس
الوقت (لأن التعليم تم بشكل منفصل لكل مهمة)، لكنها أظهرت أن التعليم
العميق يمكن أن يقلل، وربما يلغي، الحاجة إلى تصميم وضبط خاص بالمشكلة. ومع
ذلك، كما يشير</span> **Mnih** <span dir="rtl">وزملاؤه،</span> **DQN**
<span dir="rtl">ليس حلًا كاملًا لمشكلة التعليم المستقل عن المهمة. على
الرغم من أن المهارات اللازمة للتفوق في ألعاب **أتاري**</span>
**(Atari)** <span dir="rtl">كانت متنوعة بشكل ملحوظ، إلا أن جميع الألعاب
تم لعبها من خلال مراقبة **الصور الفيديوية** </span>**(Video
Images)**<span dir="rtl">، مما جعل **الشبكة العصبية الاصطناعية
الالتفافية العميقة** </span>**(Deep Convolutional ANN)**
<span dir="rtl">خيارًا طبيعيًا لهذه المجموعة من المهام. بالإضافة إلى ذلك،
كان أداء</span> **DQN** <span dir="rtl">في بعض ألعاب **أتاري
2600**</span> **(Atari 2600)** <span dir="rtl">أقل بكثير من مستويات
المهارة البشرية في هذه الألعاب. كانت الألعاب الأكثر صعوبة لـ</span>
**DQN** <span dir="rtl">خاصة</span> **Montezuma's Revenge**
<span dir="rtl">التي تعلم</span> **DQN** <span dir="rtl">اللعب فيها
بمستوى مشابه للعب العشوائي—تتطلب تخطيطًا عميقًا يتجاوز ما تم تصميم</span>
**DQN** <span dir="rtl">للقيام به. علاوة على ذلك، فإن تعلم مهارات التحكم
من خلال الممارسة المكثفة، مثلما تعلم</span> **DQN**
<span dir="rtl">كيفية لعب ألعاب **أتاري**
</span>**(Atari)**<span dir="rtl">، هو مجرد أحد أنواع التعليم التي
يحققها البشر بشكل روتيني</span>.

<span dir="rtl">على الرغم من هذه القيود، فإن</span> **DQN**
<span dir="rtl">قد تقدم بحالة الفن في **تعلم الآلة**</span> **(Machine
Learning)** <span dir="rtl">من خلال إظهار مثير للإعجاب لوعد الجمع بين
**التعليم المعزز** </span>**(Reinforcement Learning)**
<span dir="rtl">و**أساليب التعليم العميق الحديثة** </span>**(Modern
Methods of Deep Learning)**<span dir="rtl">.</span>

**<u>16.6 <span dir="rtl">إتقان لعبة الجو</span> (Mastering the Game of
Go)</u>**

<span dir="rtl">لقد تحدت لعبة **الجو**</span> **(Go)**
<span dir="rtl">الصينية القديمة الباحثين في مجال **الذكاء
الاصطناعي**</span> **<span dir="rtl">(</span>Artificial
<span dir="rtl"></span>Intelligence<span dir="rtl">)</span>**
<span dir="rtl">لعقود عديدة. لم تكن الطرق التي تحقق مهارة على مستوى
البشر، أو حتى مستوى يفوق البشر، في ألعاب أخرى ناجحة في إنتاج برامج
**جو**</span> **(Go)** <span dir="rtl">قوية. بفضل مجتمع مبرمجي **الجو**
</span>**(Go)** <span dir="rtl">النشط للغاية والمسابقات الدولية، تحسن
مستوى لعب برامج **الجو**</span> **(Go)** <span dir="rtl">بشكل كبير على
مر السنين. حتى وقت قريب، لم يكن أي برنامج **جو**</span> **(Go)**
<span dir="rtl">قادرًا على اللعب بمستوى قريب من مستوى **أستاذ جو بشري**
</span>**(Human Go Master)**<span dir="rtl">.</span>

<span dir="rtl">طور فريق في</span> **DeepMind** (Silver et al., 2016)
<span dir="rtl">برنامج</span> **AlphaGo** <span dir="rtl">الذي كسر هذا
الحاجز من خلال الجمع بين **الشبكات العصبية الاصطناعية العميقة**</span>
**<span dir="rtl">(</span>Deep Artificial<span dir="rtl">  
</span>Neural Networks <span dir="rtl"></span>- ANNs)**
(<span dir="rtl">انظر **الفصل 9.6**)، **التعليم الخاضع للإشراف**</span>
**<span dir="rtl">(</span>Supervised
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**<span dir="rtl">،
**البحث في شجرة مونت كارلو** </span>**(Monte Carlo Tree Search - MCTS)**
<span dir="rtl">  
(انظر **الفصل 8.11**)، و**التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">.</span> <span dir="rtl">بحلول وقت نشر</span>
**Silver** <span dir="rtl">وزملائه في عام 2016، كان قد أُثبت أن</span>
**AlphaGo** <span dir="rtl">أقوى بشكل حاسم من برامج **الجو**</span>
**(Go)** <span dir="rtl">الأخرى في ذلك الوقت، وكان قد هزم بطل
**الجو**</span> **(Go)** <span dir="rtl">الأوروبي</span> **Fan Hui**
<span dir="rtl">بنتيجة 5-0. كانت هذه هي الانتصارات الأولى لبرنامج
**جو**</span> **(Go)** <span dir="rtl">على لاعب **جو**</span> **(Go)**
<span dir="rtl">محترف بشري بدون إعاقات في ألعاب **جو**</span> **(Go)**
<span dir="rtl">كاملة. بعد ذلك بوقت قصير، حققت نسخة مشابهة من</span>
**AlphaGo** <span dir="rtl">انتصارات مذهلة على بطل العالم 18 مرة</span>
**Lee Sedol**<span dir="rtl">، بفوزها 4 مرات من أصل 5 مباريات في مباراة
تحدي، مما جعلها في عناوين الأخبار العالمية. كان الباحثون في **الذكاء
الاصطناعي**</span> **(Artificial Intelligence)** <span dir="rtl">يعتقدون
أنه سيستغرق الأمر العديد من السنوات، وربما عقودًا، قبل أن يصل برنامج إلى
هذا المستوى من اللعب</span>.

<span dir="rtl">هنا نصف برنامج</span> **AlphaGo**
<span dir="rtl">وبرنامجًا خليفة له يُدعى</span> **AlphaGo Zero**
<span dir="rtl">(</span>Silver et al.,
<span dir="rtl"></span>2017a<span dir="rtl">)</span>.
<span dir="rtl">حيث اعتمد</span> **AlphaGo** <span dir="rtl">بالإضافة
إلى **التعليم المعزز** </span>**(Reinforcement Learning)**
<span dir="rtl">على **التعليم الخاضع للإشراف**</span> **(Supervised
Learning)** <span dir="rtl">من قاعدة بيانات كبيرة من تحركات الخبراء
البشريين، استخدم</span> **AlphaGo Zero** **<span dir="rtl">التعليم
المعزز</span> (Reinforcement Learning)** <span dir="rtl">فقط دون أي
بيانات أو توجيه بشري باستثناء القواعد الأساسية للعبة (ومن هنا جاءت
تسمية</span> **Zero**<span dir="rtl">)</span>. <span dir="rtl">نصف
أولاً</span> **AlphaGo** <span dir="rtl">بتفصيل لنبرز البساطة النسبية
لـ</span> **AlphaGo Zero**<span dir="rtl">، الذي يتميز بأداء أعلى ويعتبر
برنامجًا أكثر نقاءً في **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">.</span>

<img src="./media/image189.png"
style="width:2.90625in;height:3.09861in" /><span dir="rtl">بطرق عديدة،
يعتبر كل من</span> **AlphaGo** <span dir="rtl">و</span>**AlphaGo Zero**
<span dir="rtl">خلفاء لـ</span> **TD-Gammon** <span dir="rtl">الخاص
بـ</span> **Tesauro** <span dir="rtl">(انظر **الفصل 16.1**)، الذي هو
نفسه خليفة للاعب **الداما**</span> **(Checkers)** <span dir="rtl">الخاص
بـ</span> **Samuel** <span dir="rtl">(انظر **الفصل 16.2**)</span>.
<span dir="rtl">تضمنت جميع هذه البرامج **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**
<span dir="rtl">عبر محاكاة ألعاب **اللعب الذاتي**
</span>**(Self-Play)**<span dir="rtl">. كما اعتمد</span> **AlphaGo**
<span dir="rtl">و</span>**AlphaGo Zero** <span dir="rtl">على التقدم الذي
أحرزته</span> **DeepMind** <span dir="rtl">في لعب ألعاب **أتاري**</span>
**(Atari)** <span dir="rtl">مع برنامج</span> **DQN**
<span dir="rtl">(انظر **الفصل 16.5**)، الذي استخدم **الشبكات العصبية
الاصطناعية الالتفافية العميقة**</span> **(Deep Convolutional ANNs)**
<span dir="rtl">لتقريب **دوال القيمة المثلى** </span>**(Optimal Value
Functions)**<span dir="rtl">.  
**الجو**</span> **(Go)** <span dir="rtl">هي لعبة بين لاعبين يتناوبان على
وضع **الأحجار السوداء والبيضاء**</span> **<span dir="rtl">(</span>Black
and <span dir="rtl"></span>White Stones<span dir="rtl">)
</span>**<span dir="rtl">على **تقاطع غير مشغول** </span>**(Unoccupied
Intersections)** <span dir="rtl">أو **نقاط**</span> **(Points)**
<span dir="rtl">على لوحة بها شبكة من 19 خطًا أفقيًا و19 خطًا عموديًا لإنتاج
مواقع مثل تلك الموضحة على اليمين. الهدف من اللعبة هو السيطرة على مساحة
من اللوحة أكبر من تلك التي يسيطر عليها الخصم. يتم التقاط الأحجار وفقًا
لقواعد بسيطة. يتم التقاط أحجار اللاعب إذا كانت محاطة تمامًا بأحجار اللاعب
الآخر، مما يعني أنه لا توجد نقطة مجاورة أفقياً أو عموديًا غير مشغولة. على
سبيل المثال، يظهر **الشكل 16.5** على اليسار ثلاثة أحجار بيضاء مع نقطة
مجاورة غير مشغولة</span> <span dir="rtl">(مسمى</span>
$`X`$<span dir="rtl">)</span>. <span dir="rtl">إذا وضع اللاعب الأسود
حجرًا على</span> $`X`$<span dir="rtl">، يتم التقاط الأحجار البيضاء
الثلاثة وإزالتها من اللوحة</span> <span dir="rtl">(**الشكل 16.5** في
الوسط). ومع ذلك، إذا وضع اللاعب الأبيض حجرًا على النقطة</span> $`X`$
<span dir="rtl">أولاً، فسيتم منع إمكانية هذا الالتقاط</span>
<span dir="rtl">(**الشكل 16.5** على اليمين). هناك قواعد أخرى ضرورية لمنع
الحلقات اللانهائية من الالتقاط/إعادة الالتقاط</span>.

<img src="./media/image190.png"
style="width:4.76708in;height:1.65014in" />

<span dir="rtl">**الشكل 16.5**:</span> <span dir="rtl">قاعدة الالتقاط في
**لعبة الجو** </span>**(Go)**<span dir="rtl">.</span>
**<span dir="rtl">يسارًا</span> (Left)**<span dir="rtl">:</span>
<span dir="rtl">الأحجار البيضاء الثلاثة غير محاطة بالكامل لأن
النقطة</span> $`X`$ <span dir="rtl">غير مشغولة.</span>
**<span dir="rtl">وسط</span> (Middle)**<span dir="rtl">:</span>
<span dir="rtl">إذا وضع اللاعب الأسود حجرًا على</span>
$`X`$<span dir="rtl">، يتم التقاط الأحجار البيضاء الثلاثة وإزالتها من
اللوحة.</span> **<span dir="rtl">يمينًا</span>
(Right)<span dir="rtl">:</span>** <span dir="rtl">إذا وضع اللاعب الأبيض
حجرًا على النقطة</span> X <span dir="rtl">أولاً، يتم منع الالتقاط</span>.

<span dir="rtl">هذه القواعد بسيطة، لكنها تنتج لعبة معقدة للغاية ولها
جاذبية واسعة لآلاف السنين. لم تعمل الطرق التي تنتج لعبًا قويًا في ألعاب
أخرى، مثل الشطرنج، بشكل جيد مع **الجو**
</span>**(Go)**<span dir="rtl">.</span> <span dir="rtl">مساحة البحث في
**الجو** </span>**(Go)** <span dir="rtl">أكبر بكثير من تلك في الشطرنج
لأن **الجو**</span> **(Go)** <span dir="rtl">يحتوي على عدد أكبر من
**الحركات القانونية** </span>**(Legal Moves)** <span dir="rtl">لكل موضع
مقارنة بالشطرنج (~250 مقابل ~35) وتستغرق ألعاب **الجو** </span>**(Go)**
<span dir="rtl">عادةً حركات أكثر من ألعاب الشطرنج (~150 مقابل ~80). ولكن
حجم مساحة البحث ليس العامل الرئيسي الذي يجعل **الجو**</span> **(Go)**
<span dir="rtl">صعبًا للغاية. البحث الشامل غير ممكن لكل من الشطرنج
و**الجو** </span>**(Go)**<span dir="rtl">، وقد أثبتت **الجو**</span>
**(Go)** <span dir="rtl">على ألواح أصغر (مثل 9 × 9) أنها صعبة للغاية
أيضًا. يتفق الخبراء على أن العقبة الرئيسية لإنشاء برامج **جو**</span>
**(Go)** <span dir="rtl">أقوى من مستوى الهواة هي صعوبة تحديد **دالة
تقييم وضعية كافية** </span>**(Adequate Position Evaluation
Function)**<span dir="rtl">.</span> <span dir="rtl">تسمح **دالة التقييم
الجيدة** </span>**(Good Evaluation Function)** <span dir="rtl">بتقصير
البحث عند عمق ممكن من خلال توفير تنبؤات سهلة نسبيًا حول ما يمكن أن ينتجه
البحث الأعمق. وفقًا لـ</span> **Müller (2002)**<span dir="rtl">:</span>
"<span dir="rtl">لن يتم العثور أبدًا على دالة تقييم بسيطة ومعقولة للـ
**الجو** </span>**(Go)**"<span dir="rtl">.</span> <span dir="rtl">كانت
خطوة كبيرة إلى الأمام هي إدخال **البحث في شجرة مونت كارلو**</span>
**(Monte Carlo Tree Search - MCTS)** <span dir="rtl">في برامج **الجو**
</span>**(Go)**<span dir="rtl">.</span> <span dir="rtl">تضمنت أقوى
البرامج في وقت تطوير</span> **AlphaGo** <span dir="rtl">جميعها</span>
**MCTS**<span dir="rtl">، لكن مهارة **مستوى الأستاذ**</span>
**(Master-Level Skill)** <span dir="rtl">بقيت صعبة المنال</span>.

<span dir="rtl">تذكر من **الفصل 8.11** أن</span> **MCTS**
<span dir="rtl">هي **إجراء تخطيط زمني للقرار**</span>
**<span dir="rtl">(</span>Decision-Time Planning
<span dir="rtl"></span>Procedure<span dir="rtl">)
</span>**<span dir="rtl">لا يحاول تعلم وتخزين **دالة تقييم عالمية**
</span>**(Global Evaluation Function)**<span dir="rtl">.</span>
<span dir="rtl">مثل **خوارزمية التنفيذ**</span> **(Rollout Algorithm)**
<span dir="rtl">(انظر **الفصل 8.10**)، يقوم</span> **MCTS**
<span dir="rtl">بتشغيل العديد من **محاكيات مونت كارلو**</span> **(Monte
Carlo Simulations)** <span dir="rtl">لحلقات كاملة (هنا، ألعاب
**جو**</span> **(Go)** <span dir="rtl">كاملة) لاختيار كل إجراء (هنا، كل
حركة في **الجو** </span>**(Go)**<span dir="rtl">:</span>
<span dir="rtl">أين يتم وضع حجر أو الاستسلام). ولكن على عكس **خوارزمية
التنفيذ البسيطة** </span>**(Simple Rollout Algorithm)**<span dir="rtl">،
فإن</span> **MCTS** <span dir="rtl">هو إجراء تكراري يوسع تدريجيًا **شجرة
البحث**</span> **(Search Tree)** <span dir="rtl">التي تمثل **العقدة
الجذرية**</span> **(Root Node)** <span dir="rtl">فيها **حالة البيئة
الحالية** </span>**(Current Environment State)**<span dir="rtl">.</span>
<span dir="rtl">كما هو موضح في **الشكل 8.10**، كل تكرار يجتاز الشجرة من
خلال **محاكاة**</span> **(Simulating)** <span dir="rtl">الإجراءات التي
توجهها الإحصائيات المرتبطة بحواف الشجرة. في نسخته الأساسية، عندما تصل
المحاكاة إلى **عقدة ورقية**</span> **(Leaf Node)** <span dir="rtl">في
شجرة البحث، يوسع</span> **MCTS** <span dir="rtl">الشجرة عن طريق إضافة
بعض أو كل **عقد الأطفال**</span> **(Child Nodes)**
<span dir="rtl">للعقدة الورقية إلى الشجرة. من العقدة الورقية أو واحدة من
**عقد الأطفال**</span> **(Child Nodes)** <span dir="rtl">الجديدة، يتم
تنفيذ **تنفيذ** </span>**(Rollout)**<span dir="rtl">: محاكاة تستمر عادةً
حتى **حالة نهائية** </span>**(Terminal State)**<span dir="rtl">، مع
تحديد الإجراءات بواسطة **سياسة التنفيذ** </span>**(Rollout
Policy)**<span dir="rtl">.</span> <span dir="rtl">عند اكتمال **التنفيذ**
</span>**(Rollout)**<span dir="rtl">، يتم تحديث الإحصائيات المرتبطة
بحواف شجرة البحث التي تم اجتيازها في هذا التكرار من خلال النسخ الاحتياطي
**للعودة** </span>**(Return)** <span dir="rtl">الناتجة عن **التنفيذ**
</span>**(Rollout)**<span dir="rtl">.</span>
<span dir="rtl">يستمر</span> **MCTS** <span dir="rtl">في هذه العملية،
بدءًا في كل مرة من **الجذر**</span> **(Root)** <span dir="rtl">لشجرة
البحث عند الحالة الحالية، لعدد من التكرارات ممكن نظرًا للقيود الزمنية.
ثم، أخيرًا، يتم اختيار إجراء من **العقدة الجذرية**</span> **(Root Node)**
<span dir="rtl">(التي لا تزال تمثل **حالة البيئة الحالية**
</span>**(Current Environment State)**<span dir="rtl">)</span>
<span dir="rtl">وفقًا للإحصائيات المتراكمة في حواف الخروج من العقدة
الجذرية. هذا هو الإجراء الذي يتخذه الوكيل. بعد انتقال البيئة إلى حالتها
التالية، يتم تنفيذ</span> **MCTS** <span dir="rtl">مرة أخرى مع تعيين
**العقدة الجذرية**</span> **(Root Node)** <span dir="rtl">لتمثيل الحالة
الحالية الجديدة</span>.

<span dir="rtl">قد تكون **شجرة البحث**</span> **(Search Tree)**
<span dir="rtl">في بداية هذه التنفيذية التالية هي فقط **العقدة الجذرية
الجديدة** </span>**(New Root Node)**<span dir="rtl">، أو قد تشمل
**سلالات**</span> **(Descendants)** <span dir="rtl">هذه العقدة المتبقية
من التنفيذ السابق لـ</span> **MCTS**<span dir="rtl">.</span>
<span dir="rtl">يتم التخلص من بقية الشجرة</span>

**<u>16.6.1 <span dir="rtl">ألفا جو</span> (AlphaGo)</u>**

<span dir="rtl">الابتكار الرئيسي الذي جعل</span> **AlphaGo**
<span dir="rtl">لاعبًا قويًا هو أنه اختار الحركات بواسطة نسخة جديدة من
**البحث في شجرة مونت كارلو** </span>**(MCTS)**<span dir="rtl">، التي تم
توجيهها بواسطة كل من **السياسة**</span> **(Policy)**
<span dir="rtl">و**دالة القيمة**</span> **(Value Function)**
<span dir="rtl">التي تم تعلمها عن طريق **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">مع **تقريب الدالة**</span> **(Function
Approximation)** <span dir="rtl">المقدم من **الشبكات العصبية الاصطناعية
الالتفافية العميقة** </span>**(Deep Convolutional
ANNs)**<span dir="rtl">.</span> <span dir="rtl">ميزة رئيسية أخرى هي أن
**التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">لم يبدأ من **أوزان شبكة عشوائية**</span>
**<span dir="rtl">(</span>Random <span dir="rtl"></span>Network
Weights<span dir="rtl">)</span>**<span dir="rtl">، بل بدأ من أوزان كانت
نتيجة **التعليم الخاضع للإشراف السابق**</span>
**<span dir="rtl">(</span>Previous <span dir="rtl"></span>Supervised
Learning<span dir="rtl">) </span>**<span dir="rtl">من مجموعة كبيرة من
حركات الخبراء البشريين</span>.

<span dir="rtl">أطلق فريق</span> **DeepMind** <span dir="rtl">على
تعديل</span> **AlphaGo** <span dir="rtl">لـ</span> **MCTS
<span dir="rtl">الأساسي</span>**
<span dir="rtl"></span>"<span dir="rtl">البحث في شجرة مونت كارلو غير
المتزامن للسياسة والقيمة</span> (Asynchronous Policy and Value
MCTS)"<span dir="rtl">، أو  
</span>**APV-MCTS**<span dir="rtl">.</span> <span dir="rtl">تم اختيار
الإجراءات عبر</span> **MCTS <span dir="rtl">الأساسي</span>**
<span dir="rtl">كما هو موضح أعلاه ولكن مع بعض التعديلات في كيفية توسيع
شجرة البحث وكيفية تقييم حواف الإجراءات. على عكس</span> **MCTS
<span dir="rtl">الأساسي</span>**<span dir="rtl">، الذي يوسع شجرة البحث
الحالية باستخدام **قيم الإجراءات المخزنة** </span>**(Stored Action
Values)** <span dir="rtl">لاختيار حافة غير مستكشفة من **عقدة ورقية**
</span>**(Leaf Node)**<span dir="rtl">، فإن</span>
**APV-MCTS**<span dir="rtl">، كما تم تنفيذه في</span>
**AlphaGo**<span dir="rtl">، قام بتوسيع شجرته من خلال اختيار حافة وفقًا
للاحتمالات التي قدمتها **شبكة عصبية اصطناعية التفافية عميقة مكونة من 13
طبقة** </span>**(13-Layer Deep Convolutional ANN)**<span dir="rtl">،
تسمى **شبكة سياسة التعليم الخاضع للإشراف** </span>**(SL-Policy
Network)**<span dir="rtl">، التي تم تدريبها سابقًا بواسطة **التعليم
الخاضع للإشراف**</span> **(Supervised Learning)** <span dir="rtl">للتنبؤ
بالحركات الموجودة في قاعدة بيانات تحتوي على ما يقرب من 30 مليون حركة
خبير بشري</span>.

<span dir="rtl">ثم، على عكس</span> **MCTS
<span dir="rtl">الأساسي</span>** <span dir="rtl">أيضًا، الذي يقيم **عقدة
الحالة الجديدة المضافة**</span> **<span dir="rtl">(</span>Newly-Added
<span dir="rtl"></span>State Node<span dir="rtl">)
</span>**<span dir="rtl">فقط من خلال عائد **التنفيذ**</span>
**(Rollout)** <span dir="rtl">الذي يبدأ منه، قام</span> **APV-MCTS**
<span dir="rtl">بتقييم العقدة بطريقتين: من خلال هذا العائد من
**التنفيذ**</span> **(Rollout)**<span dir="rtl">، ولكن أيضًا بواسطة
**دالة قيمة**</span> **<span dir="rtl">(</span>Value
<span dir="rtl"></span>Function<span dir="rtl">)</span>**<span dir="rtl">،</span>
**v\_\theta**<span dir="rtl">، التي تم تعلمها سابقًا بواسطة طريقة
**التعليم المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**.
<span dir="rtl">إذا كانت</span> $`\mathbf{s}`$ <span dir="rtl">هي
**العقدة الجديدة المضافة** </span>**(Newly-Added
Node)**<span dir="rtl">، فإن قيمتها تصبح</span>:

``` math
v(s) = (1 - \eta)v_{\theta}(s) + \eta G,
```

<span dir="rtl">حيث كان</span> $`G`$ <span dir="rtl">هو **عائد
التنفيذ**</span> **(Return of the Rollout)**
<span dir="rtl">و</span>$`\eta`$ <span dir="rtl">يتحكم في **خلط
القيم**</span> **<span dir="rtl">(</span>Mixing
<span dir="rtl"></span>of the Values<span dir="rtl">)
</span>**<span dir="rtl">الناتجة عن طريقتي التقييم هاتين. في</span>
**AlphaGo**<span dir="rtl">، تم توفير هذه القيم بواسطة **شبكة القيمة**
</span>**(Value Network)**<span dir="rtl">، وهي **شبكة عصبية اصطناعية
التفافية عميقة مكونة من 13 طبقة** </span>**(13-Layer Deep Convolutional
ANN)** <span dir="rtl">أخرى تم تدريبها كما سنوضح أدناه لإخراج القيم
المقدرة لمواقع اللوحة. كانت عمليات **التنفيذ**</span> **(Rollouts)**
<span dir="rtl">الخاصة بـ</span> **APV-MCTS** <span dir="rtl">في</span>
**AlphaGo** <span dir="rtl">ألعابًا محاكاة حيث يستخدم كلا اللاعبين
**سياسة تنفيذ سريعة**</span> **(Fast Rollout Policy)**
<span dir="rtl">مقدمة من **شبكة خطية بسيطة** </span>**(Simple Linear
Network)**<span dir="rtl">، تم تدريبها أيضًا بواسطة **التعليم الخاضع
للإشراف**</span> **(Supervised Learning)** <span dir="rtl">قبل اللعب.
خلال تنفيذها، قام</span> **APV-MCTS** <span dir="rtl">بتتبع عدد المحاكاة
التي مرت عبر كل حافة من **شجرة البحث** </span>**(Search
Tree)**<span dir="rtl">، وعند اكتمال تنفيذها، تم اختيار الحافة الأكثر
زيارة من **العقدة الجذرية**</span> **(Root Node)**
<span dir="rtl">كالإجراء الذي يجب اتخاذه، وهنا الحركة التي قام
بها</span> **AlphaGo** <span dir="rtl">فعليًا في اللعبة</span>.

<span dir="rtl">كانت **شبكة القيمة**</span> **(Value Network)**
<span dir="rtl">لها نفس بنية **شبكة سياسة التعليم الخاضع للإشراف
العميقة**</span> **(Deep Convolutional SL Policy Network)**
<span dir="rtl">باستثناء أنها تحتوي على وحدة إخراج واحدة تعطي القيم
المقدرة لمواقع اللعبة بدلاً من **توزيعات الاحتمال** </span>**(Probability
Distributions)** <span dir="rtl">للإجراءات القانونية في **شبكة السياسة**
</span>**(SL Policy Network)**<span dir="rtl">.</span>
<span dir="rtl">في الوضع المثالي، ستخرج **شبكة القيمة**</span> **(Value
Network)** **<span dir="rtl">قيم الحالة المثلى</span> (Optimal State
Values)**<span dir="rtl">، وربما كان من الممكن تقريب **دالة القيمة
المثلى**</span> **(Optimal Value Function)** <span dir="rtl">على غرار ما
تم وصفه في  
</span>**TD-Gammon** <span dir="rtl">أعلاه:</span>
**<span dir="rtl">اللعب الذاتي</span> (Self-Play)**
<span dir="rtl">مع</span> **TD(λ) <span dir="rtl">اللاخطية</span>
<span dir="rtl">(</span>Nonlinear
<span dir="rtl"></span>TD(λ)<span dir="rtl">)</span>**
<span dir="rtl">المتصلة بشبكة عصبية اصطناعية التفافية عميقة. لكن
فريق</span> **DeepMind** <span dir="rtl">اتخذ نهجًا مختلفًا يحمل وعدًا أكبر
للعبة معقدة مثل **الجو** </span>**(Go)**<span dir="rtl">.</span>
<span dir="rtl">قاموا بتقسيم عملية تدريب **شبكة القيمة** </span>**(Value
Network)** <span dir="rtl">إلى مرحلتين. في المرحلة الأولى، قاموا بإنشاء
**أفضل سياسة ممكنة (**</span>**Best Policy They Could<span dir="rtl">)
</span>**<span dir="rtl">باستخدام **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">لتدريب **شبكة سياسة التعليم
المعزز  
(**</span>**RL <span dir="rtl"></span>Policy
Network<span dir="rtl">)</span>**. <span dir="rtl">كانت هذه **شبكة عصبية
اصطناعية التفافية عميقة**</span> **<span dir="rtl">(</span>Deep
<span dir="rtl"></span>Convolutional ANN<span dir="rtl">)
</span>**<span dir="rtl">لها نفس بنية **شبكة سياسة التعليم الخاضع
للإشراف**</span> **<span dir="rtl">(</span>SL Policy
<span dir="rtl"></span>Network<span dir="rtl">)</span>**.
<span dir="rtl">تم تهيئتها بأوزان نهائية من **شبكة سياسة التعليم الخاضع
للإشراف**</span> **<span dir="rtl">(</span>SL Policy
<span dir="rtl"></span>Network<span dir="rtl">)
</span>**<span dir="rtl">التي تم تعلمها عبر **التعليم الخاضع للإشراف**
</span>**(Supervised Learning)**<span dir="rtl">، ثم تم استخدام
**التعليم المعزز بواسطة تدرج السياسة**</span>
**<span dir="rtl">(</span>Policy-Gradient Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">لتحسين **السياسة**</span> **(Policy)**
<span dir="rtl">القائمة على **التعليم الخاضع للإشراف** </span>**(SL
Policy)**<span dir="rtl">.</span> <span dir="rtl">في المرحلة الثانية من
تدريب **شبكة القيمة** </span>**(Value Network)**<span dir="rtl">، استخدم
الفريق **تقييم سياسة مونت كارلو**</span> **(Monte Carlo Policy
Evaluation)** <span dir="rtl">على البيانات التي تم الحصول عليها من عدد
كبير من ألعاب **اللعب الذاتي المحاكية**</span> **(Simulated Self-Play
Games)** <span dir="rtl">مع الحركات التي تم اختيارها بواسطة **شبكة سياسة
التعليم المعزز** </span>**(RL Policy Network)**<span dir="rtl">.</span>

<span dir="rtl">يوضح **الشكل 16.6** الشبكات المستخدمة من قبل</span>
**AlphaGo** <span dir="rtl">والخطوات التي تم اتخاذها لتدريبها فيما أسماه
فريق</span> **DeepMind** <span dir="rtl">بـ **خط أنابيب**
</span>**AlphaGo (AlphaGo Pipeline)**<span dir="rtl">.</span>
<span dir="rtl">تم تدريب جميع هذه الشبكات قبل أي لعبة حية، وظلت أوزانها
ثابتة طوال اللعب الحي</span>.

<img src="./media/image191.png"
style="width:6.26806in;height:2.78264in" />

<span dir="rtl">**الشكل 16.6**:</span> **<span dir="rtl">خط
أنابيب</span> AlphaGo (AlphaGo Pipeline)**<span dir="rtl">.</span>
<span dir="rtl">مقتبس بإذن من</span> **Macmillan
<span dir="rtl"></span>Publishers Ltd**: **Nature**<span dir="rtl">،
المجلد 529 (7587)، الصفحة 485، ©2016</span>.

<span dir="rtl">إليك بعض التفاصيل الإضافية حول **الشبكات العصبية
الاصطناعية**</span> **(ANNs)** <span dir="rtl">الخاصة بـ</span>
**AlphaGo** <span dir="rtl">وتدريبها. كانت شبكات **سياسة التعليم الخاضع
للإشراف**</span> **(SL Policy Network)** <span dir="rtl">وشبكات **سياسة
التعليم المعزز**</span> **(RL Policy Network)**
<span dir="rtl">المتماثلة في البنية مشابهة لشبكة</span> **DQN**
<span dir="rtl">العصبية الالتفافية العميقة المستخدمة في **الفصل 16.5**
للعب ألعاب **أتاري** </span>**(Atari)**<span dir="rtl">، باستثناء أنها
احتوت على 13 طبقة التفافية، مع احتواء الطبقة النهائية على وحدة</span>
**soft-max** <span dir="rtl">لكل نقطة على **لوحة الجو  
(**</span>**19 × 19 Go Board<span dir="rtl">)</span>**.
<span dir="rtl">كانت مدخلات الشبكات عبارة عن **مجموعة صور بحجم  **
</span>**(19 × 19 × 48 Image Stack)** <span dir="rtl">حيث تم تمثيل كل
نقطة على **لوحة الجو** </span>**(Go Board)** <span dir="rtl">بواسطة
قيم</span> **48 <span dir="rtl">من الميزات الثنائية أو العددية</span>
<span dir="rtl">(</span>48 Binary or Integer-Valued
Features<span dir="rtl">)</span>**. <span dir="rtl">على سبيل المثال،
أشارت إحدى الميزات إلى ما إذا كانت النقطة مشغولة بحجر من أحجار</span>
**AlphaGo** <span dir="rtl">أو بأحد أحجار الخصم، أو كانت غير مشغولة، مما
يوفر التمثيل "الخام" لتكوين اللوحة. استندت ميزات أخرى إلى قواعد **الجو**
</span>**(Go)**<span dir="rtl">، مثل عدد النقاط المجاورة التي كانت
فارغة، وعدد أحجار الخصم التي سيتم التقاطها بوضع حجر هناك، وعدد الأدوار
منذ أن تم وضع حجر هناك، وميزات أخرى اعتبرها فريق التصميم مهمة</span>.

<span dir="rtl">استغرق تدريب **شبكة سياسة التعليم الخاضع
للإشراف**</span> **(SL Policy Network)** <span dir="rtl">حوالي 3 أسابيع
باستخدام تنفيذ موزع **لتسلق التدرج العشوائي**</span> **(Stochastic
Gradient Ascent)** <span dir="rtl">على 50 معالجًا. حققت الشبكة دقة بنسبة
57%، حيث كانت أفضل دقة تم تحقيقها من قبل مجموعات أخرى في وقت النشر هي
44.4%. تم تدريب **شبكة سياسة التعليم المعزز**</span> **(RL Policy
Network)** <span dir="rtl">بواسطة **التعليم المعزز بتدرج
السياسة**</span> **(Policy Gradient Reinforcement Learning)**
<span dir="rtl">على ألعاب محاكاة بين السياسة الحالية **لشبكة سياسة
التعليم المعزز**</span> **(RL Policy Network)** <span dir="rtl">وخصوم
يستخدمون سياسات تم اختيارها عشوائيًا من السياسات التي أنتجتها تكرارات
سابقة لخوارزمية التعليم. منع اللعب ضد مجموعة مختارة عشوائيًا من الخصوم
التكيف المفرط مع السياسة الحالية. كان **إشارة المكافأة**
</span>**(Reward Signal)** +1 <span dir="rtl">إذا فازت السياسة الحالية،
-1 إذا خسرت، وصفر في غير ذلك. كانت هذه الألعاب تنافس السياسات ضد بعضها
البعض مباشرة دون استخدام</span> **MCTS**<span dir="rtl">.</span>
<span dir="rtl">من خلال محاكاة العديد من الألعاب بالتوازي على 50 معالجًا،
قام فريق</span> **DeepMind** <span dir="rtl">بتدريب **شبكة سياسة التعليم
المعزز** </span>**(RL Policy Network)** <span dir="rtl">على مليون لعبة
في يوم واحد. عند اختبار السياسة النهائية **لشبكة سياسة التعليم المعزز**
</span>**(RL Policy Network)**<span dir="rtl">، وجدوا أنها فازت بأكثر من
80% من الألعاب التي لعبت ضد **شبكة سياسة التعليم الخاضع للإشراف**
</span>**(SL Policy Network)**<span dir="rtl">، وفازت بنسبة 85% من
الألعاب التي لعبت ضد برنامج **الجو**</span> **(Go)**
<span dir="rtl">باستخدام</span> **MCTS** <span dir="rtl">الذي يحاكي
100,000 لعبة لكل حركة</span>.

<span dir="rtl">كانت **شبكة القيمة** </span>**(Value
Network)**<span dir="rtl">، التي كانت بنيتها مشابهة لبنية **شبكات سياسة
التعليم الخاضع للإشراف والتعليم المعزز**</span> **(SL and RL Policy
Networks)** <span dir="rtl">باستثناء وحدة الإخراج الفردية، تتلقى نفس
المدخلات مثل **شبكات سياسة التعليم الخاضع للإشراف والتعليم
المعزز**</span> **<span dir="rtl">(</span>SL <span dir="rtl"></span>and
RL Policy Networks<span dir="rtl">) </span>**<span dir="rtl">باستثناء
وجود ميزة ثنائية إضافية تعطي اللون الحالي للعب. تم استخدام **تقييم سياسة
مونت كارلو**</span> **(Monte Carlo Policy Evaluation)**
<span dir="rtl">لتدريب الشبكة من البيانات التي تم الحصول عليها من عدد
كبير من ألعاب **اللعب الذاتي** </span>**(Self-Play Games)**
<span dir="rtl">باستخدام **شبكة سياسة التعليم المعزز** </span>**(RL
Policy Network)**<span dir="rtl">.</span> <span dir="rtl">لتجنب التكيف
المفرط وعدم الاستقرار بسبب الارتباطات القوية بين المواقع التي تمت
مواجهتها في **اللعب الذاتي** </span>**(Self-Play)**<span dir="rtl">، قام
فريق</span> **DeepMind** <span dir="rtl">بإنشاء مجموعة بيانات تتكون من
30 مليون موقف تم اختيار كل منها عشوائيًا من لعبة **اللعب الذاتي**</span>
**(Self-Play)** <span dir="rtl">فريدة. ثم تم التدريب باستخدام 50 مليون
**دفعة صغيرة**</span>
**<span dir="rtl">(</span>Mini-Batches<span dir="rtl">)</span>**<span dir="rtl">،
كل منها تحتوي على 32 موقفًا مأخوذًا من هذه المجموعة من البيانات. استغرق
التدريب أسبوعًا واحدًا على 50 **وحدة معالجة رسومات**
</span>**(GPUs)**<span dir="rtl">.</span>

<span dir="rtl">تم تعلم **سياسة التنفيذ**</span> **(Rollout Policy)**
<span dir="rtl">قبل اللعب بواسطة **شبكة خطية بسيطة  
(**</span>**Simple Linear Network<span dir="rtl">)</span>**
<span dir="rtl">تم تدريبها بواسطة **التعليم الخاضع للإشراف  
(**</span>**Supervised Learning<span dir="rtl">)</span>**
<span dir="rtl">من مجموعة تحتوي على 8 ملايين حركة بشرية. كان على **شبكة
سياسة التنفيذ**</span> **<span dir="rtl">(</span>Rollout Policy
Network<span dir="rtl">) </span>**<span dir="rtl">أن تخرج الإجراءات
بسرعة مع الحفاظ على دقة معقولة. من حيث المبدأ، كان من الممكن استخدام
**شبكات سياسة التعليم الخاضع للإشراف أو التعليم المعزز**</span>
**<span dir="rtl">(</span>SL or RL Policy Networks<span dir="rtl">)
</span>**<span dir="rtl">في عمليات التنفيذ، ولكن التمرير الأمامي عبر هذه
الشبكات العميقة استغرق وقتًا طويلاً جدًا لاستخدام أي منها في **محاكاة
التنفيذ**</span> **<span dir="rtl">(</span>Rollout
<span dir="rtl"></span>Simulations<span dir="rtl">)</span>**<span dir="rtl">،
التي يجب أن يتم تنفيذ عدد كبير منها لكل قرار حركة أثناء اللعب الحي. لهذا
السبب، كانت **شبكة سياسة التنفيذ**</span> **(Rollout Policy Network)**
<span dir="rtl">أقل تعقيدًا من **شبكات السياسة الأخرى** </span>**(Other
Policy Networks)**<span dir="rtl">، ويمكن حساب ميزاتها بسرعة أكبر من
الميزات المستخدمة في **شبكات السياسة** </span>**(Policy
Networks)**<span dir="rtl">.</span> <span dir="rtl">سمحت **شبكة سياسة
التنفيذ**</span> **<span dir="rtl">(</span>Rollout Policy
<span dir="rtl"></span>Network<span dir="rtl">)</span>**
<span dir="rtl">بإجراء حوالي 1,000 محاكاة كاملة للعبة في الثانية على كل
من خيوط المعالجة التي استخدمها</span>
**AlphaGo**<span dir="rtl">.</span>

<span dir="rtl">قد يتساءل المرء لماذا تم استخدام **سياسة التعليم الخاضع
للإشراف**</span> **(SL Policy)** <span dir="rtl">بدلاً من **سياسة التعليم
المعزز**</span> **(RL Policy)** <span dir="rtl">الأفضل لاختيار الإجراءات
في مرحلة توسيع</span> **APV-MCTS**<span dir="rtl">. استغرقت هذه السياسات
نفس القدر من الوقت للحساب لأنها استخدمت نفس بنية الشبكة. وجد الفريق
بالفعل أن</span> **AlphaGo** <span dir="rtl">لعب بشكل أفضل ضد الخصوم
البشريين عندما استخدم</span> **APV-MCTS** **<span dir="rtl">سياسة
التعليم الخاضع للإشراف</span> (SL Policy)** <span dir="rtl">بدلاً من
**سياسة التعليم المعزز** </span>**(RL Policy)**<span dir="rtl">.</span>
<span dir="rtl">لقد افترضوا أن السبب في ذلك هو أن الأخيرة تم ضبطها
للاستجابة للحركات المثلى بدلاً من مجموعة الحركات الأوسع التي تميز اللعب
البشري. ومن المثير للاهتمام، أن الوضع كان معكوسًا بالنسبة **لدالة
القيمة**</span> **<span dir="rtl">(</span>Value
<span dir="rtl"></span>Function<span dir="rtl">)</span>**
<span dir="rtl">المستخدمة بواسطة</span>
**APV-MCTS**<span dir="rtl">.</span> <span dir="rtl">وجدوا أنه عندما
استخدم</span> **APV-MCTS** **<span dir="rtl">دالة القيمة المستمدة من
سياسة التعليم المعزز</span> <span dir="rtl">(</span>Value Function
Derived from the RL
<span dir="rtl"></span>Policy<span dir="rtl">)</span>**<span dir="rtl">،
كانت أداؤها أفضل مما لو كانت تستخدم **دالة القيمة المستمدة من سياسة
التعليم الخاضع للإشراف** </span>**(Value Function Derived from the SL
Policy)**<span dir="rtl">.</span>

<span dir="rtl">عملت عدة طرق معًا لإنتاج مهارة اللعب المثيرة للإعجاب
لـ</span> **AlphaGo**<span dir="rtl">.</span> <span dir="rtl">قام
فريق</span> **DeepMind** <span dir="rtl">بتقييم نسخ مختلفة من</span>
**AlphaGo** <span dir="rtl">من أجل تقييم المساهمات التي قدمتها هذه
المكونات المختلفة. تحكم **البارامتر**</span> **(η)** <span dir="rtl">في
المعادلة (16.4) في **خلط تقييمات حالة اللعبة**</span>
**<span dir="rtl">(</span>Mixing of Game State
<span dir="rtl"></span>Evaluations<span dir="rtl">)
</span>**<span dir="rtl">الناتجة عن **شبكة القيمة**</span> **(Value
Network)** <span dir="rtl">وعن **عمليات التنفيذ**
</span>**(Rollouts)**<span dir="rtl">. مع</span> η=0<span dir="rtl">،
استخدم</span> **AlphaGo** <span dir="rtl">فقط **شبكة القيمة**</span>
**(Value Network)** <span dir="rtl">بدون **عمليات تنفيذ**
</span>**(Rollouts)**<span dir="rtl">، ومع</span> η=1<span dir="rtl">،
اعتمد التقييم فقط على **عمليات التنفيذ**
</span>**(Rollouts)**<span dir="rtl">.</span> <span dir="rtl">وجدوا
أن</span> **AlphaGo** <span dir="rtl">الذي يستخدم فقط **شبكة
القيمة**</span> **(Value Network)** <span dir="rtl">لعب أفضل من</span>
**AlphaGo** <span dir="rtl">المعتمد على **عمليات التنفيذ**
</span>**(Rollout-Only AlphaGo)**<span dir="rtl">، وفي الواقع لعب أفضل
من أقوى جميع برامج **الجو** </span>**(Go)** <span dir="rtl">الأخرى
الموجودة في ذلك الوقت. وكانت أفضل لعبة هي عندما تم ضبط</span>
η=0.5<span dir="rtl">، مما يشير إلى أن **دمج شبكة القيمة مع عمليات
التنفيذ**</span> **<span dir="rtl">(</span>Combining the Value Network
with <span dir="rtl"></span>Rollouts<span dir="rtl">)
</span>**<span dir="rtl">كان مهمًا بشكل خاص لنجاح</span>
**AlphaGo**<span dir="rtl">.</span> <span dir="rtl">أكملت هذه طرق
التقييم بعضها البعض: حيث قامت **شبكة القيمة**</span> **(Value Network)**
<span dir="rtl">بتقييم **سياسة التعليم المعزز**</span> **(RL Policy)**
<span dir="rtl">عالية الأداء والتي كانت بطيئة جدًا لاستخدامها في اللعب
الحي، بينما أضافت **عمليات التنفيذ** </span>**(Rollouts)**
<span dir="rtl">باستخدام **سياسة التنفيذ**</span> **(Rollout Policy)**
<span dir="rtl">الأضعف ولكن الأسرع بكثير دقة لتقييمات **شبكة القيمة**
</span>**(Value Network)** <span dir="rtl">لحالات معينة حدثت خلال
الألعاب</span>.

<span dir="rtl">بشكل عام، أثار النجاح الملحوظ لـ</span> **AlphaGo**
<span dir="rtl">جولة جديدة من الحماس لوعد **الذكاء الاصطناعي**
</span>**(Artificial Intelligence)**<span dir="rtl">، وخاصة للأنظمة التي
تجمع بين **التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">و**الشبكات العصبية الاصطناعية العميقة**
</span>**(Deep ANNs)**<span dir="rtl">، لمواجهة التحديات في مجالات أخرى
صعبة</span>.

**<u>16.6.2 <span dir="rtl">ألفا جو زيرو</span> (AlphaGo Zero)</u>**

<span dir="rtl">بناءً على تجربة</span> AlphaGo<span dir="rtl">، قام فريق
من</span> DeepMind <span dir="rtl">بتطوير</span> AlphaGo Zero
<span dir="rtl">(</span>Silver et al.
<span dir="rtl"></span>2017a<span dir="rtl">)</span>.
<span dir="rtl">على عكس</span> AlphaGo<span dir="rtl">، لم يستخدم هذا
البرنامج أي بيانات بشرية أو توجيهات خارج القواعد الأساسية للعبة</span>
<span dir="rtl">(ومن هنا جاء اسم</span> "Zero"<span dir="rtl">)</span>.
<span dir="rtl">تعلم بشكل حصري من خلال **التعليم المعزز الذاتي**
</span>**(Self-Play Reinforcement Learning)**<span dir="rtl">، مع مدخلات
تصف ببساطة وضعيات الأحجار على لوحة لعبة جو. نفذت</span> AlphaGo Zero
<span dir="rtl">شكلاً من **تكرار السياسة**</span> **(Policy Iteration)**
<span dir="rtl">(الفصل 4.3)، بالتناوب بين **تقييم السياسة**</span>
**(Policy Evaluation)** <span dir="rtl">و**تحسين السياسة**</span>
**<span dir="rtl">(</span>Policy Improvement<span dir="rtl">)</span>**.
<span dir="rtl">**الشكل 16.7** يقدم نظرة عامة على خوارزمية</span>
AlphaGo Zero<span dir="rtl">.</span> <span dir="rtl">اختلاف كبير
بين</span> AlphaGo Zero <span dir="rtl">و</span>AlphaGo
<span dir="rtl">هو أن</span> AlphaGo Zero <span dir="rtl">استخدم **مونت
كارلو شجرة البحث** </span>**(MCTS - Monte Carlo Tree Search)**
<span dir="rtl">لاختيار **الإجراءات**</span> **(Actions)**
<span dir="rtl">طوال عملية **التعليم المعزز الذاتي** </span>**(Self-Play
Reinforcement Learning)**<span dir="rtl">، بينما استخدم</span> AlphaGo
**<span dir="rtl">مونت كارلو شجرة البحث</span> (MCTS - Monte Carlo Tree
Search)** <span dir="rtl">أثناء اللعب الفعلي بعد - وليس أثناء عملية
التعليم. بالإضافة إلى عدم استخدام أي بيانات بشرية أو ميزات مصممة بشريًا،
استخدم</span> AlphaGo Zero <span dir="rtl">فقط **شبكة عصبية التفاف
عميقة**</span> **<span dir="rtl">(</span>Deep Convolutional Neural
<span dir="rtl"></span>Network<span dir="rtl">)
</span>**<span dir="rtl">واحدة ونسخة أبسط من **مونت كارلو شجرة
البحث**</span> **<span dir="rtl">(</span>MCTS - Monte Carlo Tree
<span dir="rtl"></span>Search<span dir="rtl">)</span>**.

<span dir="rtl">كانت **مونت كارلو شجرة البحث**</span> **(MCTS - Monte
Carlo Tree Search)** <span dir="rtl">في</span> AlphaGo
<span dir="rtl"></span>Zero <span dir="rtl">أبسط من النسخة المستخدمة
في</span> AlphaGo <span dir="rtl">لأنها لم تشمل تكرارات كاملة للألعاب،
وبالتالي لم تكن بحاجة إلى **سياسة تكرار** </span>**(Rollout
Policy)**<span dir="rtl">.</span> <span dir="rtl">كل تكرار من **مونت
كارلو شجرة البحث**</span> **<span dir="rtl">(</span>MCTS - Monte Carlo
Tree Search<span dir="rtl">) </span>**<span dir="rtl">في</span> AlphaGo
Zero <span dir="rtl">انتهى عند عقدة ورقة في الشجرة الحالية للبحث بدلاً من
الموقف النهائي لمحاكاة لعبة كاملة. ولكن كما هو الحال في</span>
AlphaGo<span dir="rtl">، تم توجيه كل تكرار من **مونت كارلو شجرة
البحث**</span> **(MCTS - Monte Carlo Tree Search)**
<span dir="rtl">في</span> AlphaGo <span dir="rtl"></span>Zero
<span dir="rtl">بواسطة مخرجات **شبكة عصبية التفاف عميقة** </span>**(Deep
Convolutional Network)**<span dir="rtl">، والمشار إليها بالرمز</span>
**fθ** <span dir="rtl">في **الشكل 16.7**، حيث</span> **θ**
<span dir="rtl">هو متجه أوزان الشبكة. كانت مدخلات الشبكة، التي سنصفها
أدناه، تتألف من تمثيلات خام لوضعيات اللوحة، وكانت مخرجاتها تتألف من
جزئين: قيمة عددية</span> **v**<span dir="rtl">، وهي تقدير لاحتمال فوز
**العميل**</span> **(Agent)** <span dir="rtl">الحالي من الوضعية الحالية
على اللوحة، ومتجه</span> **p** <span dir="rtl">للاحتمالات الخاصة
بالحركات، واحد لكل موضع ممكن لوضع الحجر على اللوحة الحالية، بالإضافة إلى
حركة التمرير أو الاستسلام</span>.

<span dir="rtl">بدلاً من اختيار **أفعال اللعب الذاتي**</span>
**(Self-Play Actions)** <span dir="rtl">وفقًا لاحتمالات</span>
**p**<span dir="rtl">، استخدم</span> AlphaGo <span dir="rtl"></span>Zero
<span dir="rtl">هذه الاحتمالات، جنبًا إلى جنب مع مخرجات القيمة للشبكة،
لتوجيه كل تنفيذ لـ **مونت كارلو شجرة البحث** </span>**(MCTS - Monte
Carlo Tree Search)**<span dir="rtl">، الذي أعاد احتمالات حركات جديدة،
تُظهر في **الشكل 16.7** كسياسات</span> **πi**<span dir="rtl">.</span>
<span dir="rtl">استفادت هذه السياسات من العديد من المحاكاة التي أجرتها
**مونت كارلو شجرة البحث**</span> **(MCTS - Monte Carlo Tree Search)**
<span dir="rtl">في كل مرة يتم تنفيذها. والنتيجة هي أن **السياسة**</span>
**(Policy)** <span dir="rtl">التي اتبعها</span> AlphaGo Zero
<span dir="rtl">كانت تحسينًا على **السياسة** </span>**(Policy)**
<span dir="rtl">المقدمة بواسطة مخرجات الشبكة</span>
**p**<span dir="rtl">.</span> <span dir="rtl">كتب</span> **Silver et al.
(2017a)** <span dir="rtl">أن "مونت كارلو شجرة البحث</span> (MCTS - Monte
Carlo Tree Search) \*\* <span dir="rtl">قد يُنظر إليها كعامل قوي لتحسين
**السياسة** </span>**(Policy)**"<span dir="rtl">.</span>

<img src="./media/image192.png"
style="width:6.26806in;height:4.47222in" />

<span dir="rtl">الشكل 16.7: التعليم المعزز الذاتي لـ</span> AlphaGo Zero

a<span dir="rtl">) البرنامج لعب العديد من المباريات ضد نفسه، موضحة هنا
كتسلسل من وضعيات اللوحة  
</span>$`si ،\ i = 1,2,\ldots,T`$<span dir="rtl">، مع الحركات</span> ​
<span dir="rtl"></span>$`i = 1,2,\ldots,T`$<span dir="rtl">،
والفائز</span> $`z`$<span dir="rtl">.</span> <span dir="rtl">تم تحديد كل
حركة</span> $`ai`$ <span dir="rtl"></span> <span dir="rtl">بواسطة
**احتمالات الإجراء**</span> **(Action Probabilities)** $`\pi i`$
<span dir="rtl">التي أرجعتها **مونت كارلو شجرة البحث**</span>
**<span dir="rtl">(</span>MCTS – Monte <span dir="rtl"></span>Carlo Tree
Search<span dir="rtl">) </span>**<span dir="rtl">المنفذة من عقدة
الجذر</span> $`si`$ <span dir="rtl"></span> <span dir="rtl">وبتوجيه من
**شبكة عصبية التفاف عميقة** </span>**(Deep Convolutional
Network)**<span dir="rtl">، والمشار إليها هنا بالرمز</span>
$`f\theta`$​<span dir="rtl">، مع آخر أوزان</span>
θ<span dir="rtl">.</span> <span dir="rtl">يظهر هنا لوضعية واحدة</span>
$`s`$ <span dir="rtl"></span> <span dir="rtl">ولكن يتكرر لجميع</span>
$`si`$​<span dir="rtl">، كانت مدخلات الشبكة تمثيلات خام لوضعيات
اللوحة</span> $`si`$ <span dir="rtl">(جنبًا إلى جنب مع عدة وضعيات سابقة،
رغم عدم ظهورها هنا)، وكانت مخرجاتها متجهات **احتمالات الحركة**</span>
**(Move Probabilities)** $`p`$ <span dir="rtl">التي وجهت عمليات البحث
الأمامية لـ **مونت كارلو شجرة البحث** </span>**(MCTS - Monte Carlo Tree
Search)**<span dir="rtl">، وقيم عددية</span> $`v`$ <span dir="rtl"> التي
قدرت احتمالية فوز **العميل** </span>**(Agent)** <span dir="rtl">الحالي
من كل وضعية</span> $`si`$​<span dir="rtl">.</span>

b<span dir="rtl">)</span> <span dir="rtl">تدريب **الشبكة العصبية
الالتفافية العميقة** </span>**(Deep Convolutional Network
Training)**<span dir="rtl">. تم أخذ عينات تدريبية من خطوات عشوائية من
مباريات **اللعب الذاتي**</span> **(Self-Play)** <span dir="rtl">الحديثة.
تم تحديث الأوزان</span> $`\theta`$ <span dir="rtl">لتحريك **متجه
السياسة**</span> **(Policy Vector)** $`p`$ <span dir="rtl">نحو
الاحتمالات</span> $`\pi`$ <span dir="rtl">التي أرجعتها **مونت كارلو شجرة
البحث** </span>**(MCTS - Monte Carlo Tree Search)**<span dir="rtl">،
ولتشمل الفائزين</span> $`z`$ <span dir="rtl"></span> <span dir="rtl">في
تقدير احتمال الفوز</span> $`v`$<span dir="rtl">.</span>
<span dir="rtl">تم إعادة طباعة الشكل من مسودة</span> Silver et al.
(2017a) <span dir="rtl">بإذن من المؤلفين
و</span>DeepMind<span dir="rtl">.</span>

<span dir="rtl">تفاصيل إضافية حول الشبكة العصبية لـ</span> AlphaGo Zero
<span dir="rtl">وكيف تم تدريبها</span>

<span dir="rtl">أخذت الشبكة كمدخل صورة بحجم</span> 19×19×1719
<span dir="rtl">تتكون من 17 مستويًا من الخصائص الثنائية. كانت المستويات
الثمانية الأولى تمثيلات خام لمواضع أحجار اللاعب الحالي في التشكيلات
الحالية والسابقة للوحة (سبع تشكيلات سابقة): كانت قيمة الخاصية تساوي 1
إذا كانت حجارة اللاعب موجودة في النقطة المقابلة، وتساوي 0 في غير ذلك. تم
ترميز المستويات الثمانية التالية بطريقة مشابهة لمواضع أحجار الخصم. وكانت
هناك طبقة أخيرة من المدخلات ذات قيمة ثابتة تشير إلى لون اللعبة الحالي:  
1 للأسود؛ 0 للأبيض. نظرًا لأن التكرار غير مسموح به في لعبة جو وأحد
اللاعبين يحصل على عدد معين من "نقاط التعويض" لعدم الحصول على الحركة
الأولى، فإن الوضعية الحالية للوحة ليست حالة ماركوف للعبة جو. لهذا السبب
كانت هناك حاجة إلى ميزات تصف التشكيلات السابقة للوحة وميزة اللون</span>.

<span dir="rtl">كانت الشبكة "برأسين"، مما يعني أنه بعد عدد من الطبقات
الأولية، انقسمت الشبكة إلى "رأسين" منفصلين يتغذيان بشكل منفصل إلى
مجموعتين من وحدات المخرجات. في هذه الحالة، قام أحد الرأسين بتغذية 362
وحدة مخرجات تنتج 192 + 1 احتمالية حركة</span> $`p`$<span dir="rtl">،
واحدة لكل موضع محتمل لوضع الحجر بالإضافة إلى التمرير؛ والرأس الآخر قام
بتغذية وحدة مخرج واحدة فقط تنتج القيمة العددية</span> v<span dir="rtl">،
وهي تقدير لاحتمال فوز **العميل**</span> **(Agent)**
<span dir="rtl">الحالي من الوضعية الحالية للوحة. تألفت الشبكة قبل
الانقسام من 41 طبقة التفافية، تلي كل منها عملية **التطبيع بالجملة**
</span>**(Batch Normalization)**<span dir="rtl">، وتمت إضافة وصلات تخطٍ
لتنفيذ **التعليم المتبقي**</span> **(Residual Learning)**
<span dir="rtl">بواسطة أزواج من الطبقات (انظر الفصل 9.6). بشكل عام، تم
حساب احتمالات الحركات والقيم عبر 43 و44 طبقة على التوالي</span>.

<span dir="rtl">بدأت الشبكة بأوزان عشوائية وتم تدريبها باستخدام **النزول
التدرجي العشوائي**</span> **<span dir="rtl">(</span>Stochastic
<span dir="rtl"></span>Gradient Descent<span dir="rtl">)
</span>**<span dir="rtl">(مع الزخم، التنظيم، وبارامتر حجم الخطوة التي
تقل مع استمرار التدريب) باستخدام دفعات من الأمثلة مأخوذة عشوائيًا من جميع
الخطوات لأحدث 500,000 لعبة لعب ذاتي باستخدام **السياسة**</span>
**(Policy)** <span dir="rtl">الحالية الأفضل. أُضيفت ضوضاء إضافية إلى
مخرجات الشبكة</span> p <span dir="rtl">لتشجيع استكشاف جميع الحركات
الممكنة. عند نقاط فحص دورية خلال التدريب، والتي اختارها</span> **Silver
et al. (2017a)** <span dir="rtl">لتكون عند كل 1,000 خطوة تدريبية، تم
تقييم **السياسة**</span> **(Policy)** <span dir="rtl">التي تنتجها الشبكة
العصبية الاصطناعية</span> (ANN) <span dir="rtl">بأحدث الأوزان من خلال
محاكاة 400 لعبة (باستخدام **مونت كارلو شجرة البحث**</span> **(MCTS -
Monte Carlo Tree Search)** <span dir="rtl">مع 1,600 تكرار لتحديد كل
حركة) ضد **السياسة**</span> **(Policy)** <span dir="rtl">الحالية الأفضل.
إذا فازت **السياسة**</span> **(Policy)** <span dir="rtl">الجديدة (بفارق
معين لتقليل الضوضاء في النتيجة)، فإنها تصبح **السياسة**</span>
**(Policy)** <span dir="rtl">الأفضل لاستخدامها في اللعب الذاتي اللاحق.
تم تحديث أوزان الشبكة لجعل مخرجات **السياسة**</span> **(Policy)**
<span dir="rtl">الخاصة بها</span> p <span dir="rtl">تتطابق بشكل أكبر مع
**السياسة**</span> **(Policy)** <span dir="rtl">التي أرجعتها **مونت
كارلو شجرة البحث**</span> **<span dir="rtl">(</span>MCTS – Monte
<span dir="rtl"></span>Carlo Tree
Search<span dir="rtl">)</span>**<span dir="rtl">، ولجعل مخرجات
قيمتها</span> v <span dir="rtl">تتطابق بشكل أكبر مع احتمال فوز
**السياسة** </span>**(Policy)** <span dir="rtl">الحالية الأفضل من
الوضعية التي تمثلها مدخلات الشبكة</span>.

<span dir="rtl">قام فريق</span> DeepMind <span dir="rtl">بتدريب</span>
AlphaGo Zero <span dir="rtl">على أكثر من 4.9 مليون لعبة لعب ذاتي، والتي
استغرقت حوالي 3 أيام. تم اختيار كل حركة في كل لعبة عن طريق تشغيل **مونت
كارلو شجرة البحث** </span>**(MCTS - Monte Carlo Tree Search)**
<span dir="rtl">لمدة 1,600 تكرار، مما استغرق حوالي 0.4 ثانية لكل حركة.
تم تحديث أوزان الشبكة عبر 700,000 دفعة، كل منها يتألف من 2,048 تشكيل
لوحي. بعد ذلك، أجروا بطولات بمشاركة</span> **AlphaGo** **Zero**
<span dir="rtl">المدرب ضد النسخة من</span> **AlphaGo**
<span dir="rtl">التي هزمت</span> Fan Hui <span dir="rtl">بخمس مباريات
مقابل صفر، وضد النسخة التي هزمت</span> Lee Sedol <span dir="rtl">بأربع
مباريات مقابل واحدة. استخدموا نظام تصنيف</span> Elo
<span dir="rtl">لتقييم الأداء النسبي للبرامج. الفرق بين تصنيفات</span>
Elo <span dir="rtl">يهدف إلى التنبؤ بنتائج المباريات بين اللاعبين. كانت
تصنيفات</span> Elo <span dir="rtl">لكل من</span> **AlphaGo**
**Zero**<span dir="rtl">، النسخة من</span> **AlphaGo**
<span dir="rtl">التي لعبت ضد</span> Fan Hui<span dir="rtl">، والنسخة
التي لعبت ضد</span> Lee Sedol <span dir="rtl">على التوالي 4,308، 3,144،
و3,739. الفجوات في هذه التصنيفات</span> Elo <span dir="rtl">تُترجم إلى
توقعات بأن</span> **AlphaGo** **Zero** <span dir="rtl">ستهزم هذه البرامج
الأخرى باحتمالات قريبة جدًا من الواحد. في مباراة من 100 لعبة بين</span>
**AlphaGo** **Zero**<span dir="rtl">، المدرب كما هو موضح، والنسخة
الدقيقة من</span> **AlphaGo** <span dir="rtl">التي هزمت</span> Lee Sedol
<span dir="rtl">التي أجريت في نفس الظروف التي استخدمت في تلك المباراة،
هزم</span> **AlphaGo Zero** **AlphaGo** <span dir="rtl">في جميع
المباريات الـ 100</span>.

<span dir="rtl">قام فريق</span> DeepMind <span dir="rtl">أيضًا
بمقارنة</span> **AlphaGo** **Zero** <span dir="rtl">مع برنامج يستخدم
**شبكة عصبية اصطناعية** </span>**(ANN - Artificial Neural Network)**
<span dir="rtl">بنفس الهندسة المعمارية ولكنه تم تدريبه من خلال **التعليم
الخاضع للإشراف**</span> **(Supervised Learning)** <span dir="rtl">للتنبؤ
بحركات البشر في مجموعة بيانات تحتوي على ما يقرب من 30 مليون وضعية من
160,000 لعبة. ووجدوا أن اللاعب الذي تم تدريبه بالتعليم الخاضع للإشراف
لعب بشكل أفضل في البداية من</span> **AlphaGo** **Zero**<span dir="rtl">،
وكان أفضل في التنبؤ بحركات الخبراء البشر، لكنه لعب بشكل أقل بعد أن تم
تدريب</span> **AlphaGo** **Zero** <span dir="rtl">لمدة يوم واحد. وقد
أشار هذا إلى أن</span> **AlphaGo** **Zero** <span dir="rtl">اكتشف
استراتيجية للعب تختلف عن طريقة لعب البشر. في الواقع، اكتشف</span>
**AlphaGo** **Zero**<span dir="rtl">، وفضل، بعض التغييرات الجديدة على
تسلسلات الحركات الكلاسيكية</span>.

<span dir="rtl">تم إجراء الاختبارات النهائية لخوارزمية</span>
**AlphaGo** **Zero** <span dir="rtl">باستخدام نسخة ذات **شبكة عصبية
اصطناعية**</span> **(ANN - Artificial Neural Network)**
<span dir="rtl">أكبر تم تدريبها على 29 مليون لعبة لعب ذاتي، والتي
استغرقت حوالي 40 يومًا، بدءًا بأوزان عشوائية مرة أخرى. حققت هذه النسخة
تصنيف</span> Elo <span dir="rtl">قدره 5,185. وقام الفريق بمواجهة هذه
النسخة من</span> **AlphaGo** **Zero** <span dir="rtl">ضد برنامج
يسمى</span> **AlphaGo** <span dir="rtl"></span>Master<span dir="rtl">،
وهو أقوى برنامج في ذلك الوقت، والذي كان مطابقًا لـ</span> **AlphaGo**
**Zero** <span dir="rtl">ولكن، مثل</span> **AlphaGo**<span dir="rtl">،
استخدم بيانات وخصائص بشرية. كان تصنيف</span> Elo
<span dir="rtl">لـ</span> **AlphaGo** **Master** <span dir="rtl">هو
4,858، وقد هزم أقوى اللاعبين المحترفين البشريين 60 مقابل 0 في الألعاب
عبر الإنترنت. في مباراة من 100 لعبة، هزم</span> **AlphaGo** **Zero**
<span dir="rtl">ذو الشبكة الأكبر والتعليم الأكثر شمولاً</span>
**AlphaGo** **Master** <span dir="rtl">في 89 لعبة مقابل 11، مما قدم عرضًا
مقنعًا لقوة حل المشكلات لخوارزمية</span> **AlphaGo**
**Zero**<span dir="rtl">.</span>

<span dir="rtl">لقد أظهر</span> AlphaGo Zero <span dir="rtl">بوضوح أن
الأداء فوق البشري يمكن تحقيقه من خلال **التعليم المعزز النقي**
</span>**(Pure Reinforcement Learning)**<span dir="rtl">، المدعوم بنسخة
بسيطة من **مونت كارلو شجرة البحث** </span>**(MCTS - Monte Carlo Tree
Search)**<span dir="rtl">، و**شبكات عصبية عميقة**</span> **(Deep ANNs)**
<span dir="rtl">مع معرفة قليلة جدًا بالمجال ودون الاعتماد على بيانات أو
توجيهات بشرية. من المؤكد أننا سنرى أنظمة مستوحاة من إنجازات</span>
DeepMind <span dir="rtl">لكل من</span> **AlphaGo**
**<span dir="rtl">و</span>AlphaGo** **Zero** <span dir="rtl">تُطبق على
مشاكل تحديات في مجالات أخرى</span>.

<span dir="rtl">مؤخرًا، تم وصف برنامج أفضل،</span>
**AlphaZero**<span dir="rtl">، بواسطة</span> **Silver et al.
(2017b)**<span dir="rtl">، والذي لا يدمج حتى معرفة لعبة جو</span>.
<span dir="rtl"></span>AlphaZero <span dir="rtl">هو خوارزمية **تعلم معزز
عامة**</span> **<span dir="rtl">(</span>General Reinforcement
<span dir="rtl"></span>Learning Algorithm<span dir="rtl">)</span>**
<span dir="rtl">تتفوق على أفضل البرامج في العالم في الألعاب المتنوعة مثل
جو والشطرنج والشوغي</span>.

**<u>16.7 <span dir="rtl">الخدمات المخصصة عبر الويب</span> (Personalized
Web Services)</u>**

<span dir="rtl">تخصيص الخدمات عبر الويب مثل تقديم المقالات الإخبارية أو
الإعلانات هو أحد الأساليب لزيادة رضا المستخدمين عن موقع الويب أو لزيادة
عائد حملة تسويقية. يمكن ل**السياسة**</span> **(Policy)**
<span dir="rtl">أن توصي بالمحتوى الذي يُعتبر الأفضل لكل مستخدم بناءً على
ملف شخصي يتضمن اهتماماته وتفضيلاته المستنتجة من تاريخ نشاطه عبر
الإنترنت. هذا مجال طبيعي لاستخدام **التعليم الآلي**</span>
**<span dir="rtl">(</span>Machine
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**<span dir="rtl">،
وبالأخص **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">.</span> <span dir="rtl">يمكن لنظام **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">تحسين
**السياسة التوصية**</span> **<span dir="rtl">(</span>Recommendation
<span dir="rtl"></span>Policy<span dir="rtl">)
</span>**<span dir="rtl">من خلال إجراء تعديلات استجابةً لتغذية راجعة من
المستخدمين. إحدى طرق الحصول على تغذية راجعة من المستخدمين هي من خلال
استطلاعات رضا الموقع، ولكن للحصول على تغذية راجعة في الوقت الفعلي، من
الشائع مراقبة نقرات المستخدمين كمؤشرات على اهتمامهم برابط معين</span>.

<span dir="rtl">طريقة طويلة الأمد تُستخدم في التسويق تسمى</span> **A/B
<span dir="rtl">اختبار</span> (A/B Testing)** <span dir="rtl">هي نوع
بسيط من **التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">تُستخدم لتحديد أي من الإصدارين،</span> A
<span dir="rtl">أو</span> B<span dir="rtl">، من موقع الويب يفضله
المستخدمون. نظرًا لأنها غير ارتباطية، مثل مشكلة **المقامر ذو الذراعين  
(**</span>**Two-Armed Bandit
Problem<span dir="rtl">)</span>**<span dir="rtl">، فإن هذا الأسلوب لا
يخصص تقديم المحتوى. إضافة سياق يتألف من خصائص تصف المستخدمين الفرديين
والمحتوى الذي سيتم تسليمه يسمح بتخصيص الخدمة. تم تصنيف هذا بشكل رسمي
كمسألة **السياق المقامر**</span> **(Contextual Bandit Problem)**
<span dir="rtl">(أو مسألة **التعليم المعزز الارتباطي**
</span>**(Associative Reinforcement Learning)**<span dir="rtl">، الفصل
2.9) بهدف تعظيم إجمالي عدد نقرات المستخدمين. طبق</span> **Li, Chu,
Langford, and Schapire (2010)** <span dir="rtl">خوارزمية **السياق
المقامر**</span> **(Contextual Bandit Algorithm)** <span dir="rtl">على
مشكلة تخصيص الصفحة الرئيسية لموقع</span> Yahoo! Front Page Today
<span dir="rtl">(إحدى أكثر الصفحات زيارة على الإنترنت في وقت أبحاثهم) من
خلال اختيار القصة الإخبارية التي ستعرض. كان هدفهم هو تعظيم **معدل
النقرات  
(**</span>**CTR - Click-Through
Rate<span dir="rtl">)</span>**<span dir="rtl">، وهو نسبة إجمالي عدد
النقرات التي يقوم بها جميع المستخدمين على صفحة الويب إلى إجمالي عدد
زيارات الصفحة. حسنت خوارزمية **السياق المقامر**</span>
**<span dir="rtl">(</span>Contextual <span dir="rtl"></span>Bandit
Algorithm<span dir="rtl">) </span>**<span dir="rtl">بنسبة 12.5% مقارنةً
بخوارزمية **المقامر غير الارتباطي القياسية**</span> **(Standard
Non-Associative Bandit Algorithm)**<span dir="rtl">.</span>

<span dir="rtl">جادل</span> **Theocharous, Thomas, and Ghavamzadeh
(2015)** <span dir="rtl">بأن نتائج أفضل ممكنة من خلال صياغة توصيات مخصصة
كمشكلة **اتخاذ قرار ماركوف**</span> **<span dir="rtl">(</span>Markov
Decision Problem – <span dir="rtl"></span>MDP<span dir="rtl">)
</span>**<span dir="rtl">بهدف تعظيم إجمالي عدد النقرات التي يقوم بها
المستخدمون على مدار زيارات متكررة لموقع الويب</span>.
**<span dir="rtl">السياسات</span> (Policies)** <span dir="rtl">المشتقة
من صياغة **السياق المقامر**</span> **(Contextual Bandit)**
<span dir="rtl">تكون **جشعة**</span> **(Greedy)** <span dir="rtl">بمعنى
أنها لا تأخذ في الاعتبار التأثيرات طويلة المدى للإجراءات. هذه
**السياسات** </span>**(Policies)** <span dir="rtl">تعامل كل زيارة لموقع
الويب كما لو كانت زيارة جديدة من زائر تم اختياره عشوائيًا من مجموعة زوار
الموقع. بعدم استخدام حقيقة أن العديد من المستخدمين يزورون نفس المواقع
بشكل متكرر، فإن **السياسات الجشعة**</span> **(Greedy Policies)**
<span dir="rtl">لا تستفيد من الإمكانات المتاحة من التفاعلات طويلة المدى
مع المستخدمين الفرديين</span>.

<span dir="rtl">كمثال على كيفية استفادة استراتيجية تسويقية من التفاعل
طويل المدى مع المستخدمين، قارن</span> **Theocharous et al.**
<span dir="rtl">بين **سياسة جشعة**</span> **(Greedy Policy)**
<span dir="rtl">و**سياسة طويلة المدى**</span>
**<span dir="rtl">(</span>Long-Term Policy<span dir="rtl">)
</span>**<span dir="rtl">لعرض إعلانات لشراء منتج، مثل سيارة. قد تعرض
**السياسة الجشعة**</span> **<span dir="rtl">(</span>Greedy
<span dir="rtl"></span>Policy<span dir="rtl">)
</span>**<span dir="rtl">خصمًا إذا قام المستخدم بشراء السيارة فورًا. قد
يأخذ المستخدم العرض أو يغادر الموقع، وإذا عاد في أي وقت إلى الموقع، فمن
المحتمل أن يرى نفس العرض. من ناحية أخرى، يمكن **للسياسة طويلة
المدى**</span> **(Long-Term Policy)** <span dir="rtl">أن تنقل المستخدم
"عبر مسار المبيعات" قبل تقديم العرض النهائي. قد تبدأ بوصف توفر شروط
تمويل مواتية، ثم تمدح قسم الخدمة الممتاز، ثم، في الزيارة التالية، تقدم
الخصم النهائي. هذا النوع من **السياسات**</span> **(Policies)**
<span dir="rtl">يمكن أن يؤدي إلى المزيد من النقرات من المستخدم على مدار
زيارات متكررة للموقع، وإذا تم تصميم **السياسة**</span> **(Policy)**
<span dir="rtl">بشكل مناسب، يمكن أن يؤدي إلى مبيعات نهائية أكثر</span>.

<span dir="rtl">عملًا في شركة</span> Adobe Systems
Incorporated<span dir="rtl">، قام</span> **Theocharous et al.**
<span dir="rtl">بإجراء تجارب لمعرفة ما إذا كانت **السياسات**</span>
**(Policies)** <span dir="rtl">المصممة لتعظيم النقرات على المدى الطويل
يمكنها بالفعل تحسين **السياسات الجشعة قصيرة المدى** </span>**(Short-Term
Greedy Policies)**<span dir="rtl">.</span> <span dir="rtl">توفر</span>
**Adobe Marketing Cloud**<span dir="rtl">، وهي مجموعة من الأدوات التي
تستخدمها العديد من الشركات لإدارة حملات التسويق الرقمية، بنية تحتية
لأتمتة الإعلانات المستهدفة للمستخدمين وحملات جمع التبرعات. يتطلب نشر
**السياسات**</span> **(Policies)** <span dir="rtl">الجديدة باستخدام هذه
الأدوات مخاطر كبيرة لأن **السياسة** </span>**(Policy)**
<span dir="rtl">الجديدة قد تؤدي إلى أداء سيئ. لهذا السبب، كان من الضروري
لفريق البحث تقييم أداء **السياسة** </span>**(Policy)**
<span dir="rtl">إذا تم نشرها فعليًا، ولكن على أساس بيانات تم جمعها تحت
تنفيذ **سياسات** </span>**(Policies)** <span dir="rtl">أخرى. كان جانبًا
حاسمًا من هذا البحث هو **تقييم خارج السياسة** </span>**(Off-Policy
Evaluation)**<span dir="rtl">. بالإضافة إلى ذلك، أراد الفريق القيام بذلك
بثقة عالية لتقليل مخاطر نشر **السياسة الجديدة (**</span>**New
<span dir="rtl"></span>Policy<span dir="rtl">).
</span>**<span dir="rtl">على الرغم من أن **تقييم خارج السياسة**</span>
**(Off-Policy Evaluation)** <span dir="rtl">بثقة عالية كان مكونًا مركزيًا
في هذا البحث (انظر أيضًا</span> **Thomas, 2015**<span dir="rtl">؛</span>
**Thomas, Theocharous, and Ghavamzadeh, 2015**<span dir="rtl">)، فإننا
نركز هنا فقط على الخوارزميات ونتائجها</span>.

<span dir="rtl">قارن</span> **Theocharous et al.** <span dir="rtl">نتائج
خوارزميتين لتعلم **سياسات**</span> **(Policies)** <span dir="rtl">توصيات
الإعلانات. كانت الخوارزمية الأولى، التي أسموها **التحسين الجشع**
</span>**(Greedy Optimization)**<span dir="rtl">، تهدف إلى تعظيم احتمال
**النقرات الفورية**</span> **(Immediate Clicks)** <span dir="rtl">فقط.
كما في صياغة **السياق المقامر القياسية** </span>**(Standard Contextual
Bandit Formulation)**<span dir="rtl">، لم تأخذ هذه الخوارزمية بعين
الاعتبار التأثيرات طويلة المدى للتوصيات. أما الخوارزمية الأخرى، وهي
**خوارزمية تعلم معزز** </span>**(Reinforcement Learning Algorithm)**
<span dir="rtl">مبنية على صياغة **مسألة اتخاذ قرار ماركوف**
</span>**(MDP - Markov Decision Problem)**<span dir="rtl">، فقد هدفت إلى
تحسين عدد النقرات التي يقوم بها المستخدمون عبر زيارات متعددة لموقع
الويب. أسموا هذه الخوارزمية الأخيرة **تحسين قيمة العمر الافتراضي**
</span>**(LTV - Life-Time Value Optimization)**<span dir="rtl">.</span>
<span dir="rtl">واجهت كلتا الخوارزميتين مشكلات صعبة لأن **إشارة
المكافأة**</span> **(Reward Signal)** <span dir="rtl">في هذا المجال
نادرة جدًا لأن المستخدمين عادةً لا ينقرون على الإعلانات، ونقر المستخدمين
عشوائي للغاية مما يجعل العوائد ذات تباين مرتفع</span>.

<span dir="rtl">تم استخدام مجموعات بيانات من قطاع البنوك لتدريب واختبار
هذه الخوارزميات. تألفت مجموعات البيانات من العديد من المسارات الكاملة
لتفاعل العملاء مع موقع البنك على الويب والتي أظهرت لكل عميل واحدة من
مجموعة من العروض المحتملة. إذا نقر العميل، كانت المكافأة تساوي 1، وإلا
كانت تساوي 0. احتوت إحدى مجموعات البيانات على حوالي 200,000 تفاعل من شهر
واحد لحملة بنك قدمت بشكل عشوائي واحدة من 7 عروض. أما مجموعة البيانات
الأخرى من حملة بنك آخر فاحتوت على 4,000,000 تفاعل تتضمن 12 عرضًا محتملاً.
تضمنت جميع التفاعلات ميزات العملاء مثل الوقت منذ آخر زيارة للعميل إلى
الموقع، وعدد زياراتهم حتى الآن، ووقت آخر نقرة للعميل، والموقع الجغرافي،
وأحد الاهتمامات، وميزات تقدم معلومات ديموغرافية</span>.

<span dir="rtl">استند **التحسين الجشع**</span> **(Greedy Optimization)**
<span dir="rtl">إلى **تخطيط**</span> **(Mapping)** <span dir="rtl">يقدر
احتمال النقرة كدالة لميزات المستخدم. تم تعلم التخطيط عبر **التعليم
الخاضع للإشراف** </span>**(Supervised Learning)** <span dir="rtl">من
إحدى مجموعات البيانات باستخدام خوارزمية **الغابة العشوائية**
</span>**(RF - Random Forest)** <span dir="rtl"></span>(Breiman,
2001)<span dir="rtl">.</span> <span dir="rtl">تم استخدام خوارزميات
**الغابة العشوائية**</span> **(RF)** <span dir="rtl">على نطاق واسع
للتطبيقات الصناعية واسعة النطاق لأنها أدوات تنبؤية فعالة ولا تميل إلى
المبالغة في التكيف، وهي غير حساسة نسبيًا للقيم المتطرفة والضوضاء. ثم
استخدم</span> **Theocharous et al.** <span dir="rtl">التخطيط لتعريف
**سياسة  **
</span>**"-ε"<span dir="rtl">جشعة</span> (ε-Greedy Policy)**
<span dir="rtl">التي تختار باحتمالية</span> 1−ϵ <span dir="rtl">العرض
الذي تتنبأ به خوارزمية **الغابة العشوائية**</span> **(RF)**
<span dir="rtl">ليكون له أعلى احتمال لإنتاج نقرة، وباحتمالية</span> ε
<span dir="rtl">تختار من بين العروض الأخرى بشكل عشوائي</span>.

<span dir="rtl">استخدم **تحسين قيمة العمر الافتراضي**</span> **(LTV -
Life-Time Value Optimization)** **<span dir="rtl">خوارزمية تعلم معزز
دفعي</span> (Batch-Mode Reinforcement Learning Algorithm)**
<span dir="rtl">تسمى **تكرار  **
</span> **Q<span dir="rtl">الملائم</span> (FQI - Fitted Q
Iteration)**<span dir="rtl">.</span> <span dir="rtl">وهي نوع من **تكرار
القيمة الملائم  
(**</span>**Fitted Value Iteration)** (Gordon,
1999<span dir="rtl">)</span> <span dir="rtl">متكيف مع</span> **Q
<span dir="rtl">تعلم</span> (Q-Learning)**<span dir="rtl">.</span>
<span dir="rtl">يعني **النمط الدفعي**</span> **(Batch-Mode)**
<span dir="rtl">أن مجموعة البيانات الكاملة للتعليم متاحة من البداية، على
عكس **النمط عبر الإنترنت**</span> **(Online Mode)** <span dir="rtl">الذي
نركز عليه في هذا الكتاب، حيث يتم اكتساب البيانات بشكل متتابع أثناء تنفيذ
**خوارزمية التعليم** </span>**(Learning
Algorithm)**<span dir="rtl">.</span> <span dir="rtl">تكون خوارزميات
**التعليم المعزز الدفعي**</span> **(Batch-Mode Reinforcement Learning)**
<span dir="rtl">ضرورية أحيانًا عندما يكون **التعليم عبر الإنترنت**</span>
**(Online Learning)** <span dir="rtl">غير عملي، ويمكنها استخدام أي
خوارزمية تعلم خاضعة للإشراف مناسبة للنمط الدفعي، بما في ذلك الخوارزميات
المعروفة بتوسعها الجيد إلى المساحات عالية الأبعاد. يعتمد تقارب
**تكرار**</span> **Q <span dir="rtl">الملائم</span> (FQI - Fitted Q
Iteration)** <span dir="rtl">على خصائص **خوارزمية تقريب الدوال**
</span>**(Function Approximation Algorithm)** (Gordon,
1999)<span dir="rtl">.</span> <span dir="rtl">لتطبيقهم على **تحسين قيمة
العمر الافتراضي** </span>**(LTV Optimization)**<span dir="rtl">،
استخدم</span> **Theocharous et al.** <span dir="rtl">نفس خوارزمية
**الغابة العشوائية**</span> **(RF)** <span dir="rtl">التي استخدموها في
نهج **التحسين الجشع** </span>**(Greedy
Optimization)**<span dir="rtl">.</span> <span dir="rtl">نظرًا لأن **تقارب
تكرار**</span> **Q <span dir="rtl">الملائم</span> (FQI Convergence)**
<span dir="rtl">في هذه الحالة ليس أحاديًا، فقد تابع</span> **Theocharous
et al.** <span dir="rtl">أفضل</span> **FQI <span dir="rtl">سياسة</span>
(FQI Policy)** <span dir="rtl">عن طريق **تقييم خارج السياسة**
</span>**(Off-Policy Evaluation)** <span dir="rtl">باستخدام مجموعة
تدريبية للتحقق من الصحة. كانت **السياسة النهائية** </span>**(Final
Policy)** <span dir="rtl">لاختبار نهج **تحسين قيمة العمر
الافتراضي**</span> **(LTV)** <span dir="rtl">هي</span> **"-ε"
<span dir="rtl">السياسة  
(</span>ε-Greedy Policy<span dir="rtl">)
</span>**<span dir="rtl">المبنية على أفضل **سياسة**</span> **(Policy)**
<span dir="rtl">تم إنتاجها بواسطة **تكرار**</span> **Q
<span dir="rtl">الملائم</span> (FQI - Fitted Q Iteration)**
<span dir="rtl">مع تعيين **دالة قيمة الإجراء الأولية**</span>
**<span dir="rtl">(</span>Initial Action-Value
Function<span dir="rtl">)</span>** <span dir="rtl">إلى التخطيط الذي تم
إنتاجه بواسطة **الغابة العشوائية**</span> **(RF)** <span dir="rtl">لنهج
**التحسين الجشع** </span>**(Greedy
Optimization)**<span dir="rtl">.</span>

<span dir="rtl">لقياس أداء **السياسات**</span> **(Policies)**
<span dir="rtl">التي أنتجتها خوارزميات **التحسين الجشع**</span>
**(Greedy)** <span dir="rtl">و**تحسين قيمة العمر الافتراضي**
</span>**(LTV Optimization)**<span dir="rtl">، استخدم</span>
**Theocharous et al.** **<span dir="rtl">مقياس معدل النقرات</span>
(CTR - Click-Through Rate)** <span dir="rtl">ومقياسًا أسموه **مقياس قيمة
العمر الافتراضي (**</span>**LTV Metric<span dir="rtl">)</span>**.
<span dir="rtl">تتشابه هذه المقاييس، باستثناء أن **مقياس قيمة العمر
الافتراضي**</span> **(LTV Metric)** <span dir="rtl">يميز بشكل حاسم بين
زوار الموقع الفرديين</span>.

``` math
\text{CTR} = \frac{\text{Total \# of Clicks}}{\text{Total \# of Visits}}
```

``` math
\text{LTV} = \frac{\text{Total \# of Clicks}}{\text{Total \# of Visitors}}
```

<span dir="rtl">يوضح **الشكل 16.8** كيف تختلف هذه المقاييس. كل دائرة
تمثل زيارة مستخدم للموقع؛ الدوائر السوداء تمثل زيارات قام المستخدم
خلالها بالنقر. كل صف يمثل زيارات قام بها مستخدم معين. بعدم التمييز بين
الزوار، يكون **معدل النقرات**</span> **(CTR)** <span dir="rtl">لهذه
التسلسلات هو 0.35، بينما **قيمة العمر الافتراضي** </span>**(LTV)**
<span dir="rtl">هي 1.5. نظرًا لأن **قيمة العمر الافتراضي**</span>
**(LTV)** <span dir="rtl">أكبر من **معدل النقرات**</span> **(CTR)**
<span dir="rtl">إلى الحد الذي يعيد فيه المستخدمون الفرديون زيارة الموقع،
فهي مؤشر على مدى نجاح **السياسة**</span> **(Policy)** <span dir="rtl">في
تشجيع المستخدمين على الانخراط في تفاعلات ممتدة مع الموقع</span>.

<img src="./media/image193.png"
style="width:6.26806in;height:1.88264in" />

<span dir="rtl">الشكل 16.8:</span> <span dir="rtl">معدل النقرات</span>
(CTR) <span dir="rtl">مقابل قيمة العمر الافتراضي</span>
<span dir="rtl">(</span>LTV<span dir="rtl">) كل دائرة تمثل زيارة مستخدم؛
الدوائر السوداء تمثل زيارات قام المستخدم خلالها بالنقر. مقتبس من</span>
**Theocharous et al. (2015)**<span dir="rtl">.</span>

<span dir="rtl">تم اختبار **السياسات**</span> **(Policies)**
<span dir="rtl">التي أنتجتها خوارزميات **التحسين الجشع**</span>
**(Greedy)** <span dir="rtl">و**تحسين قيمة العمر الافتراضي**</span>
**(LTV Optimization)** <span dir="rtl">باستخدام طريقة **تقييم خارج
السياسة عالية الثقة** </span>**(High Confidence Off-Policy Evaluation)**
<span dir="rtl">على مجموعة اختبار تتألف من تفاعلات حقيقية مع موقع بنك
تمت خدمتها بواسطة **سياسة عشوائية** </span>**(Random
Policy)**<span dir="rtl">.</span> <span dir="rtl">كما هو متوقع، أظهرت
النتائج أن **التحسين الجشع**</span> **(Greedy Optimization)**
<span dir="rtl">حقق أفضل أداء كما تم قياسه بواسطة **معدل النقرات**
</span>**(CTR Metric)**<span dir="rtl">، بينما حقق **تحسين قيمة العمر
الافتراضي**</span> **<span dir="rtl">(</span>LTV
<span dir="rtl"></span>Optimization<span dir="rtl">)
</span>**<span dir="rtl">أفضل أداء كما تم قياسه بواسطة **مقياس قيمة
العمر الافتراضي** </span>**(LTV Metric)**<span dir="rtl">. علاوة على
ذلك، على الرغم من أننا قد أغفلنا تفاصيله، فإن طريقة **تقييم خارج السياسة
عالية الثقة** </span>**(High Confidence Off-Policy Evaluation Method)**
<span dir="rtl">قدمت ضمانات احتمالية بأن طريقة **تحسين قيمة العمر
الافتراضي**</span> **(LTV Optimization)** <span dir="rtl">ستنتج،
باحتمالية عالية، **سياسات** </span>**(Policies)** <span dir="rtl">تحسن
على **السياسات**</span> **(Policies)** <span dir="rtl">المنتشرة حاليًا.
بفضل هذه الضمانات الاحتمالية، أعلنت</span> Adobe <span dir="rtl">في عام
2016 أن **الخوارزمية الجديدة لتحسين قيمة العمر الافتراضي**</span>
**<span dir="rtl">(</span>New LTV
<span dir="rtl"></span>Algorithm<span dir="rtl">)
</span>**<span dir="rtl">ستكون مكونًا قياسيًا في</span> **Adobe Marketing
Cloud** <span dir="rtl">بحيث يمكن لبائع التجزئة إصدار سلسلة من العروض
وفقًا لسياسة من المرجح أن تحقق عائدًا أعلى من **سياسة**</span>
**(Policy)** <span dir="rtl">غير حساسة للنتائج طويلة المدى</span>.

**<u>16.8 <span dir="rtl">الارتفاع الحراري</span> (Thermal
Soaring)</u>**

<span dir="rtl">الطيور والطائرات الشراعية تستفيد من التيارات الهوائية
الصاعدة، المعروفة بالحرارية، لزيادة الارتفاع بهدف الحفاظ على الطيران مع
استهلاك قليل من الطاقة أو بدون طاقة. يُطلق على هذا السلوك اسم **الارتفاع
الحراري** </span>**(Thermal Soaring)**<span dir="rtl">، وهو مهارة معقدة
تتطلب الاستجابة لإشارات بيئية دقيقة لزيادة الارتفاع عن طريق استغلال عمود
الهواء الصاعد لأطول فترة ممكنة. استخدم</span> **Reddy, Celani,
Sejnowski, and Vergassola (2016)** **<span dir="rtl">التعليم
المعزز</span> <span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**
<span dir="rtl">للتحقيق في **سياسات الارتفاع الحراري**</span> **(Thermal
Soaring Policies)** <span dir="rtl">الفعالة في الاضطرابات الجوية القوية
التي تصاحب عادة التيارات الهوائية الصاعدة. كان هدفهم الأساسي هو تقديم
فهم أعمق للإشارات التي تستشعرها الطيور وكيفية استخدامها لتحقيق أداء مذهل
في الارتفاع الحراري، ولكن النتائج أيضًا تساهم في تطوير التكنولوجيا
المتعلقة بالطائرات الشراعية المستقلة</span>.

<span dir="rtl">تم تطبيق **التعليم المعزز**</span> **(Reinforcement
Learning)** <span dir="rtl">سابقًا على مشكلة الملاحة الفعالة للوصول إلى
منطقة التيار الصاعد الحراري</span> (Woodbury, Dunn, and Valasek,
2014)<span dir="rtl">، ولكن لم يتم تطبيقه على المشكلة الأكثر تحديًا
المتمثلة في التحليق داخل الاضطرابات المصاحبة للتيار الصاعد نفسه</span>.

<span dir="rtl">قام</span> **Reddy et al.** <span dir="rtl">بنمذجة مشكلة
الارتفاع الحراري كمسألة **اتخاذ قرار ماركوف مستمر** </span>**(Continuing
MDP)** <span dir="rtl">مع الخصم. تفاعل **العميل**</span> **(Agent)**
<span dir="rtl">مع نموذج مفصل لطائرة شراعية تطير في هواء مضطرب. وكرسوا
جهدًا كبيرًا لجعل النموذج ينتج ظروف ارتفاع حراري واقعية، بما في ذلك
التحقيق في عدة نهج مختلفة لنمذجة الغلاف الجوي</span>.

<span dir="rtl">بالنسبة لتجارب التعليم، تم نمذجة تدفق الهواء في صندوق
ثلاثي الأبعاد بأضلاع طولها كيلومتر واحد، كان أحدها عند مستوى الأرض،
باستخدام مجموعة معقدة من المعادلات التفاضلية الجزئية المستندة إلى
الفيزياء التي تتضمن سرعة الهواء، ودرجة الحرارة، والضغط. أدى إدخال
اضطرابات عشوائية صغيرة في المحاكاة العددية إلى جعل النموذج ينتج نظائرًا
للتيارات الصاعدة الحرارية والاضطرابات المصاحبة لها (الشكل 16.9 اليسار).
تم نمذجة الطيران الشراعي بواسطة معادلات ديناميكا الهواء التي تتضمن
السرعة، الرفع، السحب، وعوامل أخرى تحكم الطيران بلا طاقة للطائرة ذات
الجناح الثابت. كان التحرك بالطائرة الشراعية يتطلب تغيير زاوية الهجوم
(الزاوية بين جناح الطائرة الشراعية واتجاه تدفق الهواء) وزاوية الميلان
(الشكل 16.9 اليمين)</span>.

<img src="./media/image194.png"
style="width:6.26806in;height:2.36944in" />

### 

<span dir="rtl">لشكل 16.9: نموذج الارتفاع الحراري</span> (Thermal
Soaring Model) <span dir="rtl"></span>

**<span dir="rtl">اليسار</span>:** <span dir="rtl">لقطة لحقل السرعة
العمودية في المكعب الجوي المحاكى: تظهر المناطق ذات التدفق الصاعد الكبير
باللون الأحمر، والمناطق ذات التدفق الهابط الكبير باللون الأزرق</span>.  
**<span dir="rtl">اليمين</span>:** <span dir="rtl">رسم بياني للطيران بلا
طاقة يظهر زاوية الميلان</span> μ <span dir="rtl">وزاوية الهجوم</span>
α<span dir="rtl">.</span>  
<span dir="rtl">مقتبس بإذن من</span> **PNAS**<span dir="rtl">، المجلد
113(22)، ص</span>. E4879<span dir="rtl">، 2016،</span> **Reddy, Celani,
Sejnowski, and Vergassola, Learning to Soar in Turbulent Environments**.

<span dir="rtl">الواجهة بين **العميل**</span> **(Agent)**
<span dir="rtl">و**البيئة**</span> **(Environment)**
<span dir="rtl">تطلبت تحديد **إجراءات العميل**</span>
**<span dir="rtl">(</span>Agent's
<span dir="rtl"></span>Actions<span dir="rtl">)</span>**<span dir="rtl">،
**معلومات الحالة**</span> **(State Information)** <span dir="rtl">التي
يتلقاها **العميل**</span> **(Agent)** <span dir="rtl">من **البيئة**
</span>**(Environment)**<span dir="rtl">، و**إشارة المكافأة**
</span>**(Reward Signal)**<span dir="rtl">.</span> <span dir="rtl">من
خلال تجربة العديد من الاحتمالات، قرر</span> **Reddy et al.**
<span dir="rtl">أن ثلاث **إجراءات**</span> **(Actions)**
<span dir="rtl">لكل من **زاوية الهجوم** </span>**(Angle of Attack)**
<span dir="rtl">و**زاوية الميلان**</span> **(Bank Angle)**
<span dir="rtl">كانت كافية لأغراضهم: زيادة أو تقليل **زاوية
الميلان**</span> **<span dir="rtl">(</span>Bank
<span dir="rtl"></span>Angle<span dir="rtl">)
</span>**<span dir="rtl">و**زاوية الهجوم**</span> **(Angle of Attack)**
<span dir="rtl">الحالية بمقدار</span> 5∘ <span dir="rtl">و</span>2.5∘
<span dir="rtl">على التوالي، أو تركها دون تغيير. أدى ذلك إلى 32 **إجراءً
محتملاً** </span>**(Possible Actions)**<span dir="rtl">.</span>
<span dir="rtl">كانت **زاوية الميلان (**</span>**Bank
<span dir="rtl"></span>Angle<span dir="rtl">)
</span>**<span dir="rtl">محدودة لتبقى بين</span> −15∘
<span dir="rtl">و</span>+15∘<span dir="rtl">.</span>

<span dir="rtl">نظرًا لأن هدف دراستهم كان محاولة تحديد الحد الأدنى من
**الإشارات الحسية** </span>**(Sensory Cues)** <span dir="rtl">اللازمة
لارتفاع حراري فعال، سواء لتسليط الضوء على الإشارات التي قد تستخدمها
الطيور أثناء الارتفاع أو لتقليل تعقيد الاستشعار المطلوب للطيران الشراعي
الآلي، حاول المؤلفون استخدام مجموعات مختلفة من **الإشارات**</span>
**(Signals)** <span dir="rtl">كمدخلات لـ **العميل**
</span>**(Reinforcement Learning Agent)**<span dir="rtl">.</span>
<span dir="rtl">بدأوا باستخدام **تجميع الحالة**</span> **(State
Aggregation)** <span dir="rtl">(الفصل 9.3) لمساحة حالة رباعية الأبعاد
بأبعاد تعطي سرعة الرياح الرأسية المحلية، تسارع الرياح الرأسية المحلية،
عزم الدوران اعتمادًا على الفرق بين سرعات الرياح الرأسية عند أطراف الجناح
الأيسر والأيمن، ودرجة الحرارة المحلية. تم تقسيم كل بُعد إلى ثلاث **فئات**
</span>**(Bins)**<span dir="rtl">:</span> <span dir="rtl">عالية إيجابية،
عالية سلبية، وصغيرة. أظهرت النتائج، كما هو موضح أدناه، أن بعدين فقط من
هذه الأبعاد كانا حاسمين لسلوك ارتفاع فعال</span>.

<span dir="rtl">الهدف العام من الارتفاع الحراري هو زيادة الارتفاع قدر
الإمكان من كل عمود هوائي صاعد. حاول</span> **Reddy et al.**
<span dir="rtl">استخدام إشارة مكافأة بسيطة تكافئ **العميل**</span>
**(Agent)** <span dir="rtl">في نهاية كل حلقة بناءً على الارتفاع المكتسب
خلال الحلقة، وإشارة مكافأة سلبية كبيرة إذا لامست الطائرة الشراعية الأرض،
وصفر في الحالات الأخرى. وجدوا أن التعليم لم يكن ناجحًا مع هذه **إشارة
المكافأة** </span>**(Reward Signal)** <span dir="rtl">للحلقات ذات المدة
الواقعية، وأن **آثار الأهلية**</span> **(Eligibility Traces)**
<span dir="rtl">لم تساعد. من خلال تجربة إشارات مكافأة مختلفة، وجدوا أن
التعليم كان أفضل مع **إشارة مكافأة**</span> **(Reward Signal)**
<span dir="rtl">تجمع خطيًا في كل خطوة زمنية بين سرعة الرياح الرأسية
وتسارع الرياح الرأسية الملاحظة في الخطوة الزمنية السابقة</span>.

<span dir="rtl">تم التعليم باستخدام خوارزمية **سارسا**</span> **(Sarsa)
<span dir="rtl">ذات خطوة واحدة</span>**<span dir="rtl">، مع اختيار
**الإجراءات**</span> **(Actions)** <span dir="rtl">وفقًا لتوزيع
**سوفت-ماكس**</span> **(Soft-Max Distribution)** <span dir="rtl">استنادًا
إلى **قيم الإجراءات المعيارية**</span> **(Normalized Action
Values)**<span dir="rtl">. تم حساب احتمالات **الإجراءات**</span>
**(Action Probabilities)** <span dir="rtl">وفقًا للمعادلة (13.2) مع
تفضيلات **الإجراءات** </span>**(Action
Preferences)**<span dir="rtl">.</span>

``` math
h(s,a,\theta) = \frac{\widehat{q}(s,a,\theta) - \min_{b}\widehat{q}(s,b,\theta)}{\tau\left( \max_{b}\widehat{q}(s,b,\theta) - \min_{b}\widehat{q}(s,b,\theta) \right)}
```

<span dir="rtl">حيث أن</span> θ <span dir="rtl">هو **متجه
بارامترات**</span> **(Parameter Vector)** <span dir="rtl">يحتوي على مكون
واحد لكل **إجراء** </span>**(Action)** <span dir="rtl">ولكل مجموعة مجمعة
من **الحالات** </span>**(States)**<span dir="rtl">، وتقوم</span>
$`q\hat{}(s,a,\theta)`$ <span dir="rtl">فقط بإرجاع المكون المقابل
لـ</span> $`s,as`$ <span dir="rtl">بالطريقة المعتادة لأساليب **تجميع
الحالات** </span>**(State Aggregation Methods)**<span dir="rtl">. تكون
**تفضيلات الإجراءات**</span> **(Action Preferences)** <span dir="rtl">في
المعادلة أعلاه من خلال **تطبيع** </span>**(Normalizing)**
**<span dir="rtl">القيم التقديرية للإجراءات</span> (Approximate Action
Values)** <span dir="rtl">إلى الفترة</span> \[0,1\] <span dir="rtl">ثم
قسمتها على</span> $`\tau`$<span dir="rtl">، وهي **بارامتر درجة
الحرارة**</span> **(Temperature Parameter)** <span dir="rtl">الموجب.
كلما زادت</span> τ<span dir="rtl">، تقل اعتمادية احتمال اختيار
**الإجراء**</span> **(Action)** <span dir="rtl">على تفضيله؛ وكلما
قلت</span> $`\tau`$ <span dir="rtl">باتجاه الصفر، يقترب احتمال اختيار
الإجراء الأكثر تفضيلًا من الواحد، مما يجعل **السياسة**</span>
**(Policy)** <span dir="rtl">تقترب من **السياسة الجشعة**
</span>**(Greedy Policy)**<span dir="rtl">.</span> <span dir="rtl">تم
ضبط **بارامتر درجة الحرارة** </span>**(Temperature
Parameter)<span dir="rtl">  
</span>**$`\tau`$ <span dir="rtl">في البداية إلى 2.0 وتم تقليله تدريجيًا
إلى 0.2 أثناء التعليم. تم حساب **تفضيلات الإجراءات** </span>**(Action
Preferences)** <span dir="rtl">من التقديرات الحالية للقيم التقديرية
للإجراءات: تم إعطاء الإجراء ذو القيمة التقديرية القصوى **تفضيل**
</span>**(Preference)** $`1\tau`$ ​<span dir="rtl">، بينما تم إعطاء
الإجراء ذو القيمة التقديرية الدنيا تفضيل 0، وتم قياس تفضيلات الإجراءات
الأخرى بين هذين القيمتين</span>.

<span dir="rtl">تم تثبيت **بارامتري حجم الخطوة**</span> **(Step-Size)**
<span dir="rtl">و**معدل الخصم**</span> **(Discount-Rate)**
<span dir="rtl">عند 0.1 و0.98 على التوالي. جرت كل **حلقة تعلم**</span>
**(Learning Episode)** <span dir="rtl">مع **العميل**</span> **(Agent)**
<span dir="rtl">الذي يتحكم في الطيران المحاكى خلال فترة مستقلة من
**تيارات الهواء المضطربة المحاكية**</span>
**<span dir="rtl">(</span>Simulated <span dir="rtl"></span>Turbulent Air
Currents<span dir="rtl">)</span>**. <span dir="rtl">استمرت كل حلقة لمدة
2.5 دقيقة محاكاة مع خطوة زمنية قدرها  
1 ثانية.</span> **<span dir="rtl">تعلّم</span> (Learning)**
<span dir="rtl">تقارب فعليًا بعد بضع مئات من الحلقات. يظهر **اللوحة
اليسرى من الشكل 16.10** مسارًا عشوائيًا قبل التعليم حيث يختار
**العميل**</span> **(Agent)** **<span dir="rtl">الإجراءات</span>
(Actions)** <span dir="rtl">بشكل عشوائي. بدءًا من أعلى الحجم الموضح، يكون
مسار الطائرة الشراعية في الاتجاه المشار إليه بالسهم وسرعان ما تفقد
الارتفاع</span>. <span dir="rtl">**اللوحة اليمنى من الشكل 16.10** تظهر
مسارًا بعد التعليم. تبدأ الطائرة الشراعية من نفس المكان (هنا تظهر في أسفل
الحجم) وتزيد ارتفاعها بالدوران داخل عمود الهواء الصاعد. على الرغم من
أن</span> **Reddy et al.** <span dir="rtl">وجدوا أن الأداء يختلف بشكل
كبير عبر فترات محاكاة مختلفة لتدفق الهواء، إلا أن عدد المرات التي لامست
فيها الطائرة الشراعية الأرض قد انخفض بشكل مستمر إلى ما يقرب من الصفر مع
تقدم التعليم</span>.

<img src="./media/image195.png"
style="width:6.26806in;height:2.82431in" />

<span dir="rtl">الشكل 16.10:</span> <span dir="rtl">مسارات الارتفاع
الحراري</span> (Thermal Soaring Trajectories)
<span dir="rtl">النموذجية</span>

<span dir="rtl">الأسهم تظهر اتجاه الطيران من نفس نقطة البداية (لاحظ أن
مقاييس الارتفاع مختلفة)</span>.  
**<span dir="rtl">اليسار</span>:** <span dir="rtl">قبل التعليم: يختار
**العميل**</span> **(Agent)** **<span dir="rtl">الإجراءات</span>
(Actions)** <span dir="rtl">عشوائيًا وتنخفض الطائرة الشراعية</span>.
<span dir="rtl"></span>

**<span dir="rtl">اليمين:</span>** <span dir="rtl">بعد التعليم: تكتسب
الطائرة الشراعية ارتفاعًا باتباع مسار حلزوني</span>.  
<span dir="rtl">مقتبس بإذن من</span> **PNAS**<span dir="rtl">، المجلد
113(22)، ص</span>. E4879<span dir="rtl">، 2016،</span> **Reddy, Celani,
Sejnowski, and Vergassola, Learning to Soar in Turbulent
Environments**<span dir="rtl">.</span>

<span dir="rtl">بعد تجربة مجموعات مختلفة من الميزات المتاحة لـ **العميل
التعليمي** </span>**(Learning Agent)**<span dir="rtl">، تبين أن الجمع
بين **تسارع الرياح العمودية**</span> **(Vertical Wind Acceleration)**
<span dir="rtl">و**عزم الدوران** </span>**(Torques)**
<span dir="rtl">كان الأكثر فعالية. افترض المؤلفون أن هذه الميزات توفر
معلومات حول تدرج سرعة الرياح العمودية في اتجاهين مختلفين، مما يسمح لجهاز
التحكم بالاختيار بين الالتفاف عن طريق تغيير **زاوية الميلان**
</span>**(Bank Angle)** <span dir="rtl">أو الاستمرار في نفس المسار بترك
زاوية الميلان كما هي. يتيح ذلك للطائرة الشراعية البقاء داخل عمود الهواء
الصاعد</span>. **<span dir="rtl">سرعة الرياح العمودية</span> (Vertical
Wind Velocity)** <span dir="rtl">تشير إلى قوة التيار الحراري ولكنها لا
تساعد في البقاء داخل التدفق. وجدوا أن الحساسية لدرجة الحرارة لم تكن ذات
فائدة كبيرة. كما وجدوا أن التحكم في **زاوية الهجوم**</span> **(Angle of
Attack)** <span dir="rtl">ليس مفيدًا للبقاء داخل تيار حراري معين، بل يكون
مفيدًا عند الانتقال بين التيارات الحرارية عند تغطية مسافات كبيرة، كما هو
الحال في الطيران عبر البلاد وهجرة الطيور</span>.

<span dir="rtl">نظرًا لأن التحليق في مستويات مختلفة من الاضطرابات يتطلب
**سياسات**</span> **(Policies)** <span dir="rtl">مختلفة، تم التدريب في
ظروف تتراوح بين اضطرابات ضعيفة وقوية. في الاضطرابات القوية، أدى التغير
السريع في سرعة الرياح والطائرة الشراعية إلى تقليل الوقت المتاح لجهاز
التحكم للرد. أدى ذلك إلى تقليل مقدار التحكم الممكن مقارنة بما كان ممكنًا
عند المناورة عندما كانت التقلبات ضعيفة. قام</span> **Reddy et al.**
<span dir="rtl">بفحص **السياسات**</span> **(Policies)**
<span dir="rtl">التي تعلمتها خوارزمية **سارسا**</span> **(Sarsa)**
<span dir="rtl">في ظل هذه الظروف المختلفة. كانت السمات المشتركة بين
السياسات التي تم تعلمها في جميع الأنظمة هي: عند استشعار **تسارع الرياح
السلبية** </span>**(Negative Wind Acceleration)**<span dir="rtl">، قم
بالميلان بحدة في اتجاه الجناح ذي الرفع الأعلى؛ وعند استشعار **تسارع
الرياح الإيجابية الكبيرة**</span> **<span dir="rtl">(</span>Large
Positive Wind <span dir="rtl"></span>Acceleration<span dir="rtl">)
</span>**<span dir="rtl">وعدم وجود عزم دوران، لا تفعل شيئًا. ومع ذلك، أدت
مستويات الاضطراب المختلفة إلى اختلافات في **السياسات**
</span>**(Policies)**<span dir="rtl">.</span> <span dir="rtl">كانت
السياسات التي تعلمت في الاضطرابات القوية أكثر تحفظًا من حيث تفضيلها
**زاويا الميلان الصغيرة** </span>**(Small Bank
Angles)**<span dir="rtl">، بينما في الاضطرابات الضعيفة، كان الإجراء
الأفضل هو الالتفاف قدر الإمكان عن طريق الميلان الحاد. قادت الدراسة
المنهجية لزوايا الميلان المفضلة من قبل **السياسات**</span>
**(Policies)** <span dir="rtl">التي تم تعلمها تحت الظروف المختلفة
المؤلفين إلى اقتراح أنه من خلال اكتشاف متى يتجاوز تسارع الرياح العمودية
عتبة معينة يمكن لجهاز التحكم ضبط **السياسة**</span> **(Policy)**
<span dir="rtl">للتكيف مع أنظمة الاضطراب المختلفة</span>.

<span dir="rtl">كما أجرى</span> **Reddy et al.** <span dir="rtl">تجارب
للتحقيق في تأثير **بارامتر معدل الخصم**</span>
**<span dir="rtl">(</span>Discount-Rate
<span dir="rtl"></span>Parameter<span dir="rtl">)</span>** γ
<span dir="rtl">على أداء **السياسات**</span> **(Policies)**
<span dir="rtl">المتعلمة. وجدوا أن الارتفاع المكتسب في حلقة معينة ازداد
مع زيادة</span> γ<span dir="rtl">، حيث بلغ الحد الأقصى عند</span>
γ=0.99<span dir="rtl">، مما يشير إلى أن التحليق الحراري الفعال يتطلب أخذ
التأثيرات طويلة المدى لقرارات التحكم بعين الاعتبار</span>.

<span dir="rtl">توضح هذه الدراسة الحاسوبية حول التحليق الحراري كيف يمكن
أن يُحرز **التعليم المعزز** </span>**(Reinforcement Learning)**
<span dir="rtl">تقدمًا نحو تحقيق أهداف مختلفة. تساهم **السياسات**
</span>**(Policies)** <span dir="rtl">المتعلمة التي لديها وصول إلى
مجموعات مختلفة من الإشارات البيئية وأفعال التحكم في كل من الهدف الهندسي
المتمثل في تصميم الطائرات الشراعية المستقلة والهدف العلمي المتمثل في
تحسين فهم مهارات التحليق لدى الطيور. في كلتا الحالتين، يمكن اختبار
الفرضيات الناتجة عن تجارب التعليم في الميدان عن طريق تركيب أدوات في
الطائرات الشراعية الحقيقية ومقارنة التنبؤات مع سلوك التحليق الملاحظ لدى
الطيور</span>.

**<span dir="rtl">الفصل 17: الحدود</span> (Frontiers)**

<span dir="rtl">في هذا الفصل الأخير، نتطرق إلى بعض المواضيع التي تتجاوز
نطاق هذا الكتاب، ولكننا نرى أنها ذات أهمية خاصة لمستقبل **التعليم
المعزز** </span>**(Reinforcement Learning)**<span dir="rtl">.</span>
<span dir="rtl">العديد من هذه المواضيع تأخذنا إلى ما هو أبعد مما هو
معروف بشكل موثوق، وبعضها يتجاوز إطار **عملية اتخاذ القرار
المتسلسلة**</span> **(MDP framework)**

**<u>17.1 <span dir="rtl">الدوال القيمة العامة والمهام المساعدة</span>
<span dir="rtl">(</span>General Value Functions and Auxiliary
Tasks<span dir="rtl">)</span></u>**

<span dir="rtl">على مدار هذا الكتاب، أصبحت فكرتنا عن **دالة
القيمة**</span> **(Value Function)** <span dir="rtl">عامة بشكل كبير. مع
**التعلم خارج السياسة**</span> **<span dir="rtl">(</span>Off-policy
Learning<span dir="rtl">)</span>**<span dir="rtl">، سمحنا لدالة القيمة
بأن تكون مشروطة على **سياسة هدف**</span> **(Target Policy)**
<span dir="rtl">عشوائية. ثم في القسم 12.8، عممنا الخصم ليشمل **دالة
الإنهاء** </span>**(Termination Function)** γ:S→
\[0,1\]<span dir="rtl">، بحيث يمكن تطبيق معدل خصم مختلف في كل خطوة زمنية
عند تحديد **العائد** </span>**(Return)** (12.17)<span dir="rtl">. هذا
سمح لنا بالتعبير عن التنبؤات حول مقدار المكافأة التي سنحصل عليها خلال
أفق يعتمد على الحالة. الخطوة التالية، وربما الأخيرة، هي التعميم لما
يتجاوز المكافآت للسماح بالتنبؤات حول إشارات عشوائية. بدلاً من التنبؤ
بمجموع المكافآت المستقبلية، قد نتنبأ بمجموع القيم المستقبلية لإحساس صوتي
أو لوني، أو لإشارة داخلية معالجة بشكل كبير مثل تنبؤ آخر. أيا كانت
الإشارة التي يتم جمعها بهذه الطريقة في تنبؤ يشبه **دالة القيمة**
</span>**(Value Function)**<span dir="rtl">، نسميها المتراكم</span>
(Cumulant) <span dir="rtl">لهذا التنبؤ. نقوم بتشكيلها في إشارة
متراكم</span> Ct​∈R<span dir="rtl">.</span> <span dir="rtl">باستخدام هذا،
تُكتب **دالة القيمة العامة** </span>**(General Value
Function)**<span dir="rtl">، أو</span> **GVF**<span dir="rtl">،
كالتالي</span>:

``` math
v_{\pi,\gamma,C}(s) = E\left\lbrack \sum_{k = t}^{\infty}{\left( \prod_{i = t + 1}^{k}{\gamma\left( S_{i} \right)} \right)C_{k + 1}}\, \middle| \, S_{t} = s,A_{t:\infty} \sim \pi \right\rbrack
```

<span dir="rtl">كما هو الحال مع **دوال القيمة التقليدية (مثل**
</span>$`\mathbf{v\pi}`$ <span dir="rtl"></span>**​ <span dir="rtl">أو
</span>**$`\mathbf{q*}`$<span dir="rtl">**)**، فإن هذه الدالة المثالية
التي نسعى لتقريبها بواسطة نموذج ذو **بارامتر** </span>**(Parameterized
Form)**<span dir="rtl">، والذي قد نستمر في تسميته</span>
$`v\hat{}(s,w)`$<span dir="rtl">، على الرغم من أنه بالطبع يجب أن يكون
هناك</span> $`w`$ <span dir="rtl">مختلف لكل تنبؤ، أي لكل اختيار
من</span> $`\pi`$<span dir="rtl">،</span> $`\gamma`$<span dir="rtl">،
و</span>$`C`$<span dir="rtl">. نظرًا لأن **الدالة القيمة العامة**</span>
**(GVF)** <span dir="rtl">ليس لها اتصال ضروري بالمكافأة، فقد يكون من
الخطأ تسميتها **دالة قيمة** </span>**(Value
Function)**<span dir="rtl">.</span> <span dir="rtl">يمكن ببساطة تسميتها
**تنبؤًا**</span> **(Prediction)** <span dir="rtl">أو، لجعلها أكثر تميزًا،
**توقعًا**</span> **(Forecast)** <span dir="rtl">(وفقًا لـ</span>
Ring<span dir="rtl">، في التحضير)</span>. <span dir="rtl">مهما كان
اسمها، فإنها في شكل **دالة قيمة** </span>**(Value Function)**
<span dir="rtl">وبالتالي يمكن تعلمها بالطرق المعتادة باستخدام الأساليب
التي تم تطويرها في هذا الكتاب لتعلم **دوال القيمة التقريبية**
</span>**(Approximate Value Functions)** <span dir="rtl">إلى جانب
**التنبؤات المكتسبة**</span> **<span dir="rtl">(</span>Learned
Predictions<span dir="rtl">)</span>**<span dir="rtl">، يمكننا أيضًا تعلم
**السياسات**</span> **(Policies)** <span dir="rtl">لزيادة هذه
**التنبؤات** </span>**(Predictions)** <span dir="rtl">بالطرق المعتادة من
خلال **التكرار العام للسياسات**</span>
**<span dir="rtl">(</span>Generalized Policy
<span dir="rtl"></span>Iteration<span dir="rtl">)</span>**
<span dir="rtl">(القسم 4.6) أو بواسطة **طرق الممثل-الناقد**
</span>**(Actor-Critic Methods)**<span dir="rtl">.</span>
<span dir="rtl">بهذه الطريقة، يمكن أن يتعلم **العميل**</span>
**(Agent)** <span dir="rtl">التنبؤ والتحكم في العديد من الإشارات، وليس
فقط المكافأة طويلة الأجل</span>.

<span dir="rtl">لماذا قد يكون من المفيد التنبؤ والتحكم في إشارات أخرى
غير المكافأة طويلة الأجل؟ هذه هي **المهام المساعدة**</span> **(Auxiliary
Tasks)** <span dir="rtl">لأنها إضافية، بالإضافة إلى المهمة الرئيسية
المتمثلة في تعظيم **المكافأة**
</span>**(Reward)**<span dir="rtl">.</span> <span dir="rtl">أحد الإجابات
هو أن القدرة على التنبؤ والتحكم في مجموعة متنوعة من الإشارات يمكن أن
تشكل نوعًا قويًا من **النماذج البيئية** </span>**(Environmental
Model)**<span dir="rtl">.</span> <span dir="rtl">كما رأينا في **الفصل
8**، يمكن أن يمكن **النموذج الجيد**</span> **(Good Model)**
<span dir="rtl">العميل من الحصول على **المكافأة** </span>**(Reward)**
<span dir="rtl">بشكل أكثر كفاءة. يتطلب الأمر بعض المفاهيم الإضافية
لتطوير هذا الجواب بوضوح، لذلك نؤجله إلى القسم التالي. أولاً، دعنا نعتبر
طريقتين أبسط يمكن من خلالها أن تكون التنبؤات المتنوعة مفيدة **للعميل في
التعليم المعزز** </span>**(Reinforcement Learning
Agent)**<span dir="rtl">.</span>

<span dir="rtl">إحدى الطرق البسيطة التي يمكن أن تساعد فيها **المهام
المساعدة**</span> **(Auxiliary Tasks)** <span dir="rtl">في المهمة
الرئيسية هي أنها قد تتطلب بعضًا من نفس التمثيلات اللازمة للمهمة الرئيسية.
قد تكون بعض **المهام المساعدة**</span> **(Auxiliary Tasks)**
<span dir="rtl">أسهل، مع تأخير أقل وارتباط أوضح بين **الإجراءات**
</span>**(Actions)** <span dir="rtl">والنتائج. إذا أمكن العثور على
**ميزات جيدة**</span> **(Good Features)** <span dir="rtl">في وقت مبكر
على **المهام المساعدة السهلة** </span>**(Easy Auxiliary
Tasks)**<span dir="rtl">، فقد تسرع هذه الميزات بشكل كبير من التعلم على
**المهمة الرئيسية** </span>**(Main Task)**<span dir="rtl">.</span>
<span dir="rtl">لا يوجد سبب ضروري لماذا يجب أن يكون هذا صحيحًا، ولكن في
العديد من الحالات يبدو أنه ممكن. على سبيل المثال، إذا تعلمت التنبؤ
والتحكم في **مستشعراتك** </span>**(Sensors)** <span dir="rtl">على مقاييس
زمنية قصيرة، لنقل ثوانٍ، فقد تتوصل بشكل معقول إلى جزء من فكرة **الأشياء**
</span>**(Objects)**<span dir="rtl">، مما سيساعدك كثيرًا في التنبؤ
والتحكم في **المكافأة طويلة الأجل** </span>**(Long-Term
Reward)**<span dir="rtl">.</span>

<span dir="rtl">يمكننا تخيل **شبكة عصبية اصطناعية**</span> **(Artificial
Neural Network - ANN)** <span dir="rtl">حيث يتم تقسيم الطبقة الأخيرة إلى
أجزاء متعددة، أو **رؤوس** </span>**(Heads)**<span dir="rtl">، كل منها
يعمل على **مهمة مختلفة** </span>**(Different
Task)<span dir="rtl">.</span>** <span dir="rtl">قد ينتج **رأس
واحد**</span> **(One Head)** <span dir="rtl">دالة القيمة التقريبية
للمهمة الرئيسية (مع **المكافأة**</span> **(Reward)**
<span dir="rtl">كمجموعة تراكماتها) بينما تنتج **الرؤوس الأخرى**</span>
**(Other Heads)** <span dir="rtl">حلولاً لمختلف **المهام المساعدة**
</span>**(Auxiliary Tasks)**<span dir="rtl">.</span>
<span dir="rtl">يمكن لجميع **الرؤوس**</span> **(Heads)**
<span dir="rtl">نشر الأخطاء باستخدام **الانحدار العشوائي
المتدرج**</span> **(Stochastic Gradient Descent)** <span dir="rtl">في
نفس الجسم - الجزء المشترك السابق من **الشبكة**</span> **(Network)** -
<span dir="rtl">والذي سيحاول بعد ذلك تشكيل **تمثيلات**
</span>**(Representations)**<span dir="rtl">، في **طبقته قبل الأخيرة**
</span>**(Next-to-last Layer)**<span dir="rtl">، لدعم جميع
**الرؤوس**</span>
**<span dir="rtl">(</span>Heads<span dir="rtl">).</span>**
<span dir="rtl">جرب الباحثون **المهام المساعدة**</span> **(Auxiliary
Tasks)** <span dir="rtl">مثل التنبؤ بالتغير في **البكسلات**
</span>**(Pixels)**<span dir="rtl">، التنبؤ بمكافأة الخطوة الزمنية
التالية، والتنبؤ بتوزيع **العائد**
</span>**(Return)**<span dir="rtl">.</span> <span dir="rtl">في كثير من
الحالات، أظهر هذا النهج تسريعًا كبيرًا في التعلم على **المهمة الرئيسية**
</span>**(Main Task)** (Jaderberg et al., 2017)<span dir="rtl">.</span>
<span dir="rtl">تم اقتراح **التنبؤات المتعددة**</span> **(Multiple
Predictions)** <span dir="rtl">بشكل متكرر كطريقة لتوجيه بناء **تقديرات
الحالة** </span>**(State Estimates)** <span dir="rtl">(انظر القسم
17.3)</span>.

<span dir="rtl">طريقة بسيطة أخرى يمكن من خلالها أن يحسن تعلم **المهام
المساعدة**</span> **(Auxiliary Tasks)** <span dir="rtl">من الأداء هي
أفضل تفسير لها من خلال التشبيه مع الظاهرة النفسية **التكييف
الكلاسيكي**</span> **<span dir="rtl">(</span>Classical
<span dir="rtl"></span>Conditioning<span dir="rtl">)
</span>**<span dir="rtl">القسم (14.2). إحدى طرق فهم **التكييف
الكلاسيكي**</span> **<span dir="rtl">(</span>Classical
<span dir="rtl"></span>Conditioning<span dir="rtl">)
</span>**<span dir="rtl">هي أن **التطور**</span> **(Evolution)**
<span dir="rtl">قد بنى **ارتباطًا انعكاسيًا**</span>
**<span dir="rtl">(</span>Reflexive
<span dir="rtl"></span>Association<span dir="rtl">)
</span>**<span dir="rtl">(غير مكتسب) لإجراء معين من **تنبؤ إشارة
معينة**</span> **<span dir="rtl">(</span>Prediction of a
<span dir="rtl"></span>Particular Signal<span dir="rtl">).</span>**
<span dir="rtl">على سبيل المثال، يبدو أن البشر والعديد من الحيوانات
الأخرى لديهم **رد فعل انعكاسي**</span> **(Reflex)** <span dir="rtl">مدمج
لإغلاق عيونهم كلما تجاوز **تنبؤهم**</span> **(Prediction)**
<span dir="rtl">بالتعرض لضربة في العين عتبة معينة. يتم تعلم **التنبؤ**
</span>**(Prediction)**<span dir="rtl">، ولكن **الارتباط**</span>
**(Association)** <span dir="rtl">من **التنبؤ** </span>**(Prediction)
<span dir="rtl"></span>**<span dir="rtl">إلى **إغلاق العين**</span>
**(Eye Closure)** <span dir="rtl">مدمج، وبالتالي يتم إنقاذ الحيوان من
العديد من الضربات غير المحمية في عينه. وبالمثل، يمكن بناء
**الارتباط**</span> **(Association)** <span dir="rtl">من **الخوف**
</span>**(Fear)** <span dir="rtl">إلى زيادة **معدل ضربات القلب**
</span>**(Heart Rate)**<span dir="rtl">، أو إلى **التجميد**
</span>**(Freezing)**<span dir="rtl">.</span> <span dir="rtl">يمكن
لمصممي **العملاء**</span> **(Agent Designers)** <span dir="rtl">أن
يفعلوا شيئًا مشابهًا، يربطون عن طريق التصميم (بدون تعلم) **تنبؤات أحداث
معينة**</span> **(Predictions of Specific Events)** <span dir="rtl">مع
**إجراءات محددة مسبقًا** </span>**(Predetermined
Actions)<span dir="rtl">.</span>** <span dir="rtl">على سبيل المثال، يمكن
تزويد **السيارة ذاتية القيادة  
(**</span>**Self-driving Car<span dir="rtl">)
</span>**<span dir="rtl">التي تتعلم التنبؤ بما إذا كان التقدم للأمام
سيؤدي إلى اصطدام **بنية انعكاسية** </span>**(Built-in Reflex)
<span dir="rtl"></span>**<span dir="rtl">للتوقف، أو الانحراف بعيدًا، كلما
كان **التنبؤ**</span> **(Prediction)** <span dir="rtl">أعلى من عتبة
معينة. أو اعتبر **روبوت تنظيف**</span> **(Vacuum-Cleaning Robot)**
<span dir="rtl">تعلم التنبؤ بما إذا كان قد ينفد شحن البطارية قبل العودة
إلى **الشاحن**</span> **(Charger)** <span dir="rtl">وأنه يتجه تلقائيًا
إلى **الشاحن**</span> **(Charger)** <span dir="rtl">كلما أصبح
**التنبؤ**</span> **(Prediction)** <span dir="rtl">غير صفري. تعتمد
**التنبؤات الصحيحة**</span> **(Correct Prediction)** <span dir="rtl">على
**حجم المنزل**</span> **<span dir="rtl">(</span>Size of the
House<span dir="rtl">)</span>**<span dir="rtl">، **الغرفة التي يتواجد
فيها الروبوت**</span> **<span dir="rtl">(</span>Room the Robot is
<span dir="rtl"></span>In<span dir="rtl">)</span>**<span dir="rtl">،
و**عمر البطارية** </span>**(Age of the Battery)**<span dir="rtl">، وكلها
ستكون صعبة لمصمم **الروبوت**</span> **<span dir="rtl">(</span>Robot
<span dir="rtl"></span>Designer<span dir="rtl">)</span>**
<span dir="rtl">ليعرفها. سيكون من الصعب على المصمم بناء **خوارزمية
موثوقة**</span> **<span dir="rtl">(</span>Reliable
<span dir="rtl"></span>Algorithm<span dir="rtl">)</span>**
<span dir="rtl">لتحديد ما إذا كان يجب العودة إلى **الشاحن**</span>
**(Charger)** <span dir="rtl">من حيث **الإشارات الحسية**</span>
**(Sensory Terms)**<span dir="rtl">، ولكن قد يكون من السهل القيام بذلك
من حيث **التنبؤ المتعلم**</span> **<span dir="rtl">(</span>Learned
<span dir="rtl"></span>Prediction<span dir="rtl">)</span>**.
<span dir="rtl">نتوقع العديد من الطرق المحتملة مثل هذه التي قد تجمع فيها
**التنبؤات المتعلمة** </span>**(Learned Predictions)
<span dir="rtl"></span>**<span dir="rtl">بشكل مفيد مع **الخوارزميات
المدمجة** </span>**(Built-in Algorithms)** <span dir="rtl">للتحكم في
**السلوك** </span>**(Behavior)**<span dir="rtl">.</span>

<span dir="rtl">أخيرًا، ربما يكون الدور الأكثر أهمية **للمهام
المساعدة**</span> **(Auxiliary Tasks)** <span dir="rtl">هو تجاوز
الافتراض الذي قمنا به طوال هذا الكتاب بأن **تمثيل الحالة**</span>
**(State Representation)** <span dir="rtl">ثابت ويُعطى **للعميل**
</span>**(Agent)<span dir="rtl">.</span>** <span dir="rtl">لشرح هذا
الدور، علينا أولاً أن نتراجع قليلًا لتقدير حجم هذا الافتراض وتبعات إزالته.
سنقوم بذلك في **القسم 17.3**</span>.

**<u>17.2 <span dir="rtl">التجريد الزمني عبر الخيارات</span> (Temporal
Abstraction via Options)</u>**

<span dir="rtl">جانب جذاب من **إطار عملية اتخاذ القرار
المتسلسلة**</span> **(MDP)** <span dir="rtl">هو أنه يمكن تطبيقه بشكل
مفيد على المهام في العديد من المقاييس الزمنية المختلفة. يمكن استخدامه
لتشكيل المهمة المتمثلة في تحديد أي العضلات يجب أن تنقبض للإمساك بشيء ما،
أي رحلة طيران يجب اتخاذها للوصول بشكل مريح إلى مدينة بعيدة، وأي وظيفة
يجب قبولها لتحقيق حياة مُرضية. تختلف هذه المهام بشكل كبير في مقاييسها
الزمنية، ومع ذلك يمكن تشكيل كل منها على أنها **عملية اتخاذ القرار
المتسلسلة**</span> **(MDP)** <span dir="rtl">يمكن حلها عبر التخطيط أو
عمليات التعلم كما هو موضح في هذا الكتاب. جميعها تتضمن التفاعل مع العالم،
اتخاذ القرارات المتسلسلة، وهدفًا يمكن تصوره بشكل مفيد على أنه تجميع
للمكافآت على مر الزمن، وبالتالي يمكن تشكيل جميعها كـ **عملية اتخاذ
القرار المتسلسلة** </span>**(MDPs)**<span dir="rtl">.</span>

<span dir="rtl">على الرغم من أنه يمكن تشكيل جميع هذه المهام كـ **عملية
اتخاذ القرار المتسلسلة** </span>**(MDPs)**<span dir="rtl">، قد يظن المرء
أنه لا يمكن تشكيلها كـ</span> **MDP** <span dir="rtl">واحدة. فهي تتضمن
مقاييس زمنية مختلفة، مفاهيم مختلفة للاختيار والعمل! لن يكون من المفيد،
على سبيل المثال، تخطيط رحلة عبر القارة على مستوى تقلصات العضلات. ومع
ذلك، بالنسبة لمهام أخرى، مثل الإمساك بالأشياء، أو رمي السهام، أو ضرب كرة
البيسبول، قد تكون تقلصات العضلات على المستوى المنخفض هي المستوى المناسب
تمامًا. يقوم الناس بكل هذه الأشياء بسلاسة دون أن يظهروا وكأنهم ينتقلون
بين المستويات. هل يمكن توسيع إطار</span> **MDP** <span dir="rtl">ليشمل
جميع المستويات في وقت واحد؟</span>

<span dir="rtl">ربما يمكن ذلك. إحدى الأفكار الشائعة هي تشكيل</span>
**MDP** <span dir="rtl">على مستوى مفصل، مع خطوة زمنية صغيرة، ولكن تمكين
التخطيط على مستويات أعلى باستخدام مسارات عمل ممتدة تتوافق مع العديد من
الخطوات الزمنية الأساسية. للقيام بذلك، نحتاج إلى مفهوم مسار العمل الذي
يمتد على العديد من الخطوات الزمنية ويتضمن مفهوم الإنهاء. طريقة عامة
لتشكيل هاتين الفكرتين هي على شكل **سياسة**
</span>**(Policy)**<span dir="rtl">،</span> π<span dir="rtl">، و**دالة
إنهاء تعتمد على الحالة** </span>**(State-dependent Termination
Function)**<span dir="rtl">،</span> γ<span dir="rtl">، كما هو الحال في
**الدوال القيمة العامة** </span>**(GVFs)**<span dir="rtl">.</span>
<span dir="rtl">نُعرف زوجًا من هذين المفهومين على أنه **مفهوم عام للعمل**
</span>**(Generalized Notion of Action)** <span dir="rtl">يُسمى
**الخيار** </span>**(Option)**<span dir="rtl">.</span>
<span dir="rtl">لتنفيذ خيار</span>
$`\omega = \langle\pi\omega,\gamma\omega\rangle`$ <span dir="rtl">في
الزمن</span> $`t`$ <span dir="rtl">هو الحصول على **الإجراء**</span>
**(Action)** <span dir="rtl">الذي يجب اتخاذه،</span>
$`At`$​<span dir="rtl">، من</span>
$`\pi\omega( \cdot \mid St)`$<span dir="rtl">، ثم ينتهي في الزمن</span>
$`t + 1`$ <span dir="rtl">باحتمال</span>
γω($`S_{t + 1}`$)<span dir="rtl">.</span> <span dir="rtl">إذا لم ينته
الخيار عند</span> $`t + 1`$<span dir="rtl">، فإن</span> $`A_{t + 1}`$
<span dir="rtl"></span>​ <span dir="rtl">يُختار من</span>
$`\pi\omega( \cdot \mid S_{t + 1})`$<span dir="rtl">، وينتهي الخيار
عند</span> $`t + 2`$ <span dir="rtl"></span>
<span dir="rtl">باحتمال</span> $`\gamma\omega(St + 2)`$<span dir="rtl">،
وهكذا حتى الإنهاء النهائي. من الملائم اعتبار **الإجراءات على المستوى
المنخفض**</span> **(Low-level Actions)** <span dir="rtl">كحالات خاصة من
**الخيارات** </span>**(Options)** <span dir="rtl">كل إجراء</span> a
<span dir="rtl">يتوافق مع خيار</span>
$`\langle\pi\omega,\gamma\omega\rangle`$ <span dir="rtl">حيث تختار
**السياسة**</span>**(Policy)** <span dir="rtl">الإجراء
(</span>$`\pi\omega(s) = a`$ <span dir="rtl">لكل</span>
$`s \in S`$<span dir="rtl">)</span> <span dir="rtl">و**دالة
الإنهاء**</span> **(Termination Function)** <span dir="rtl">هي
صفر</span>  
<span dir="rtl">(</span>
$`\gamma\omega(s) = 0`$<span dir="rtl">لكل</span>
$`Ss \in S`$<span dir="rtl">)</span>. <span dir="rtl">تقوم
**الخيارات**</span> **(Options)** <span dir="rtl">فعليًا بتوسيع مساحة
**الإجراءات** </span>**(Action Space)**<span dir="rtl">.</span>
<span dir="rtl">يمكن لـ **العميل**</span> **(Agent)**
<span dir="rtl">إما اختيار **إجراء على المستوى المنخفض**</span>
**<span dir="rtl">(</span>Low-level
Action/Option<span dir="rtl">)</span>**<span dir="rtl">، ينتهي بعد خطوة
زمنية واحدة، أو اختيار **خيار ممتد**</span>
**<span dir="rtl">(</span>Extended
<span dir="rtl"></span>Option<span dir="rtl">)
</span>**<span dir="rtl">قد ينفذ لعدة خطوات زمنية قبل الإنهاء</span>.

<span dir="rtl">تم تصميم **الخيارات**</span> **(Options)**
<span dir="rtl">بحيث تكون قابلة للتبديل مع **الإجراءات على المستوى
المنخفض** </span>**(Low-level Actions)**<span dir="rtl">. على سبيل
المثال، **مفهوم دالة قيمة الإجراء**</span>
**<span dir="rtl">(</span>Action-Value
<span dir="rtl"></span>Function<span dir="rtl">) </span>**$`q_{\pi}`$
<span dir="rtl">يتعمم بشكل طبيعي إلى **دالة قيمة الخيار**</span>
**<span dir="rtl">(</span>Option-Value
<span dir="rtl"></span>Function<span dir="rtl">)</span>**
<span dir="rtl">التي تأخذ **الحالة**</span> **(State)**
<span dir="rtl">و**الخيار**</span> **(Option)** <span dir="rtl">كمدخلات
وتعيد **العائد المتوقع** </span>**(Expected Return)**
<span dir="rtl">بدءًا من تلك **الحالة**
</span>**(State)**<span dir="rtl">، وتنفيذ ذلك **الخيار**</span>
**(Option)** <span dir="rtl">حتى الإنهاء، وبعد ذلك تتبع **السياسة**
</span>**(Policy)** π<span dir="rtl">. يمكننا أيضًا تعميم مفهوم
**السياسة**</span> **(Policy)** <span dir="rtl">إلى **سياسة
هرمية**</span> **<span dir="rtl">(</span>Hierarchical
<span dir="rtl"></span>Policy<span dir="rtl">)
</span>**<span dir="rtl">تختار من بين **الخيارات**</span> **(Options)**
<span dir="rtl">بدلاً من **الإجراءات**
</span>**(Actions)**<span dir="rtl">، حيث تنفذ **الخيارات**
</span>**(Options)**<span dir="rtl">، عند اختيارها، حتى الإنهاء. مع هذه
الأفكار، يمكن تعميم العديد من **الخوارزميات** </span>**(Algorithms)**
<span dir="rtl">في هذا الكتاب لتعلم **دوال قيمة الخيار
التقريبية**</span> **<span dir="rtl">(</span>Approximate Option-Value
Functions<span dir="rtl">)</span>** <span dir="rtl">و**السياسات
الهرمية** </span>**(Hierarchical Policies)**<span dir="rtl">.</span>
<span dir="rtl">في أبسط الحالات، تتم عملية التعلم من بداية الخيار حتى
إنهائه، مع إجراء التحديث فقط عند إنهاء الخيار. بشكل أكثر تعقيدًا، يمكن
إجراء التحديثات في كل خطوة زمنية، باستخدام **خوارزميات التعلم داخل
الخيار**</span> **<span dir="rtl">(</span>Intra-Option Learning
Algorithms<span dir="rtl">)</span>**<span dir="rtl">، التي تتطلب عادةً
**التعلم خارج السياسة  
(**</span>**Off-policy Learning<span dir="rtl">)</span>**.

<span dir="rtl">ربما يكون **التعميم**</span> **(Generalization)**
<span dir="rtl">الأكثر أهمية الذي تتيحه أفكار **الخيار**</span>
**(Option)** <span dir="rtl">هو **النموذج البيئي**</span>
**(Environmental Model)** <span dir="rtl">كما تم تطويره في **الفصول 3 و4
و8**.</span> **<span dir="rtl">النموذج التقليدي للإجراء</span>
(Conventional Model of an Action)** <span dir="rtl">هو **احتمالات
الانتقال بين الحالات**</span> **<span dir="rtl">(</span>State-Transition
Probabilities<span dir="rtl">) </span>**<span dir="rtl">والمكافأة
**المباشرة المتوقعة**</span> **<span dir="rtl">(</span>Expected
Immediate Reward<span dir="rtl">)</span>** <span dir="rtl">لاتخاذ
**الإجراء**</span> **(Action)** <span dir="rtl">في كل **حالة**
</span>**(State)**<span dir="rtl">.</span> <span dir="rtl">كيف تتعمم
**نماذج الإجراءات التقليدية**</span> **(Conventional Action Models)
<span dir="rtl"></span>**<span dir="rtl">إلى **نماذج الخيارات**
</span>**(Option Models)**<span dir="rtl">؟ بالنسبة للخيارات، يكون
**النموذج المناسب**</span> **(Appropriate Model)** <span dir="rtl">مرة
أخرى من جزأين، أحدهما يتوافق مع **انتقال الحالة الناتج عن تنفيذ
الخيار**</span> **<span dir="rtl">(</span>State Transition Resulting
from Executing the
<span dir="rtl"></span>Option<span dir="rtl">)</span>**
<span dir="rtl">والآخر يتوافق مع **المكافأة التراكمية المتوقعة على طول
الطريق**</span> **<span dir="rtl">(</span>Expected
<span dir="rtl"></span>Cumulative Reward Along the
Way<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">الجزء الخاص **بمكافأة النموذج للخيار**</span>
**<span dir="rtl">(</span>Reward <span dir="rtl"></span>Part of an
Option Model<span dir="rtl">)</span>**<span dir="rtl">، والذي يشبه
**المكافأة المتوقعة لأزواج الحالة-الإجراء**</span>
**<span dir="rtl">(</span>Expected <span dir="rtl"></span>Reward for
State-Action Pairs<span dir="rtl">)</span>**
<span dir="rtl"></span>(3.5)<span dir="rtl">، هو</span>...

``` math
r(s,\omega) = E\left\lbrack R_{1} + \gamma R_{2} + \gamma^{2}R_{3} + \cdots + \gamma^{\tau - 1}R_{\tau}\, \middle| \, S_{0} = s,A_{0:\tau - 1} \sim \pi_{\omega},\tau \sim \gamma_{\omega} \right\rbrack
```

<span dir="rtl">بالنسبة لجميع الخيارات</span> $`\omega`$
<span dir="rtl">وجميع الحالات</span> $`s \in S`$<span dir="rtl">،
حيث</span> $`\tau`$ <span dir="rtl">هو الخطوة الزمنية العشوائية التي
ينتهي عندها الخيار وفقًا لدالة الإنهاء</span>
$`\gamma\omega`$​<span dir="rtl">.</span> <span dir="rtl">لاحظ دور
**بارامتر الخصم** </span>**(Discounting Parameter)**
<span dir="rtl"></span>γ <span dir="rtl">في هذه المعادلة—يتم الخصم وفقًا
لـ</span> $`\gamma`$<span dir="rtl">، ولكن إنهاء الخيار يتم وفقًا
لـ</span> $`\gamma\omega`$ ​<span dir="rtl">.</span> <span dir="rtl">جزء
**انتقال الحالة** </span>**(State-Transition)** <span dir="rtl">في نموذج
الخيار أكثر دقة. هذا الجزء من النموذج يحدد **احتمالية كل حالة ناتجة
ممكنة**</span> **(Probability of Each Possible Resulting State)**
<span dir="rtl">(كما في المعادلة</span> (3.4)<span dir="rtl">)، ولكن
الآن قد تكون هذه الحالة ناتجة بعد عدة خطوات زمنية، حيث يجب خصم كل منها
بشكل مختلف. يحدد **نموذج الخيار** </span>**(Option Model)**
$`\omega`$<span dir="rtl">، لكل حالة</span> $`s`$ <span dir="rtl">التي
قد يبدأ الخيار</span> $`\omega`$ <span dir="rtl">تنفيذها فيها، ولكل
حالة</span> s′ <span dir="rtl">التي قد ينتهي فيها الخيار</span>
ω<span dir="rtl">،</span> ...

``` math
p\left( s' \middle| s,\omega \right) = \sum_{k = 1}^{\infty}\gamma^{k}\Pr\text{\{}S_{k} = s',\tau = k \mid S_{0} = s,A_{0:k - 1} \sim \pi_{\omega},\tau \sim \gamma_{\omega}\text{\}}.
```

<span dir="rtl">لاحظ أنه بسبب عامل</span> $`\gamma k`$<span dir="rtl">،
فإن</span> $`p(s' \mid s,\omega)`$ <span dir="rtl">لم يعد يمثل **احتمال
الانتقال**</span> **<span dir="rtl">(</span>Transition
<span dir="rtl"></span>Probability<span dir="rtl">)
</span>**<span dir="rtl">ولم يعد يجمع إلى واحد على جميع قيم</span>
$`s'`$<span dir="rtl">.</span> <span dir="rtl">(ومع ذلك، نستمر في  
استخدام تدوين '</span>$`|`$<span dir="rtl">' في</span>
$`p`$<span dir="rtl">)</span>.

<span dir="rtl">تعريف الجزء الانتقالي لنموذج الخيار أعلاه يسمح لنا
بصياغة **معادلات بيلمان**</span> **<span dir="rtl">(</span>Bellman
<span dir="rtl"></span>Equations<span dir="rtl">)
</span>**<span dir="rtl">و**خوارزميات البرمجة الديناميكية**
</span>**(Dynamic Programming Algorithms)** <span dir="rtl">التي تنطبق
على جميع الخيارات، بما في ذلك **الإجراءات على المستوى المنخفض**</span>
**<span dir="rtl">(</span>Low-level
<span dir="rtl"></span>Actions<span dir="rtl">)</span>**
<span dir="rtl">كحالة خاصة. على سبيل المثال، معادلة بيلمان العامة لقيم
الحالات في **السياسة الهرمية** </span>**(Hierarchical Policy)**
<span dir="rtl"></span>$`\pi`$ <span dir="rtl">هي</span>:

``` math
v_{\pi}(s) = \sum_{\omega \in \Omega(s)}^{}{\pi\left( \omega \middle| s \right)\left\lbrack r(s,\omega) + \sum_{s'}^{}{p\left( s' \middle| s,\omega \right)v_{\pi}\left( s' \right)} \right\rbrack}
```

<span dir="rtl">حيث تشير</span> $`\Omega(s)`$ <span dir="rtl">إلى مجموعة
الخيارات المتاحة في الحالة</span> $`s`$<span dir="rtl">.</span>
<span dir="rtl">إذا كانت</span> $`\Omega(s)`$ <span dir="rtl">تشمل فقط
**الإجراءات على المستوى المنخفض** </span>**(Low-level
Actions)**<span dir="rtl">، فإن هذه المعادلة تختزل إلى نسخة من **معادلة
بيلمان المعتادة** </span>**(3.14)**<span dir="rtl">، باستثناء بالطبع
أن</span> γ <span dir="rtl">مضمنة في</span> $`p`$ <span dir="rtl">الجديد
(17.3) وبالتالي لا تظهر. وبالمثل، فإن **خوارزميات التخطيط
المقابلة**</span> **(Corresponding Planning Algorithms)**
<span dir="rtl">لا تحتوي أيضًا على</span>
$`\gamma`$<span dir="rtl">.</span> <span dir="rtl">على سبيل المثال،
**خوارزمية التكرار القيمي** </span>**(Value Iteration Algorithm)**
<span dir="rtl">مع **الخيارات** </span>**(Options)**<span dir="rtl">،
التي تشبه (4.10)، هي</span>:

``` math
v_{k + 1}(s) = \max_{\omega \in \Omega(s)}\left\lbrack r(s,\omega) + \sum_{s'}^{}{p\left( s' \middle| s,\omega \right)v_{k}\left( s' \right)} \right\rbrack,\quad\text{for all }s \in S.
```

<span dir="rtl">إذا كانت</span> $`\Omega(s)`$ <span dir="rtl">تشمل جميع
**الإجراءات على المستوى المنخفض**</span>
**<span dir="rtl">(</span>Low-level Actions**<span dir="rtl">)</span>
<span dir="rtl">المتاحة في كل حالة</span> $`s`$<span dir="rtl">، فإن هذه
الخوارزمية تتقارب إلى</span> $`\mathbf{v*}`$
**<span dir="rtl">التقليدية</span>
<span dir="rtl">(</span>Conventional**
$`\mathbf{v*}`$<span dir="rtl">**)**، والتي يمكن من خلالها حساب
**السياسة المثلى**</span> **<span dir="rtl">(</span>Optimal
Policy<span dir="rtl">).</span>** <span dir="rtl">ومع ذلك، فإن التخطيط
باستخدام **الخيارات** </span>**(Options)** <span dir="rtl">يكون مفيدًا
بشكل خاص عندما يتم اعتبار مجموعة فرعية فقط من الخيارات الممكنة
(في</span> $`\Omega(s)`$ <span dir="rtl">في كل حالة. عندئذٍ، سيتقارب
**التكرار القيمي**</span> **(Value Iteration)** <span dir="rtl">إلى
**أفضل سياسة هرمية (**</span>**Best Hierarchical
Policy<span dir="rtl">)</span>** <span dir="rtl">مقتصرة على مجموعة
الخيارات المحدودة. على الرغم من أن هذه السياسة قد تكون دون المثالية، إلا
أن التقارب يمكن أن يكون أسرع بكثير نظرًا لقلة الخيارات التي يتم اعتبارها
ولكون كل خيار يمكنه تخطي العديد من الخطوات الزمنية</span>.

<span dir="rtl">للتخطيط باستخدام **الخيارات**
</span>**(Options)**<span dir="rtl">، يجب إما أن يتم تزويدك بنماذج
الخيارات، أو أن تتعلمها. إحدى الطرق الطبيعية لتعلم نموذج الخيار هي
تشكيله كمجموعة من **الدوال القيمة العامة** </span>**(GVFs)**
<span dir="rtl">(كما تم تعريفها في القسم السابق) ثم تعلم **الدوال القيمة
العامة**</span> **(GVFs)** <span dir="rtl">باستخدام الأساليب المقدمة في
هذا الكتاب. ليس من الصعب أن نرى كيف يمكن القيام بذلك بالنسبة للجزء
المتعلق بالمكافأة في نموذج الخيار. ببساطة يتم اختيار التراكم لأحد
**الدوال القيمة العامة**</span> **(GVFs)** <span dir="rtl">ليكون
**المكافأة**
</span>**(**$`\mathbf{C\_ t\  = \ R\_ t}`$**)**<span dir="rtl">، وتكون
سياسته هي **سياسة الخيار** </span>**(Policy of the Option)**
($`\pi = \pi\omega`$​)<span dir="rtl">، وتكون **دالة الإنهاء**
</span>**(Termination Function)** <span dir="rtl">هي معدل الخصم مضروبًا
في **دالة الإنهاء للخيار (**</span>**Termination
<span dir="rtl"></span>Function of the Option<span dir="rtl">)</span>**
$`(\gamma(s) = \gamma \cdot \gamma\omega(s))`$ <span dir="rtl">عندئذٍ
تكون **الدالة القيمة العامة الحقيقية** </span>**(True GVF)**
<span dir="rtl">مساوية للجزء المتعلق بالمكافأة من نموذج الخيار،</span>
$`v\pi,\gamma,C(s) = r(s,\omega)`$<span dir="rtl">, ويمكن استخدام طرق
التعلم الموصوفة في هذا الكتاب لتقريبها. الجزء المتعلق بانتقال الحالة في
نموذج الخيار أكثر تعقيدًا قليلاً. يحتاج المرء إلى تخصيص **دالة قيمة
عامة**</span> **(GVF)** <span dir="rtl">لكل حالة قد ينتهي فيها الخيار.
نحن لا نريد أن تقوم هذه **الدوال القيمة العامة** </span>**(GVFs)**
<span dir="rtl">بتجميع أي شيء باستثناء عندما ينتهي الخيار، وعندها فقط
عندما يحدث الإنهاء في الحالة المناسبة. يمكن تحقيق ذلك عن طريق اختيار
التراكم **لدالة القيمة العامة**</span> **(GVF)** <span dir="rtl">التي
تتنبأ بالانتقال إلى الحالة</span> $`s'`$ <span dir="rtl">ليكون</span>
$`Ct = \gamma(St) \cdot I\lbrack St = s'\rbrack`$<span dir="rtl">.</span>
<span dir="rtl">يتم اختيار **سياسة دالة القيمة العامة**</span>
**<span dir="rtl">(</span>Policy of the
<span dir="rtl"></span>GVF<span dir="rtl">)
</span>**<span dir="rtl">و**دوال الإنهاء** </span>**(Termination
Functions)** <span dir="rtl">بنفس الطريقة التي تم اختيارها للجزء المتعلق
بالمكافأة من نموذج الخيار. عندئذٍ تكون **الدالة القيمة العامة
الحقيقية**</span> **(True GVF)** <span dir="rtl">مساوية للجزء الخاص
بالحالة</span> $`s'`$ <span dir="rtl">من نموذج انتقال الحالة
للخيار،</span> $`v\pi,\gamma,C(s) = p(s' \mid s,\omega)`$​
<span dir="rtl">ويمكن مرة أخرى استخدام طرق هذا الكتاب لتعلمها. على الرغم
من أن كل من هذه الخطوات تبدو طبيعية، فإن جمعها جميعًا (بما في ذلك تقريب
الدوال والمكونات الأساسية الأخرى) يعد تحديًا كبيرًا ويتجاوز الحالة الفنية
الحالية</span>.

<span dir="rtl">**<u>تمرين 17.1:</u>** لقد قدم هذا القسم
**الخيارات**</span> **(Options)** <span dir="rtl">لحالة الخصم، ولكن يمكن
القول إن الخصم غير مناسب للتحكم عند استخدام **تقريب الدوال**</span>
**(Function Approximation)** <span dir="rtl">(القسم 10.4). ما هي
**معادلة بيلمان الطبيعية**</span> **(Natural Bellman Equation)**
<span dir="rtl">لسياسة هرمية، والتي تشبه (17.4)، ولكن لإعداد متوسط
المكافأة</span> (Section 10.3)<span dir="rtl">؟ ما هما الجزآن من نموذج
الخيار، المشابهان  
لـ (17.2) و(17.3)، لإعداد متوسط المكافأة؟</span>

**<u>17.3 <span dir="rtl">الملاحظات والحالة</span> (Observations and
State)</u>**

<span dir="rtl">طوال هذا الكتاب، كتبنا **دوال القيمة التقريبية
المكتسبة**</span> **<span dir="rtl">(</span>Learned Approximate Value
<span dir="rtl"></span>Functions<span dir="rtl">)</span>**
<span dir="rtl">و**السياسات**</span> **(Policies)** <span dir="rtl">في
الفصل 13) كدوال تعتمد على **حالة البيئة**</span>
**<span dir="rtl">(</span>State of <span dir="rtl"></span>the
Environment<span dir="rtl">)</span>** <span dir="rtl">هذا يشكل قيدًا
كبيرًا على الأساليب المقدمة في **الجزء الأول**</span>
**<span dir="rtl">(</span>Part
I<span dir="rtl">)</span>**<span dir="rtl">، حيث تم تنفيذ **دالة القيمة
المكتسبة**</span> **(Learned Value Function)** <span dir="rtl">كجدول
بحيث يمكن تقريب أي **دالة قيمة**</span> **(Value Function)**
<span dir="rtl">بدقة؛ هذه الحالة تعادل افتراض أن **حالة البيئة**</span>
**<span dir="rtl">(</span>State of the
<span dir="rtl"></span>Environment<span dir="rtl">)</span>**
<span dir="rtl">يتم ملاحظتها بالكامل من قبل **العميل**
</span>**(Agent)**<span dir="rtl">.</span> <span dir="rtl">ولكن في
العديد من الحالات المهمة، وبالتأكيد في حياة جميع الذكاءات الطبيعية، تعطي
المدخلات الحسية معلومات جزئية فقط عن حالة العالم. قد تكون بعض الأشياء
محجوبة من قبل أشياء أخرى، أو خلف **العميل**</span>
**<span dir="rtl">(</span>Agent<span dir="rtl">)</span>**<span dir="rtl">،
أو على بعد أميال. في هذه الحالات، تكون جوانب مهمة محتملة من **حالة
البيئة**</span> **<span dir="rtl">(</span>State of the
<span dir="rtl"></span>Environment<span dir="rtl">)</span>**
<span dir="rtl">غير قابلة للملاحظة مباشرةً، ويعد افتراض أن **دالة القيمة
المكتسبة**</span> **<span dir="rtl">(</span>Learned
<span dir="rtl"></span>Value Function<span dir="rtl">)</span>**
<span dir="rtl">يتم تنفيذها كجدول فوق **فضاء حالة البيئة**</span>
**<span dir="rtl">(</span>Environment's State
<span dir="rtl"></span>Space<span dir="rtl">)</span>**
<span dir="rtl">افتراضًا قويًا، وغير واقعي، ويحد من الإمكانيات</span>.

<span dir="rtl">إطار **تقريب الدوال المعلَمة**</span> **(Parametric
Function Approximation)** <span dir="rtl">الذي طورناه في **الجزء
الثاني**</span> **(Part II)** <span dir="rtl">أقل تقييدًا بكثير، ويمكن
القول إنه لا يمثل قيدًا على الإطلاق. في **الجزء الثاني** </span>**(Part
II)**<span dir="rtl">، احتفظنا بالافتراض أن **دوال القيمة المكتسبة**
</span>**(Learned Value Functions)**
<span dir="rtl">و**السياسات**</span> **(Policies**) <span dir="rtl">هي
دوال تعتمد على **حالة البيئة** </span>**(State of the
Environment)**<span dir="rtl">، ولكن سمحنا بتقييد هذه الدوال بشكل عشوائي
من خلال التعميم. من المدهش إلى حد ما وغير المعترف به على نطاق واسع أن
**تقريب الدوال**</span> **(Function Approximation)**
<span dir="rtl">يشمل جوانب مهمة من **الملاحظة الجزئية**</span>
**(Partial Observability)** <span dir="rtl">على سبيل المثال، إذا كان
هناك **متغير حالة**</span> **<span dir="rtl">(</span>State
<span dir="rtl"></span>Variable<span dir="rtl">)</span>**
<span dir="rtl">غير قابل للملاحظة، فيمكن اختيار التعميم بحيث لا تعتمد
**القيمة التقريبية** </span>**(Approximate Value)
<span dir="rtl"></span>**<span dir="rtl">على ذلك **المتغير**</span>
**(State Variable)** <span dir="rtl">يكون التأثير تمامًا كما لو أن
**متغير الحالة**</span> **(State Variable)** <span dir="rtl">غير قابل
للملاحظة. بسبب هذا، تنطبق جميع النتائج التي تم الحصول عليها للحالة
المعلَمة على **الملاحظة الجزئية**</span> **(Partial Observability)**
<span dir="rtl">دون تغيير. بهذا المعنى، تشمل حالة **تقريب الدوال
المعلَمة** </span>**(Parameterized Function Approximation)**
<span dir="rtl">حالة **الملاحظة الجزئية** </span>**(Partial
Observability)**<span dir="rtl">.</span>

<span dir="rtl">ومع ذلك، هناك العديد من القضايا التي لا يمكن التحقيق
فيها دون معالجة أكثر وضوحًا لـ **الملاحظة الجزئية**</span> **(Partial
Observability)** <span dir="rtl">على الرغم من أننا لا يمكننا أن نعطيها
معالجة كاملة هنا، يمكننا أن نحدد التغييرات التي ستكون مطلوبة للقيام
بذلك. هناك أربع خطوات</span>.

<span dir="rtl">أولاً، سنغير **المشكلة**
</span>**(Problem)**<span dir="rtl">.</span> <span dir="rtl">بدلاً من أن
تصدر **البيئة**</span> **(Environment)** <span dir="rtl">حالاتها، فإنها
ستصدر فقط **الملاحظات**</span> **(Observations)**—<span dir="rtl">إشارات
تعتمد على **حالتها**</span> **(State)** <span dir="rtl">ولكنها، مثل
مستشعرات الروبوت، توفر معلومات جزئية فقط عنها. للراحة، دون فقدان
العمومية، نفترض أن **المكافأة** </span>**(Reward)
<span dir="rtl"></span>**<span dir="rtl">هي دالة مباشرة ومعروفة
**للملاحظة**</span> **(Observation)** (<span dir="rtl">ربما تكون
**الملاحظة** </span>**(Observation)
<span dir="rtl"></span>**<span dir="rtl">متجهًا، وتكون
**المكافأة**</span> **(Reward)** <span dir="rtl">إحدى مكوناته). سيكون
**التفاعل البيئي** </span>**(Environmental Interaction)
<span dir="rtl"></span>**<span dir="rtl">بعد ذلك بدون **حالات أو مكافآت
صريحة**</span> **<span dir="rtl">(</span>Explicit States
<span dir="rtl"></span>or
Rewards<span dir="rtl">)</span>**<span dir="rtl">، بل سيكون مجرد تسلسل
متناوب من **الإجراءات**</span> **<span dir="rtl">(</span>Actions
At​∈A<span dir="rtl">)</span>** <span dir="rtl">و**الملاحظات**
</span>**(Observations Ot∈O)**<span dir="rtl">:</span>

``` math
A_{0},O_{1},A_{1},O_{2},A_{2},O_{3},A_{3},O_{4},.....
```

... <span dir="rtl">مستمرًا إلى الأبد (راجع المعادلة 3.1) أو تشكيل
**حلقات**</span> **(Episodes)** <span dir="rtl">كل منها ينتهي **بملاحظة
نهائية خاصة**</span> **(Special Terminal Observation)**.
<span dir="rtl">ثانيًا، يمكننا استعادة فكرة **الحالة** </span>**(State)**
<span dir="rtl">كما استخدمت في هذا الكتاب من تسلسل **الملاحظات**</span>
**(Observations)** <span dir="rtl">و**الإجراءات**
</span>**(Actions)**<span dir="rtl">. دعنا نستخدم كلمة **تاريخ**
</span>**(History)**<span dir="rtl">، والرمز</span>
$`Ht`$​<span dir="rtl">، لجزء أولي من المسار يصل إلى ملاحظة</span>:
<span dir="rtl"></span>$`Ht = A0,O1,\ldots,At - 1,Ot`$
<span dir="rtl">يُمثل **التاريخ**</span> **(History)**
<span dir="rtl">أقصى ما يمكننا معرفته عن الماضي دون النظر خارج تيار
البيانات (لأن **التاريخ**</span> **(History)** <span dir="rtl">هو كل
تيار البيانات الماضي). بالطبع، ينمو **التاريخ** </span>**(History)**
<span dir="rtl">مع الزمن</span> t <span dir="rtl">ويمكن أن يصبح كبيرًا
وصعب التعامل معه. فكرة **الحالة**</span> **(State)** <span dir="rtl">هي
أنها تلخيص مدمج **للتاريخ** </span>**(History)
<span dir="rtl"></span>**<span dir="rtl">يكون مفيدًا مثل **التاريخ
الفعلي**</span> **(Actual History)** <span dir="rtl">للتنبؤ بالمستقبل.
دعونا نكون واضحين بشأن ما يعنيه هذا بالضبط. لكي تكون **ملخصًا
للتاريخ**</span> **<span dir="rtl">(</span>Summary
<span dir="rtl"></span>of the
History<span dir="rtl">)</span>**<span dir="rtl">، يجب أن تكون
**الحالة**</span> **(State)** <span dir="rtl">دالة تعتمد على **التاريخ**
</span>**(History)**<span dir="rtl">،</span>
$`St = f(Ht)`$<span dir="rtl">، ولكي تكون مفيدة بنفس القدر في التنبؤ
بالمستقبل مثل **التاريخ الكامل (**</span>**Whole
<span dir="rtl"></span>History<span dir="rtl">)</span>**<span dir="rtl">،
يجب أن تكون لها **خاصية ماركوف**</span> **(Markov Property)**
<span dir="rtl">المعروفة. بشكل رسمي، هذه خاصية للدالة</span>
f<span dir="rtl">. تكون الدالة</span> f <span dir="rtl">حاصلة على
**خاصية ماركوف**</span> **(Markov Property)** <span dir="rtl">إذا وفقط
إذا كانت أي **تاريخين** </span>**(Histories)**
<span dir="rtl"></span>$`h`$ <span dir="rtl">و</span>$`h'`$
<span dir="rtl">يُحولان بواسطة</span> $`f`$ <span dir="rtl">إلى نفس
**الحالة** </span>**(State)** $`(f(h) = f(h'))`$ <span dir="rtl">يمتلكان
أيضًا نفس الاحتمالات **للملاحظة التالية** </span>**(Next
Observation)**<span dir="rtl">.</span>

``` math
f(h) = f\left( h' \right) \rightarrow \Pr\text{\{}O_{t + 1} = o \mid H_{t} = h,A_{t} = a\text{\}} = \Pr\text{\{}O_{t + 1} = o \mid H_{t} = h',A_{t} = a\text{\}},
```

<span dir="rtl">لكل</span> $`o \in O`$
<span dir="rtl">و</span>$`a \in A`$<span dir="rtl">.</span>
<span dir="rtl">إذا كانت</span> $`f`$ <span dir="rtl">تمتلك **خاصية
ماركوف** </span>**(Markov Property)**<span dir="rtl">، فإن  
</span>$`St = f(Ht)S\_ t`$ <span dir="rtl">هو **حالة**</span>
**(State)** <span dir="rtl">كما استخدمنا المصطلح في هذا الكتاب. دعونا
نسميها من الآن فصاعدًا **حالة ماركوف**</span> **(Markov State)**
<span dir="rtl">لتمييزها عن **الحالات**</span> **(States)**
<span dir="rtl">التي تلخص **التاريخ**</span> **(History)**
<span dir="rtl">لكنها تفتقر إلى **خاصية ماركوف**</span> **(Markov
Property)** <span dir="rtl">(والتي سننظر فيها قريبًا)</span>.

**<span dir="rtl">حالة ماركوف</span> (Markov State)** <span dir="rtl">هي
أساس جيد للتنبؤ **بالملاحظة التالية** </span>**(Next Observation)**
<span dir="rtl"></span>(17.5)<span dir="rtl">، ولكن الأهم من ذلك أنها
أيضًا أساس جيد للتنبؤ بأي شيء أو التحكم فيه. على سبيل المثال، دعونا نعتبر
**الاختبار**</span> **(Test)** <span dir="rtl">هو أي تسلسل محدد من
**الإجراءات والملاحظات المتبادلة** </span>**(Alternating Actions and
Observations) <span dir="rtl"></span>**<span dir="rtl">التي قد تحدث في
المستقبل. على سبيل المثال، يتم تمثيل اختبار ثلاثي الخطوات كالتالي</span>
$`\tau = a1\ o1\ a2\ o2\ a3\ o3`$​<span dir="rtl">.</span>
<span dir="rtl">يتم تعريف **احتمالية هذا الاختبار** </span>**(The
Probability of This Test)** <span dir="rtl">بالنظر إلى **تاريخ
معين**</span> **(Specific History)** h <span dir="rtl">على أنها</span>:

``` math
p\left( \tau\mid h \right) = \Pr\text{\{}O_{t + 1} = o_{1},O_{t + 2} = o_{2},O_{t + 3} = o_{3} \mid H_{t} = h,A_{t} = a_{1},A_{t + 1} = a_{2},A_{t + 2} = a_{3}\text{\}}.
```

<span dir="rtl">إذا كانت</span> $`f`$ <span dir="rtl">تمتلك **خاصية
ماركوف** </span>**(Markov)**<span dir="rtl">، وكان</span> $`h`$
<span dir="rtl">و</span>$`h'`$ <span dir="rtl">أي **تاريخين**</span>
**(Histories)** <span dir="rtl">يتم تحويلهما إلى نفس **الحالة**</span>
**(State)** <span dir="rtl">بواسطة</span> $`f`$<span dir="rtl">، فإنه
بالنسبة لأي **اختبار**</span> **(Test)** $`\tau`$ <span dir="rtl">من أي
طول، يجب أن تكون **احتمالاته**</span> **(Its Probabilities)**
<span dir="rtl">بالنظر إلى **التاريخين**</span> **(The Two Histories)**
<span dir="rtl">متماثلة أيضًا</span>:

``` math
f(h) = f\left( h' \right) \rightarrow \left( \tau\mid h \right) = p\left( \tau\mid h' \right).
```

<span dir="rtl">بمعنى آخر، تلخص **حالة ماركوف**</span> **(Markov
State)** <span dir="rtl">جميع المعلومات في **التاريخ**
</span>**(History)** <span dir="rtl">اللازمة لتحديد **احتمالية أي
اختبار**</span> **(Test's Probability)** <span dir="rtl">في الواقع، تلخص
كل ما هو ضروري **لإجراء أي تنبؤ** </span>**(Making Any
Prediction)**<span dir="rtl">، بما في ذلك أي **دالة قيمة عامة**
</span>**(GVF)**<span dir="rtl">، وللتصرف بشكل مثالي (إذا كانت</span>
$`f`$ <span dir="rtl">تمتلك **خاصية ماركوف**
</span>**(Markov)**<span dir="rtl">، فإنه يوجد دائمًا دالة حتمية</span>
$`\pi`$ <span dir="rtl">بحيث يكون اختيار</span> $`At = \pi(f(Ht))`$
<span dir="rtl">هو الأمثل)</span>.

<span dir="rtl">الخطوة الثالثة في توسيع **التعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">لتشمل **الملاحظة الجزئية**
</span>**(Partial Observability)
<span dir="rtl"></span>**<span dir="rtl">هي التعامل مع بعض الاعتبارات
الحاسوبية. على وجه الخصوص، نريد أن تكون **الحالة**</span> **(State)**
<span dir="rtl">ملخصًا مضغوطًا **للتاريخ**
</span>**(History)**<span dir="rtl">.</span> <span dir="rtl">على سبيل
المثال، **دالة الهوية** </span>**(Identity Function)**
<span dir="rtl">تفي تمامًا بشروط **ماركوف**</span> **(Markov)**
<span dir="rtl">لـ</span> $`f`$<span dir="rtl">، لكنها مع ذلك ستكون
قليلة الفائدة لأن **الحالة المقابلة**</span> **(State**
$`\mathbf{St = Ht}`$**)** <span dir="rtl">ستنمو مع الزمن وتصبح غير قابلة
للإدارة، كما ذكرنا سابقًا، ولكن بشكل أكثر جوهرية لأنها لن تتكرر أبدًا؛
**العميل**</span> **(Agent)** <span dir="rtl">لن يواجه نفس **الحالة**
</span>**(State)** <span dir="rtl">مرتين (في مهمة مستمرة) وبالتالي لن
يتمكن أبدًا من الاستفادة من **طريقة التعلم الجدولية**</span>
**<span dir="rtl">(</span>Tabular <span dir="rtl"></span>Learning
Method<span dir="rtl">) </span>**<span dir="rtl">نريد أن تكون
**حالاتنا**</span> **(States)** <span dir="rtl">مضغوطة بالإضافة إلى أن
تكون **ماركوف** </span>**(Markov)** <span dir="rtl">هناك مسألة مماثلة
تتعلق بكيفية الحصول على **الحالة**</span> **(State)**
<span dir="rtl">وتحديثها. نحن لا نريد حقًا دالة</span> f
<span dir="rtl">التي تأخذ **التواريخ الكاملة**</span> **(Whole
Histories)** <span dir="rtl">بدلاً من ذلك، لأسباب حاسوبية نفضل الحصول على
نفس تأثير</span> f <span dir="rtl">مع تحديث تدريجي ومتكرر يحسب</span>
$`S_{t + 1}`$ <span dir="rtl">من</span> $`St`$​<span dir="rtl">، ويضم
الزيادة التالية من البيانات،</span> $`At`$
<span dir="rtl">و</span>$`Ot + 1`$ <span dir="rtl"></span>​:

``` math
S_{t + 1} = u\left( S_{t},A_{t},O_{t + 1} \right),\quad\text{for all }t \geq 0,
```

<span dir="rtl">مع إعطاء **الحالة الأولى** </span>**(First State)**
$`S0`$​<span dir="rtl">.</span> <span dir="rtl">تُسمى **الدالة**</span>
**(Function)** $`u`$ **<span dir="rtl">دالة تحديث الحالة</span>
(State-Update Function)** <span dir="rtl">على سبيل المثال، إذا
كانت</span> $`f`$ <span dir="rtl">هي دالة الهوية</span>
($`St = Ht`$)<span dir="rtl">، فستقوم</span> $`u`$ <span dir="rtl">فقط
بتمديد</span> $`St`$ <span dir="rtl">بإلحاق</span> $`At`$
<span dir="rtl">و</span>Ot+1 <span dir="rtl"></span>​
<span dir="rtl">إليها. نظرًا للدالة</span> $`f`$<span dir="rtl">، من
الممكن دائمًا بناء **دالة** </span>**(Function)** $`u`$
<span dir="rtl">المقابلة، ولكن قد لا تكون ملائمة من الناحية الحاسوبية،
وكما في مثال الهوية، قد لا تنتج **حالة مضغوطة**</span>
**<span dir="rtl">(</span>Compact State<span dir="rtl">)</span>**
<span dir="rtl">تُعد **دالة تحديث الحالة**</span> **(State-Update
Function)** <span dir="rtl">جزءًا مركزيًا من أي بنية **عميل**</span>
**(Agent)** <span dir="rtl">تتعامل مع **الملاحظة الجزئية**
</span>**(Partial Observability)**<span dir="rtl">.</span>
<span dir="rtl">يجب أن تكون قابلة للحساب بكفاءة، حيث لا يمكن اتخاذ أي
**إجراءات**</span> **(Actions)** <span dir="rtl">أو **تنبؤات**
</span>**(Predictions)** <span dir="rtl">حتى تتوفر **الحالة**
</span>**(State)**<span dir="rtl">.</span> <span dir="rtl">يظهر مخطط عام
لهذه **البنية المعمارية للعميل**</span> **<span dir="rtl">(</span>Agent
<span dir="rtl"></span>Architecture<span dir="rtl">)
</span>**<span dir="rtl">في **الشكل 17.1**</span>.

<span dir="rtl">مثال على الحصول على **حالات ماركوف**</span> **(Markov
States)** <span dir="rtl">من خلال **دالة تحديث الحالة**</span>
**<span dir="rtl">(</span>State-Update
Function<span dir="rtl">)</span>** <span dir="rtl">يُقدمه **النهج البايزي
الشائع المعروف باسم عمليات اتخاذ القرار المتسلسلة الجزئية
الملاحظة**</span> **<span dir="rtl">(</span>Partially Observable MDPs
<span dir="rtl">أو</span>
POMDPs<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">في هذا النهج، يُفترض أن البيئة تحتوي على **حالة كامنة
محددة جيدًا**</span> **(Well-Defined Latent State)** $`Xt`$
<span dir="rtl">التي تقوم بتوليد **ملاحظات البيئة**
</span>**(Environment’s Observations)**<span dir="rtl">، لكنها ليست
متاحة أبدًا **للعميل** </span>**(Agent)** <span dir="rtl">ويجب عدم الخلط
بينها وبين **الحالة**</span> **(State** $`\mathbf{St}`$**)**
<span dir="rtl">التي يستخدمها **العميل**</span> **(Agent)**
<span dir="rtl">لاتخاذ التنبؤات والقرارات</span>).
**<span dir="rtl">الحالة الطبيعية لماركوف</span> (Natural Markov
State)**<span dir="rtl">،</span> $`St`$​<span dir="rtl">، لـ</span>
**POMDP <span dir="rtl"></span>**<span dir="rtl">هي **التوزيع
الاحتمالي**</span> **(Distribution)** <span dir="rtl">على **الحالات
الكامنة**</span> **(Latent States)** <span dir="rtl">المعطاة **للتاريخ**
</span>**(History)**<span dir="rtl">، والتي تسمى **حالة الاعتقاد**
</span>**(Belief State)**<span dir="rtl">.</span> <span dir="rtl">لأجل
الوضوح، نفترض الحالة المعتادة التي تكون فيها هناك عدد محدود من **الحالات
المخفية** </span>**(Hidden States)**<span dir="rtl">،</span>
$`Xt \in \{ 1,2,\ldots,d\}`$, <span dir="rtl">إذن، **حالة الاعتقاد**
</span>**(Belief State)** <span dir="rtl">هي **المتجه**
</span>**(Vector) <span dir="rtl"></span>** $`St = st \in Rd`$
<span dir="rtl">مع مكوناته</span>...

``` math
s_{t}\lbrack i\rbrack = \Pr\text{\{}X_{t} = i \mid H_{t}\text{\}},\quad\text{for all possible latent states }i \in \text{\{}1,2,\ldots,d\text{\}}.
```

**<span dir="rtl">حالة الاعتقاد</span> (Belief State)**
<span dir="rtl">تبقى بنفس الحجم (نفس عدد المكونات) مهما كان نمو
الزمن</span> t<span dir="rtl">.</span> <span dir="rtl">يمكن أيضًا تحديثها
تدريجيًا باستخدام **قاعدة بايز** </span>**(Bayes'
Rule)**<span dir="rtl">، بافتراض أن لديك معرفة كاملة بالآليات الداخلية
للبيئة. على وجه التحديد، المكون</span> i <span dir="rtl">من **دالة تحديث
حالة الاعتقاد**</span> **<span dir="rtl">(</span>Belief-State
<span dir="rtl"></span>Update Function<span dir="rtl">)
</span>**<span dir="rtl">هو</span>:

``` math
u(s,a,o)\lbrack i\rbrack = \frac{\sum_{x = 1}^{d}{s\lbrack x\rbrack}\, p\left( i,o\mid x,a \right)}{\sum_{x = 1}^{d}{\sum_{x' = 1}^{d}{s\lbrack x\rbrack}}\, p\left( x',o\mid x,a \right)}
```

<span dir="rtl">لكل</span> $`a \in A`$<span dir="rtl">،</span>
$`o \in O`$<span dir="rtl">، و**حالات الاعتقاد**</span> **(Belief
States)** $`s \in Rds`$ <span dir="rtl">مع مكونات</span>
$`s\lbrack x\rbrack`$<span dir="rtl">، حيث إن **دالة**
</span>$`\mathbf{p}`$ **<span dir="rtl">ذات الأربعة معطيات</span>
(Four-Argument p Function)** <span dir="rtl">هنا ليست الدالة المعتادة
لـ</span> **MDPs** <span dir="rtl">  
(كما في **الفصل 3**)، ولكنها الدالة النظيرة لـ</span> **POMDPs**
<span dir="rtl">من حيث **الحالة الكامنة** </span>**(Latent
State)**<span dir="rtl">:</span>
$`p(x',o \mid x,a) = Pr\{ Xt = x',Ot = o \mid Xt - 1 = x,At - 1 = a = x,At - 1 = a\}`$.
<span dir="rtl">هذا النهج شائع في الأعمال النظرية وله العديد من
التطبيقات المهمة، ولكن افتراضاته وتعقيداته الحسابية تتزايد بشكل سيئ ولا
نوصي به كنهج للذكاء الاصطناعي</span>.

<span dir="rtl">مثال آخر على **حالات ماركوف**</span> **(Markov States)**
<span dir="rtl">يُقدمه **تمثيل الحالات التنبؤية**</span>
**<span dir="rtl">(</span>Predictive <span dir="rtl"></span>State
Representations <span dir="rtl">أو</span>PSRs<span dir="rtl">)</span>**.
<span dir="rtl"></span>**PSRs** <span dir="rtl">تعالج نقطة الضعف في
نهج</span> **POMDP** <span dir="rtl">أن دلالة **حالة العميل**</span>
**(Agent State St​)** <span dir="rtl">مرتبطة **بحالة البيئة**
</span>**(Environment State** $`\mathbf{Xt}`$**)**<span dir="rtl">، التي
لا يتم ملاحظتها أبدًا وبالتالي يصعب التعرف عليها. في</span> **PSRs**
<span dir="rtl">والنهج ذات الصلة، تكون دلالة **حالة العميل**
</span>**(Agent State)** <span dir="rtl">مرتبطة بالتنبؤات حول
**الملاحظات والإجراءات المستقبلية**</span>
**<span dir="rtl">(</span>Future <span dir="rtl"></span>Observations and
Actions<span dir="rtl">)</span>**<span dir="rtl">، والتي يمكن ملاحظتها
بسهولة. في</span> **PSRs**<span dir="rtl">، يتم تعريف **حالة
ماركوف**</span> **(Markov State)** <span dir="rtl">على أنها متجه ذو
أبعاد</span> $`d`$ <span dir="rtl">من **احتمالات** </span>$`\mathbf{d}`$
<span dir="rtl">لاختبارات "أساسية" مختارة بشكل خاص كما تم تعريفها أعلاه
(17.6). ثم يتم تحديث المتجه بواسطة **دالة تحديث الحالة**</span>
**<span dir="rtl">(</span>State-Update Function<span dir="rtl">)
</span>**$`u`$ <span dir="rtl">التي تشبه **قاعدة بايز** </span>**(Bayes
Rule)**<span dir="rtl">، ولكن بدلالة مرتبطة بالبيانات القابلة للملاحظة،
مما يجعلها أكثر سهولة في التعلم. تم تمديد هذا النهج بطرق عديدة، بما في
ذلك اختبارات النهاية، الاختبارات التركيبية، الأساليب "الطيفية" القوية،
واختبارات الحلقات المغلقة والاختبارات الزمنية المجردة التي يتم تعلمها
بواسطة **طرق** </span>**TD**<span dir="rtl">.</span> <span dir="rtl">بعض
أفضل التطورات النظرية هي لأنظمة معروفة باسم **نماذج المشغل القابل
للملاحظة**</span> **<span dir="rtl">(</span>Observable Operator Models
<span dir="rtl">أو</span>OOMs<span dir="rtl">)</span>**
<span dir="rtl">و**أنظمة متتابعة**</span> **(Sequential Systems)**
<span dir="rtl">(</span>Thon<span dir="rtl">، 2017)</span>.

<img src="./media/image196.png"
style="width:6.28669in;height:3.76697in" />

<span dir="rtl">**الشكل 17.1**:</span> <span dir="rtl">بنية معمارية
تصورية **لوكيل**</span> **(Agent)** <span dir="rtl">تشمل **نموذجًا**
</span>**(Model)**<span dir="rtl">، و**مخططًا**
</span>**(Planner)**<span dir="rtl">، و**دالة تحديث الحالة**
</span>**(State-Update Function)**<span dir="rtl">.</span>
<span dir="rtl">يتلقى **العالم**</span> **(World)** <span dir="rtl">في
هذه الحالة **الإجراءات**</span> **(Actions A)** <span dir="rtl">ويصدر
**الملاحظات** </span>**(Observations O)**<span dir="rtl">.</span>
<span dir="rtl">تُستخدم **الملاحظات** </span>**(Observations)**
<span dir="rtl">ونسخة من **الإجراء**</span> **(Action)**
<span dir="rtl">بواسطة **دالة تحديث الحالة**</span>
**<span dir="rtl">(</span>State-Update <span dir="rtl"></span>Function
u<span dir="rtl">) </span>**<span dir="rtl">لإنتاج **الحالة الجديدة**
</span>**(New State)**<span dir="rtl">.</span> <span dir="rtl">يتم إدخال
**الحالة الجديدة**</span> **(New State)** <span dir="rtl">إلى
**السياسة**</span> **(Policy)** <span dir="rtl">و**دالة القيمة**
</span>**(Value Function)**<span dir="rtl">، مما ينتج عنه **الإجراء
التالي**</span> **<span dir="rtl">(</span>Next
<span dir="rtl"></span>Action<span dir="rtl">)</span>**<span dir="rtl">،
ويتم أيضًا إدخالها إلى **المخطط**</span> **(Planner)**
<span dir="rtl">(وإلى</span> $`u`$<span dir="rtl">).</span>
<span dir="rtl">تتدفق المعلومات الأكثر مسؤولية عن **التعلم**</span>
**(Learning)** <span dir="rtl">عبر **الخطوط المتقطعة**</span> **(Dashed
Lines)** <span dir="rtl">التي تمر بشكل مائل عبر الصناديق التي تقوم
بتغييرها.</span> **<span dir="rtl">المكافأة</span> (Reward**
$`\mathbf{R}`$**)** <span dir="rtl">تغير مباشرة **السياسة**</span>
**(Policy)** <span dir="rtl">و**دالة القيمة** </span>**(Value
Function)**<span dir="rtl">. **الإجراء**
</span>**(Action)**<span dir="rtl">، **المكافأة**
</span>**(Reward)**<span dir="rtl">، و**الحالة**</span> **(State)**
<span dir="rtl">تغير **النموذج** </span>**(Model)**<span dir="rtl">،
الذي يعمل بشكل وثيق مع **المخطط**</span> **(Planner)**
<span dir="rtl">لتغيير **السياسة**</span> **(Policy)**
<span dir="rtl">و**دالة القيمة** </span>**(Value Function)**
<span dir="rtl">أيضًا. لاحظ أن تشغيل **المخطط**</span> **(Planner)**
<span dir="rtl">يمكن فصله عن تفاعل **العميل–البيئة**
</span>**(Agent–Environment Interaction)**<span dir="rtl">، في حين يجب
أن تعمل العمليات الأخرى في توافق مع هذا التفاعل لمواكبة وصول البيانات
الجديدة. لاحظ أيضًا أن **النموذج والمخطط**</span>
**<span dir="rtl">(</span>Model <span dir="rtl"></span>and
Planner<span dir="rtl">) </span>**<span dir="rtl">لا يتعاملان مع
**الملاحظات**</span> **(Observations)** <span dir="rtl">بشكل مباشر، ولكن
فقط مع **الحالات**</span> **(States)** <span dir="rtl">التي
تنتجها</span> u<span dir="rtl">، والتي يمكن أن تعمل كأهداف **لتعلم
النموذج  
(**</span>**Model
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**.

<span dir="rtl">الخطوة الرابعة والأخيرة في مخططنا المختصر لكيفية التعامل
مع **الملاحظة الجزئية**</span> **<span dir="rtl">(</span>Partial
<span dir="rtl"></span>Observability)**<span dir="rtl">) في **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">هي إعادة
إدخال **التقريب** </span>**(Approximation)**<span dir="rtl">. كما نوقش
في مقدمة **الجزء الثاني** </span>**(Part II)**<span dir="rtl">، للوصول
إلى **الذكاء الاصطناعي** </span>**(Artificial Intelligence)**
<span dir="rtl">بطموح، يجب تبني **التقريب**
</span>**(Approximation)**<span dir="rtl">.</span> <span dir="rtl">هذا
ينطبق على **الحالات**</span> **(States)** <span dir="rtl">كما هو الحال
مع **دوال القيمة** </span>**(Value Functions)**<span dir="rtl">.</span>
<span dir="rtl">يجب أن نقبل ونعمل مع مفهوم تقريبي **للحالة**
</span>**(State)**<span dir="rtl">.</span> <span dir="rtl">ستلعب
**الحالة التقريبية**</span> **(Approximate State)** <span dir="rtl">نفس
الدور في **الخوارزميات**</span> **(Algorithms)** <span dir="rtl">كما كان
من قبل، لذلك نستمر في استخدام الترميز</span> St <span dir="rtl">للإشارة
إلى **الحالة**</span> **(State)** <span dir="rtl">التي يستخدمها
**العميل** </span>**(Agent)**<span dir="rtl">، حتى وإن لم تكن
ماركوف</span>.

<span dir="rtl">ربما يكون أبسط مثال على **الحالة التقريبية**</span>
**(Approximate State)** <span dir="rtl">هو مجرد **الملاحظة الأخيرة**
</span>**(Latest Observation)**<span dir="rtl">،</span>
$`St = Ot`$​<span dir="rtl">.</span> <span dir="rtl">بالطبع، لا يمكن لهذا
النهج التعامل مع أي معلومات **حالة مخفية** </span>**(Hidden
State)**<span dir="rtl">.</span> <span dir="rtl">سيكون من الأفضل استخدام
**آخر** </span>$`\mathbf{k}`$ **<span dir="rtl">من الملاحظات
والإجراءات</span> <span dir="rtl">(</span>Last
<span dir="rtl"></span>**$`\mathbf{k}`$ **Observations and
Actions<span dir="rtl">)</span>**<span dir="rtl">،</span>
$`St = Ot,At - 1,Ot - 1,\ldots`$,​<span dir="rtl">، لبعض</span>
$`k \geq 1k`$<span dir="rtl">، والذي يمكن تحقيقه عن طريق **دالة تحديث
الحالة**</span> **(State-Update Function)** <span dir="rtl">التي تحرك
البيانات الجديدة للداخل وتخرج البيانات الأقدم. هذا النهج من التاريخ من
الرتبة</span> $`k`$ <span dir="rtl">لا يزال بسيطًا جدًا، ولكنه يمكن أن
يزيد بشكل كبير من قدرات **العميل**</span> **(Agent)**
<span dir="rtl">مقارنة بمحاولة استخدام **الملاحظة المباشرة الوحيدة**
</span>**(Single Immediate Observation)** <span dir="rtl">كحالة
مباشرة</span>.

<span dir="rtl">ماذا يحدث عندما تكون **خاصية ماركوف**</span> **(Markov
Property)** (17.5) <span dir="rtl">مستوفاة تقريبًا فقط؟ لسوء الحظ، يمكن
أن يتدهور أداء **التنبؤ طويل الأمد**</span> **(Long-term Prediction)**
<span dir="rtl">بشكل كبير عندما تصبح **التنبؤات الخطوة الواحدة**</span>
**(One-step Predictions)** <span dir="rtl">التي تحدد **خاصية ماركوف**
</span>**(Markov Property)** <span dir="rtl">غير دقيقة حتى بشكل طفيف. قد
تكون **الاختبارات طويلة الأمد**</span>
**<span dir="rtl">(</span>Longer-term
Tests<span dir="rtl">)</span>**<span dir="rtl">، و**الدوال القيمة
العامة** </span>**(GVFs)**<span dir="rtl">، و**دوال تحديث
الحالة**</span> **<span dir="rtl">(</span>State-Update
<span dir="rtl"></span>Functions<span dir="rtl">)</span>**
<span dir="rtl">كلها ذات تقريب ضعيف.</span> **<span dir="rtl">الأهداف
قصيرة الأمد</span> (Short-term)** <span dir="rtl">و**طويلة الأمد**
</span>**(Long-term Approximation Objectives)** <span dir="rtl">مختلفة،
ولا توجد ضمانات نظرية مفيدة حاليًا</span>.

<span dir="rtl">ومع ذلك، هناك أسباب تدعو إلى الاعتقاد بأن الفكرة العامة
الموضحة في هذا القسم تنطبق على الحالة التقريبية. الفكرة العامة هي أن
**الحالة**</span> **(State)** <span dir="rtl">التي تكون جيدة لبعض
**التنبؤات**</span> **(Predictions)** <span dir="rtl">تكون أيضًا جيدة
لغيرها (بشكل خاص، أن **حالة ماركوف**</span> **(Markov State)**
<span dir="rtl">التي تكفي **للتنبؤات الخطوة الواحدة** </span>**(One-step
Predictions)**<span dir="rtl">، تكفي أيضًا لجميع التنبؤات الأخرى). إذا
تراجعنا عن تلك النتيجة المحددة لحالة ماركوف، فالفكرة العامة مشابهة لما
ناقشناه في **القسم 17.1** مع **التعلم متعدد الرؤوس**</span>
**(Multi-headed Learning)** <span dir="rtl">و**المهام المساعدة**</span>
**(Auxiliary Tasks)**. <span dir="rtl">ناقشنا كيف أن
**التمثيلات**</span> **(Representations)** <span dir="rtl">التي كانت
جيدة **للمهام المساعدة**</span> **(Auxiliary Tasks)**
<span dir="rtl">كانت غالبًا جيدة أيضًا **للمهمة الرئيسية** </span>**(Main
Task)**<span dir="rtl">.</span> <span dir="rtl">مجتمعة، تشير هذه إلى نهج
**للملاحظة الجزئية** </span>**(Partial Observability)**
<span dir="rtl">و**تعلم التمثيل**</span> **(Representation Learning)**
<span dir="rtl">حيث يتم السعي وراء **تنبؤات متعددة**</span> **(Multiple
Predictions)** <span dir="rtl">واستخدامها لتوجيه بناء **ميزات
الحالة**</span> **<span dir="rtl">(</span>State
<span dir="rtl"></span>Features<span dir="rtl">)</span>**.
<span dir="rtl">الضمان الذي توفره **خاصية ماركوف المثالية**</span>
**<span dir="rtl">(</span>Perfect-but-Impractical
<span dir="rtl"></span>Markov Property<span dir="rtl">)
</span>**<span dir="rtl">يتم استبداله بالـ</span> **Heuristic**
<span dir="rtl">الذي يقول بأن ما هو جيد لبعض التنبؤات قد يكون جيدًا
لأخرى</span>.

<span dir="rtl">يتوسع هذا النهج جيدًا مع **الموارد الحاسوبية**
</span>**(Computational Resources)**<span dir="rtl">.</span>
<span dir="rtl">مع آلة كبيرة، يمكن للمرء أن يجرب **عددًا كبيرًا من
التنبؤات** </span>**(Large Numbers of Predictions)**<span dir="rtl">،
ربما يفضل تلك التي تشبه التنبؤات ذات الاهتمام النهائي أو تلك التي من
الأسهل تعلمها بشكل موثوق، أو وفقًا لبعض المعايير الأخرى. من المهم هنا أن
نتجاوز اختيار **التنبؤات**</span> **(Predictions)**
<span dir="rtl">يدويًا. يجب على **العميل**</span> **(Agent)**
<span dir="rtl">القيام بذلك. سيتطلب ذلك لغة عامة **للتنبؤات**
</span>**(Predictions)**<span dir="rtl">، حتى يتمكن **العميل**
</span>**(Agent)** <span dir="rtl">من استكشاف مساحة كبيرة من **التنبؤات
الممكنة**</span> **(Possible Predictions)** <span dir="rtl">بشكل منهجي،
ويقوم بفرزها للعثور على تلك الأكثر فائدة</span>.

<span dir="rtl">على وجه الخصوص، يمكن تطبيق كلا من نهجي</span> **POMDP
<span dir="rtl">و</span>PSR <span dir="rtl"></span>**<span dir="rtl">مع
**الحالات التقريبية** </span>**(Approximate
States)**<span dir="rtl">.</span> <span dir="rtl">تكون **دلالة
الحالة**</span> **(Semantics of the State)** <span dir="rtl">مفيدة في
تشكيل **دالة تحديث الحالة** </span>**(State-Update
Function)**<span dir="rtl">، كما هو الحال في هذين النهجين وفي نهج
**الرتبة** </span>$`\mathbf{k}`$<span dir="rtl">.</span>
<span dir="rtl">لا يوجد حاجة قوية لأن تكون **الدلالة**</span>
**(Semantics)** <span dir="rtl">صحيحة للاحتفاظ بمعلومات مفيدة في
**الحالة** </span>**(State)**<span dir="rtl">.</span>
<span dir="rtl">بعض الأساليب **لتوسيع الحالة** </span>**(State
Augmentation)**<span dir="rtl">، مثل **شبكات الحالة الصدوية**
</span>**(Echo State Networks)**
<span dir="rtl">(</span>Jaeger<span dir="rtl">، 2002)، تحتفظ بمعلومات
تقريبًا تعسفية عن **التاريخ** </span>**(History)** <span dir="rtl">ومع
ذلك يمكنها أن تعمل بشكل جيد. هناك العديد من الاحتمالات ونتوقع المزيد من
العمل والأفكار في هذا المجال</span>. **<span dir="rtl">تعلم دالة تحديث
الحالة</span> <span dir="rtl">(</span>Learning the State-Update
Function<span dir="rtl">)</span>** <span dir="rtl">لـ **حالة
تقريبية**</span> **(Approximate State)** <span dir="rtl">هو جزء كبير من
مشكلة **تعلم التمثيل** </span>**(Representation Learning)
<span dir="rtl"></span>**<span dir="rtl">كما تنشأ في **التعليم
المعزز**</span> **<span dir="rtl">(</span>Reinforcement
Learning**<span dir="rtl">).</span>

**<u>17.4 <span dir="rtl">تصميم إشارات المكافأة</span> (Designing Reward
Signals)</u>**

<span dir="rtl">تتمثل ميزة رئيسية **للتعليم المعزز**</span>
**(Reinforcement Learning)** <span dir="rtl">مقارنة **بالتعلم الخاضع
للإشراف** </span>**(Supervised Learning)
<span dir="rtl"></span>**<span dir="rtl">في أن التعليم المعزز لا يعتمد
على معلومات تعليمية تفصيلية: إنشاء إشارة المكافأة لا يعتمد على معرفة ما
يجب أن تكون عليه **الإجراءات الصحيحة (**</span>**Correct
Actions<span dir="rtl">) للعميل</span> (Agent)** <span dir="rtl">لكن
نجاح تطبيق **التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">يعتمد بشدة على مدى جودة **إشارة المكافأة**</span>
**(Reward Signal)** <span dir="rtl">في تحديد هدف **مصمم التطبيق**</span>
**<span dir="rtl">(</span>Application's
<span dir="rtl"></span>Designer<span dir="rtl">)
</span>**<span dir="rtl">ومدى دقة هذه الإشارة في تقييم التقدم نحو تحقيق
هذا الهدف. لهذه الأسباب، يعد **تصميم إشارة المكافأة**</span>
**(Designing a Reward Signal)** <span dir="rtl">جزءًا حيويًا من أي تطبيق
**للتعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">.</span>

<span dir="rtl">نعني **بتصميم إشارة المكافأة**</span> **(Designing a
Reward Signal)** <span dir="rtl">تصميم الجزء من **بيئة العميل**
</span>**(Agent's Environment)
<span dir="rtl"></span>**<span dir="rtl">المسؤول عن حساب كل **مكافأة
عددية** </span>**(Scalar Reward)** $`Rt`$ <span dir="rtl">وإرسالها إلى
**العميل**</span> **(Agent)** <span dir="rtl">في كل وقت</span>
t<span dir="rtl">.</span> <span dir="rtl">في مناقشتنا للمصطلحات في نهاية
**الفصل 14**، قلنا إن</span> $`Rt`$ <span dir="rtl">يشبه أكثر
**إشارةً**</span> **(Signal)** <span dir="rtl">يتم توليدها داخل **دماغ
الحيوان**</span> **(Animal's Brain)** <span dir="rtl">أكثر من كونه مثل
**شيء أو حدث**</span> **(Object or Event)** <span dir="rtl">في **البيئة
الخارجية للحيوان**</span> **<span dir="rtl">(</span>Animal's External
<span dir="rtl"></span>Environment<span dir="rtl">).</span>**
<span dir="rtl">الأجزاء من **أدمغتنا**</span> **(Our Brains)**
<span dir="rtl">التي تولد هذه **الإشارات**</span> **(Signals)**
<span dir="rtl">لنا تطورت على مدى ملايين السنين لتكون مناسبة جيدًا
للتحديات التي كان على أسلافنا مواجهتها في كفاحهم لنقل جيناتهم إلى
الأجيال القادمة. لذلك، يجب ألا نفكر في أن تصميم **إشارة مكافأة جيدة**
</span>**(Good Reward Signal)
<span dir="rtl"></span>**<span dir="rtl">هو دائمًا أمر سهل</span>!

<span dir="rtl">أحد التحديات هو تصميم **إشارة مكافأة**</span> **(Reward
Signal)** <span dir="rtl">بحيث يتقارب **سلوك العميل**</span>
**<span dir="rtl">(</span>Agent's
<span dir="rtl"></span>Behavior<span dir="rtl">)</span>**<span dir="rtl">،
ومع التعلم، ليحقق في النهاية ما يريده **مصمم التطبيق
(**</span>**Application's Designer<span dir="rtl">)
</span>**<span dir="rtl">بالفعل. قد يكون هذا سهلاً إذا كان هدف
**المصمم**</span> **(Designer)** <span dir="rtl">بسيطًا وسهل التحديد، مثل
إيجاد حل لمشكلة محددة جيدًا أو تحقيق **درجة عالية**</span> **(High
Score)** <span dir="rtl">في **لعبة محددة جيدًا**</span>
**<span dir="rtl">(</span>Well-Defined
<span dir="rtl"></span>Game<span dir="rtl">).</span>**
<span dir="rtl">في حالات مثل هذه، من المعتاد مكافأة **العميل**</span>
**(Agent)** <span dir="rtl">وفقًا لنجاحه في حل المشكلة أو نجاحه في تحسين
**درجته** </span>**(Score)**<span dir="rtl">.</span> <span dir="rtl">لكن
بعض المشاكل تتضمن أهدافًا يصعب ترجمتها إلى **إشارات مكافأة**
</span>**(Reward Signals)**<span dir="rtl">.</span> <span dir="rtl">هذا
صحيح بشكل خاص عندما تتطلب المشكلة من **العميل** </span>**(Agent)**
<span dir="rtl">أداءً ماهرًا لمهمة معقدة أو مجموعة من المهام، مثلما يتطلب
الأمر من **مساعد روبوتي منزلي مفيد** </span>**(Useful Household Robotic
Assistant)<span dir="rtl">.</span>** <span dir="rtl">بالإضافة إلى ذلك،
يمكن **لوكلاء التعليم المعزز** </span>**(Reinforcement Learning Agents)
<span dir="rtl"></span>**<span dir="rtl">اكتشاف طرق غير متوقعة لجعل
بيئاتهم تقدم **المكافأة** </span>**(Reward)**<span dir="rtl">، بعضها قد
يكون غير مرغوب فيه أو حتى خطيرًا. هذه تحديات مستمرة وحاسمة لأي
**طريقة**</span> **(Method)** <span dir="rtl">تعتمد على **التحسين**
</span>**(Optimization)**<span dir="rtl">، مثل **التعليم المعزز**
</span>**(Reinforcement Learning)<span dir="rtl">.</span>**
<span dir="rtl">نناقش هذه المسألة أكثر في **القسم 17.6**، القسم الأخير
من هذا الكتاب</span>.

<span dir="rtl">حتى عندما يكون هناك هدف بسيط وسهل التحديد، غالبًا ما تنشأ
مشكلة **المكافأة النادرة**</span> **<span dir="rtl">(</span>Sparse
<span dir="rtl"></span>Reward<span dir="rtl">)</span>**
<span dir="rtl">يمكن أن يكون تقديم **مكافأة غير صفرية**</span>
**(Non-Zero Reward)** <span dir="rtl">بشكل متكرر بما يكفي للسماح
**للعميل**</span> **(Agent)** <span dir="rtl">بتحقيق الهدف مرة واحدة،
ناهيك عن التعلم لتحقيقه بكفاءة من حالات بدء متعددة، تحديًا شاقًا. قد تكون
أزواج **الحالة–الإجراء**</span> **(State-Action Pairs)**
<span dir="rtl">التي تستحق بوضوح تحفيز **المكافأة**</span> **(Reward)**
<span dir="rtl">قليلة ومتباعدة، وقد تكون **المكافآت**</span>
**(Rewards)** <span dir="rtl">التي تشير إلى التقدم نحو الهدف نادرة لأن
التقدم صعب أو حتى مستحيل الكشف عنه. قد يتجول **العميل**</span>
**(Agent)** <span dir="rtl">بلا هدف لفترات طويلة من الزمن (ما أطلق عليه
مينسكي، 1961، **"مشكلة الهضبة"**)</span>.

<span dir="rtl">في الممارسة العملية، غالبًا ما يُترك **تصميم إشارة
المكافأة** </span>**(Designing a Reward Signal)** <span dir="rtl">لبحث
غير رسمي يعتمد على التجربة والخطأ لإيجاد **إشارة**</span> **(Signal)**
<span dir="rtl">تنتج نتائج مقبولة. إذا فشل **العميل**</span> **(Agent)**
<span dir="rtl">في التعلم، أو تعلم ببطء شديد، أو تعلم الشيء الخطأ، فإن
**المصمم** </span>**(Designer)** <span dir="rtl">يقوم بتعديل **إشارة
المكافأة**</span> **(Reward Signal)** <span dir="rtl">ويجرب مرة أخرى.
للقيام بذلك، يقوم **المصمم** </span>**(Designer)
<span dir="rtl"></span>**<span dir="rtl">بتقييم **أداء العميل**</span>
**(Agent's Performance)** <span dir="rtl">وفقًا للمعايير التي يحاول
ترجمتها إلى **إشارة مكافأة**</span> **(Reward Signal)**
<span dir="rtl">بحيث يتوافق هدف **العميل**</span> **(Agent)**
<span dir="rtl">مع هدف **المصمم**
</span>**(Designer)**<span dir="rtl">.</span> <span dir="rtl">وإذا كان
**التعلم**</span> **(Learning)** <span dir="rtl">بطيئًا جدًا، فقد يحاول
**المصمم**</span> **(Designer)** <span dir="rtl">تصميم **إشارة مكافأة
غير نادرة**</span> **(Non-Sparse Reward Signal)** <span dir="rtl">توجه
**التعلم**</span> **(Learning)** <span dir="rtl">بفعالية طوال تفاعل
**العميل**</span> **(Agent)** <span dir="rtl">مع **بيئته**
</span>**(Environment)**<span dir="rtl">.</span>

<span dir="rtl">من المغري معالجة مشكلة **المكافأة النادرة**</span>
**(Sparse Reward)** <span dir="rtl">من خلال **مكافأة العميل**
</span>**(Rewarding the Agent)
<span dir="rtl"></span>**<span dir="rtl">لتحقيق **الأهداف
الفرعية**</span> **(Subgoals)** <span dir="rtl">التي يعتقد **المصمم**
</span>**(Designer) <span dir="rtl"></span>**<span dir="rtl">أنها
**محطات مهمة**</span> **(Important Way Stations)**
<span dir="rtl">للوصول إلى الهدف العام. ولكن قد يؤدي تعزيز **إشارة
المكافأة**</span> **(Reward Signal)** **<span dir="rtl">بمكافآت إضافية
حسنة النية</span> <span dir="rtl">(</span>Well-Intentioned Supplemental
Rewards<span dir="rtl">) </span>**<span dir="rtl">إلى جعل
**العميل**</span> **(Agent)** <span dir="rtl">يتصرف بطريقة مختلفة تمامًا
عما هو مقصود؛ قد ينتهي **العميل**</span> **(Agent)**
<span dir="rtl">بعدم تحقيق الهدف العام على الإطلاق. الطريقة الأفضل
لتوفير مثل هذا التوجيه هي ترك **إشارة المكافأة**</span> **(Reward
Signal)** <span dir="rtl">كما هي وبدلاً من ذلك تعزيز **تقريب دالة
القيمة**</span> **(Value-Function Approximation)**
**<span dir="rtl">بتخمين أولي</span> <span dir="rtl">(</span>Initial
<span dir="rtl"></span>Guess<span dir="rtl">)</span>**
<span dir="rtl">لما يجب أن يكون عليه في النهاية، أو تعزيزه **بتخمينات
أولية**</span> **(Initial Guesses)** <span dir="rtl">حول ما يجب أن تكون
عليه بعض أجزائه. على سبيل المثال، افترض أن أحدهم يريد تقديم</span>
$`v0:S \rightarrow R`$<span dir="rtl">:</span> <span dir="rtl">كـ
**تخمين أولي**</span> **(Initial Guess)** <span dir="rtl">للدالة المثلى
الحقيقية</span> $`v*`$<span dir="rtl">، وأنه يستخدم **تقريب دالة خطية
(**</span>**Linear <span dir="rtl"></span>Function
Approximation<span dir="rtl">) </span>**<span dir="rtl">مع **ميزات**
</span>**(Features)** $`x:S \rightarrow Rd`$<span dir="rtl">. عندئذٍ،
سيحدد **تقريب دالة القيمة الأولي**</span> **(Initial Value Function
Approximation)** <span dir="rtl">كالتالي</span>:

``` math
v(s,w) = w^{\top}x(s) + v_{0}(s),
```

<span dir="rtl">وتحديث الأوزان</span> $`w`$ <span dir="rtl">كما هو
معتاد. إذا كان **المتجه الأولي للأوزان**</span> **(Initial Weight
Vector)** <span dir="rtl">يساوي صفرًا، فإن **دالة القيمة الأولية**</span>
**(Initial Value Function)** <span dir="rtl">ستكون</span>
$`v0`$​<span dir="rtl">، ولكن **جودة الحل النهائي** </span>**(Asymptotic
Solution Quality)** <span dir="rtl">ستُحدد بواسطة **متجهات
الميزات**</span> **<span dir="rtl">(</span>Feature
Vectors<span dir="rtl">)</span>** <span dir="rtl">كما هو معتاد. يمكن
تنفيذ هذا التهيئة لأي **مقربات غير خطية**</span>
**<span dir="rtl">(</span>Nonlinear
Approximators<span dir="rtl">)</span>** <span dir="rtl">وأي شكل من
أشكال</span> $`v0`$​<span dir="rtl">، على الرغم من أنه لا يضمن دائمًا
تسريع **التعلم**</span>
**<span dir="rtl">(</span>Learning<span dir="rtl">).</span>**
<span dir="rtl"></span>

<span dir="rtl">نهج فعال بشكل خاص لمشكلة **المكافأة النادرة**</span>
**(Sparse Reward Problem)** <span dir="rtl">هو **تقنية التشكيل**
</span>**(Shaping Technique)** <span dir="rtl">التي قدمها عالم النفس بي.
إف. سكينر ووُصِفت في **القسم 14.3**.</span> <span dir="rtl">تعتمد فعالية
هذه التقنية على حقيقة أن **المكافأة النادرة**</span> **(Sparse Reward
Problems)** <span dir="rtl">ليست فقط مشاكل تتعلق بإشارة المكافأة؛ بل هي
أيضًا مشاكل تتعلق **بسياسة العميل**</span> **(Agent's Policy)**
<span dir="rtl">في منع العميل من الوصول المتكرر إلى **الحالات المكافئة**
</span>**(Rewarding States)**<span dir="rtl">.</span>
<span dir="rtl">يتضمن **التشكيل** </span>**(Shaping)
<span dir="rtl"></span>**<span dir="rtl">تغيير **إشارة المكافأة**</span>
**(Reward Signal)** <span dir="rtl">مع تقدم **التعلم**</span>
**<span dir="rtl">(</span>Learning<span dir="rtl">)</span>**<span dir="rtl">،
بدءًا من **إشارة مكافأة**</span> **(Reward Signal)** <span dir="rtl">غير
نادرة بالنظر إلى **سلوك العميل الأولي**</span>
**<span dir="rtl">(</span>Agent's Initial
<span dir="rtl"></span>Behavior<span dir="rtl">)</span>**<span dir="rtl">،
وتعديلها تدريجيًا نحو **إشارة المكافأة**</span> **(Reward Signal)**
<span dir="rtl">المناسبة للمشكلة الأصلية. يتم إجراء كل تعديل بحيث يتم
مكافأة العميل بشكل متكرر بالنظر إلى **سلوكه الحالي**</span>
**<span dir="rtl">(</span>Current
<span dir="rtl"></span>Behavior<span dir="rtl">)</span>**
<span dir="rtl">يواجه العميل سلسلة من مشاكل **التعلم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement Learning
<span dir="rtl"></span>Problems<span dir="rtl">)</span>**
<span dir="rtl">تزداد صعوبة تدريجيًا، حيث يجعل ما تم تعلمه في كل مرحلة
المشكلة التالية الأسهل نسبيًا لأن العميل الآن يواجه **المكافأة**</span>
**(Reward)** <span dir="rtl">بشكل متكرر أكثر مما لو لم يكن لديه تجربة
سابقة مع المشاكل الأسهل. هذا النوع من **التشكيل**</span> **(Shaping)**
<span dir="rtl">هو تقنية أساسية في تدريب الحيوانات، وهو فعال أيضًا في
**التعليم المعزز الحاسوبي**</span>
**<span dir="rtl">(</span>Computational Reinforcement
Learning<span dir="rtl">).</span>**

<span dir="rtl">ماذا لو لم يكن لديك أي فكرة عن **المكافآت**</span>
**(Rewards)** <span dir="rtl">التي يجب أن تكون ولكن هناك **عميل
آخر**</span> **(Another Agent)**<span dir="rtl">، ربما يكون شخصًا، خبيرًا
بالفعل في المهمة ويمكن ملاحظة **سلوكه**</span>
**(Behavior)**<span dir="rtl">؟ في هذه الحالة، يمكن استخدام **الأساليب
المعروفة بأسماء مختلفة مثل "التعلم بالتقليد"**</span> **(Imitation
Learning)<span dir="rtl">،</span> "<span dir="rtl">التعلم من
العرض</span> (Learning from Demonstration) <span dir="rtl">و  
"التعلم بالتمرين"</span>(Apprenticeship Learning)
<span dir="rtl"></span>**<span dir="rtl">الفكرة هنا هي الاستفادة من
**العميل الخبير** </span>**(Expert Agent)
<span dir="rtl"></span>**<span dir="rtl">ولكن مع إبقاء الباب مفتوحًا
لإمكانية الأداء الأفضل في النهاية. يمكن تعلم **سلوك الخبير**</span>
**(Expert's Behavior)** <span dir="rtl">إما **بتعلم مباشر**</span>
**(Directly by Supervised Learning)** <span dir="rtl">أو **باستخراج
إشارة مكافأة**</span> **(Reward Signal)** <span dir="rtl">باستخدام ما
يعرف بـ</span> **"<span dir="rtl">التعليم المعزز العكسي"</span> (Inverse
Reinforcement Learning) <span dir="rtl"></span>**<span dir="rtl">ثم
استخدام **خوارزمية التعليم المعزز** </span>**(Reinforcement Learning
Algorithm) <span dir="rtl"></span>** <span dir="rtl">مع تلك **إشارة
المكافأة**</span> **(Reward Signal)** **<span dir="rtl">لتعلم
سياسة</span> (Policy)** <span dir="rtl">مهمة **التعليم المعزز
العكسي**</span> **(Inverse Reinforcement Learning)** <span dir="rtl">كما
استكشفها نغ ورسل (2000) هي محاولة استعادة **إشارة مكافأة الخبير**</span>
**<span dir="rtl">(</span>Expert's Reward
<span dir="rtl"></span>Signal<span dir="rtl">)</span>**
<span dir="rtl">من **سلوك الخبير فقط**</span> **(Expert's Behavior
Alone)** <span dir="rtl">لا يمكن القيام بذلك بدقة لأن **السياسة**</span>
**(Policy)** <span dir="rtl">يمكن أن تكون مثلى فيما يتعلق بالعديد من
**إشارات المكافأة المختلفة**</span> **(Different Reward Signals)**
<span dir="rtl">على سبيل المثال، أي **إشارة مكافأة**</span> **(Reward
Signal)** <span dir="rtl">تعطي نفس المكافأة لجميع **الحالات والإجراءات**
</span>**(States and Actions**)<span dir="rtl">، ولكن من الممكن العثور
على **مرشحين معقولين لإشارة المكافأة** </span>**(Plausible Reward Signal
Candidates)**<span dir="rtl">.</span> <span dir="rtl">لسوء الحظ، تتطلب
هذه الطريقة فرضيات قوية، بما في ذلك معرفة ديناميكيات البيئة و**متجهات
الميزات**</span> **<span dir="rtl">(</span>Feature
Vectors<span dir="rtl">)</span>** <span dir="rtl">التي تكون فيها **إشارة
المكافأة**</span> **(Reward Signal)** <span dir="rtl">خطية. تتطلب
الطريقة أيضًا حل المشكلة بالكامل (على سبيل المثال، باستخدام **طرق البرمجة
الديناميكية**</span> **<span dir="rtl">(</span>Dynamic
<span dir="rtl"></span>Programming Methods**<span dir="rtl">)</span>
<span dir="rtl">عدة مرات. على الرغم من هذه الصعوبات، يجادل أبيل ونغ
(2004) بأن **نهج التعليم المعزز العكسي**</span> **(Inverse Reinforcement
Learning Approach)** <span dir="rtl">يمكن أن يكون أحيانًا أكثر فعالية من
**التعلم الخاضع للإشراف**</span> **(Supervised Learning)**
<span dir="rtl">للاستفادة من **سلوك الخبير**</span>
**<span dir="rtl">(</span>Expert's Behavior<span dir="rtl">).</span>**
<span dir="rtl"></span>

<span dir="rtl">نهج آخر للعثور على **إشارة مكافأة جيدة**</span> **(Good
Reward Signal)** <span dir="rtl">هو **أتمتة** </span>**(Automate)**
<span dir="rtl">البحث **بالتجربة والخطأ**</span> **(Trial-and-Error
Search)** <span dir="rtl">عن **إشارة جيدة**</span> **(Good Signal)**
<span dir="rtl">التي ذكرناها أعلاه. من منظور **التطبيق**
</span>**(Application Perspective)**<span dir="rtl">، تعد **إشارة
المكافأة**</span> **<span dir="rtl">(</span>Reward
Signal<span dir="rtl">)</span>** **<span dir="rtl">بارامترًا لخوارزمية
التعلم</span> (Parameter of the Learning
Algorithm)**<span dir="rtl">.</span> <span dir="rtl">كما هو الحال مع
**البارامترات الأخرى للخوارزميات**</span>
**<span dir="rtl">(</span>Other Algorithm
Parameters<span dir="rtl">)</span>**<span dir="rtl">، يمكن **أتمتة
البحث**</span> **(Automate the Search)** <span dir="rtl">عن **إشارة
مكافأة جيدة**</span> **(Good Reward Signal)** <span dir="rtl">من خلال
تعريف مساحة من **المرشحين الممكنين**</span> **(Feasible Candidates)**
<span dir="rtl">وتطبيق **خوارزمية تحسين** </span>**(Optimization
Algorithm) <span dir="rtl"></span>**<span dir="rtl">تقوم **خوارزمية
التحسين**</span> **(Optimization Algorithm)** <span dir="rtl">بتقييم كل
**إشارة مكافأة مرشحة**</span> **(Candidate Reward Signal)**
<span dir="rtl">من خلال تشغيل نظام **التعليم المعزز**
</span>**(Reinforcement Learning System) <span dir="rtl"></span>**
<span dir="rtl">مع تلك **الإشارة**</span> **(Signal)**
<span dir="rtl">لعدد معين من الخطوات، ثم تسجيل النتيجة الإجمالية
باستخدام **دالة هدف عالية المستوى**</span>
**<span dir="rtl">(</span>High-Level Objective
<span dir="rtl"></span>Function<span dir="rtl">)</span>**
<span dir="rtl">تهدف إلى تشفير هدف المصمم الحقيقي، مع تجاهل قيود العميل.
يمكن تحسين **إشارات المكافأة**</span> **(Reward Signals)**
<span dir="rtl">حتى **من خلال الصعود عبر الانحدار عبر الإنترنت**</span>
**<span dir="rtl">(</span>Online <span dir="rtl"></span>Gradient
Ascent<span dir="rtl">)</span>**<span dir="rtl">، حيث يكون
**الانحدار**</span> **(Gradient)** <span dir="rtl">هو **انحدار دالة
الهدف عالية المستوى** </span>**(High-Level Objective Function)** (Sorg,
Lewis, and Singh, 2010)<span dir="rtl">.</span> <span dir="rtl">عند ربط
هذا النهج بالعالم الطبيعي، فإن **الخوارزمية لتحسين دالة الهدف عالية
المستوى**</span> **<span dir="rtl">(</span>Algorithm for
<span dir="rtl"></span>Optimizing the High-Level Objective
Function<span dir="rtl">)</span>** <span dir="rtl">تعادل **التطور**
</span>**(Evolution)**<span dir="rtl">، حيث أن **دالة الهدف عالية
المستوى**</span> **(High-Level Objective Function)** <span dir="rtl">هي
**اللياقة التطورية للحيوان**</span> **(Animal's Evolutionary Fitness)**
<span dir="rtl">التي تحددها **عدد نسله**</span>
**<span dir="rtl">(</span>Number of its
<span dir="rtl"></span>Offspring<span dir="rtl">)</span>**
<span dir="rtl">الذين يبقون على قيد الحياة حتى يصلوا إلى **سن التكاثر**
</span>**(Reproductive Age)**<span dir="rtl">.</span>

<span dir="rtl">أكدت التجارب الحاسوبية مع هذا النهج **لتحسين ثنائي
المستوى**</span> **<span dir="rtl">(</span>Bilevel Optimization
<span dir="rtl"></span>Approach<span dir="rtl">)</span>**
—<span dir="rtl">مستوى واحد يعادل **التطور**
</span>**(Evolution)**<span dir="rtl">، والآخر ناتج عن **التعليم المعزز
من قبل الوكلاء الفرديين**</span> **(Reinforcement Learning by Individual
Agents)** —<span dir="rtl">أن الحدس وحده ليس دائمًا كافيًا لتصميم **إشارة
مكافأة جيدة**</span> **(Good Reward Signal)**
<span dir="rtl">(</span>Singh, Lewis, and <span dir="rtl"></span>Barto,
2009<span dir="rtl">).</span> <span dir="rtl">قد تكون **أداء وكيل
التعليم المعزز**</span> **<span dir="rtl">(</span>Performance of a
Reinforcement <span dir="rtl"></span>Learning
Agent<span dir="rtl">)</span>** <span dir="rtl">كما تم تقييمه بواسطة
**دالة الهدف عالية المستوى**</span> **<span dir="rtl">(</span>High-Level
<span dir="rtl"></span>Objective Function<span dir="rtl">)</span>**
<span dir="rtl">حساسًا جدًا لتفاصيل **إشارة المكافأة**</span> **(Reward
Signal)** <span dir="rtl">الخاصة بالعميل بطرق خفية تحددها قيود العميل
والبيئات التي يعمل ويتعلم فيها. كما أظهرت هذه التجارب أن **هدف
العميل**</span> **(Agent's Goal)** <span dir="rtl">لا ينبغي دائمًا أن
يكون نفس **هدف المصمم** </span>**(Designer’s
Goal)**<span dir="rtl">.</span>

<span dir="rtl">في البداية، قد يبدو هذا غير بديهي، ولكن قد يكون من
المستحيل على العميل تحقيق **هدف المصمم** </span>**(Designer’s Goal)
<span dir="rtl"></span>**<span dir="rtl">بغض النظر عن **إشارة
المكافأة**</span> **(Reward Signal)** <span dir="rtl">الخاصة به. يجب على
العميل أن يتعلم تحت أنواع مختلفة من القيود، مثل **القدرة الحاسوبية
المحدودة**</span> **<span dir="rtl">(</span>Limited
<span dir="rtl"></span>Computational
Power<span dir="rtl">)</span>**<span dir="rtl">، **الوصول المحدود إلى
المعلومات عن بيئته**</span> **<span dir="rtl">(</span>Limited Access
<span dir="rtl"></span>to Information about its
Environment<span dir="rtl">)</span>**<span dir="rtl">، أو **الوقت
المحدود للتعلم**</span> **<span dir="rtl">(</span>Limited Time to
<span dir="rtl"></span>Learn<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">عندما تكون هناك قيود مثل هذه، قد يؤدي التعلم لتحقيق هدف
يختلف عن هدف المصمم أحيانًا إلى الاقتراب من تحقيق **هدف المصمم**</span>
**(Designer’s Goal)** <span dir="rtl">أكثر مما لو تم السعي وراء ذلك
الهدف مباشرة</span> <span dir="rtl">(</span>Sorg, Singh, and Lewis,
2010<span dir="rtl">؛</span> Sorg, 2011<span dir="rtl">).</span>
<span dir="rtl">من السهل العثور على أمثلة على ذلك في العالم الطبيعي.
نظرًا لأننا لا نستطيع تقييم **القيمة الغذائية لمعظم الأطعمة**
</span>**(Nutritional Value of Most Foods)
<span dir="rtl"></span>**<span dir="rtl">بشكل مباشر، فقد أعطانا
التطور</span>—**<span dir="rtl">مصمم إشارة المكافأة الخاصة بنا</span>
(Designer of Our Reward Signal)** —**<span dir="rtl">إشارة مكافأة</span>
(Reward Signal)** <span dir="rtl">تجعلنا نسعى وراء **أذواق معينة**
</span>**(Certain Tastes)**<span dir="rtl">.</span> <span dir="rtl">على
الرغم من أنها ليست معصومة من الخطأ بل قد تكون ضارة في البيئات التي تختلف
بطرق معينة عن **البيئات الأجدادية**</span>
**<span dir="rtl">(</span>Ancestral
<span dir="rtl"></span>Environments**<span dir="rtl">)، إلا أن هذا يعوض
عن العديد من **قيودنا** </span>**(Our
Limitations)**<span dir="rtl">:</span> **<span dir="rtl">قدراتنا الحسية
المحدودة</span> (Our Limited Sensory Abilities)**<span dir="rtl">،
**الوقت المحدود الذي يمكننا التعلم فيه** </span>**(Limited Time Over
Which We Can Learn)**<span dir="rtl">، والمخاطر المرتبطة بإيجاد **نظام
غذائي صحي**</span> **(Healthy Diet)** <span dir="rtl">من خلال **التجربة
الشخصية** </span>**(Personal Experimentation)**<span dir="rtl">.
وبالمثل، نظرًا لأن **الحيوان**</span> **(Animal)** <span dir="rtl">لا
يمكنه ملاحظة **لياقته التطورية**</span> **<span dir="rtl">(</span>Its
Own <span dir="rtl"></span>Evolutionary
Fitness<span dir="rtl">)</span>**<span dir="rtl">، فإن **دالة الهدف
عالية المستوى**</span> **<span dir="rtl">(</span>High-Level Objective
<span dir="rtl"></span>Function<span dir="rtl">)</span>**
<span dir="rtl">لا تعمل كإشارة مكافأة للتعلم. بدلاً من ذلك، يوفر التطور
**إشارات مكافأة**</span> **<span dir="rtl">(</span>Reward
<span dir="rtl"></span>Signals<span dir="rtl">)</span>**
<span dir="rtl">تكون حساسة **للمؤشرات القابلة للملاحظة للياقة
التطورية**</span> **<span dir="rtl">(</span>Observable Predictors
<span dir="rtl"></span>of Evolutionary
Fitness<span dir="rtl">)</span>**.

<span dir="rtl">أخيرًا، تذكر أن **وكيل التعليم المعزز**</span>
**(Reinforcement Learning Agent)** <span dir="rtl">ليس بالضرورة مثل كائن
حي كامل أو روبوت؛ يمكن أن يكون **مكونًا من نظام أكبر يتصرف**</span>
**<span dir="rtl">(</span>Component of a <span dir="rtl"></span>Larger
Behaving System<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">هذا يعني أن **إشارات المكافأة**</span> **(Reward
Signals)** <span dir="rtl">قد تتأثر بأشياء داخل **الوكيل الأكبر**
</span>**(Larger Behaving Agent)**<span dir="rtl">، مثل **الحالات
الدافعية**</span> **<span dir="rtl">(</span>Motivational
<span dir="rtl"></span>States<span dir="rtl">)</span>**<span dir="rtl">،
**الذكريات** </span>**(Memories)**<span dir="rtl">، **الأفكار**
</span>**(Ideas)**<span dir="rtl">، أو حتى **الهلوسات**
</span>**(Hallucinations)**<span dir="rtl">.</span> <span dir="rtl">قد
تعتمد **إشارات المكافأة**</span> **(Reward Signals)**
<span dir="rtl">أيضًا على خصائص **عملية التعلم نفسها**</span>
**<span dir="rtl">(</span>Learning Process
Itself<span dir="rtl">)</span>**<span dir="rtl">، مثل مقاييس مقدار
التقدم الذي يحرزه **التعلم**
</span>**(Learning)**<span dir="rtl">.</span> <span dir="rtl">جعل
**إشارات المكافأة**</span> **(Reward Signals)
<span dir="rtl"></span>**<span dir="rtl">حساسة للمعلومات حول العوامل
الداخلية مثل هذه يتيح **للعميل** </span>**(Agent)** <span dir="rtl">تعلم
كيفية التحكم في</span> **"<span dir="rtl">البنية المعرفية</span>"
(Cognitive Architecture)** <span dir="rtl">التي هو جزء منها، وكذلك
اكتساب **المعرفة والمهارات**</span> **(Knowledge and Skills)**
<span dir="rtl">التي سيكون من الصعب تعلمها من **إشارة مكافأة تعتمد فقط
على الأحداث الخارجية**</span> **<span dir="rtl">(</span>Reward Signal
that Depended Only on External Events<span dir="rtl">)</span>**
<span dir="rtl">مثل هذه الاحتمالات أدت إلى فكرة</span>
**"<span dir="rtl">التعليم المعزز ذو الدوافع الذاتية</span>"
<span dir="rtl">(</span>Intrinsically-Motivated Reinforcement
Learning<span dir="rtl">)</span>** <span dir="rtl">التي نناقشها بإيجاز
في نهاية القسم التالي</span>.

**<u>17.5 <span dir="rtl">القضايا المتبقية</span> (Remaining
Issues)</u>**

<span dir="rtl">في هذا الكتاب، قدمنا الأسس **لنهج التعليم المعزز**
</span>**(Reinforcement Learning Approach)** <span dir="rtl">نحو
**الذكاء الاصطناعي** </span>**(Artificial
Intelligence)**<span dir="rtl">.</span> <span dir="rtl">بشكل عام، يعتمد
هذا النهج على **الأساليب الخالية من النماذج**</span> **(Model-Free
Methods)** <span dir="rtl">و**الأساليب المستندة إلى النماذج**</span>
**<span dir="rtl">(</span>Model-Based Methods<span dir="rtl">)</span>**
<span dir="rtl">التي تعمل معًا، كما هو الحال في **بنية**</span> **Dyna**
<span dir="rtl">في **الفصل 8**، مدمجة مع **تقريب الدوال**</span>
**(Function Approximation)** <span dir="rtl">كما تم تطويره في **الجزء
الثاني** </span>**(Part II)**<span dir="rtl">.</span>
<span dir="rtl">ركزنا على **الخوارزميات عبر الإنترنت**</span> **(Online
Algorithms)** <span dir="rtl">و**الخوارزميات التكرارية**</span>
**<span dir="rtl">(</span>Incremental
<span dir="rtl"></span>Algorithms<span dir="rtl">)</span>**<span dir="rtl">،
التي نراها أساسية حتى بالنسبة للأساليب المستندة إلى النماذج، وعلى كيفية
تطبيق هذه الأساليب في مواقف **التدريب خارج السياسة**
</span>**(Off-Policy Training Situations)**<span dir="rtl">.</span>
<span dir="rtl">لقد تم تقديم المبرر الكامل للأخيرة فقط في هذا الفصل
الأخير. أي أننا قدمنا  
**التعلم خارج السياسة**</span> **(Off-Policy Learning)**
<span dir="rtl">طوال الوقت كطريقة جذابة للتعامل مع معضلة
**الاستكشاف/الاستغلال** </span>**(Explore/Exploit
Dilemma)**<span dir="rtl">، ولكننا ناقشنا فقط في هذا الفصل **التعلم عن
العديد من المهام المساعدة المتنوعة**</span>
**<span dir="rtl">(</span>Learning about Many Diverse Auxiliary
<span dir="rtl"></span>Tasks<span dir="rtl">)</span>**
<span dir="rtl">في نفس الوقت مع **الدوال القيمة العامة**</span>
**(GVFs)** <span dir="rtl">و**التعلم عن العالم بشكل هرمي**
</span>**(Learning about the World Hierarchically)
<span dir="rtl"></span>** <span dir="rtl">من حيث **نماذج الخيارات
المؤقتة المجردة** </span>**(Temporally-Abstract Option
Models)**<span dir="rtl">، وكلاهما يتضمن **التعلم خارج السياسة  
(**</span>**Off-Policy Learning<span dir="rtl">)</span>**.

<span dir="rtl">ما زال هناك الكثير من العمل ليتم إنجازه، كما أشرنا طوال
الكتاب وكما يتضح من الاتجاهات للبحث الإضافي التي تمت مناقشتها في هذا
الفصل. ولكن لنفترض أننا كنا كرماء ووافقنا على الخطوط العريضة لكل ما قمنا
به في الكتاب وكل ما تم توضيحه حتى الآن في هذا الفصل. ماذا سيبقى بعد ذلك؟
بالطبع لا يمكننا أن نعرف على وجه اليقين ما الذي سيكون مطلوبًا، لكن يمكننا
تقديم بعض التخمينات. في هذا القسم، نسلط الضوء على ست قضايا إضافية يبدو
لنا أنها ستحتاج إلى معالجة من قبل البحث المستقبلي</span>.

<span dir="rtl">أولاً، ما زلنا بحاجة إلى **أساليب تقريب دوال بارامترية
قوية**</span> **<span dir="rtl">(</span>Powerful Parametric Function
<span dir="rtl"></span>Approximation Methods<span dir="rtl">)</span>**
<span dir="rtl">تعمل بشكل جيد في البيئات **التكرارية بالكامل وعبر
الإنترنت** </span>**(Fully Incremental and Online
Settings)<span dir="rtl">.</span>** <span dir="rtl">تعتبر الأساليب
المستندة إلى **التعلم العميق** </span>**(Deep Learning)
<span dir="rtl"></span>**<span dir="rtl">والشبكات **العصبية
الاصطناعية**</span> **(ANNs)** <span dir="rtl">خطوة كبيرة في هذا
الاتجاه، ولكنها لا تزال تعمل بشكل جيد فقط مع **التدريب الجماعي**</span>
**(Batch Training)** <span dir="rtl">على **مجموعات بيانات كبيرة**
</span>**(Large Data Sets)**<span dir="rtl">، مع التدريب من **اللعب
الذاتي المكثف دون اتصال  
(**</span>**(Extensive Offline Self-Play**<span dir="rtl">، أو مع التعلم
من **تجارب متعددة للوكلاء في نفس المهمة** </span>**(Interleaved
Experience of Multiple Agents on the Same
Task)<span dir="rtl">.</span>** <span dir="rtl">هذه البيئات وغيرها هي
طرق للتحايل على قيد أساسي من **أساليب التعلم العميق الحالية**</span>
**<span dir="rtl">(</span>Today's Deep <span dir="rtl"></span>Learning
Methods<span dir="rtl">)</span>**<span dir="rtl">، والتي تكافح للتعلم
بسرعة في **البيئات التكرارية وعبر الإنترنت** </span>**(Incremental,
Online Settings) <span dir="rtl"></span>**<span dir="rtl">التي هي الأكثر
طبيعية **لخوارزميات التعليم المعزز** </span>**(Reinforcement Learning
Algorithms) <span dir="rtl"></span>**<span dir="rtl">التي تم التركيز
عليها في هذا الكتاب. يوصف المشكلة أحيانًا بأنها</span>
**"<span dir="rtl">التدخل الكارثي</span> (Catastrophic Interference)"**
<span dir="rtl">أو  
</span>**"<span dir="rtl">البيانات المترابطة</span> (Correlated Data)"**
<span dir="rtl">عندما يتم تعلم شيء جديد، فإنه يميل إلى استبدال ما تم
تعلمه سابقًا بدلاً من إضافته، مما يؤدي إلى فقدان فوائد التعلم القديم.
غالبًا ما تستخدم تقنيات مثل</span> **"<span dir="rtl">مخازن إعادة
التشغيل</span> (Replay Buffers)"** <span dir="rtl">للاحتفاظ **بالبيانات
القديمة وإعادة تشغيلها**</span> **<span dir="rtl">(</span>Retain and
Replay Old Data<span dir="rtl">) </span>**<span dir="rtl">بحيث لا تفقد
فوائدها بشكل دائم. التقييم الصادق يجب أن يكون أن **أساليب التعلم العميق
الحالية**</span> **(Current Deep Learning Methods)**
<span dir="rtl">ليست مناسبة بشكل جيد **للتعلم عبر الإنترنت**
</span>**(Online Learning)**<span dir="rtl">.</span> <span dir="rtl">لا
نرى سببًا يجعل هذا القيد غير قابل للتجاوز، ولكن **الخوارزميات**</span>
**(Algorithms)** <span dir="rtl">التي تتعامل معه، مع الاحتفاظ في نفس
الوقت بمزايا **التعلم العميق** </span>**(Deep
Learning)**<span dir="rtl">، لم يتم تطويرها بعد. معظم أبحاث **التعلم
العميق الحالية**</span> **<span dir="rtl">(</span>Current Deep
<span dir="rtl"></span>Learning Research<span dir="rtl">)</span>**
<span dir="rtl">موجهة نحو التحايل على هذا القيد بدلاً من إزالته</span>.

<span dir="rtl">ثانيًا (وربما مرتبط بشكل وثيق)، ما زلنا بحاجة إلى **طرق
لتعلم الميزات**</span> **<span dir="rtl">(</span>Methods for Learning
<span dir="rtl"></span>Features<span dir="rtl">)
</span>**<span dir="rtl">بحيث **يتعمم التعلم اللاحق بشكل جيد**</span>
**<span dir="rtl">(</span>Subsequent Learning Generalizes
<span dir="rtl"></span>Well<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">هذه القضية هي مثال على مشكلة عامة تعرف بأسماء مختلفة
مثل  
</span>**"<span dir="rtl">تعلم التمثيل</span> (Representation
Learning)**<span dir="rtl">،</span> **"<span dir="rtl">الاستقراء
البنائي</span> <span dir="rtl">(</span>Constructive
<span dir="rtl"></span>Induction<span dir="rtl">)</span>**<span dir="rtl">،
و**"التعلم الفوقي**</span> **(Meta-Learning)**" **<span dir="rtl">كيف
يمكننا استخدام التجربة ليس فقط لتعلم دالة معينة مطلوبة</span>
<span dir="rtl">(</span>Given Desired Function<span dir="rtl">)، ولكن
لتعلم التحيزات الاستقرائية</span> <span dir="rtl">(</span>Inductive
<span dir="rtl"></span>Biases<span dir="rtl">)</span>
<span dir="rtl">بحيث يتعمم التعلم المستقبلي بشكل أفضل</span>
<span dir="rtl">(</span>Future Learning Generalizes
<span dir="rtl"></span>Better<span dir="rtl">) ويصبح أسرع</span>
<span dir="rtl">(</span>Faster<span dir="rtl">)؟ هذه مشكلة قديمة، تعود
إلى أصول الذكاء الاصطناعي</span> (Origins of Artificial Intelligence)
<span dir="rtl">والتعرف</span>** <span dir="rtl">على الأنماط</span>
(**Pattern** **Recognition**) <span dir="rtl">في خمسينيات وستينيات القرن
الماضي. مثل هذا العمر يجب أن يجعل المرء يتوقف قليلاً. ربما لا يوجد حل.
ولكن من المحتمل أيضًا أن الوقت لإيجاد حل وإثبات فعاليته لم يأت بعد.
اليوم، يتم **التعلم الآلي** </span>**(Machine Learning)**
<span dir="rtl">على نطاق أكبر بكثير مما كان عليه في الماضي، وأصبحت
الفوائد المحتملة **لأسلوب تعلم التمثيل الجيد**</span> **(Good
Representation Learning Method)** <span dir="rtl">أكثر وضوحًا. نلاحظ أن
هناك **مؤتمر سنوي جديد**</span> **(New Annual Conference)**
—**<span dir="rtl">المؤتمر الدولي لتعلم التمثيلات</span> (International
Conference on Learning Representations)** —<span dir="rtl">يستكشف هذا
الموضوع والموضوعات ذات الصلة كل عام منذ عام 2013. أيضًا، من غير الشائع
استكشاف **تعلم التمثيل**</span> **(Representation Learning)**
<span dir="rtl">في **سياق التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement <span dir="rtl"></span>Learning
Context<span dir="rtl">) </span>**<span dir="rtl">التعليم
**المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">يجلب بعض
الاحتمالات الجديدة لهذه القضية القديمة، مثل **المهام المساعدة**</span>
**(Auxiliary Tasks)** <span dir="rtl">التي تمت مناقشتها في **القسم
17.1**.</span> <span dir="rtl">في **التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">، يمكن تحديد مشكلة
**تعلم التمثيل** </span>**(Representation Learning)
<span dir="rtl"></span>**<span dir="rtl">مع مشكلة **تعلم دالة تحديث
الحالة**</span> **<span dir="rtl">(</span>Learning the State-Update
Function<span dir="rtl">) </span>**<span dir="rtl">التي تم مناقشتها في
**القسم 17.3**</span>.

<span dir="rtl">ثالثًا، ما زلنا بحاجة إلى **طرق قابلة للتوسع**</span>
**(Scalable Methods)** **<span dir="rtl">للتخطيط باستخدام نماذج البيئة
المكتسبة</span> (Planning with Learned Environment
Models)**<span dir="rtl">.</span> <span dir="rtl">أثبتت **طرق التخطيط**
</span>**(Planning Methods)** <span dir="rtl">فعاليتها الكبيرة في
التطبيقات مثل</span> **AlphaGo Zero <span dir="rtl">وشطرنج
الكمبيوتر</span> (Computer Chess)**<span dir="rtl">، حيث أن **نموذج
البيئة**</span> **(Model of the Environment)** <span dir="rtl">معروف من
**قواعد اللعبة**</span> **(Rules of the Game)** <span dir="rtl">أو يمكن
توفيره من قبل **المصممين البشريين**</span>
**<span dir="rtl">(</span>Human
<span dir="rtl"></span>Designers<span dir="rtl">)
</span>**<span dir="rtl">ولكن حالات **التعلم المعزز المستند إلى النموذج
بالكامل**</span> **<span dir="rtl">(</span>Full Model-Based
<span dir="rtl"></span>Reinforcement
Learning<span dir="rtl">)</span>**<span dir="rtl">، حيث يتم تعلم **نموذج
البيئة**</span> **(Environment Model)** <span dir="rtl">من البيانات ثم
استخدامه **للتخطيط**</span>
**<span dir="rtl">(</span>Planning<span dir="rtl">)</span>**<span dir="rtl">،
نادرة.</span> **<span dir="rtl">نظام</span> Dyna**
<span dir="rtl">الموصوف في **الفصل 8** هو مثال واحد، ولكن كما هو موضح
هناك وفي معظم الأعمال اللاحقة، فإنه يستخدم **نموذجًا جدوليًا بدون تقريب
دالة**</span> **<span dir="rtl">(</span>Tabular Model Without Function
Approximation<span dir="rtl">)</span>**<span dir="rtl">، مما يقيد بشكل
كبير قابليته للتطبيق. فقط عدد قليل من الدراسات شملت **نماذج خطية
مكتسبة**</span> **<span dir="rtl">(</span>Learned Linear
Models<span dir="rtl">)</span>**<span dir="rtl">، وعدد أقل من ذلك استكشف
أيضًا **شمل النماذج المجردة الزمنية**</span>
**<span dir="rtl">(</span>Temporally-Abstract
Models<span dir="rtl">)</span>** <span dir="rtl">باستخدام
**الخيارات**</span> **(Options)** <span dir="rtl">كما تمت مناقشته في
**القسم 17.2**</span>.

<span dir="rtl">هناك حاجة إلى المزيد من العمل قبل أن يصبح **التخطيط
باستخدام النماذج المكتسبة**</span> **<span dir="rtl">(</span>Planning
<span dir="rtl"></span>with Learned Models<span dir="rtl">)
</span>**<span dir="rtl">فعالًا. على سبيل المثال، يجب أن يكون **تعلم
النموذج**</span> **<span dir="rtl">(</span>Learning of
<span dir="rtl"></span>the Model<span dir="rtl">)</span>**
<span dir="rtl">انتقائيًا لأن نطاق **النموذج**</span> **(Model)**
<span dir="rtl">يؤثر بشدة على **كفاءة التخطيط**</span>
**<span dir="rtl">(</span>Planning
<span dir="rtl"></span>Efficiency<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">إذا ركز **النموذج**</span> **(Model)**
<span dir="rtl">على **العواقب الرئيسية**</span> **(Key Consequences)**
**<span dir="rtl">لأهم الخيارات</span> (Most Important
Options)**<span dir="rtl">، يمكن أن يكون **التخطيط**</span>
**(Planning)** <span dir="rtl">فعالًا وسريعًا، ولكن إذا شمل
**النموذج**</span> **(Model)** <span dir="rtl">تفاصيل **عواقب غير مهمة
للخيارات**</span> **<span dir="rtl">(</span>Unimportant
<span dir="rtl"></span>Consequences of Options<span dir="rtl">)
</span>**<span dir="rtl">التي من غير المرجح أن يتم اختيارها، فقد يكون
**التخطيط** </span>**(Planning)
<span dir="rtl"></span>**<span dir="rtl">عديم الفائدة تقريبًا. يجب بناء
**نماذج البيئة**</span> **(Environment Models)** <span dir="rtl">بحكمة
فيما يتعلق **بحالاتها ودينامياتها**</span> **(Their States and
Dynamics)** <span dir="rtl">بهدف **تحسين عملية التخطيط**
</span>**(Optimizing the Planning Process)** <span dir="rtl">يجب مراقبة
الأجزاء المختلفة من **النموذج** </span>**(Model)**
<span dir="rtl">باستمرار لمعرفة **مدى مساهمتها**</span> **(Degree to
Which They Contribute)** <span dir="rtl">أو إضرارها **بكفاءة
التخطيط**</span> **(Planning Efficiency)**. <span dir="rtl">لم يتعامل
المجال بعد مع هذا التعقيد من القضايا أو يصمم **طرق تعلم النموذج**</span>
**(Model-Learning Methods)** <span dir="rtl">التي تأخذ في الاعتبار
آثارها</span>.

<span dir="rtl">القضية الرابعة التي تحتاج إلى معالجة في الأبحاث
المستقبلية هي **أتمتة اختيار المهام** </span>**(Automating the Choice of
Tasks)** <span dir="rtl">التي يعمل عليها **العميل**</span> **(Agent)**
<span dir="rtl">ويستخدمها **لتنظيم كفاءته المتنامية**
</span>**(Structure Its Developing Competence)**<span dir="rtl">.</span>
<span dir="rtl">من المعتاد في **التعلم الآلي** </span>**(Machine
Learning) <span dir="rtl"></span>**<span dir="rtl">أن يقوم **المصممون
البشريون**</span> **(Human Designers)** <span dir="rtl">بتحديد المهام
التي يُتوقع أن يتقنها **العميل التعلمي**</span> **(Learning Agent)**
<span dir="rtl">نظرًا لأن هذه المهام معروفة مسبقًا وتظل ثابتة، يمكن
تضمينها في **كود خوارزمية التعلم** </span>**(Learning Algorithm
Code)**<span dir="rtl">.</span> <span dir="rtl">ولكن، بالنظر إلى
المستقبل، سنرغب في أن يتخذ **العميل**</span> **(Agent)**
<span dir="rtl">قراراته الخاصة حول المهام التي يجب عليه محاولة إتقانها.
قد تكون هذه **مهامًا فرعية**</span> **(Subtasks)** <span dir="rtl">لمهمة
عامة معروفة بالفعل، أو قد تكون **كتل بناء** </span>**(Building Blocks)**
<span dir="rtl">تُستخدم لتسهيل التعلم **لعدد كبير من المهام
المختلفة**</span> **<span dir="rtl">(</span>Many Different
<span dir="rtl"></span>Tasks<span dir="rtl">)</span>**
<span dir="rtl">التي من المحتمل أن يواجهها **العميل**</span> **(Agent)**
<span dir="rtl">في المستقبل ولكنها غير معروفة حاليًا</span>.

<span dir="rtl">قد تكون هذه المهام مثل **المهام المساعدة**</span>
**(Auxiliary Tasks)** <span dir="rtl">أو **الدوال القيمة العامة**
</span>**(GVFs)** <span dir="rtl">التي تمت مناقشتها في **القسم 17.1**،
أو **المهام التي تُحل بواسطة الخيارات**</span>
**<span dir="rtl">(</span>Tasks Solved by
<span dir="rtl"></span>Options<span dir="rtl">)</span>**
<span dir="rtl">كما نوقش في **القسم 17.2**.</span> <span dir="rtl">على
سبيل المثال، عند تشكيل **دالة قيمة عامة**
</span>**(GVF)**<span dir="rtl">، ما الذي يجب أن يكون **العنصر
التراكمي** </span>**(Cumulant)**<span dir="rtl">، **السياسة**
</span>**(Policy)**<span dir="rtl">، و**دالة الإنهاء**
</span>**(Termination Function)**<span dir="rtl">؟ **الحالة الفنية
الحالية**</span> **(Current State of the Art)** <span dir="rtl">هي تحديد
هذه الأمور يدويًا، ولكن ستكون هناك قوة وعمومية أكبر بكثير إذا تم **اختيار
هذه المهام تلقائيًا**</span> **(Making These Task Choices
Automatically)**<span dir="rtl">، خاصة عندما تكون مشتقة من **ما سبق أن
قام العميل ببنائه**</span> **(What the Agent Has Previously
Constructed)** <span dir="rtl">نتيجة **لتعلم التمثيل**</span>
**(Representation Learning)** <span dir="rtl">أو **التجربة مع المشاكل
الفرعية السابقة**</span> **<span dir="rtl">(</span>Experience
<span dir="rtl"></span>with Previous
Subproblems<span dir="rtl">)</span>** <span dir="rtl"></span>
<span dir="rtl">إذا تم **أتمتة تصميم الدالة القيمة العامة**</span>
**<span dir="rtl">(</span>Automated <span dir="rtl"></span>GVF
Design<span dir="rtl">)</span>**<span dir="rtl">، فعندئذٍ يجب أن تكون
**خيارات التصميم**</span> **(Design Choices)** <span dir="rtl">ممثلة
بشكل صريح. بدلاً من أن تكون **اختيارات المهام**</span> **(Task Choices)**
<span dir="rtl">في ذهن **المصمم**</span> **(Designer)**
<span dir="rtl">ومضمنة في **الكود** </span>**(Code)**<span dir="rtl">،
سيكون من الضروري أن تكون داخل **الآلة**</span> **(Machine Itself)**
<span dir="rtl">بطريقة يمكن ضبطها وتغييرها ومراقبتها وتصفيتها والبحث
عنها تلقائيًا. يمكن بعد ذلك بناء المهام بشكل هرمي فوق بعضها البعض كما هي
الحال مع **الميزات**</span> **(Features)** <span dir="rtl">في **الشبكات
العصبية الاصطناعية** </span>**(ANN)**<span dir="rtl">.</span>
**<span dir="rtl">المهام</span> (Tasks)
<span dir="rtl"></span>**<span dir="rtl">هي **الأسئلة**</span>
**<span dir="rtl">(</span>Questions<span dir="rtl">)،</span>**
<span dir="rtl">و**محتويات الشبكة العصبية الاصطناعية**</span>
**<span dir="rtl">(</span>Contents of <span dir="rtl"></span>the
ANN<span dir="rtl">) </span>**<span dir="rtl">هي **إجابات على تلك
الأسئلة**</span> **(Answers to Those Questions)** <span dir="rtl">نتوقع
أن هناك حاجة إلى وجود **تسلسل هرمي كامل للأسئلة**</span> **(Full
Hierarchy of Questions)** <span dir="rtl">ليتوافق مع **التسلسل الهرمي
للإجابات**</span> **(Hierarchy of Answers)** <span dir="rtl">التي توفرها
**طرق التعلم العميق الحديثة** </span>**(Modern Deep Learning
Methods)**<span dir="rtl">.</span>

<span dir="rtl">القضية الخامسة التي نود تسليط الضوء عليها للبحث
المستقبلي هي **التفاعل بين السلوك والتعلم** </span>**(Interaction
Between Behavior and Learning)
<span dir="rtl"></span>**<span dir="rtl">من خلال **بعض النظائر الحاسوبية
للفضول**</span> **(Computational Analog of Curiosity)**
<span dir="rtl">في هذا الفصل، كنا نتخيل **بيئة**</span>
**<span dir="rtl">(</span>Setting<span dir="rtl">)
</span>**<span dir="rtl">يتم فيها **تعلم العديد من المهام**</span>
**(Many Tasks)** <span dir="rtl">في وقت واحد، باستخدام **الأساليب خارج
السياسة** </span>**(Off-Policy Methods)**<span dir="rtl">، من نفس **تيار
الخبرة**</span> **(Stream of Experience)** <span dir="rtl">بالطبع، ستؤثر
**الإجراءات المتخذة**</span> **(Actions Taken)** <span dir="rtl">على هذا
**تيار الخبرة** </span>**(Stream of Experience)**<span dir="rtl">، مما
سيحدد بدوره **مقدار التعلم**</span> **(How Much Learning Occurs)**
<span dir="rtl">و**المهام التي يتم تعلمها**</span>
**<span dir="rtl">(</span>Which <span dir="rtl"></span>Tasks Are
Learned<span dir="rtl">)</span>** <span dir="rtl">عندما لا تكون
**المكافأة**</span> **(Reward)** <span dir="rtl">متاحة، أو لا تتأثر بشكل
كبير **بالسلوك** </span>**(Behavior)**<span dir="rtl">، يكون
**العميل**</span> **(Agent)** <span dir="rtl">حرًا في اختيار
**الإجراءات**</span> **(Actions)** <span dir="rtl">التي تزيد من
**التعلم**</span> **(Learning)** <span dir="rtl">على **المهام**</span>
**(Tasks)** <span dir="rtl">إلى أقصى حد، أي استخدام بعض **مقاييس تقدم
التعلم** </span>**(Measure of Learning Progress)
<span dir="rtl"></span>** <span dir="rtl">كـ **مكافأة داخلية أو
"ذاتية"** </span>**(Intrinsic Reward)**<span dir="rtl">، مما يُنفذ **شكلًا
حاسوبيًا من الفضول**</span> **(Computational Form of Curiosity)**
<span dir="rtl">بالإضافة إلى **قياس تقدم التعلم** </span>**(Measuring
Learning Progress)**<span dir="rtl">، يمكن لـ **المكافأة
الذاتية**</span> **<span dir="rtl">(</span>Intrinsic
<span dir="rtl"></span>Reward<span dir="rtl">)</span>**<span dir="rtl">،
من بين إمكانيات أخرى، أن **تشير إلى استقبال مدخلات غير متوقعة أو جديدة
أو مثيرة للاهتمام بطريقة أخرى**</span> **<span dir="rtl">(</span>Signal
the Receipt of Unexpected, Novel, or Otherwise
<span dir="rtl"></span>Interesting
Input<span dir="rtl">)</span>**<span dir="rtl">، أو يمكن أن تقيم **قدرة
العميل على إحداث تغييرات في بيئته**</span>
**<span dir="rtl">(</span>Agent’s <span dir="rtl"></span>Ability to
Cause Changes in Its Environment<span dir="rtl">).</span>**
<span dir="rtl">يمكن استخدام **إشارات المكافأة الذاتية**
</span>**(Intrinsic Reward Signals)
<span dir="rtl"></span>**<span dir="rtl">المولدة بهذه الطرق من قبل
**العميل**</span> **(Agent)** <span dir="rtl">لطرح **مهام لنفسه**
</span>**(Tasks for Itself) <span dir="rtl"></span>**<span dir="rtl">من
خلال **تحديد المهام المساعدة** </span>**(Defining Auxiliary
Tasks)**<span dir="rtl">، **الدوال القيمة العامة**
</span>**(GVFs)**<span dir="rtl">، أو **الخيارات**
</span>**(Options)**<span dir="rtl">، كما تمت مناقشته أعلاه، بحيث تساهم
**المهارات المكتسبة بهذه الطريقة**</span> **(Skills Learned in This
Way)** <span dir="rtl">في **قدرة العميل على إتقان المهام
المستقبلية**</span> **(Agent’s Ability to Master Future Tasks)**
<span dir="rtl">النتيجة هي **نظير حاسوبي لشيء مثل اللعب**</span>
**(Computational Analog of Something Like Play)** <span dir="rtl">تم
إجراء العديد من الدراسات الأولية حول مثل هذه الاستخدامات **لإشارات
المكافأة الذاتية**</span> **<span dir="rtl">(</span>Intrinsic Reward
<span dir="rtl"></span>Signals<span dir="rtl">)</span>**<span dir="rtl">،
ولا تزال الموضوعات المثيرة للبحث المستقبلي موجودة في هذا المجال
العام</span>.

<span dir="rtl">القضية الأخيرة التي تتطلب اهتمامًا في الأبحاث المستقبلية
هي تطوير **طرق لجعل من المقبول أمنيًا تضمين وكلاء التعليم المعزز في
البيئات الفيزيائية**</span> **<span dir="rtl">(</span>Methods to Make It
Acceptably <span dir="rtl"></span>Safe to Embed Reinforcement Learning
Agents into Physical
Environment<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">هذه واحدة من أكثر المجالات إلحاحًا للبحث المستقبلي،
وسنناقشها بشكل أكبر في **القسم التالي** </span>**(Following
Section)**<span dir="rtl">.</span>

**<u>17.6 <span dir="rtl">مستقبل الذكاء الاصطناعي</span> (The Future of
Artificial Intelligence)</u>**

<span dir="rtl">عندما كنا نكتب النسخة الأولى من هذا الكتاب في منتصف
التسعينيات، كان **الذكاء الاصطناعي** </span>**(Artificial Intelligence)
<span dir="rtl"></span>**<span dir="rtl">يحقق تقدمًا ملحوظًا وكان له تأثير
على المجتمع، على الرغم من أن معظم هذا التأثير كان لا يزال في إطار
**الوعود**</span> **(Promises)** <span dir="rtl">التي يقدمها الذكاء
الاصطناعي والتي كانت تلهم التطورات. كان **التعلم الآلي**</span>
**(Machine Learning)** <span dir="rtl">جزءًا من هذا التوجه، لكنه لم يصبح
بعد جزءًا لا غنى عنه **للذكاء الاصطناعي** </span>**(Artificial
Intelligence)**<span dir="rtl">.</span> <span dir="rtl">بحلول اليوم،
انتقلت هذه الوعود إلى **تطبيقات**</span> **(Applications)**
<span dir="rtl">تغير حياة ملايين الناس، وأصبح **التعلم الآلي**</span>
**<span dir="rtl">(</span>Machine
<span dir="rtl"></span>Learning<span dir="rtl">)
</span>**<span dir="rtl">تقنية رئيسية. أثناء كتابة هذه الطبعة الثانية،
كانت بعض التطورات الأكثر إثارة للدهشة في مجال **الذكاء
الاصطناعي**</span> **(Artificial Intelligence)** <span dir="rtl">تتعلق
**بالتعليم المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">)</span>**<span dir="rtl">،
وخاصةً</span> **"<span dir="rtl">التعليم المعزز العميق</span> (Deep
Reinforcement Learning)"**—**<span dir="rtl">التعليم المعزز</span>
(Reinforcement Learning)** <span dir="rtl">مع **تقريب الدوال بواسطة
الشبكات العصبية العميقة** </span>**(Function Approximation by Deep
Artificial Neural Networks) <span dir="rtl"></span>**
<span dir="rtl">نحن في بداية موجة من **التطبيقات الواقعية للذكاء
الاصطناعي**</span> **<span dir="rtl">(</span>Real-World Applications of
Artificial
<span dir="rtl"></span>Intelligence<span dir="rtl">)</span>**<span dir="rtl">،
العديد منها سيشمل **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">، سواءً العميق أو غيره، والتي ستؤثر على حياتنا
بطرق يصعب التنبؤ بها</span>.

<span dir="rtl">ولكن، **وفرة التطبيقات الواقعية الناجحة**</span>
**<span dir="rtl">(</span>Abundance of Successful Real-World
<span dir="rtl"></span>Applications<span dir="rtl">)</span>**
<span dir="rtl">لا تعني أن **الذكاء الاصطناعي الحقيقي**</span> **(True
Artificial Intelligence)** <span dir="rtl">قد وصل. على الرغم من التقدم
الكبير في العديد من المجالات، لا يزال هناك فجوة كبيرة بين **الذكاء
الاصطناعي**</span> **(Artificial Intelligence)** <span dir="rtl">و**ذكاء
البشر** </span>**(Human Intelligence)**<span dir="rtl">، وحتى **ذكاء
الحيوانات الأخرى** </span>**(Other Animals)**<span dir="rtl">.</span>
<span dir="rtl">يمكن تحقيق أداء يفوق **قدرات البشر**</span>
**<span dir="rtl">(</span>Superhuman
<span dir="rtl"></span>Performance<span dir="rtl">)</span>**
<span dir="rtl">في بعض المجالات، حتى في المجالات الصعبة مثل
**لعبة**</span> <span dir="rtl"></span>**Go**<span dir="rtl">، ولكن لا
يزال من الصعب تطوير أنظمة تشبهنا في كونها **عملاء كاملين
وتفاعليين**</span> **<span dir="rtl">(</span>Complete, Interactive
<span dir="rtl"></span>Agents<span dir="rtl">)</span>**
<span dir="rtl">تمتلك **قابلية التكيف العامة ومهارات حل
المشكلات**</span> **<span dir="rtl">(</span>General Adaptability and
<span dir="rtl"></span>Problem-Solving
Skills<span dir="rtl">)</span>**<span dir="rtl">، **الذكاء العاطفي**
</span>**(Emotional Sophistication)**<span dir="rtl">، **الإبداع**
</span>**(Creativity)**<span dir="rtl">، والقدرة على **التعلم السريع من
التجربة** </span>**(Learn Quickly from Experience)** <span dir="rtl">مع
تركيزه على **التعلم من خلال التفاعل مع البيئات الديناميكية**</span>
**<span dir="rtl">(</span>Learning by Interacting
<span dir="rtl"></span>with Dynamic
Environments<span dir="rtl">)</span>**<span dir="rtl">، سيكون **التعليم
المعزز** </span>**(Reinforcement Learning)**<span dir="rtl">، مع تطوره
في المستقبل، مكونًا حاسمًا **للعملاء**</span> **(Agents)**
<span dir="rtl">الذين يمتلكون هذه القدرات</span>.

<span dir="rtl">تؤكد **اتصالات التعليم المعزز بعلم النفس وعلم
الأعصاب**</span> **<span dir="rtl">(</span>Reinforcement Learning’s
<span dir="rtl"></span>Connections to Psychology and
Neuroscience<span dir="rtl">)</span>**<span dir="rtl">، كما في **الفصول
14 و15**، على أهميته في تحقيق هدف آخر طويل الأمد **للذكاء الاصطناعي**
</span>**(Artificial Intelligence)**<span dir="rtl">:</span>
**<span dir="rtl">إلقاء الضوء على الأسئلة الأساسية حول العقل وكيف ينبثق
من الدماغ</span> <span dir="rtl">(</span>Shedding Light on
<span dir="rtl"></span>Fundamental Questions about the Mind and How It
Emerges from the Brain<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">يساهم **نظري التعليم المعزز**</span> **(Reinforcement
Learning Theory)** <span dir="rtl">بالفعل في فهمنا **لعمليات المكافأة
والتحفيز واتخاذ القرارات في الدماغ**</span>
**<span dir="rtl">(</span>Brain’s Reward, Motivation, and
<span dir="rtl"></span>Decision-Making
Processes<span dir="rtl">)</span>**<span dir="rtl">، وهناك أسباب وجيهة
للاعتقاد بأنه من خلال روابطه **بالطب النفسي الحاسوبي**</span>
**<span dir="rtl">(</span>Computational
Psychiatry<span dir="rtl">)</span>**<span dir="rtl">، سيساهم **نظري
التعليم المعزز** </span>**(Reinforcement Learning Theory)
<span dir="rtl"></span>**<span dir="rtl">في تطوير **طرق علاج الاضطرابات
النفسية** </span>**(Methods for Treating Mental
Disorders)**<span dir="rtl">، بما في ذلك **إدمان المخدرات**</span>
**<span dir="rtl">(</span>Drug
<span dir="rtl"></span>Abuse<span dir="rtl">)</span>**
<span dir="rtl">و**الإدمان**
</span>**(Addiction)**<span dir="rtl">.</span>

<span dir="rtl">يمكن أن يقدم **التعليم المعزز**</span> **(Reinforcement
Learning)** <span dir="rtl">مساهمة أخرى في المستقبل كمساعد **لاتخاذ
القرارات البشرية**</span> **(Aid to Human Decision Making)**
**<span dir="rtl">السياسات</span> (Policies)** <span dir="rtl">المستمدة
من **التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">في **البيئات المحاكاة**</span>
**<span dir="rtl">(</span>Simulated
<span dir="rtl"></span>Environments<span dir="rtl">)</span>**
<span dir="rtl">يمكن أن تنصح **متخذي القرارات البشرية**</span> **(Human
Decision Makers)** <span dir="rtl">في مجالات مثل **التعليم**
</span>**(Education)**<span dir="rtl">، **الرعاية الصحية**
</span>**(Healthcare)**<span dir="rtl">،  
**النقل** </span>**(Transportation)**<span dir="rtl">، **الطاقة**
</span>**(Energy)**<span dir="rtl">، و**تخصيص الموارد في القطاع
العام**</span> **<span dir="rtl">(</span>Public-Sector Resource
Allocation<span dir="rtl">)</span>**<span dir="rtl">.**الخاصية
الرئيسية**</span> **(Key Feature)** <span dir="rtl">التي تجعل **التعليم
المعزز**</span> **(Reinforcement Learning)** <span dir="rtl">ذا صلة خاصة
هي أنه يأخذ في الاعتبار **العواقب طويلة الأمد للقرارات**</span>
<span dir="rtl"></span>**(Long-Term Consequences of
Decisions)**<span dir="rtl">.</span> <span dir="rtl">يتضح هذا بوضوح في
الألعاب مثل **الطاولة**</span> **(Backgammon)**
<span dir="rtl">و**لعبة** </span>**Go**<span dir="rtl">، حيث تم عرض بعض
من أكثر النتائج إثارة للإعجاب **للتعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">، ولكنه أيضًا خاصية للعديد من **القرارات ذات
المخاطر العالية**</span> **(High-Stakes Decisions)**
<span dir="rtl">التي تؤثر على حياتنا وعلى كوكبنا</span>.
**<span dir="rtl">التعليم المعزز</span> (Reinforcement Learning)**
<span dir="rtl">يتبع الأساليب ذات الصلة **لتقديم المشورة لاتخاذ القرارات
البشرية**</span> **(Advising Human Decision Making)**
<span dir="rtl">التي تم تطويرها في الماضي من قبل **محللي
القرارات**</span> **(Decision Analysts)** <span dir="rtl">في العديد من
**التخصصات** </span>**(Disciplines)**<span dir="rtl">.</span>
<span dir="rtl">مع **طرق تقريب الدوال المتقدمة**</span> **(Advanced
Function Approximation Methods)** <span dir="rtl">و**القوة الحسابية
الهائلة** </span>**(Massive Computational Power)**<span dir="rtl">، فإن
**طرق التعليم المعزز**</span> **<span dir="rtl">(</span>Reinforcement
<span dir="rtl"></span>Learning Methods<span dir="rtl">)</span>**
<span dir="rtl">لديها القدرة على التغلب على بعض الصعوبات في توسيع
**أساليب دعم اتخاذ القرارات التقليدية**</span> **(Traditional
Decision-Support Methods)** <span dir="rtl">لتشمل **مشاكل أكبر وأكثر
تعقيدًا** </span>**(Larger and More Complex
Problems)**<span dir="rtl">.</span>

<span dir="rtl">لقد أدى **الوتيرة السريعة للتقدم في الذكاء
الاصطناعي**</span> **<span dir="rtl">(</span>Rapid Pace of Advances in
<span dir="rtl"></span>Artificial Intelligence<span dir="rtl">)</span>**
<span dir="rtl">إلى تحذيرات بأن **الذكاء الاصطناعي**
</span>**(Artificial Intelligence)** <span dir="rtl">يشكل تهديدات خطيرة
لمجتمعاتنا، بل للبشرية نفسها</span>. **<span dir="rtl">العالم البارز
ورائد الذكاء الاصطناعي</span> (Renowned Scientist and Artificial
Intelligence Pioneer) <span dir="rtl"></span>**<span dir="rtl">هربرت
سيمون توقع التحذيرات التي نسمعها اليوم في عرض تقديمي في **ندوة**</span>
**Earthware <span dir="rtl">في جامعة كارنيجي ميلون في عام 2000</span>
(Earthware Symposium at CMU in 2000)** (Simon,
2000)<span dir="rtl">.</span> <span dir="rtl">تحدث عن الصراع الأبدي بين
**الوعد والمخاطر لأي معرفة جديدة**</span>
**<span dir="rtl">(</span>Promise and Perils of Any New
<span dir="rtl"></span>Knowledge<span dir="rtl">)</span>**<span dir="rtl">،
مذكرًا إيانا **بالأساطير اليونانية**</span> **(Greek Myths)**
<span dir="rtl">مثل **بروميثيوس**
</span>**(Prometheus)**<span dir="rtl">، **بطل العلم الحديث**
</span>**(Hero of Modern Science)**<span dir="rtl">، الذي سرق النار من
الآلهة **لفائدة البشرية**</span> **<span dir="rtl">(</span>For the
Benefit of Mankind<span dir="rtl">)</span>**<span dir="rtl">،
و**باندورا**</span>
**<span dir="rtl">(</span>Pandora<span dir="rtl">)</span>**<span dir="rtl">،
التي يمكن أن يفتح صندوقها بفعل صغير وبريء ليطلق **مخاطر لا توصف على
العالم**</span> **<span dir="rtl">(</span>Untold Perils on the
World<span dir="rtl">)</span>** <span dir="rtl">بينما قبل سيمون بأن هذا
الصراع **حتمي** </span>**(Inevitable)**<span dir="rtl">، حثنا على
الاعتراف بأننا كمصممين لمستقبلنا وليس **مجرد مشاهدين**</span>
<span dir="rtl"></span>**(Mere Spectators)**<span dir="rtl">، يمكن
**للقرارات التي نتخذها**</span> **<span dir="rtl">(</span>Decisions
<span dir="rtl"></span>We Make<span dir="rtl">)</span>**
<span dir="rtl">أن **تميل الكفة لصالح بروميثيوس**</span>
**<span dir="rtl">(</span>Tilt the Scale in Prometheus’
Favor<span dir="rtl">) </span>**<span dir="rtl">هذا صحيح بالتأكيد
**للتعليم المعزز** </span>**(Reinforcement Learning)**<span dir="rtl">،
الذي يمكن أن **يفيد المجتمع** </span>**(Benefit Society)**
<span dir="rtl">ولكن يمكن أيضًا أن ينتج عنه **نتائج غير مرغوب
فيها**</span> **<span dir="rtl">(</span>Undesirable
<span dir="rtl"></span>Outcomes<span dir="rtl">)</span>**
<span dir="rtl">إذا تم نشره بإهمال. وبالتالي، فإن **سلامة تطبيقات الذكاء
الاصطناعي التي تشمل التعليم المعزز**</span>
**<span dir="rtl">(</span>Safety of Artificial Intelligence Applications
Involving <span dir="rtl"></span>Reinforcement
Learning<span dir="rtl">)</span>** <span dir="rtl">هو موضوع يستحق
**عناية دقيقة** </span>**(Careful Attention)**<span dir="rtl">.</span>

<span dir="rtl">يمكن **لوكيل التعليم المعزز**</span> **(Reinforcement
Learning Agent)** <span dir="rtl">أن يتعلم من خلال التفاعل **مع العالم
الحقيقي أو مع محاكاة لجزء من العالم الحقيقي**</span>
**<span dir="rtl">(</span>With Either the Real World or
<span dir="rtl"></span>with a Simulation of Some Piece of the Real
World<span dir="rtl">)</span>**<span dir="rtl">، أو من خلال **مزيج من
هذين المصدرين للتجربة**</span> **<span dir="rtl">(</span>Mixture of
These Two Sources of Experience<span dir="rtl">)</span>**
<span dir="rtl">توفر **المحاكيات** </span>**(Simulators)
<span dir="rtl"></span>**<span dir="rtl">بيئات آمنة يمكن للوكيل أن
**يستكشف ويتعلم فيها**</span> **(Explore and Learn)**
<span dir="rtl">دون المخاطرة **بضرر حقيقي لنفسه أو لبيئته**</span>
**<span dir="rtl">(</span>Risking Real Damage to Itself or to Its
<span dir="rtl"></span>Environment<span dir="rtl">)</span>**
<span dir="rtl">في معظم التطبيقات الحالية، يتم **تعلم السياسات**</span>
**(Policies Are Learned)** <span dir="rtl">من **تجربة محاكاة**</span>
**(Simulated Experience)** <span dir="rtl">بدلاً من التفاعل المباشر مع
**العالم الحقيقي**</span> **<span dir="rtl">(</span>Real
<span dir="rtl"></span>World<span dir="rtl">)</span>**
<span dir="rtl">بالإضافة إلى **تجنب العواقب غير المرغوب فيها في العالم
الحقيقي**</span> **<span dir="rtl">(</span>Avoiding
<span dir="rtl"></span>Undesirable Real-World
Consequences<span dir="rtl">)</span>**<span dir="rtl">، يمكن أن يوفر
**التعلم من تجربة محاكاة** </span>**(Learning from Simulated Experience)
<span dir="rtl">بيانات غير محدودة تقريبًا</span>
<span dir="rtl">(</span>Virtually <span dir="rtl"></span>Unlimited
Data<span dir="rtl">)</span>** <span dir="rtl">للتعلم، **بتكلفة أقل بشكل
عام**</span> **(Generally at Less Cost)** <span dir="rtl">مما يلزم
للحصول على **تجربة حقيقية** </span>**(Real
Experience)**<span dir="rtl">، ولأن **المحاكاة عادة ما تعمل بشكل أسرع من
الوقت الحقيقي**</span> <span dir="rtl"></span>**(Simulations Typically
Run Much Faster Than Real Time)**<span dir="rtl">، يمكن أن يحدث
**التعلم**</span> **(Learning)** <span dir="rtl">في كثير من الأحيان
**بشكل أسرع**</span> **(More Quickly)** <span dir="rtl">مما لو كان يعتمد
على **التجربة الحقيقية** </span>**(Real
Experience)**<span dir="rtl">.</span>

<span dir="rtl">ومع ذلك، يتطلب **الاستفادة الكاملة من التعليم المعزز
(**</span>**Full Potential of Reinforcement
<span dir="rtl"></span>Learning<span dir="rtl">) تضمين وكلاء التعليم
المعزز في تدفق التجربة الواقعية</span> <span dir="rtl">(</span>Embedding
<span dir="rtl"></span>Reinforcement Learning Agents into the Flow of
Real-World Experience<span dir="rtl">)</span>**<span dir="rtl">، حيث
**يعملون، يستكشفون، ويتعلمون في عالمنا، وليس فقط في عوالمهم**</span>
**<span dir="rtl">(</span>They Act, Explore, <span dir="rtl"></span>and
Learn in Our World, and Not Just in Their
Worlds<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">بعد كل شيء، **خوارزميات التعليم المعزز**</span>
**(Reinforcement Learning Algorithms)** <span dir="rtl">على الأقل تلك
التي نركز عليها في هذا الكتاب مصممة **للتعلم عبر الإنترنت** </span>**(To
Learn Online)**<span dir="rtl">، وهي تحاكي العديد من جوانب **كيفية بقاء
الحيوانات في بيئات غير مستقرة وعدائية**</span>
**<span dir="rtl">(</span>How Animals Are Able to Survive in
<span dir="rtl"></span>Nonstationary and Hostile
Environments<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">يمكن أن يكون **تضمين وكلاء التعليم المعزز في العالم
الحقيقي**</span> **<span dir="rtl">(</span>Embedding Reinforcement
Learning Agents in the Real
<span dir="rtl"></span>World<span dir="rtl">)
</span>**<span dir="rtl">تحويليًا في **تحقيق وعود الذكاء
الاصطناعي**</span> **<span dir="rtl">(</span>Realizing the Promises of
<span dir="rtl"></span>Artificial Intelligence<span dir="rtl">)
</span>**<span dir="rtl">في **تعزيز وتوسيع القدرات البشرية**</span>
**<span dir="rtl">(</span>Amplify and Extend
<span dir="rtl"></span>Human Abilities<span dir="rtl">)</span>**.

<span dir="rtl">السبب الرئيسي **لرغبة في أن يعمل ويتعلم وكيل التعليم
المعزز في العالم الحقيقي**</span> **<span dir="rtl">(</span>Major Reason
<span dir="rtl"></span>for Wanting a Reinforcement Learning Agent to Act
and Learn in the Real World<span dir="rtl">)</span>** <span dir="rtl">هو
أنه غالبًا ما يكون **من الصعب** </span>**(Difficult)**<span dir="rtl">،
وأحيانًا **من المستحيل** </span>**(Impossible)**<span dir="rtl">،
**محاكاة التجربة الواقعية بدقة كافية**</span>
**<span dir="rtl">(</span>Simulate Real-World Experience with Enough
<span dir="rtl"></span>Fidelity<span dir="rtl">)</span>**
<span dir="rtl">لجعل **السياسات الناتجة، سواء كانت مستمدة من التعليم
المعزز أو من طرق أخرى، تعمل بشكل جيد وآمن عند توجيه الإجراءات
الحقيقية**</span> **<span dir="rtl">(</span>Resulting Policies, Whether
Derived <span dir="rtl"></span>by Reinforcement Learning or by Other
Methods, Work Well—and Safely—When Directing Real
Actions<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">هذا صحيح بشكل خاص **للبيئات التي تعتمد ديناميكياتها على
سلوك البشر**</span> **<span dir="rtl">(</span>Environments Whose
Dynamics Depend on the Behavior of
Humans<span dir="rtl">)</span>**<span dir="rtl">، مثل **التعليم**
</span>**(Education)**<span dir="rtl">، **الرعاية الصحية**</span>
**<span dir="rtl">(</span>Healthcare<span dir="rtl">)</span>**<span dir="rtl">،
**النقل** </span>**(Transportation)**<span dir="rtl">، و**السياسات
العامة**</span> **<span dir="rtl">(</span>Public
Policy<span dir="rtl">)</span>**<span dir="rtl">، وهي مجالات يمكن أن
تستفيد بالتأكيد من **تحسين اتخاذ القرارات**</span> **(Improved Decision
Making)** <span dir="rtl">ومع ذلك، **بالنسبة للوكلاء المدمجين في العالم
الحقيقي** </span>**(Real-World Embedded Agents)**<span dir="rtl">، يجب
أن تؤخذ التحذيرات بشأن **المخاطر المحتملة للذكاء الاصطناعي على محمل
الجد**</span> **<span dir="rtl">(</span>Potential Dangers of Artificial
<span dir="rtl"></span>Intelligence Need to Be
Heeded<span dir="rtl">)</span>**.

<span dir="rtl">بعض هذه التحذيرات ذات صلة خاصة **بالتعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">.</span>
<span dir="rtl">نظرًا لأن **التعليم المعزز**</span> **(Reinforcement
Learning)** <span dir="rtl">يعتمد على **التحسين**
</span>**(Optimization)**<span dir="rtl">، فإنه يرث **إيجابيات
وسلبيات**</span> **(Pluses and Minuses)** <span dir="rtl">جميع **أساليب
التحسين**</span> **<span dir="rtl">(</span>Optimization
<span dir="rtl"></span>Method<span dir="rtl">)</span>**.
<span dir="rtl">من الجانب السلبي، توجد مشكلة **تصميم دوال الهدف**</span>
**<span dir="rtl">(</span>Objective
Functions<span dir="rtl">)</span>**<span dir="rtl">، أو **إشارات
المكافأة**</span> **(Reward Signals)** <span dir="rtl">في حالة **التعليم
المعزز** </span>**(Reinforcement Learning)**<span dir="rtl">، بحيث ينتج
عن **التحسين**</span> **(Optimization)** <span dir="rtl">النتائج
المرغوبة مع تجنب النتائج غير المرغوبة. لقد ذكرنا في **القسم 17.4** أن
**وكلاء التعليم المعزز**</span> **(Reinforcement Learning Agents)**
<span dir="rtl">يمكنهم اكتشاف طرق غير متوقعة لجعل بيئاتهم تقدم
**المكافآت** </span>**(Rewards)**<span dir="rtl">، بعضها قد يكون غير
مرغوب فيه أو حتى خطيرًا. عندما نحدد ما نريده من النظام ليتعلمه بشكل غير
مباشر، كما نفعل عند تصميم **إشارة مكافأة نظام التعليم المعزز**</span>
**<span dir="rtl">(</span>Reinforcement Learning System’s Reward
<span dir="rtl"></span>Signal<span dir="rtl">)</span>**<span dir="rtl">،
لن نعرف مدى قرب **العميل**</span> **(Agent)** <span dir="rtl">من تحقيق
رغبتنا حتى يكتمل **التعلم** </span>**(Learning)** <span dir="rtl">هذه
ليست مشكلة جديدة مع **التعليم المعزز** </span>**(Reinforcement
Learning)**<span dir="rtl">؛ فالاعتراف بها له تاريخ طويل في كل من
**الأدب والهندسة** </span>**(Literature and
Engineering)**<span dir="rtl">.</span> <span dir="rtl">على سبيل المثال،
في قصيدة جوته</span> **"<span dir="rtl">الساحر وتلميذه</span> (The
Sorcerer’s Apprentice)"** (Goethe, 1878)<span dir="rtl">، يستخدم
**التلميذ**</span> **(Apprentice)** <span dir="rtl">السحر لتسخير
**مكنسة**</span> **(Broom)** <span dir="rtl">للقيام بعمله في جلب الماء،
لكن النتيجة كانت **فيضانات غير مقصودة**</span> **(Unintended Flood)**
<span dir="rtl">بسبب **نقص معرفة التلميذ بالسحر** </span>**(Apprentice’s
Inadequate Knowledge of Magic)** <span dir="rtl"></span>
<span dir="rtl">في السياق الهندسي، حذر **نوربرت وينر** </span>**(Norbert
Wiener)**<span dir="rtl">، مؤسس **علم التحكم الآلي**
</span>**(Cybernetics)**<span dir="rtl">، من هذه المشكلة قبل أكثر من نصف
قرن من خلال سرد القصة الخارقة</span> **"<span dir="rtl">قرد اليد</span>
<span dir="rtl">(</span>The Monkey’s Paw)"** (Wiener,
1964<span dir="rtl">)</span>: "... <span dir="rtl">تمنحك ما تطلبه، وليس
ما كان يجب أن تطلبه أو ما تنوي" (ص. 59). كما تم مناقشة المشكلة بتفصيل
كبير في سياق حديث من قبل **نيك بوستروم** </span>**(Nick Bostrom)**
(2014)<span dir="rtl">.</span> <span dir="rtl">أي شخص لديه خبرة مع
**التعليم المعزز**</span> **(Reinforcement Learning)**
<span dir="rtl">من المحتمل أن يكون قد شاهد أنظمتهم تكتشف طرقًا غير متوقعة
**للحصول على الكثير من المكافآت**</span>
**<span dir="rtl">(</span>Obtain a Lot of
<span dir="rtl"></span>Reward<span dir="rtl">)</span>**
<span dir="rtl">أحيانًا يكون **السلوك غير المتوقع**</span> **(Unexpected
Behavior)** <span dir="rtl">جيدًا: فهو يحل مشكلة بطريقة جديدة وجميلة. في
حالات أخرى، ما **يتعلمه العميل**</span> **(Agent Learns)**
<span dir="rtl">ينتهك  
**اعتبارات**</span>
**<span dir="rtl">(</span>Considerations<span dir="rtl">)
</span>**<span dir="rtl">قد لا يكون **مصمم النظام**</span> **(System
Designer)** <span dir="rtl">قد فكر فيها أبدًا.</span>
**<span dir="rtl">التصميم الدقيق لإشارات المكافأة</span> (Careful Design
of Reward Signals)** <span dir="rtl">ضروري إذا كان من المقرر أن **يعمل
العميل في العالم الحقيقي**</span> **(Agent Act in the Real World)**
<span dir="rtl">دون **فرصة لمراجعة الإنسان**</span> **(Opportunity for
Human Vetting)** <span dir="rtl">لأفعاله أو وسائل **إيقاف سلوكه بسهولة**
</span>**(Easily Interrupt Its Behavior)**<span dir="rtl">.</span>

<span dir="rtl">على الرغم من إمكانية حدوث **عواقب سلبية غير
مقصودة**</span> **<span dir="rtl">(</span>Unintended Negative
<span dir="rtl"></span>Consequences<span dir="rtl">)</span>**<span dir="rtl">،
فقد تم استخدام **التحسين**</span> **(Optimization)**
<span dir="rtl">لمئات السنين من قبل **المهندسين**
</span>**(Engineers)**<span dir="rtl">، **المهندسين المعماريين**
</span>**(Architects)**<span dir="rtl">، وغيرهم من الذين كانت
**تصميماتهم لها تأثير إيجابي على العالم** </span>**(Designs Have
Positively Impacted the World)**<span dir="rtl">.</span>
<span dir="rtl">نحن مدينون بالعديد من الأشياء الجيدة في
**بيئتنا**</span> **(Our Environment)** <span dir="rtl">لتطبيق **أساليب
التحسين** </span>**(Optimization Methods)** <span dir="rtl"></span>
<span dir="rtl">تم تطوير العديد من **النهج**</span> **(Approaches)**
<span dir="rtl">للتخفيف من **مخاطر التحسين** </span>**(Risk of
Optimization)**<span dir="rtl">، مثل **إضافة القيود الصارمة
والمرنة**</span> **<span dir="rtl">(</span>Adding Hard and Soft
Constraints<span dir="rtl">)</span>**<span dir="rtl">، **تقييد التحسين
على السياسات القوية والحساسة للمخاطر**</span>
**<span dir="rtl">(</span>Restricting Optimization to Robust and
Risk-Sensitive Policies<span dir="rtl">)</span>**<span dir="rtl">،
و**التحسين باستخدام دوال هدف متعددة**</span> **(Optimizing with Multiple
Objective Functions)** <span dir="rtl">تم **تكييف بعض هذه الأساليب مع
التعليم المعزز** </span>**(Adapted to Reinforcement
Learning)**<span dir="rtl">، وهناك حاجة إلى **مزيد من البحث لمعالجة هذه
المخاوف**</span> **<span dir="rtl">(</span>More Research Is Needed to
Address These Concerns<span dir="rtl">)</span>** <span dir="rtl">تبقى
**مشكلة ضمان توافق هدف وكيل التعليم المعزز مع هدفنا**</span>
**<span dir="rtl">(</span>Problem of Ensuring That a Reinforcement
Learning Agent’s Goal Is Attuned to Our Own <span dir="rtl"></span>**
<span dir="rtl">تحديًا قائمًا</span>.

<span dir="rtl">تحدٍ آخر إذا كان من المقرر **لوكيل التعليم
المعزز**</span> **(Reinforcement Learning Agent)** <span dir="rtl">أن
يعمل ويتعلم في **العالم الحقيقي**</span> **(Real World)**
<span dir="rtl">ليس فقط حول ما قد يتعلمه في النهاية، ولكن حول **كيفية
سلوكه أثناء التعلم** </span>**(How They Will Behave While They Are
Learning)**<span dir="rtl">.</span> <span dir="rtl">كيف تضمن أن يحصل
**العميل**</span> **(Agent)** <span dir="rtl">على **ما يكفي من
التجربة**</span> **(Enough Experience)** <span dir="rtl">لتعلم **سياسة
عالية الأداء** </span>**(High-Performing Policy)**<span dir="rtl">، كل
ذلك دون **إلحاق الضرر ببيئته** </span>**(Harming Its
Environment)**<span dir="rtl">، أو **بغيره من الوكلاء** </span>**(Other
Agents)**<span dir="rtl">، أو **بنفسه** </span>**(Itself)**
<span dir="rtl">(أو بواقعية أكثر، مع الحفاظ على **احتمالية الضرر عند
مستوى مقبول**</span> **<span dir="rtl">(</span>Keeping the Probability
of Harm Acceptably Low<span dir="rtl">))</span>**<span dir="rtl">؟ هذه
المشكلة ليست جديدة ولا فريدة من نوعها **للتعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">.</span>
**<span dir="rtl">إدارة المخاطر</span> (Risk Management)**
<span dir="rtl">و**التخفيف من آثارها**</span> **(Mitigation)**
<span dir="rtl">للتعليم المعزز المدمج مشابهة لما كان على **مهندسي
التحكم** </span>**(Control Engineers) <span dir="rtl"></span>**
<span dir="rtl">مواجهته منذ البداية **باستخدام التحكم الآلي**</span>
**<span dir="rtl">(</span>Using Automatic Control<span dir="rtl">)
</span>** <span dir="rtl">في المواقف التي يمكن أن يكون لسلوك
**المتحكم**</span> **(Controller’s Behavior)** <span dir="rtl">فيها
**عواقب غير مقبولة، وربما كارثية**</span>
**<span dir="rtl">(</span>Unacceptable, Possibly Catastrophic,
Consequences<span dir="rtl">)</span>**<span dir="rtl">، كما في **التحكم
في الطائرات**</span> **(Control of an Aircraft)** <span dir="rtl">أو
**عملية كيميائية دقيقة**</span> **(Delicate Chemical Process)**
<span dir="rtl">تعتمد تطبيقات **التحكم** </span>**(Control
Applications)** <span dir="rtl">على **النمذجة الدقيقة للنظام**
</span>**(Careful System Modeling)**<span dir="rtl">، **التحقق من صحة
النموذج**</span> **(Model Validation)**<span dir="rtl">، و**الاختبار
المكثف** </span>**(Extensive Testing)**<span dir="rtl">، وهناك جسم نظري
متطور للغاية يهدف إلى **ضمان التقارب والاستقرار**</span> **(Ensuring
Convergence and Stability)** **<span dir="rtl">لأجهزة التحكم
التكيفية</span> (Adaptive Controllers)** <span dir="rtl">المصممة
للاستخدام عندما لا تكون **ديناميكيات النظام المراد التحكم فيه معروفة
بالكامل**</span> **<span dir="rtl">(</span>Dynamics of the System to Be
Controlled Are Not Fully Known<span dir="rtl">) الضمانات النظرية</span>
(Theoretical Guarantees)** <span dir="rtl">ليست دائمًا مؤكدة بشكل كامل
لأنها تعتمد على **صلاحية الافتراضات الأساسية للرياضيات**</span>
**<span dir="rtl">(</span>Validity of the
<span dir="rtl"></span>Assumptions Underlying the
Mathematics<span dir="rtl">)</span>**<span dir="rtl">، ولكن بدون هذه
النظرية، إلى جانب **إدارة المخاطر**</span> **(Risk Management)**
<span dir="rtl">و**ممارسات التخفيف** </span>**(Mitigation
Practices)**<span dir="rtl">، لم يكن **التحكم الآلي**</span>
**(Automatic Control)**—<span dir="rtl">التكيفي وغير التكيفي—ليكون
**مفيدًا كما هو  
اليوم**</span> **(As Beneficial as It Is Today)** <span dir="rtl">في
تحسين **جودة وكفاءة وفعالية العمليات**</span>
**<span dir="rtl">(</span>Quality, <span dir="rtl"></span>Efficiency,
and Cost-Effectiveness of Processes<span dir="rtl">)
</span>**<span dir="rtl">التي أصبحنا نعتمد عليها. واحدة من أكثر المجالات
إلحاحًا **لبحث التعليم المعزز المستقبلي**</span>
**<span dir="rtl">(</span>For Future Reinforcement
<span dir="rtl"></span>Learning Research<span dir="rtl">)</span>**
<span dir="rtl">هي **تكييف وتوسيع الأساليب المطورة في هندسة
التحكم**</span> **<span dir="rtl">(</span>Adapt and
<span dir="rtl"></span>Extend Methods Developed in Control
Engineering<span dir="rtl">)</span>** <span dir="rtl">بهدف جعل **دمج
وكلاء التعليم المعزز في البيئات المادية آمنًا بشكل مقبول**</span>
**<span dir="rtl">(</span>Making It Acceptably Safe to Fully
<span dir="rtl"></span>Embed Reinforcement Learning Agents into Physical
Environments<span dir="rtl">)</span>**.

<span dir="rtl">في الختام، نعود إلى **دعوة سيمون**</span> **(Simon’s
Call)** <span dir="rtl">للاعتراف بأننا **مصممو مستقبلنا**
</span>**(Designers of Our Future)
<span dir="rtl"></span>**<span dir="rtl">وليس مجرد **مشاهدين**
</span>**(Spectators)**<span dir="rtl">.</span> <span dir="rtl">من خلال
**القرارات التي نتخذها كأفراد** </span>**(Decisions We Make as
Individuals)**<span dir="rtl">، ومن خلال **التأثير الذي يمكننا ممارسته
على كيفية حكم مجتمعاتنا**</span> **<span dir="rtl">(</span>Influence We
Can Exert on How Our Societies <span dir="rtl"></span>Are
Governed<span dir="rtl">)</span>**<span dir="rtl">، يمكننا **العمل على
ضمان أن تفوق الفوائد التي توفرها التكنولوجيا الجديدة (**</span>**Work
Toward Ensuring That the Benefits Made Possible by a New
<span dir="rtl"></span>Technology Outweigh<span dir="rtl">)</span>**
**<span dir="rtl">الأضرار التي يمكن أن تسببها</span> (The Harm It Can
Cause)** <span dir="rtl">هناك **فرصة واسعة للقيام بذلك**</span> **(Ample
Opportunity to Do This)** <span dir="rtl">في حالة **التعليم المعزز**
</span>**(Reinforcement Learning)**<span dir="rtl">، الذي يمكن أن يساعد
في **تحسين جودة الحياة وعدالتها واستدامتها**</span> **(Improve the
Quality, Fairness, and Sustainability of Life)** <span dir="rtl">على
كوكبنا، ولكنه يمكن أيضًا أن يطلق **مخاطر جديدة** </span>**(New
Perils)**<span dir="rtl">.</span> <span dir="rtl">تهديد **موجود
بالفعل**</span> **<span dir="rtl">(</span>Already
<span dir="rtl"></span>Here<span dir="rtl">)</span>** <span dir="rtl">هو
**إزاحة الوظائف الناجمة عن تطبيقات الذكاء الاصطناعي**</span>
**<span dir="rtl">(</span>Displacement of Jobs
<span dir="rtl"></span>Caused by Applications of Artificial
Intelligence<span dir="rtl">)</span>**<span dir="rtl">.</span>
<span dir="rtl">ومع ذلك، هناك أسباب وجيهة للاعتقاد بأن **فوائد الذكاء
الاصطناعي**</span> **(Benefits of Artificial Intelligence)**
<span dir="rtl">يمكن أن تفوق **الاضطراب الذي يسببه**
</span>**(Disruption It Causes)**<span dir="rtl">.</span>
<span dir="rtl">أما بالنسبة **للسلامة**
</span>**(Safety)**<span dir="rtl">، فإن **المخاطر المحتملة مع التعليم
المعزز**</span> **(Hazards Possible with Reinforcement Learning)**
<span dir="rtl">ليست **مختلفة تمامًا**</span> **(Completely Different)**
<span dir="rtl">عن تلك التي **تمت إدارتها بنجاح**</span>
**<span dir="rtl">(</span>Managed
<span dir="rtl"></span>Successfully<span dir="rtl">)</span>**
<span dir="rtl">لتطبيقات **تحسين**</span> **(Optimization)**
<span dir="rtl">و**أساليب التحكم** </span>**Control Methods)**
<span dir="rtl">المماثلة. مع **انتقال التعليم المعزز إلى العالم
الحقيقي**</span> **<span dir="rtl">(</span>As Reinforcement Learning
<span dir="rtl"></span>Moves Out into the Real World<span dir="rtl">)
</span>**<span dir="rtl">في التطبيقات المستقبلية، **لدى المطورين التزام
باتباع أفضل الممارسات التي تطورت للتقنيات المماثلة**</span>
**<span dir="rtl">(</span>Developers Have an Obligation to
<span dir="rtl"></span>Follow Best Practices That Have Evolved for
Similar Technologies<span dir="rtl">)</span>**<span dir="rtl">، وفي
الوقت نفسه **توسيعها**</span> **(Extending Them)**
<span dir="rtl">للتأكد من أن **بروميثيوس**</span> **(Prometheus)**
<span dir="rtl">يظل **مسيطرًا** </span>**(Keeps the Upper
Hand)**<span dir="rtl">.</span>
