# الفصل الثاني:
## Multi-armed Bandits
### الماكينات ذات الأذرع المتعددة

أكثر ميزات التعليم المعزز تميزًا عن أنواع التعليم الأخرى هي أنه يستخدم معلومات التدريب التي تقيم الأفعال المتخذة بدلاً من توجيهها عبر تقديم الأفعال الصحيحة. هذا ما يخلق الحاجة للاستكشاف النشط، أي البحث الصريح عن سلوك جيد. تشير التغذية الراجعة التقييمية (Evaluative Feedback) البحتة إلى مدى جودة الفعل المتخذ، ولكنها لا تحدد ما إذا كان الفعل هو الأفضل أو الأسوأ الممكن. من ناحية أخرى، تشير التغذية الراجعة التوجيهية (Instructive Feedback) إلى الفعل الصحيح الذي يجب اتخاذه، بغض النظر عن الفعل المتخذ بالفعل. هذا النوع من التغذية الراجعة هو أساس التعليم الخاضع للإشراف، الذي يشمل أجزاء كبيرة من تصنيف الأنماط، والشبكات العصبية الاصطناعية، وتحديد الأنظمة. في أشكالها النقية، تكون هاتان النوعيتان من التغذية الراجعة مميزتين تمامًا: التغذية الراجعة التقييمية (Evaluative Feedback) تعتمد بالكامل على الفعل المتخذ، بينما التغذية الراجعة التوجيهية (Instructive Feedback) مستقلة عن الفعل المتخذ.

في هذا الفصل، ندرس الجانب التقييمي للتعليم المعزز في إطار مبسط، لا يتضمن تعلم التصرف في أكثر من حالة واحدة. هذا الإطار غير الترابطي هو الإطار الذي تم فيه إنجاز معظم الأعمال السابقة التي تتضمن التغذية الراجعة التقييمية (Evaluative Feedback)، وهو يتجنب الكثير من تعقيد مشكلة التعليم المعزز الكامل. دراسة هذه الحالة تمكننا من رؤية الفرق بين التغذية الراجعة التقييمية (Evaluative Feedback) والتغذية الراجعة التوجيهية (Instructive Feedback) بوضوح، وكيف يمكن دمجهما.

المشكلة غير الترابطية، التقييمية التي نستكشفها هي نسخة بسيطة من مشكلة الماكينة ذات الأذرع المتعددة. نستخدم هذه المشكلة لتقديم عدد من الأساليب الأساسية للتعليم التي نوسعها في الفصول اللاحقة لتطبيقها على مشكلة التعليم المعزز الكاملة. في نهاية هذا الفصل، نقترب خطوة من مشكلة التعليم المعزز الكامل من خلال مناقشة ما يحدث عندما تصبح مشكلة الماكينة ذات الأذرع المتعددة ترابطية، أي عندما تُتخذ الأفعال في أكثر من حالة واحدة.

---

## 2.1 مشكلة الماكينة ذات الأذرع المتعددة (k-armed Bandit Problem)

فكر بعناية في مشكلة التعليم التالية. تواجه باستمرار خيارًا بين عدة خيارات مختلفة $k$ أو إجراءات مختلفة. بعد كل اختيار تتلقى مكافأة رقمية يتم اختيارها من توزيع احتمالي ثابت يعتمد على الإجراء الذي قمت باختياره. هدفك هو تعظيم إجمالي المكافأة المتوقعة على مدار فترة زمنية معينة، على سبيل المثال، على مدار 1000 اختيار للإجراءات أو خطوات زمنية.

هذه هي الصيغة الأصلية لمشكلة الماكينات ذات الأذرع المتعددة، والتي سُميت بهذا الاسم بالتشابه مع آلة القمار ذات الذراع الواحد، باستثناء أنها تحتوي على $k$ أذرع بدلاً من واحدة. كل اختيار للعمل يشبه سحب ذراع من أذرع آلة القمار، والعوائد هي الأرباح التي يتم الحصول عليها عند ضرب الجائزة الكبرى. من خلال تكرار اختيار الأذرع، هدفك هو زيادة مكاسبك عن طريق التركيز على الأذرع الأفضل. تشبيه آخر هو أن الطبيب يختار بين علاجات تجريبية لمجموعة من المرضى بشكل خطير. كل إجراء هو اختيار علاج، وكل عائد هو بقاء أو صحة المريض. اليوم، يُستخدم مصطلح "مشكلة الماكينات ذات الأذرع المتعددة" أحياناً للإشارة إلى تعميم للمشكلة الموصوفة أعلاه، ولكن في هذا الكتاب نستخدمه للإشارة فقط إلى هذه الحالة البسيطة.

في مشكلة الماكينات ذات الأذرع المتعددة، كل واحد من الأذرع $k$ له عائد متوقع أو متوسط يُعطى عند اختيار هذا الذراع؛ دعنا نطلق على هذا العائد قيمة هذا الذراع. نُسمي الذراع الذي يُختار في الزمن $t$ بـ $A_t$، والعائد المقابل له بـ $R_t$. إذاً، قيمة أي ذراع عشوائي $a$، التي نُشير إليها بـ $q^*(a)$، هي العائد المتوقع عند اختيار $a$:

$$
q^*(a) = \mathrm{E}[R_t \mid A_t = a]
$$

إذا كنت تعرف قيمة كل إجراء، فستكون مشكلة الماكينات ذات الأذرع المتعددة سهلة الحل: ستقوم دائماً باختيار الإجراء الذي له أعلى قيمة. نحن نفترض أنك لا تعرف قيم الأفعال بيقين، رغم أنك قد تكون لديك تقديرات. نُسمي تقدير قيمة الإجراء $a$ في الزمن $t$ بـ $Q_t(a)$، ونرغب في أن يكون $Q_t(a)$ قريباً من $q^*(a)$.

إذا كنت تحتفظ بتقديرات لقيم الأفعال، فإن في أي وقت سيكون هناك على الأقل إجراء واحد له أعلى قيمة تقديرية. نُطلق على هذه الأفعال اسم الأفعال الجشعة (greedy actions). عندما تختار واحداً من هذه الأفعال، نقول إنك تستغل (exploiting) معرفتك الحالية بقيم الأفعال. إذا كنت تختار بدلاً من ذلك واحداً من الأفعال غير الجشعة (nongreedy actions)، فنحن نقول إنك تستكشف (exploring)، لأن هذا يتيح لك تحسين تقديرك لقيمة الإجراء غير الجشع.

الاستغلال (exploitation) هو الشيء الصحيح لزيادة العائد المتوقع في الخطوة الواحدة، ولكن الاستكشاف (exploration) قد يؤدي إلى تحقيق عائد إجمالي أكبر على المدى الطويل.

على سبيل المثال، افترض أن قيمة الإجراء الجشع معروفة بيقين، بينما يُقدَّر أن عدة أفعال أخرى قريبة من حيث القيمة ولكن مع عدم يقين كبير. عدم اليقين هو أن أحد هذه الأفعال الأخرى ربما يكون في الواقع أفضل من الإجراء الجشع، لكنك لا تعرف أي منها. إذا كان لديك العديد من الخطوات الزمنية القادمة لاختيار الأفعال، فقد يكون من الأفضل استكشاف الأفعال غير الجشعة واكتشاف أي منها أفضل من الإجراء الجشع. العائد أقل على المدى القصير خلال الاستكشاف، ولكنه أعلى على المدى الطويل لأنك بعد اكتشاف الأفعال الأفضل يمكنك استغلالها عدة مرات. بما أنه ليس من الممكن استكشاف واستغلال الإجراء في نفس الوقت، فإننا غالباً ما نستخدم مصطلح الصراع (conflict) بين الاستكشاف والاستغلال.

في أي حالة معينة، سواء كان من الأفضل الاستكشاف أو الاستغلال يعتمد بشكل معقد على القيم الدقيقة للتقديرات، وعدم اليقين، وعدد الخطوات المتبقية. هناك العديد من الأساليب المتطورة لتحقيق توازن بين الاستكشاف والاستغلال في صياغات رياضية محددة لمشكلة الماكينات ذات الأذرع المتعددة والمشاكل ذات الصلة. ومع ذلك، فإن معظم هذه الأساليب تقوم على افتراضات قوية بشأن الثبات والمعرفة السابقة التي إما تُخالف أو يكون من المستحيل التحقق منها في التطبيقات وفي مشكلة التعليم المعزز الكامل التي سنناقشها في الفصول التالية. ضمانات الأمثلية أو الخسارة المحدودة لهذه الأساليب لا تقدم راحة كبيرة عندما لا تنطبق افتراضات نظريتها.

في هذا الكتاب، لا نركز على تحقيق توازن بين الاستكشاف والاستغلال بطريقة متطورة؛ نحن نهتم فقط بتحقيق التوازن بينهما بشكل عام. في هذا الفصل، نقدم عدة طرق بسيطة لتحقيق التوازن لمشكلة الماكينات ذات الأذرع المتعددة ونوضح أنها تعمل بشكل أفضل بكثير من الأساليب التي تعتمد دائماً على الاستغلال. الحاجة إلى تحقيق توازن بين الاستكشاف والاستغلال هي تحدٍ مميز يظهر في التعليم المعزز؛ وبساطة نسخة المشكلة لدينا من الماكينات ذات الأذرع المتعددة تمكننا من عرض هذا التحدي بشكل واضح جداً.

---

## 2.2 طرق تقييم الأفعال (Action-value Methods)

نبدأ بالنظر عن كثب إلى الطرق المستخدمة لتقدير قيم الأفعال واستخدام التقديرات لاتخاذ قرارات اختيار الأفعال، والتي نطلق عليها مجتمعةً اسم طرق تقييم الأفعال (action-value methods). تذكر أن القيمة الحقيقية للإجراء هي العائد المتوسط عند اختيار ذلك الإجراء. إحدى الطرق الطبيعية لتقدير ذلك هي عن طريق حساب متوسط العوائد التي تم الحصول عليها بالفعل:

$$
Q_t(a) = \frac{\text{Sum of Rewards when } a \text{ taken prior to } t}{\text{Number of times } a \text{ taken prior to } t} = \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}} \quad\quad (2.1)
$$

حيث يشير $\mathbb{1}$ إلى المتغير العشوائي الذي يكون 1 إذا كان الشرط صحيحًا و0 إذا لم يكن كذلك. إذا كان المقام صفرًا، فإننا نعرّف $Q_t(a)$ بدلاً من ذلك على أنه قيمة افتراضية، مثل 0. عندما يذهب المقام نحو اللانهاية، وفقاً لقانون الأعداد الكبيرة، يتقارب $Q_t(a)$ إلى $q^*(a)$. نُطلق على هذه الطريقة اسم طريقة المتوسط التجريبي (sample-average method) لتقدير قيم الأفعال لأن كل تقدير هو متوسط العوائد ذات الصلة. بالطبع، هذه مجرد طريقة واحدة لتقدير قيم الأفعال، وليست بالضرورة الأفضل. ومع ذلك، دعنا نلتزم الآن بهذه الطريقة البسيطة للتقدير وننتقل إلى مسألة كيفية استخدام التقديرات لاختيار الأفعال.

أبسط قاعدة لاختيار الأفعال هي اختيار واحد من الأفعال التي لها أعلى قيمة تقديرية، أي واحد من الأفعال الجشعة كما هو محدد في القسم السابق. إذا كان هناك أكثر من إجراء جشع واحد، يتم اختيار واحد منها بطريقة عشوائية أو بأي طريقة أخرى. نكتب هذه الطريقة لاختيار الأفعال الجشعة كما يلي:

$$
A_t = \arg\max_a Q_t(a) \quad\quad (2.2)
$$

حيث يشير $\arg\max_a$ إلى الإجراء $a$ الذي يتم تعظيم التعبير الذي يليه (مرة أخرى، مع كسر الروابط بطريقة عشوائية). اختيار الأفعال الجشعة (greedy action selection) يستغل دائمًا المعرفة الحالية لتعظيم العائد الفوري؛ ولا يقضي أي وقت في تجربة الأفعال التي تبدو أقل فعالية لمعرفة ما إذا كانت قد تكون أفضل فعلاً. بديل بسيط هو التصرف بطريقة جشعة معظم الوقت، ولكن بين الحين والآخر، مثلاً باحتمالية صغيرة $\varepsilon$، اختيار عشوائي من بين جميع الأفعال مع احتمال متساوٍ، بشكل مستقل عن تقديرات قيم الأفعال. نُطلق على الطرق التي تستخدم قاعدة اختيار الأفعال القريبة من الجشع هذه اسم طرق-الجشع ($\varepsilon$-greedy methods). ميزة هذه الطرق هي أنه، في الحد الذي تزداد فيه عدد الخطوات، سيتم تجربة كل إجراء عددًا لانهائيًا من المرات، مما يضمن أن جميع $Q_t(a)$ تتقارب إلى $q^*(a)$ وهذا بالطبع يعني أن احتمالية اختيار الإجراء الأمثل تتقارب إلى أكثر من $1-\varepsilon$، أي تقريباً إلى يقين كبير. ومع ذلك، فإن هذه مجرد ضمانات أفقية، ولا تعطي الكثير عن فعالية الطرق في الممارسة العملية.

**التمرين 2.1:** في اختيار الأفعال القريبة من الجشع ($\varepsilon$-greedy action selection)، في حالة وجود إجراءين و$\varepsilon = 0.5$، ما هي احتمالية اختيار الإجراء الجشع؟

---

## 2.3 اختبار الماكينات ذات الأذرع العشر (The 10-armed Testbed)

لتقييم فعالية طرق تقييم الأفعال greedy و$\varepsilon$-greedy بشكل تقريبي، قمنا بمقارنتهما رقميًا على مجموعة من مشاكل الاختبار. كانت هذه مجموعة من 2000 مشكلة ماكينات ذات أذرع متعددة تم توليدها عشوائيًا، حيث $k=10$. لكل مشكلة ماكينات ذات أذرع متعددة، مثل تلك المعروضة في الشكل 2.1، كانت قيم الأفعال،

$$
q^*(a), \quad a=1,\ldots,10
$$

---

**الشكل 2.1:** مثال على مشكلة الماكينات ذات الأذرع المتعددة من مجموعة الاختبار ذات العشر أذرع. تم اختيار القيمة الحقيقية $q^*(a)$ لكل من الأفعال العشرة وفقًا لتوزيع طبيعي بمتوسط صفر وتباين وحدة، ثم تم اختيار المكافآت الفعلية وفقًا لتوزيع طبيعي بمتوسط $q^*(a)$ وتباين وحدة، كما تقترح هذه التوزيعات الرمادية.

تم اختيار القيم وفقًا لتوزيع طبيعي (غوسي) بمتوسط 0 وتباين 1. ثم، عندما طبق طريقة تعلم على هذه المشكلة واختارت الإجراء $A_t$ في الخطوة الزمنية $t$، تم اختيار المكافأة الفعلية $R_t$ من توزيع طبيعي بمتوسط $q^*(A_t)$ وتباين 1. تظهر هذه التوزيعات باللون الرمادي في الشكل 2.1. نسمي مجموعة مهام الاختبار هذه مجموعة الاختبار ذات العشر أذرع. بالنسبة لأي طريقة تعلم، يمكننا قياس أدائها وسلوكها مع تحسنها مع الخبرة على مدى 1000 خطوة زمنية عند تطبيقها على إحدى مشاكل الماكينات ذات الأذرع المتعددة. هذه تشكل تجربة واحدة. بتكرار ذلك على 2000 تجربة مستقلة، كل منها مع مشكلة ماكينات ذات أذرع متعددة مختلفة، حصلنا على مقاييس للسلوك المتوسط لخوارزمية التعليم.

---

**الشكل 2.2** يقارن بين طريقة greedy وطريقتين $\varepsilon$-greedy ($\varepsilon=0.01$ و$\varepsilon=0.1$)، كما هو موضح أعلاه، على مجموعة الاختبار ذات العشر أذرع. جميع الطرق شكلت تقديراتها لقيم الأفعال باستخدام تقنية المتوسط الحسابي للعينات. الرسم البياني العلوي يظهر الزيادة في المكافأة المتوقعة مع الخبرة. تحسنت طريقة greedy بشكل طفيف أسرع من الطرق الأخرى في البداية، لكنها استقرت عند مستوى أدنى. حققت مكافأة لكل خطوة تبلغ حوالي 1 فقط، مقارنةً بأفضل مكافأة ممكنة والتي تبلغ حوالي 1.55 في هذه المجموعة. كانت طريقة greedy أسوأ بشكل ملحوظ على المدى الطويل.

---

**الشكل 2.2:** الأداء المتوسط لطرق تقدير قيم الأفعال $\varepsilon$-greedy على مجموعة الاختبار ذات العشر أذرع. هذه البيانات هي متوسطات على مدى 2000 تجربة مع مشاكل ماكينات ذات أذرع متعددة مختلفة. جميع الطرق استخدمت المتوسطات الحسابية للعينات كتقديرات لقيم الأفعال.

غالبًا ما كانت طريقة greedy تعلق في أداء أفعال دون المستوى الأمثل. يظهر الرسم البياني السفلي أن طريقة greedy وجدت الإجراء الأمثل في حوالي ثلث المهام فقط. في الثلثين الآخرين، كانت عيناتها الأولية من الإجراء الأمثل مخيبة للآمال، ولم تعُد إليه أبدًا. كانت طرق $\varepsilon$-greedy تؤدي في النهاية بشكل أفضل لأنها استمرت في الاستكشاف وتحسين فرصها في التعرف على الإجراء الأمثل. كانت طريقة $\varepsilon=0.1$ تستكشف أكثر، وعادة ما تجد الإجراء الأمثل في وقت أبكر، لكنها لم تختار ذلك الإجراء أكثر من 91% من الوقت. تحسنت طريقة $\varepsilon=0.01$ بشكل أبطأ، لكنها في النهاية كانت تؤدي بشكل أفضل من طريقة $\varepsilon=0.1$ على كلا مقياسي الأداء المعروضين في الشكل. من الممكن أيضًا تقليل $\varepsilon$ مع مرور الوقت لمحاولة الحصول على أفضل ما في القيم العالية والمنخفضة.

ميزة طرق $\varepsilon$-greedy على طرق greedy تعتمد على المهمة. على سبيل المثال، لنفترض أن تباين المكافآت كان أكبر، مثلاً 10 بدلاً من 1. مع المكافآت الأكثر ضوضاء، يتطلب الأمر المزيد من الاستكشاف للعثور على الإجراء الأمثل، ويجب أن تكون طرق $\varepsilon$-greedy أفضل نسبيًا مقارنةً بطريقة greedy. من ناحية أخرى، إذا كانت تباينات المكافآت صفرية، فإن طريقة greedy ستكون قادرة على معرفة القيمة الحقيقية لكل إجراء بعد تجربته مرة واحدة. في هذه الحالة، قد تكون طريقة greedy هي الأفضل لأنها ستجد الإجراء الأمثل بسرعة ثم لا تستكشف أبدًا. ولكن حتى في الحالة الحتمية، هناك ميزة كبيرة للاستكشاف إذا قمنا بتخفيف بعض الافتراضات الأخرى. على سبيل المثال، لنفترض أن مهمة الماكينات ذات الأذرع المتعددة كانت غير ثابتة، أي أن القيم الحقيقية للأفعال تتغير مع مرور الوقت. في هذه الحالة، يكون الاستكشاف ضروريًا حتى في الحالة الحتمية للتأكد من أن أحد الأفعال غير الجشعة لم يتغير ليصبح أفضل من الإجراء الجشع. كما سنرى في الفصول التالية، تعتبر عدم الثبات هي الحالة الأكثر شيوعًا التي يتم مواجهتها في التعليم المعزز. حتى إذا كانت المهمة الأساسية ثابتة وحتمية، يواجه المتعلم مجموعة من مهام اتخاذ القرار الشبيهة بالماكينات ذات الأذرع المتعددة، والتي تتغير كل منها مع مرور الوقت مع تقدم التعليم وتغير سياسة اتخاذ القرار لدى الوكيل. يتطلب التعليم المعزز توازنًا بين الاستكشاف والاستغلال.

---

**التمرين 2.2:** مثال على الماكينات ذات الأذرع المتعددة  
اعتبر مشكلة ماكينات ذات أذرع متعددة $k$ حيث $k=4$ أفعال، والتي يُرمز لها بـ 1، 2، 3، و4. افترض تطبيق خوارزمية ماكينات ذات أذرع متعددة باستخدام اختيار الأفعال بطريقة $\varepsilon$-greedy، وتقديرات قيمة الأفعال باستخدام المتوسط الحسابي للعينات، وتقديرات أولية لـ $Q_1(a)=0$ لجميع $a$. لنفترض أن تسلسل الأفعال والمكافآت الأولي هو:

$$
A_1 = 1, \quad R_1 = -1 \\
A_2 = 2, \quad R_2 = 1 \\
A_3 = 2, \quad R_3 = -2 \\
A_4 = 2, \quad R_4 = 2 \\
A_5 = 3, \quad R_5 = 0
$$

في بعض الأحيان، عند حساب تقديرات القيمة التالية بعد الخطوة 5، قد نحتاج إلى حساب المتوسط الحسابي للعينات، اشرح كيف يتم حساب $Q_6(2)$ و$Q_6(1)$.

---

## 2.4 التحديث التكراري للقيم (Incremental Implementation)

حساب المتوسط الحسابي للعينات في الصيغة (2.1) يمكن أن يكون مكلفًا حسابيًا إذا كان هناك عدد كبير من الخطوات السابقة، لأننا نحتاج إلى جمع جميع المكافآت السابقة في كل مرة نقوم بتحديث تقدير قيمة الإجراء. هناك صيغة تكرارية يمكن استخدامها لحساب المتوسط الحسابي دون الحاجة إلى الاحتفاظ بكل المكافآت السابقة، وتُعرف بالتحديث التكراري:

$$
Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n] \quad\quad (2.3)
$$

حيث $Q_n$ هو المتوسط بعد $n-1$ عينة، و$R_n$ هي العينة الجديدة. الصيغة تعبر عن أن المتوسط الجديد هو المتوسط القديم زائد تعديل صغير يعتمد على الفرق بين العينة الجديدة والمتوسط القديم، مقسوم على عدد العينات.

---

## 2.5 طرق الخطوة الثابتة (Constant-step-size Methods)

في حالة البيئات غير الثابتة أو المتغيرة، لا يكون المتوسط الحسابي للعينات هو الطريقة الأفضل لأنه يعطي نفس الوزن لكل مكافأة تم استلامها، مما يجعل تقدير القيمة بطيئًا في التكيف مع التغيرات. بدلاً من ذلك، يمكن استخدام تحديثات ذات خطوة ثابتة:

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n] \quad\quad (2.4)
$$

حيث $\alpha$ هو معدل التعلم (learning rate) أو حجم الخطوة (step size) بين 0 و1. هذا النوع من التحديثات يعطي وزنًا أكبر للمكافآت الحديثة، مما يسمح بتكيف أسرع مع التغيرات في القيم الحقيقية للأفعال.

---

## 2.6 طرق التحديث التكرارية والذاكرة المحدودة

يُعد استخدام معدل خطوة ثابت مفيدًا خصوصًا في البيئات غير الثابتة، حيث يمكن للوكيل التكيف بسرعة مع التغيرات في القيم الحقيقية. من ناحية أخرى، في البيئات الثابتة، يكون استخدام المتوسط الحسابي للعينات مناسبًا لأنه يتقارب إلى القيمة الحقيقية.

---

## 2.7 الخلاصة

في هذا الفصل، تعرفنا على المشكلة الأساسية للماكينات ذات الأذرع المتعددة كمقدمة لتحديات التعليم المعزز، وخاصة الصراع بين الاستكشاف والاستغلال. ناقشنا طرقًا بسيطة لتقدير قيم الأفعال واختيار الأفعال باستخدام الطرق الجشعة و$\varepsilon$-greedy، وبيّنا أهمية الاستكشاف لتحقيق أداء جيد في المدى الطويل. كذلك استعرضنا طرق التحديث التكراري والقيمة الثابتة لتقديرات قيم الأفعال.

---

**ملاحظة:** يمكنني كتابة فصول أخرى أو تفصيل أكثر حسب طلبك.
# الفصل الثاني:
## Multi-armed Bandits
### الماكينات ذات الأذرع المتعددة

أكثر ميزات التعليم المعزز تميزًا عن أنواع التعليم الأخرى هي أنه يستخدم معلومات التدريب التي تقيم الأفعال المتخذة بدلاً من توجيهها عبر تقديم الأفعال الصحيحة. هذا ما يخلق الحاجة للاستكشاف النشط، أي البحث الصريح عن سلوك جيد. تشير التغذية الراجعة التقييمية (Evaluative Feedback) البحتة إلى مدى جودة الفعل المتخذ، ولكنها لا تحدد ما إذا كان الفعل هو الأفضل أو الأسوأ الممكن. من ناحية أخرى، تشير التغذية الراجعة التوجيهية (Instructive Feedback) إلى الفعل الصحيح الذي يجب اتخاذه، بغض النظر عن الفعل المتخذ بالفعل. هذا النوع من التغذية الراجعة هو أساس التعليم الخاضع للإشراف، الذي يشمل أجزاء كبيرة من تصنيف الأنماط، والشبكات العصبية الاصطناعية، وتحديد الأنظمة. في أشكالها النقية، تكون هاتان النوعيتان من التغذية الراجعة مميزتين تمامًا: التغذية الراجعة التقييمية (Evaluative Feedback) تعتمد بالكامل على الفعل المتخذ، بينما التغذية الراجعة التوجيهية (Instructive Feedback) مستقلة عن الفعل المتخذ.

في هذا الفصل، ندرس الجانب التقييمي للتعليم المعزز في إطار مبسط، لا يتضمن تعلم التصرف في أكثر من حالة واحدة. هذا الإطار غير الترابطي هو الإطار الذي تم فيه إنجاز معظم الأعمال السابقة التي تتضمن التغذية الراجعة التقييمية (Evaluative Feedback)، وهو يتجنب الكثير من تعقيد مشكلة التعليم المعزز الكامل. دراسة هذه الحالة تمكننا من رؤية الفرق بين التغذية الراجعة التقييمية (Evaluative Feedback) والتغذية الراجعة التوجيهية (Instructive Feedback) بوضوح، وكيف يمكن دمجهما.

المشكلة غير الترابطية، التقييمية التي نستكشفها هي نسخة بسيطة من مشكلة الماكينة ذات الأذرع المتعددة. نستخدم هذه المشكلة لتقديم عدد من الأساليب الأساسية للتعليم التي نوسعها في الفصول اللاحقة لتطبيقها على مشكلة التعليم المعزز الكاملة. في نهاية هذا الفصل، نقترب خطوة من مشكلة التعليم المعزز الكامل من خلال مناقشة ما يحدث عندما تصبح مشكلة الماكينة ذات الأذرع المتعددة ترابطية، أي عندما تُتخذ الأفعال في أكثر من حالة واحدة.

---

## 2.1 مشكلة الماكينة ذات الأذرع المتعددة (k-armed Bandit Problem)

فكر بعناية في مشكلة التعليم التالية. تواجه باستمرار خيارًا بين عدة خيارات مختلفة $k$ أو إجراءات مختلفة. بعد كل اختيار تتلقى مكافأة رقمية يتم اختيارها من توزيع احتمالي ثابت يعتمد على الإجراء الذي قمت باختياره. هدفك هو تعظيم إجمالي المكافأة المتوقعة على مدار فترة زمنية معينة، على سبيل المثال، على مدار 1000 اختيار للإجراءات أو خطوات زمنية.

هذه هي الصيغة الأصلية لمشكلة الماكينات ذات الأذرع المتعددة، والتي سُميت بهذا الاسم بالتشابه مع آلة القمار ذات الذراع الواحد، باستثناء أنها تحتوي على $k$ أذرع بدلاً من واحدة. كل اختيار للعمل يشبه سحب ذراع من أذرع آلة القمار، والعوائد هي الأرباح التي يتم الحصول عليها عند ضرب الجائزة الكبرى. من خلال تكرار اختيار الأذرع، هدفك هو زيادة مكاسبك عن طريق التركيز على الأذرع الأفضل. تشبيه آخر هو أن الطبيب يختار بين علاجات تجريبية لمجموعة من المرضى بشكل خطير. كل إجراء هو اختيار علاج، وكل عائد هو بقاء أو صحة المريض. اليوم، يُستخدم مصطلح "مشكلة الماكينات ذات الأذرع المتعددة" أحياناً للإشارة إلى تعميم للمشكلة الموصوفة أعلاه، ولكن في هذا الكتاب نستخدمه للإشارة فقط إلى هذه الحالة البسيطة.

في مشكلة الماكينات ذات الأذرع المتعددة، كل واحد من الأذرع $k$ له عائد متوقع أو متوسط يُعطى عند اختيار هذا الذراع؛ دعنا نطلق على هذا العائد قيمة هذا الذراع. نُسمي الذراع الذي يُختار في الزمن $t$ بـ $A_t$، والعائد المقابل له بـ $R_t$. إذاً، قيمة أي ذراع عشوائي $a$، التي نُشير إليها بـ $q^*(a)$، هي العائد المتوقع عند اختيار $a$:

$$
q^*(a) = \mathrm{E}[R_t \mid A_t = a]
$$

إذا كنت تعرف قيمة كل إجراء، فستكون مشكلة الماكينات ذات الأذرع المتعددة سهلة الحل: ستقوم دائماً باختيار الإجراء الذي له أعلى قيمة. نحن نفترض أنك لا تعرف قيم الأفعال بيقين، رغم أنك قد تكون لديك تقديرات. نُسمي تقدير قيمة الإجراء $a$ في الزمن $t$ بـ $Q_t(a)$، ونرغب في أن يكون $Q_t(a)$ قريباً من $q^*(a)$.

إذا كنت تحتفظ بتقديرات لقيم الأفعال، فإن في أي وقت سيكون هناك على الأقل إجراء واحد له أعلى قيمة تقديرية. نُطلق على هذه الأفعال اسم الأفعال الجشعة (greedy actions). عندما تختار واحداً من هذه الأفعال، نقول إنك تستغل (exploiting) معرفتك الحالية بقيم الأفعال. إذا كنت تختار بدلاً من ذلك واحداً من الأفعال غير الجشعة (nongreedy actions)، فنحن نقول إنك تستكشف (exploring)، لأن هذا يتيح لك تحسين تقديرك لقيمة الإجراء غير الجشع.

الاستغلال (exploitation) هو الشيء الصحيح لزيادة العائد المتوقع في الخطوة الواحدة، ولكن الاستكشاف (exploration) قد يؤدي إلى تحقيق عائد إجمالي أكبر على المدى الطويل.

على سبيل المثال، افترض أن قيمة الإجراء الجشع معروفة بيقين، بينما يُقدَّر أن عدة أفعال أخرى قريبة من حيث القيمة ولكن مع عدم يقين كبير. عدم اليقين هو أن أحد هذه الأفعال الأخرى ربما يكون في الواقع أفضل من الإجراء الجشع، لكنك لا تعرف أي منها. إذا كان لديك العديد من الخطوات الزمنية القادمة لاختيار الأفعال، فقد يكون من الأفضل استكشاف الأفعال غير الجشعة واكتشاف أي منها أفضل من الإجراء الجشع. العائد أقل على المدى القصير خلال الاستكشاف، ولكنه أعلى على المدى الطويل لأنك بعد اكتشاف الأفعال الأفضل يمكنك استغلالها عدة مرات. بما أنه ليس من الممكن استكشاف واستغلال الإجراء في نفس الوقت، فإننا غالباً ما نستخدم مصطلح الصراع (conflict) بين الاستكشاف والاستغلال.

في أي حالة معينة، سواء كان من الأفضل الاستكشاف أو الاستغلال يعتمد بشكل معقد على القيم الدقيقة للتقديرات، وعدم اليقين، وعدد الخطوات المتبقية. هناك العديد من الأساليب المتطورة لتحقيق توازن بين الاستكشاف والاستغلال في صياغات رياضية محددة لمشكلة الماكينات ذات الأذرع المتعددة والمشاكل ذات الصلة. ومع ذلك، فإن معظم هذه الأساليب تقوم على افتراضات قوية بشأن الثبات والمعرفة السابقة التي إما تُخالف أو يكون من المستحيل التحقق منها في التطبيقات وفي مشكلة التعليم المعزز الكامل التي سنناقشها في الفصول التالية. ضمانات الأمثلية أو الخسارة المحدودة لهذه الأساليب لا تقدم راحة كبيرة عندما لا تنطبق افتراضات نظريتها.

في هذا الكتاب، لا نركز على تحقيق توازن بين الاستكشاف والاستغلال بطريقة متطورة؛ نحن نهتم فقط بتحقيق التوازن بينهما بشكل عام. في هذا الفصل، نقدم عدة طرق بسيطة لتحقيق التوازن لمشكلة الماكينات ذات الأذرع المتعددة ونوضح أنها تعمل بشكل أفضل بكثير من الأساليب التي تعتمد دائماً على الاستغلال. الحاجة إلى تحقيق توازن بين الاستكشاف والاستغلال هي تحدٍ مميز يظهر في التعليم المعزز؛ وبساطة نسخة المشكلة لدينا من الماكينات ذات الأذرع المتعددة تمكننا من عرض هذا التحدي بشكل واضح جداً.

---

## 2.2 طرق تقييم الأفعال (Action-value Methods)

نبدأ بالنظر عن كثب إلى الطرق المستخدمة لتقدير قيم الأفعال واستخدام التقديرات لاتخاذ قرارات اختيار الأفعال، والتي نطلق عليها مجتمعةً اسم طرق تقييم الأفعال (action-value methods). تذكر أن القيمة الحقيقية للإجراء هي العائد المتوسط عند اختيار ذلك الإجراء. إحدى الطرق الطبيعية لتقدير ذلك هي عن طريق حساب متوسط العوائد التي تم الحصول عليها بالفعل:

$$
Q_t(a) = \frac{\text{Sum of Rewards when } a \text{ taken prior to } t}{\text{Number of times } a \text{ taken prior to } t} = \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}} \quad\quad (2.1)
$$

حيث يشير $\mathbb{1}$ إلى المتغير العشوائي الذي يكون 1 إذا كان الشرط صحيحًا و0 إذا لم يكن كذلك. إذا كان المقام صفرًا، فإننا نعرّف $Q_t(a)$ بدلاً من ذلك على أنه قيمة افتراضية، مثل 0. عندما يذهب المقام نحو اللانهاية، وفقاً لقانون الأعداد الكبيرة، يتقارب $Q_t(a)$ إلى $q^*(a)$. نُطلق على هذه الطريقة اسم طريقة المتوسط التجريبي (sample-average method) لتقدير قيم الأفعال لأن كل تقدير هو متوسط العوائد ذات الصلة. بالطبع، هذه مجرد طريقة واحدة لتقدير قيم الأفعال، وليست بالضرورة الأفضل. ومع ذلك، دعنا نلتزم الآن بهذه الطريقة البسيطة للتقدير وننتقل إلى مسألة كيفية استخدام التقديرات لاختيار الأفعال.

أبسط قاعدة لاختيار الأفعال هي اختيار واحد من الأفعال التي لها أعلى قيمة تقديرية، أي واحد من الأفعال الجشعة كما هو محدد في القسم السابق. إذا كان هناك أكثر من إجراء جشع واحد، يتم اختيار واحد منها بطريقة عشوائية أو بأي طريقة أخرى. نكتب هذه الطريقة لاختيار الأفعال الجشعة كما يلي:

$$
A_t = \arg\max_a Q_t(a) \quad\quad (2.2)
$$

حيث يشير $\arg\max_a$ إلى الإجراء $a$ الذي يتم تعظيم التعبير الذي يليه (مرة أخرى، مع كسر الروابط بطريقة عشوائية). اختيار الأفعال الجشعة (greedy action selection) يستغل دائمًا المعرفة الحالية لتعظيم العائد الفوري؛ ولا يقضي أي وقت في تجربة الأفعال التي تبدو أقل فعالية لمعرفة ما إذا كانت قد تكون أفضل فعلاً. بديل بسيط هو التصرف بطريقة جشعة معظم الوقت، ولكن بين الحين والآخر، مثلاً باحتمالية صغيرة $\varepsilon$، اختيار عشوائي من بين جميع الأفعال مع احتمال متساوٍ، بشكل مستقل عن تقديرات قيم الأفعال. نُطلق على الطرق التي تستخدم قاعدة اختيار الأفعال القريبة من الجشع هذه اسم طرق-الجشع ($\varepsilon$-greedy methods). ميزة هذه الطرق هي أنه، في الحد الذي تزداد فيه عدد الخطوات، سيتم تجربة كل إجراء عددًا لانهائيًا من المرات، مما يضمن أن جميع $Q_t(a)$ تتقارب إلى $q^*(a)$ وهذا بالطبع يعني أن احتمالية اختيار الإجراء الأمثل تتقارب إلى أكثر من $1-\varepsilon$، أي تقريباً إلى يقين كبير. ومع ذلك، فإن هذه مجرد ضمانات أفقية، ولا تعطي الكثير عن فعالية الطرق في الممارسة العملية.

**التمرين 2.1:** في اختيار الأفعال القريبة من الجشع ($\varepsilon$-greedy action selection)، في حالة وجود إجراءين و$\varepsilon = 0.5$، ما هي احتمالية اختيار الإجراء الجشع؟

---

## 2.3 اختبار الماكينات ذات الأذرع العشر (The 10-armed Testbed)

لتقييم فعالية طرق تقييم الأفعال greedy و$\varepsilon$-greedy بشكل تقريبي، قمنا بمقارنتهما رقميًا على مجموعة من مشاكل الاختبار. كانت هذه مجموعة من 2000 مشكلة ماكينات ذات أذرع متعددة تم توليدها عشوائيًا، حيث $k=10$. لكل مشكلة ماكينات ذات أذرع متعددة، مثل تلك المعروضة في الشكل 2.1، كانت قيم الأفعال،

$$
q^*(a), \quad a=1,\ldots,10
$$

---

**الشكل 2.1:** مثال على مشكلة الماكينات ذات الأذرع المتعددة من مجموعة الاختبار ذات العشر أذرع. تم اختيار القيمة الحقيقية $q^*(a)$ لكل من الأفعال العشرة وفقًا لتوزيع طبيعي بمتوسط صفر وتباين وحدة، ثم تم اختيار المكافآت الفعلية وفقًا لتوزيع طبيعي بمتوسط $q^*(a)$ وتباين وحدة، كما تقترح هذه التوزيعات الرمادية.

تم اختيار القيم وفقًا لتوزيع طبيعي (غوسي) بمتوسط 0 وتباين 1. ثم، عندما طبق طريقة تعلم على هذه المشكلة واختارت الإجراء $A_t$ في الخطوة الزمنية $t$، تم اختيار المكافأة الفعلية $R_t$ من توزيع طبيعي بمتوسط $q^*(A_t)$ وتباين 1. تظهر هذه التوزيعات باللون الرمادي في الشكل 2.1. نسمي مجموعة مهام الاختبار هذه مجموعة الاختبار ذات العشر أذرع. بالنسبة لأي طريقة تعلم، يمكننا قياس أدائها وسلوكها مع تحسنها مع الخبرة على مدى 1000 خطوة زمنية عند تطبيقها على إحدى مشاكل الماكينات ذات الأذرع المتعددة. هذه تشكل تجربة واحدة. بتكرار ذلك على 2000 تجربة مستقلة، كل منها مع مشكلة ماكينات ذات أذرع متعددة مختلفة، حصلنا على مقاييس للسلوك المتوسط لخوارزمية التعليم.

---

**الشكل 2.2** يقارن بين طريقة greedy وطريقتين $\varepsilon$-greedy ($\varepsilon=0.01$ و$\varepsilon=0.1$)، كما هو موضح أعلاه، على مجموعة الاختبار ذات العشر أذرع. جميع الطرق شكلت تقديراتها لقيم الأفعال باستخدام تقنية المتوسط الحسابي للعينات. الرسم البياني العلوي يظهر الزيادة في المكافأة المتوقعة مع الخبرة. تحسنت طريقة greedy بشكل طفيف أسرع من الطرق الأخرى في البداية، لكنها استقرت عند مستوى أدنى. حققت مكافأة لكل خطوة تبلغ حوالي 1 فقط، مقارنةً بأفضل مكافأة ممكنة والتي تبلغ حوالي 1.55 في هذه المجموعة. كانت طريقة greedy أسوأ بشكل ملحوظ على المدى الطويل.

---

**الشكل 2.2:** الأداء المتوسط لطرق تقدير قيم الأفعال $\varepsilon$-greedy على مجموعة الاختبار ذات العشر أذرع. هذه البيانات هي متوسطات على مدى 2000 تجربة مع مشاكل ماكينات ذات أذرع متعددة مختلفة. جميع الطرق استخدمت المتوسطات الحسابية للعينات كتقديرات لقيم الأفعال.

غالبًا ما كانت طريقة greedy تعلق في أداء أفعال دون المستوى الأمثل. يظهر الرسم البياني السفلي أن طريقة greedy وجدت الإجراء الأمثل في حوالي ثلث المهام فقط. في الثلثين الآخرين، كانت عيناتها الأولية من الإجراء الأمثل مخيبة للآمال، ولم تعُد إليه أبدًا. كانت طرق $\varepsilon$-greedy تؤدي في النهاية بشكل أفضل لأنها استمرت في الاستكشاف وتحسين فرصها في التعرف على الإجراء الأمثل. كانت طريقة $\varepsilon=0.1$ تستكشف أكثر، وعادة ما تجد الإجراء الأمثل في وقت أبكر، لكنها لم تختار ذلك الإجراء أكثر من 91% من الوقت. تحسنت طريقة $\varepsilon=0.01$ بشكل أبطأ، لكنها في النهاية كانت تؤدي بشكل أفضل من طريقة $\varepsilon=0.1$ على كلا مقياسي الأداء المعروضين في الشكل. من الممكن أيضًا تقليل $\varepsilon$ مع مرور الوقت لمحاولة الحصول على أفضل ما في القيم العالية والمنخفضة.

ميزة طرق $\varepsilon$-greedy على طرق greedy تعتمد على المهمة. على سبيل المثال، لنفترض أن تباين المكافآت كان أكبر، مثلاً 10 بدلاً من 1. مع المكافآت الأكثر ضوضاء، يتطلب الأمر المزيد من الاستكشاف للعثور على الإجراء الأمثل، ويجب أن تكون طرق $\varepsilon$-greedy أفضل نسبيًا مقارنةً بطريقة greedy. من ناحية أخرى، إذا كانت تباينات المكافآت صفرية، فإن طريقة greedy ستكون قادرة على معرفة القيمة الحقيقية لكل إجراء بعد تجربته مرة واحدة. في هذه الحالة، قد تكون طريقة greedy هي الأفضل لأنها ستجد الإجراء الأمثل بسرعة ثم لا تستكشف أبدًا. ولكن حتى في الحالة الحتمية، هناك ميزة كبيرة للاستكشاف إذا قمنا بتخفيف بعض الافتراضات الأخرى. على سبيل المثال، لنفترض أن مهمة الماكينات ذات الأذرع المتعددة كانت غير ثابتة، أي أن القيم الحقيقية للأفعال تتغير مع مرور الوقت. في هذه الحالة، يكون الاستكشاف ضروريًا حتى في الحالة الحتمية للتأكد من أن أحد الأفعال غير الجشعة لم يتغير ليصبح أفضل من الإجراء الجشع. كما سنرى في الفصول التالية، تعتبر عدم الثبات هي الحالة الأكثر شيوعًا التي يتم مواجهتها في التعليم المعزز. حتى إذا كانت المهمة الأساسية ثابتة وحتمية، يواجه المتعلم مجموعة من مهام اتخاذ القرار الشبيهة بالماكينات ذات الأذرع المتعددة، والتي تتغير كل منها مع مرور الوقت مع تقدم التعليم وتغير سياسة اتخاذ القرار لدى الوكيل. يتطلب التعليم المعزز توازنًا بين الاستكشاف والاستغلال.

---

**التمرين 2.2:** مثال على الماكينات ذات الأذرع المتعددة  
اعتبر مشكلة ماكينات ذات أذرع متعددة $k$ حيث $k=4$ أفعال، والتي يُرمز لها بـ 1، 2، 3، و4. افترض تطبيق خوارزمية ماكينات ذات أذرع متعددة باستخدام اختيار الأفعال بطريقة $\varepsilon$-greedy، وتقديرات قيمة الأفعال باستخدام المتوسط الحسابي للعينات، وتقديرات أولية لـ $Q_1(a)=0$ لجميع $a$. لنفترض أن تسلسل الأفعال والمكافآت الأولي هو:

$$
A_1 = 1, \quad R_1 = -1 \\
A_2 = 2, \quad R_2 = 1 \\
A_3 = 2, \quad R_3 = -2 \\
A_4 = 2, \quad R_4 = 2 \\
A_5 = 3, \quad R_5 = 0
$$

في بعض الأحيان، عند حساب تقديرات القيمة التالية بعد الخطوة 5، قد نحتاج إلى حساب المتوسط الحسابي للعينات، اشرح كيف يتم حساب $Q_6(2)$ و$Q_6(1)$.

---

## 2.4 التحديث التكراري للقيم (Incremental Implementation)

حساب المتوسط الحسابي للعينات في الصيغة (2.1) يمكن أن يكون مكلفًا حسابيًا إذا كان هناك عدد كبير من الخطوات السابقة، لأننا نحتاج إلى جمع جميع المكافآت السابقة في كل مرة نقوم بتحديث تقدير قيمة الإجراء. هناك صيغة تكرارية يمكن استخدامها لحساب المتوسط الحسابي دون الحاجة إلى الاحتفاظ بكل المكافآت السابقة، وتُعرف بالتحديث التكراري:

$$
Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n] \quad\quad (2.3)
$$

حيث $Q_n$ هو المتوسط بعد $n-1$ عينة، و$R_n$ هي العينة الجديدة. الصيغة تعبر عن أن المتوسط الجديد هو المتوسط القديم زائد تعديل صغير يعتمد على الفرق بين العينة الجديدة والمتوسط القديم، مقسوم على عدد العينات.

---

## 2.5 طرق الخطوة الثابتة (Constant-step-size Methods)

في حالة البيئات غير الثابتة أو المتغيرة، لا يكون المتوسط الحسابي للعينات هو الطريقة الأفضل لأنه يعطي نفس الوزن لكل مكافأة تم استلامها، مما يجعل تقدير القيمة بطيئًا في التكيف مع التغيرات. بدلاً من ذلك، يمكن استخدام تحديثات ذات خطوة ثابتة:

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n] \quad\quad (2.4)
$$

حيث $\alpha$ هو معدل التعلم (learning rate) أو حجم الخطوة (step size) بين 0 و1. هذا النوع من التحديثات يعطي وزنًا أكبر للمكافآت الحديثة، مما يسمح بتكيف أسرع مع التغيرات في القيم الحقيقية للأفعال.

---

## 2.6 طرق التحديث التكرارية والذاكرة المحدودة

يُعد استخدام معدل خطوة ثابت مفيدًا خصوصًا في البيئات غير الثابتة، حيث يمكن للوكيل التكيف بسرعة مع التغيرات في القيم الحقيقية. من ناحية أخرى، في البيئات الثابتة، يكون استخدام المتوسط الحسابي للعينات مناسبًا لأنه يتقارب إلى القيمة الحقيقية.

---

## 2.7 الخلاصة

في هذا الفصل، تعرفنا على المشكلة الأساسية للماكينات ذات الأذرع المتعددة كمقدمة لتحديات التعليم المعزز، وخاصة الصراع بين الاستكشاف والاستغلال. ناقشنا طرقًا بسيطة لتقدير قيم الأفعال واختيار الأفعال باستخدام الطرق الجشعة و$\varepsilon$-greedy، وبيّنا أهمية الاستكشاف لتحقيق أداء جيد في المدى الطويل. كذلك استعرضنا طرق التحديث التكراري والقيمة الثابتة لتقديرات قيم الأفعال.

---

**ملاحظة:** يمكنني كتابة فصول أخرى أو تفصيل أكثر حسب طلبك.
# الفصل الثاني:
## Multi-armed Bandits
### الماكينات ذات الأذرع المتعددة

أكثر ميزات التعليم المعزز تميزًا عن أنواع التعليم الأخرى هي أنه يستخدم معلومات التدريب التي تقيم الأفعال المتخذة بدلاً من توجيهها عبر تقديم الأفعال الصحيحة. هذا ما يخلق الحاجة للاستكشاف النشط، أي البحث الصريح عن سلوك جيد. تشير التغذية الراجعة التقييمية (Evaluative Feedback) البحتة إلى مدى جودة الفعل المتخذ، ولكنها لا تحدد ما إذا كان الفعل هو الأفضل أو الأسوأ الممكن. من ناحية أخرى، تشير التغذية الراجعة التوجيهية (Instructive Feedback) إلى الفعل الصحيح الذي يجب اتخاذه، بغض النظر عن الفعل المتخذ بالفعل. هذا النوع من التغذية الراجعة هو أساس التعليم الخاضع للإشراف، الذي يشمل أجزاء كبيرة من تصنيف الأنماط، والشبكات العصبية الاصطناعية، وتحديد الأنظمة. في أشكالها النقية، تكون هاتان النوعيتان من التغذية الراجعة مميزتين تمامًا: التغذية الراجعة التقييمية (Evaluative Feedback) تعتمد بالكامل على الفعل المتخذ، بينما التغذية الراجعة التوجيهية (Instructive Feedback) مستقلة عن الفعل المتخذ.

في هذا الفصل، ندرس الجانب التقييمي للتعليم المعزز في إطار مبسط، لا يتضمن تعلم التصرف في أكثر من حالة واحدة. هذا الإطار غير الترابطي هو الإطار الذي تم فيه إنجاز معظم الأعمال السابقة التي تتضمن التغذية الراجعة التقييمية (Evaluative Feedback)، وهو يتجنب الكثير من تعقيد مشكلة التعليم المعزز الكامل. دراسة هذه الحالة تمكننا من رؤية الفرق بين التغذية الراجعة التقييمية (Evaluative Feedback) والتغذية الراجعة التوجيهية (Instructive Feedback) بوضوح، وكيف يمكن دمجهما.

المشكلة غير الترابطية، التقييمية التي نستكشفها هي نسخة بسيطة من مشكلة الماكينة ذات الأذرع المتعددة. نستخدم هذه المشكلة لتقديم عدد من الأساليب الأساسية للتعليم التي نوسعها في الفصول اللاحقة لتطبيقها على مشكلة التعليم المعزز الكاملة. في نهاية هذا الفصل، نقترب خطوة من مشكلة التعليم المعزز الكامل من خلال مناقشة ما يحدث عندما تصبح مشكلة الماكينة ذات الأذرع المتعددة ترابطية، أي عندما تُتخذ الأفعال في أكثر من حالة واحدة.

---

## 2.1 مشكلة الماكينة ذات الأذرع المتعددة (k-armed Bandit Problem)

فكر بعناية في مشكلة التعليم التالية. تواجه باستمرار خيارًا بين عدة خيارات مختلفة $k$ أو إجراءات مختلفة. بعد كل اختيار تتلقى مكافأة رقمية يتم اختيارها من توزيع احتمالي ثابت يعتمد على الإجراء الذي قمت باختياره. هدفك هو تعظيم إجمالي المكافأة المتوقعة على مدار فترة زمنية معينة، على سبيل المثال، على مدار 1000 اختيار للإجراءات أو خطوات زمنية.

هذه هي الصيغة الأصلية لمشكلة الماكينات ذات الأذرع المتعددة، والتي سُميت بهذا الاسم بالتشابه مع آلة القمار ذات الذراع الواحد، باستثناء أنها تحتوي على $k$ أذرع بدلاً من واحدة. كل اختيار للعمل يشبه سحب ذراع من أذرع آلة القمار، والعوائد هي الأرباح التي يتم الحصول عليها عند ضرب الجائزة الكبرى. من خلال تكرار اختيار الأذرع، هدفك هو زيادة مكاسبك عن طريق التركيز على الأذرع الأفضل. تشبيه آخر هو أن الطبيب يختار بين علاجات تجريبية لمجموعة من المرضى بشكل خطير. كل إجراء هو اختيار علاج، وكل عائد هو بقاء أو صحة المريض. اليوم، يُستخدم مصطلح "مشكلة الماكينات ذات الأذرع المتعددة" أحياناً للإشارة إلى تعميم للمشكلة الموصوفة أعلاه، ولكن في هذا الكتاب نستخدمه للإشارة فقط إلى هذه الحالة البسيطة.

في مشكلة الماكينات ذات الأذرع المتعددة، كل واحد من الأذرع $k$ له عائد متوقع أو متوسط يُعطى عند اختيار هذا الذراع؛ دعنا نطلق على هذا العائد قيمة هذا الذراع. نُسمي الذراع الذي يُختار في الزمن $t$ بـ $A_t$، والعائد المقابل له بـ $R_t$. إذاً، قيمة أي ذراع عشوائي $a$، التي نُشير إليها بـ $q^*(a)$، هي العائد المتوقع عند اختيار $a$:

$$
q^*(a) = \mathrm{E}[R_t \mid A_t = a]
$$

إذا كنت تعرف قيمة كل إجراء، فستكون مشكلة الماكينات ذات الأذرع المتعددة سهلة الحل: ستقوم دائماً باختيار الإجراء الذي له أعلى قيمة. نحن نفترض أنك لا تعرف قيم الأفعال بيقين، رغم أنك قد تكون لديك تقديرات. نُسمي تقدير قيمة الإجراء $a$ في الزمن $t$ بـ $Q_t(a)$، ونرغب في أن يكون $Q_t(a)$ قريباً من $q^*(a)$.

إذا كنت تحتفظ بتقديرات لقيم الأفعال، فإن في أي وقت سيكون هناك على الأقل إجراء واحد له أعلى قيمة تقديرية. نُطلق على هذه الأفعال اسم الأفعال الجشعة (greedy actions). عندما تختار واحداً من هذه الأفعال، نقول إنك تستغل (exploiting) معرفتك الحالية بقيم الأفعال. إذا كنت تختار بدلاً من ذلك واحداً من الأفعال غير الجشعة (nongreedy actions)، فنحن نقول إنك تستكشف (exploring)، لأن هذا يتيح لك تحسين تقديرك لقيمة الإجراء غير الجشع.

الاستغلال (exploitation) هو الشيء الصحيح لزيادة العائد المتوقع في الخطوة الواحدة، ولكن الاستكشاف (exploration) قد يؤدي إلى تحقيق عائد إجمالي أكبر على المدى الطويل.

على سبيل المثال، افترض أن قيمة الإجراء الجشع معروفة بيقين، بينما يُقدَّر أن عدة أفعال أخرى قريبة من حيث القيمة ولكن مع عدم يقين كبير. عدم اليقين هو أن أحد هذه الأفعال الأخرى ربما يكون في الواقع أفضل من الإجراء الجشع، لكنك لا تعرف أي منها. إذا كان لديك العديد من الخطوات الزمنية القادمة لاختيار الأفعال، فقد يكون من الأفضل استكشاف الأفعال غير الجشعة واكتشاف أي منها أفضل من الإجراء الجشع. العائد أقل على المدى القصير خلال الاستكشاف، ولكنه أعلى على المدى الطويل لأنك بعد اكتشاف الأفعال الأفضل يمكنك استغلالها عدة مرات. بما أنه ليس من الممكن استكشاف واستغلال الإجراء في نفس الوقت، فإننا غالباً ما نستخدم مصطلح الصراع (conflict) بين الاستكشاف والاستغلال.

في أي حالة معينة، سواء كان من الأفضل الاستكشاف أو الاستغلال يعتمد بشكل معقد على القيم الدقيقة للتقديرات، وعدم اليقين، وعدد الخطوات المتبقية. هناك العديد من الأساليب المتطورة لتحقيق توازن بين الاستكشاف والاستغلال في صياغات رياضية محددة لمشكلة الماكينات ذات الأذرع المتعددة والمشاكل ذات الصلة. ومع ذلك، فإن معظم هذه الأساليب تقوم على افتراضات قوية بشأن الثبات والمعرفة السابقة التي إما تُخالف أو يكون من المستحيل التحقق منها في التطبيقات وفي مشكلة التعليم المعزز الكامل التي سنناقشها في الفصول التالية. ضمانات الأمثلية أو الخسارة المحدودة لهذه الأساليب لا تقدم راحة كبيرة عندما لا تنطبق افتراضات نظريتها.

في هذا الكتاب، لا نركز على تحقيق توازن بين الاستكشاف والاستغلال بطريقة متطورة؛ نحن نهتم فقط بتحقيق التوازن بينهما بشكل عام. في هذا الفصل، نقدم عدة طرق بسيطة لتحقيق التوازن لمشكلة الماكينات ذات الأذرع المتعددة ونوضح أنها تعمل بشكل أفضل بكثير من الأساليب التي تعتمد دائماً على الاستغلال. الحاجة إلى تحقيق توازن بين الاستكشاف والاستغلال هي تحدٍ مميز يظهر في التعليم المعزز؛ وبساطة نسخة المشكلة لدينا من الماكينات ذات الأذرع المتعددة تمكننا من عرض هذا التحدي بشكل واضح جداً.

---

## 2.2 طرق تقييم الأفعال (Action-value Methods)

نبدأ بالنظر عن كثب إلى الطرق المستخدمة لتقدير قيم الأفعال واستخدام التقديرات لاتخاذ قرارات اختيار الأفعال، والتي نطلق عليها مجتمعةً اسم طرق تقييم الأفعال (action-value methods). تذكر أن القيمة الحقيقية للإجراء هي العائد المتوسط عند اختيار ذلك الإجراء. إحدى الطرق الطبيعية لتقدير ذلك هي عن طريق حساب متوسط العوائد التي تم الحصول عليها بالفعل:

$$
Q_t(a) = \frac{\text{Sum of Rewards when } a \text{ taken prior to } t}{\text{Number of times } a \text{ taken prior to } t} = \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}} \quad\quad (2.1)
$$

حيث يشير $\mathbb{1}$ إلى المتغير العشوائي الذي يكون 1 إذا كان الشرط صحيحًا و0 إذا لم يكن كذلك. إذا كان المقام صفرًا، فإننا نعرّف $Q_t(a)$ بدلاً من ذلك على أنه قيمة افتراضية، مثل 0. عندما يذهب المقام نحو اللانهاية، وفقاً لقانون الأعداد الكبيرة، يتقارب $Q_t(a)$ إلى $q^*(a)$. نُطلق على هذه الطريقة اسم طريقة المتوسط التجريبي (sample-average method) لتقدير قيم الأفعال لأن كل تقدير هو متوسط العوائد ذات الصلة. بالطبع، هذه مجرد طريقة واحدة لتقدير قيم الأفعال، وليست بالضرورة الأفضل. ومع ذلك، دعنا نلتزم الآن بهذه الطريقة البسيطة للتقدير وننتقل إلى مسألة كيفية استخدام التقديرات لاختيار الأفعال.

أبسط قاعدة لاختيار الأفعال هي اختيار واحد من الأفعال التي لها أعلى قيمة تقديرية، أي واحد من الأفعال الجشعة كما هو محدد في القسم السابق. إذا كان هناك أكثر من إجراء جشع واحد، يتم اختيار واحد منها بطريقة عشوائية أو بأي طريقة أخرى. نكتب هذه الطريقة لاختيار الأفعال الجشعة كما يلي:

$$
A_t = \arg\max_a Q_t(a) \quad\quad (2.2)
$$

حيث يشير $\arg\max_a$ إلى الإجراء $a$ الذي يتم تعظيم التعبير الذي يليه (مرة أخرى، مع كسر الروابط بطريقة عشوائية). اختيار الأفعال الجشعة (greedy action selection) يستغل دائمًا المعرفة الحالية لتعظيم العائد الفوري؛ ولا يقضي أي وقت في تجربة الأفعال التي تبدو أقل فعالية لمعرفة ما إذا كانت قد تكون أفضل فعلاً. بديل بسيط هو التصرف بطريقة جشعة معظم الوقت، ولكن بين الحين والآخر، مثلاً باحتمالية صغيرة $\varepsilon$، اختيار عشوائي من بين جميع الأفعال مع احتمال متساوٍ، بشكل مستقل عن تقديرات قيم الأفعال. نُطلق على الطرق التي تستخدم قاعدة اختيار الأفعال القريبة من الجشع هذه اسم طرق-الجشع ($\varepsilon$-greedy methods). ميزة هذه الطرق هي أنه، في الحد الذي تزداد فيه عدد الخطوات، سيتم تجربة كل إجراء عددًا لانهائيًا من المرات، مما يضمن أن جميع $Q_t(a)$ تتقارب إلى $q^*(a)$ وهذا بالطبع يعني أن احتمالية اختيار الإجراء الأمثل تتقارب إلى أكثر من $1-\varepsilon$، أي تقريباً إلى يقين كبير. ومع ذلك، فإن هذه مجرد ضمانات أفقية، ولا تعطي الكثير عن فعالية الطرق في الممارسة العملية.

**التمرين 2.1:** في اختيار الأفعال القريبة من الجشع ($\varepsilon$-greedy action selection)، في حالة وجود إجراءين و$\varepsilon = 0.5$، ما هي احتمالية اختيار الإجراء الجشع؟

---

## 2.3 اختبار الماكينات ذات الأذرع العشر (The 10-armed Testbed)

لتقييم فعالية طرق تقييم الأفعال greedy و$\varepsilon$-greedy بشكل تقريبي، قمنا بمقارنتهما رقميًا على مجموعة من مشاكل الاختبار. كانت هذه مجموعة من 2000 مشكلة ماكينات ذات أذرع متعددة تم توليدها عشوائيًا، حيث $k=10$. لكل مشكلة ماكينات ذات أذرع متعددة، مثل تلك المعروضة في الشكل 2.1، كانت قيم الأفعال،

$$
q^*(a), \quad a=1,\ldots,10
$$

---

**الشكل 2.1:** مثال على مشكلة الماكينات ذات الأذرع المتعددة من مجموعة الاختبار ذات العشر أذرع. تم اختيار القيمة الحقيقية $q^*(a)$ لكل من الأفعال العشرة وفقًا لتوزيع طبيعي بمتوسط صفر وتباين وحدة، ثم تم اختيار المكافآت الفعلية وفقًا لتوزيع طبيعي بمتوسط $q^*(a)$ وتباين وحدة، كما تقترح هذه التوزيعات الرمادية.

تم اختيار القيم وفقًا لتوزيع طبيعي (غوسي) بمتوسط 0 وتباين 1. ثم، عندما طبق طريقة تعلم على هذه المشكلة واختارت الإجراء $A_t$ في الخطوة الزمنية $t$، تم اختيار المكافأة الفعلية $R_t$ من توزيع طبيعي بمتوسط $q^*(A_t)$ وتباين 1. تظهر هذه التوزيعات باللون الرمادي في الشكل 2.1. نسمي مجموعة مهام الاختبار هذه مجموعة الاختبار ذات العشر أذرع. بالنسبة لأي طريقة تعلم، يمكننا قياس أدائها وسلوكها مع تحسنها مع الخبرة على مدى 1000 خطوة زمنية عند تطبيقها على إحدى مشاكل الماكينات ذات الأذرع المتعددة. هذه تشكل تجربة واحدة. بتكرار ذلك على 2000 تجربة مستقلة، كل منها مع مشكلة ماكينات ذات أذرع متعددة مختلفة، حصلنا على مقاييس للسلوك المتوسط لخوارزمية التعليم.

---

**الشكل 2.2** يقارن بين طريقة greedy وطريقتين $\varepsilon$-greedy ($\varepsilon=0.01$ و$\varepsilon=0.1$)، كما هو موضح أعلاه، على مجموعة الاختبار ذات العشر أذرع. جميع الطرق شكلت تقديراتها لقيم الأفعال باستخدام تقنية المتوسط الحسابي للعينات. الرسم البياني العلوي يظهر الزيادة في المكافأة المتوقعة مع الخبرة. تحسنت طريقة greedy بشكل طفيف أسرع من الطرق الأخرى في البداية، لكنها استقرت عند مستوى أدنى. حققت مكافأة لكل خطوة تبلغ حوالي 1 فقط، مقارنةً بأفضل مكافأة ممكنة والتي تبلغ حوالي 1.55 في هذه المجموعة. كانت طريقة greedy أسوأ بشكل ملحوظ على المدى الطويل.

---

**الشكل 2.2:** الأداء المتوسط لطرق تقدير قيم الأفعال $\varepsilon$-greedy على مجموعة الاختبار ذات العشر أذرع. هذه البيانات هي متوسطات على مدى 2000 تجربة مع مشاكل ماكينات ذات أذرع متعددة مختلفة. جميع الطرق استخدمت المتوسطات الحسابية للعينات كتقديرات لقيم الأفعال.

غالبًا ما كانت طريقة greedy تعلق في أداء أفعال دون المستوى الأمثل. يظهر الرسم البياني السفلي أن طريقة greedy وجدت الإجراء الأمثل في حوالي ثلث المهام فقط. في الثلثين الآخرين، كانت عيناتها الأولية من الإجراء الأمثل مخيبة للآمال، ولم تعُد إليه أبدًا. كانت طرق $\varepsilon$-greedy تؤدي في النهاية بشكل أفضل لأنها استمرت في الاستكشاف وتحسين فرصها في التعرف على الإجراء الأمثل. كانت طريقة $\varepsilon=0.1$ تستكشف أكثر، وعادة ما تجد الإجراء الأمثل في وقت أبكر، لكنها لم تختار ذلك الإجراء أكثر من 91% من الوقت. تحسنت طريقة $\varepsilon=0.01$ بشكل أبطأ، لكنها في النهاية كانت تؤدي بشكل أفضل من طريقة $\varepsilon=0.1$ على كلا مقياسي الأداء المعروضين في الشكل. من الممكن أيضًا تقليل $\varepsilon$ مع مرور الوقت لمحاولة الحصول على أفضل ما في القيم العالية والمنخفضة.

ميزة طرق $\varepsilon$-greedy على طرق greedy تعتمد على المهمة. على سبيل المثال، لنفترض أن تباين المكافآت كان أكبر، مثلاً 10 بدلاً من 1. مع المكافآت الأكثر ضوضاء، يتطلب الأمر المزيد من الاستكشاف للعثور على الإجراء الأمثل، ويجب أن تكون طرق $\varepsilon$-greedy أفضل نسبيًا مقارنةً بطريقة greedy. من ناحية أخرى، إذا كانت تباينات المكافآت صفرية، فإن طريقة greedy ستكون قادرة على معرفة القيمة الحقيقية لكل إجراء بعد تجربته مرة واحدة. في هذه الحالة، قد تكون طريقة greedy هي الأفضل لأنها ستجد الإجراء الأمثل بسرعة ثم لا تستكشف أبدًا. ولكن حتى في الحالة الحتمية، هناك ميزة كبيرة للاستكشاف إذا قمنا بتخفيف بعض الافتراضات الأخرى. على سبيل المثال، لنفترض أن مهمة الماكينات ذات الأذرع المتعددة كانت غير ثابتة، أي أن القيم الحقيقية للأفعال تتغير مع مرور الوقت. في هذه الحالة، يكون الاستكشاف ضروريًا حتى في الحالة الحتمية للتأكد من أن أحد الأفعال غير الجشعة لم يتغير ليصبح أفضل من الإجراء الجشع. كما سنرى في الفصول التالية، تعتبر عدم الثبات هي الحالة الأكثر شيوعًا التي يتم مواجهتها في التعليم المعزز. حتى إذا كانت المهمة الأساسية ثابتة وحتمية، يواجه المتعلم مجموعة من مهام اتخاذ القرار الشبيهة بالماكينات ذات الأذرع المتعددة، والتي تتغير كل منها مع مرور الوقت مع تقدم التعليم وتغير سياسة اتخاذ القرار لدى الوكيل. يتطلب التعليم المعزز توازنًا بين الاستكشاف والاستغلال.

---

**التمرين 2.2:** مثال على الماكينات ذات الأذرع المتعددة  
اعتبر مشكلة ماكينات ذات أذرع متعددة $k$ حيث $k=4$ أفعال، والتي يُرمز لها بـ 1، 2، 3، و4. افترض تطبيق خوارزمية ماكينات ذات أذرع متعددة باستخدام اختيار الأفعال بطريقة $\varepsilon$-greedy، وتقديرات قيمة الأفعال باستخدام المتوسط الحسابي للعينات، وتقديرات أولية لـ $Q_1(a)=0$ لجميع $a$. لنفترض أن تسلسل الأفعال والمكافآت الأولي هو:

$$
A_1 = 1, \quad R_1 = -1 \\
A_2 = 2, \quad R_2 = 1 \\
A_3 = 2, \quad R_3 = -2 \\
A_4 = 2, \quad R_4 = 2 \\
A_5 = 3, \quad R_5 = 0
$$

في بعض الأحيان، عند حساب تقديرات القيمة التالية بعد الخطوة 5، قد نحتاج إلى حساب المتوسط الحسابي للعينات، اشرح كيف يتم حساب $Q_6(2)$ و$Q_6(1)$.

---

## 2.4 التحديث التكراري للقيم (Incremental Implementation)

حساب المتوسط الحسابي للعينات في الصيغة (2.1) يمكن أن يكون مكلفًا حسابيًا إذا كان هناك عدد كبير من الخطوات السابقة، لأننا نحتاج إلى جمع جميع المكافآت السابقة في كل مرة نقوم بتحديث تقدير قيمة الإجراء. هناك صيغة تكرارية يمكن استخدامها لحساب المتوسط الحسابي دون الحاجة إلى الاحتفاظ بكل المكافآت السابقة، وتُعرف بالتحديث التكراري:

$$
Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n] \quad\quad (2.3)
$$

حيث $Q_n$ هو المتوسط بعد $n-1$ عينة، و$R_n$ هي العينة الجديدة. الصيغة تعبر عن أن المتوسط الجديد هو المتوسط القديم زائد تعديل صغير يعتمد على الفرق بين العينة الجديدة والمتوسط القديم، مقسوم على عدد العينات.

---

## 2.5 طرق الخطوة الثابتة (Constant-step-size Methods)

في حالة البيئات غير الثابتة أو المتغيرة، لا يكون المتوسط الحسابي للعينات هو الطريقة الأفضل لأنه يعطي نفس الوزن لكل مكافأة تم استلامها، مما يجعل تقدير القيمة بطيئًا في التكيف مع التغيرات. بدلاً من ذلك، يمكن استخدام تحديثات ذات خطوة ثابتة:

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n] \quad\quad (2.4)
$$

حيث $\alpha$ هو معدل التعلم (learning rate) أو حجم الخطوة (step size) بين 0 و1. هذا النوع من التحديثات يعطي وزنًا أكبر للمكافآت الحديثة، مما يسمح بتكيف أسرع مع التغيرات في القيم الحقيقية للأفعال.

---

## 2.6 طرق التحديث التكرارية والذاكرة المحدودة

يُعد استخدام معدل خطوة ثابت مفيدًا خصوصًا في البيئات غير الثابتة، حيث يمكن للوكيل التكيف بسرعة مع التغيرات في القيم الحقيقية. من ناحية أخرى، في البيئات الثابتة، يكون استخدام المتوسط الحسابي للعينات مناسبًا لأنه يتقارب إلى القيمة الحقيقية.

---

## 2.7 الخلاصة

في هذا الفصل، تعرفنا على المشكلة الأساسية للماكينات ذات الأذرع المتعددة كمقدمة لتحديات التعليم المعزز، وخاصة الصراع بين الاستكشاف والاستغلال. ناقشنا طرقًا بسيطة لتقدير قيم الأفعال واختيار الأفعال باستخدام الطرق الجشعة و$\varepsilon$-greedy، وبيّنا أهمية الاستكشاف لتحقيق أداء جيد في المدى الطويل. كذلك استعرضنا طرق التحديث التكراري والقيمة الثابتة لتقديرات قيم الأفعال.

---

**ملاحظة:** يمكنني كتابة فصول أخرى أو تفصيل أكثر حسب طلبك.
