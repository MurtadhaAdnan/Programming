<div dir="rtl" style="text-align: justify;">

# الفصل الثاني:
## Multi-armed Bandits  
### الماكينات ذات الأذرع المتعددة  

أكثر ميزات التعليم المعزز تميزًا عن أنواع التعليم الأخرى هي أنه يستخدم معلومات التدريب التي تقيم الأفعال المتخذة بدلاً من توجيهها عبر تقديم الأفعال الصحيحة. هذا ما يخلق الحاجة للاستكشاف النشط، أي البحث الصريح عن سلوك جيد. تشير التغذية الراجعة التقييمية (Evaluative Feedback) البحتة إلى مدى جودة الفعل المتخذ، ولكنها لا تحدد ما إذا كان الفعل هو الأفضل أو الأسوأ الممكن. من ناحية أخرى، تشير التغذية الراجعة التوجيهية (Instructive Feedback) إلى الفعل الصحيح الذي يجب اتخاذه، بغض النظر عن الفعل المتخذ بالفعل. هذا النوع من التغذية الراجعة هو أساس التعليم الخاضع للإشراف، الذي يشمل أجزاء كبيرة من تصنيف الأنماط، والشبكات العصبية الاصطناعية، وتحديد الأنظمة. في أشكالها النقية، تكون هاتان النوعيتان من التغذية الراجعة مميزتين تمامًا: التغذية الراجعة التقييمية (Evaluative Feedback) تعتمد بالكامل على الفعل المتخذ، بينما التغذية الراجعة التوجيهية (Instructive Feedback) مستقلة عن الفعل المتخذ.

في هذا الفصل، ندرس الجانب التقييمي للتعليم المعزز في إطار مبسط، لا يتضمن تعلم التصرف في أكثر من حالة واحدة. هذا الإطار غير الترابطي هو الإطار الذي تم فيه إنجاز معظم الأعمال السابقة التي تتضمن التغذية الراجعة التقييمية (Evaluative Feedback)، وهو يتجنب الكثير من تعقيد مشكلة التعليم المعزز الكامل. دراسة هذه الحالة تمكننا من رؤية الفرق بين التغذية الراجعة التقييمية (Evaluative Feedback) والتغذية الراجعة التوجيهية (Instructive Feedback) بوضوح، وكيف يمكن دمجهما.

المشكلة غير الترابطية، التقييمية التي نستكشفها هي نسخة بسيطة من مشكلة الماكينة ذات الأذرع المتعددة. نستخدم هذه المشكلة لتقديم عدد من الأساليب الأساسية للتعليم التي نوسعها في الفصول اللاحقة لتطبيقها على مشكلة التعليم المعزز الكاملة. في نهاية هذا الفصل، نقترب خطوة من مشكلة التعليم المعزز الكامل من خلال مناقشة ما يحدث عندما تصبح مشكلة الماكينة ذات الأذرع المتعددة ترابطية، أي عندما تُتخذ الأفعال في أكثر من حالة واحدة.

---

## 2.1 مشكلة الماكينة ذات الأذرع المتعددة (k-armed Bandit Problem)

فكر بعناية في مشكلة التعليم التالية. تواجه باستمرار خيارًا بين عدة خيارات مختلفة $$k$$ أو إجراءات مختلفة. بعد كل اختيار تتلقى مكافأة رقمية يتم اختيارها من توزيع احتمالي ثابت يعتمد على الإجراء الذي قمت باختياره. هدفك هو تعظيم إجمالي المكافأة المتوقعة على مدار فترة زمنية معينة، على سبيل المثال، على مدار 1000 اختيار للإجراءات أو خطوات زمنية.

هذه هي الصيغة الأصلية لمشكلة الماكينات ذات الأذرع المتعددة، والتي سُميت بهذا الاسم بالتشابه مع آلة القمار ذات الذراع الواحد، باستثناء أنها تحتوي على $$k$$ أذرع بدلاً من واحدة. كل اختيار للعمل يشبه سحب ذراع من أذرع آلة القمار، والعوائد هي الأرباح التي يتم الحصول عليها عند ضرب الجائزة الكبرى. من خلال تكرار اختيار الأذرع، هدفك هو زيادة مكاسبك عن طريق التركيز على الأذرع الأفضل. تشبيه آخر هو أن الطبيب يختار بين علاجات تجريبية لمجموعة من المرضى بشكل خطير. كل إجراء هو اختيار علاج، وكل عائد هو بقاء أو صحة المريض. اليوم، يُستخدم مصطلح "مشكلة الماكينات ذات الأذرع المتعددة" أحياناً للإشارة إلى تعميم للمشكلة الموصوفة أعلاه، ولكن في هذا الكتاب نستخدمه للإشارة فقط إلى هذه الحالة البسيطة.

في مشكلة الماكينات ذات الأذرع المتعددة، كل واحد من الأذرع $$k$$ له عائد متوقع أو متوسط يُعطى عند اختيار هذا الذراع؛ دعنا نطلق على هذا العائد قيمة هذا الذراع. نُسمي الذراع الذي يُختار في الزمن $$t$$ بـ $$A_t$$، والعائد المقابل له بـ $$R_t$$. إذاً، قيمة أي ذراع عشوائي $$a$$، التي نُشير إليها بـ $$q^*(a)$$، هي العائد المتوقع عند اختيار $$a$$:

$$
$$
q^*(a) = \mathrm{E}[R_t \mid A_t = a]
$$
$$

إذا كنت تعرف قيمة كل إجراء، فستكون مشكلة الماكينات ذات الأذرع المتعددة سهلة الحل: ستقوم دائماً باختيار الإجراء الذي له أعلى قيمة. نحن نفترض أنك لا تعرف قيم الأفعال بيقين، رغم أنك قد تكون لديك تقديرات. نُسمي تقدير قيمة الإجراء $$a$$ في الزمن $$t$$ بـ $$Q_t(a)$$، ونرغب في أن يكون $$Q_t(a)$$ قريباً من $$q^*(a)$$.

إذا كنت تحتفظ بتقديرات لقيم الأفعال، فإن في أي وقت سيكون هناك على الأقل إجراء واحد له أعلى قيمة تقديرية. نُطلق على هذه الأفعال اسم الأفعال الجشعة (greedy actions). عندما تختار واحداً من هذه الأفعال، نقول إنك تستغل (exploiting) معرفتك الحالية بقيم الأفعال. إذا كنت تختار بدلاً من ذلك واحداً من الأفعال غير الجشعة (nongreedy actions)، فنحن نقول إنك تستكشف (exploring)، لأن هذا يتيح لك تحسين تقديرك لقيمة الإجراء غير الجشع.

الاستغلال (exploitation) هو الشيء الصحيح لزيادة العائد المتوقع في الخطوة الواحدة، ولكن الاستكشاف (exploration) قد يؤدي إلى تحقيق عائد إجمالي أكبر على المدى الطويل.

على سبيل المثال، افترض أن قيمة الإجراء الجشع معروفة بيقين، بينما يُقدَّر أن عدة أفعال أخرى قريبة من حيث القيمة ولكن مع عدم يقين كبير. عدم اليقين هو أن أحد هذه الأفعال الأخرى ربما يكون في الواقع أفضل من الإجراء الجشع، لكنك لا تعرف أي منها. إذا كان لديك العديد من الخطوات الزمنية القادمة لاختيار الأفعال، فقد يكون من الأفضل استكشاف الأفعال غير الجشعة واكتشاف أي منها أفضل من الإجراء الجشع. العائد أقل على المدى القصير خلال الاستكشاف، ولكنه أعلى على المدى الطويل لأنك بعد اكتشاف الأفعال الأفضل يمكنك استغلالها عدة مرات. بما أنه ليس من الممكن استكشاف واستغلال الإجراء في نفس الوقت، فإننا غالباً ما نستخدم مصطلح الصراع (conflict) بين الاستكشاف والاستغلال.

في أي حالة معينة، سواء كان من الأفضل الاستكشاف أو الاستغلال يعتمد بشكل معقد على القيم الدقيقة للتقديرات، وعدم اليقين، وعدد الخطوات المتبقية. هناك العديد من الأساليب المتطورة لتحقيق توازن بين الاستكشاف والاستغلال في صياغات رياضية محددة لمشكلة الماكينات ذات الأذرع المتعددة والمشاكل ذات الصلة. ومع ذلك، فإن معظم هذه الأساليب تقوم على افتراضات قوية بشأن الثبات والمعرفة السابقة التي إما تُخالف أو يكون من المستحيل التحقق منها في التطبيقات وفي مشكلة التعليم المعزز الكامل التي سنناقشها في الفصول التالية. ضمانات الأمثلية أو الخسا
