<div dir="rtl">

# الفصل الثاني: (Multi-armed Bandits)  
## ماكينات القمار متعددة الأذرع  

### المقدمة  
أهم ما يُميز **التعلم التعزيزي** (Reinforcement Learning) عن غيره من أنواع التعلم هو اعتماده على **معلومات تقييمية** (Evaluative Feedback) تُقيّم الأفعال المُتخذة بدلاً من تقديم إرشادات محددة. هذا النهج يخلق حاجة ماسة للاستكشاف النشط (Active Exploration) والبحث عن السلوك الأمثل.  

- **التغذية الراجعة التقييمية**: تُخبرنا بجودة الفعل المُتخذ دون تحديد ما إذا كان الأفضل أم لا.  
- **التغذية الراجعة الإرشادية**: تُحدد الفعل الصحيح مباشرةً (كما في التعلم الخاضع للإشراف **Supervised Learning**).  

في هذا الفصل، ندرس مشكلة مبسطة تسمى **"مشكلة اللص متعدد الأذرع"** (k-armed Bandit Problem)، والتي تُعتبر نموذجًا أساسيًا لفهم التوازن بين **الاستغلال** (Exploitation) و**الاستكشاف** (Exploration).  

### 2.1 مشكلة اللص متعدد الأذرع  
تخيل أنك أمام مجموعة من **آلات القمار** (Slot Machines) لكل منها ذراع، وتُسمى هذه الآلات **"اللصوص"** (Bandits). كلما سحبت ذراعًا (اتخذت فعلًا)، تحصل على **مكافأة** (Reward) من توزيع احتمالي ثابت خاص بذلك الذراع. هدفك هو **تعظيم مجموع المكافآت** على مدى عدة محاولات (مثل 1000 محاولة).  

- **التشبيه الطبي**: مثل طبيب يُجرب علاجات مختلفة على مرضى، حيث كل علاج (فعل) يعطي نتيجة (مكافأة) مختلفة.  
- **قيمة الفعل** (Action Value): القيمة المتوقعة للمكافأة عند اختيار فعل معين، تُرمز بـ \( q_*(a) \):  

\[
q_*(a) \doteq \mathbb{E}[R_t \mid A_t = a]
\]

إذا عرفنا هذه القيم مسبقًا، فالحل يكون بسيطًا: **اختر الفعل ذو القيمة الأعلى دائمًا**. لكن في الواقع، لا نعرفها بل نُقدّرها بناءً على التجارب السابقة.  

### 2.2 طرق تقدير القيمة (Action-value Methods)  
نستخدم **متوسط العينات** (Sample-average Method) لتقدير قيمة كل فعل:  

\[
Q_t(a) \doteq \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}}
\]

حيث:  
- \( \mathbb{1}_{A_i = a} \) هو دالة مؤشر تُساوي 1 إذا تم اختيار الفعل \( a \) في الخطوة \( i \).  
- إذا لم يُختار الفعل أبدًا، نُعين له قيمة افتراضية (مثل 0).  

#### الاختيار الطماع (Greedy Action Selection)  
نختار الفعل ذو التقدير الأعلى حاليًا:  

\[
A_t \doteq \arg\max_a Q_t(a)
\]

لكن هذا الأسلوب **يُهمل الاستكشاف**، مما قد يؤدي إلى التعلق بأفعال دون الأمثل.  

#### أسلوب إبسيلون-طماع (ε-Greedy Method)  
لتحقيق التوازن، نختار الفعل الطماع بنسبة \( 1 - \varepsilon \)، ونختار عشوائيًا بنسبة \( \varepsilon \).  

**مثال**: إذا كان \( \varepsilon = 0.1 \)، فسنستكشف أفعالًا عشوائية في 10% من المحاولات.  

### 2.3 بيئة الاختبار (10-armed Testbed)  
لاختبار الأداء، نستخدم 2000 نموذج عشوائي لآلات ذات 10 أذرع، حيث:  
- \( q_*(a) \) تُختار من توزيع طبيعي (mean = 0, variance = 1).  
- المكافأة \( R_t \) تُعطى من توزيع طبيعي (mean = \( q_*(A_t) \), variance = 1).  

**النتائج**:  
- **الطماع البحت** (Greedy) يؤدي بشكل جيد مبكرًا لكنه يتوقف عند أداء دون الأمثل.  
- **ε-Greedy** (بـ \( \varepsilon = 0.01 \) أو \( 0.1 \)) يتفوق على المدى البعيد بسبب الاستكشاف.  

### 2.4 التحديث التزايدي (Incremental Implementation)  
بدلاً من تخزين كل المكافآت، نستخدم **تحديثًا تزايديًا** (Incremental Update) لحساب المتوسط:  

\[
Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n]
\]

هذه الصيغة تُقلل من متطلبات الذاكرة والحساب.  

### 2.5 المشاكل غير الثابتة (Nonstationary Problems)  
إذا تغيرت قيم الأفعال بمرور الزمن (مشكلة غير ثابتة)، نستخدم **مُعامل خطى ثابت** (Constant Step-size \( \alpha \)):  

\[
Q_{n+1} = Q_n + \alpha [R_n - Q_n]
\]

حيث يُعطي وزنًا أكبر للمكافآت الحديثة.  

### 2.6 القيم المبدئية المتفائلة (Optimistic Initial Values)  
ببدء التقديرات بقيم عالية (مثل +5 بدلاً من 0)، نُحفز النموذج على **استكشاف جميع الأفعال** مبكرًا قبل الاستقرار على الأمثل.  

### 2.7 اختيار الحد الأعلى للثقة (UCB Action Selection)  
نختار الأفعال بناءً على:  

\[
A_t \doteq \arg\max_a \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]
\]

حيث:  
- \( c \sqrt{\frac{\ln t}{N_t(a)}} \) يقيس **عدم اليقين** في تقدير الفعل \( a \).  
- \( N_t(a) \): عدد مرات اختيار الفعل \( a \).  

### 2.8 خوارزميات التدرج (Gradient Bandit Algorithms)  
بدلاً من تقدير القيم، نتعلم **تفضيلات** (Preferences) للأفعال باستخدام:  

\[
\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}}
\]

ثم نُحدّث التفضيلات حسب المكافأة النسبية:  

\[
H_{t+1}(a) = H_t(a) + \alpha (R_t - \bar{R}_t)(\mathbb{1}_{A_t = a} - \pi_t(a))
\]

### 2.9 البحث الترابطي (Associative Search)  
عندما تتغير المشكلة حسب السياق (مثل لون الآلة)، نتعلم **سياسة** (Policy) تربط كل سياق بأفضل فعل.  

### 2.10 الملخص  
- **ε-Greedy**: يوازن بين الاستغلال والاستكشاف بشكل بسيط.  
- **UCB**: يُفضل الأفعال الأقل استكشافًا.  
- **التدرج**: يعمل جيدًا مع التفضيلات النسبية.  
- **القيم المتفائلة**: تحفز الاستكشاف المبكر.  

الخوارزميات الأفضل تعتمد على طبيعة المشكلة، لكن **UCB** يظهر أداءً متميزًا في الاختبارات.  

