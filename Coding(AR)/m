
===== الصفحة 1 =====

374

الشكل 12.3
مستويات فصل محتملة. هناك العديد من المصنفات الخطية (الخطوط الخضراء) التي تفصل بين العلامات البرتقالية والأقراص الزرقاء.

التصنيف باستخدام آلات ناقلات الدعم (SVM)

12.2 آلة ناقلات الدعم الأولية

بناءً على مفهوم المسافات من النقاط إلى مستوى الفصل، نحن الآن في وضع يسمح لنا بمناقشة آلة ناقلات الدعم. لمجموعة بيانات \(\{(x_1, y_1), \ldots, (x_N, y_N)\}\) التي يمكن فصلها خطيًا، لدينا عدد لا حصر له من مستويات الفصل المرشحة (انظر الشكل 12.3)، وبالتالي مصنفات تحل مشكلة التصنيف دون أي أخطاء (تدريبية). للعثور على حل فريد، إحدى الأفكار هي اختيار مستوى الفصل الذي يعظم الهامش بين الأمثلة الإيجابية والسلبية. بمعنى آخر، نريد أن تكون الأمثلة الإيجابية والسلبية مفصولة بهامش كبير (القسم 12.2.1). في ما يلي، نحسب المسافة بين مثال ومستوى الفصل لاشتقاق الهامش. تذكر أن أقرب نقطة على مستوى الفصل إلى نقطة معينة (المثال \(x_n\)) يتم الحصول عليها عن طريق الإسقاط العمودي (القسم 3.8).

المصنف ذو الهامش الكبير يظهر أداءً جيدًا في التعميم (Steinwart and Christmann, 2008).

هامش
يمكن أن يكون هناك مثالان أو أكثر أقرب إلى مستوى الفصل.

12.2.1 مفهوم الهامش

مفهوم الهامش بسيط بشكل حدسي: إنه المسافة من مستوى الفصل إلى أقرب الأمثلة في مجموعة البيانات، بافتراض أن مجموعة البيانات قابلة للفصل خطيًا. ومع ذلك، عند محاولة صياغة هذه المسافة، هناك تعقيد تقني قد يكون مربكًا. هذا التعقيد هو أننا بحاجة إلى تحديد مقياس لقياس المسافة. أحد المقاييس المحتملة هو النظر إلى مقياس البيانات، أي القيم الخام لـ \(x_n\). هناك مشاكل في هذا، حيث يمكننا تغيير وحدات قياس \(x_n\) وتغيير القيم في \(x_n\)، وبالتالي تغيير المسافة إلى مستوى الفصل. كما سنرى قريبًا، نحدد المقياس بناءً على معادلة مستوى الفصل (12.3) نفسها.

ضع في اعتبارك مستوى فصل \(\langle w, x \rangle + b\)، ومثال \(x_a\) كما هو موضح في الشكل 12.4. دون فقدان العمومية، يمكننا اعتبار المثال \(x_a\) على الجانب الإيجابي من مستوى الفصل، أي \(\langle w, x_a \rangle + b > 0\). نود حساب المسافة \(r > 0\) لـ \(x_a\) من مستوى الفصل. نفعل ذلك عن طريق النظر في الإسقاط العمودي (القسم 3.8) لـ \(x_a\) على مستوى الفصل، والذي نرمز له بـ \(x'_a\). بما أن \(w\) عمودي على مستوى الفصل، نعلم أن المسافة \(r\) هي مجرد قياس لهذا المتجه \(w\). إذا كان طول \(w\) معروفًا، فيمكننا استخدام عامل القياس هذا \(r\) لحساب المسافة المطلقة بين \(x_a\) و \(x'_a\). للراحة، نختار استخدام متجه بطول وحدة (معياره 1) ونحصل على ذلك بقسمة \(w\) على معياره، \(\frac{w}{\|w\|}\). باستخدام جمع المتجهات (القسم 2.4)، نحصل على

\[x_a = x'_a + r \frac{w}{\|w\|}.\]

(12.8)

طريقة أخرى للتفكير في \(r\) هي أنها إحداثي \(x_a\) في الفضاء الجزئي الممتد بواسطة \(w / \|w\|\). لقد عبرنا الآن عن مسافة \(x_a\) من مستوى الفصل بـ \(r\)، وإذا اخترنا \(x_a\) ليكون النقطة الأقرب إلى مستوى الفصل، فإن هذه المسافة \(r\) هي الهامش.

تذكر أننا نود أن تكون الأمثلة الإيجابية أبعد من \(r\) من مستوى الفصل، وأن تكون الأمثلة السلبية أبعد من مسافة \(r\) (في الاتجاه السلبي) من مستوى الفصل. بشكل مشابه للجمع بين (12.5) و (12.6) في (12.7)، نصوغ هذا الهدف كـ

\[y_n (\langle w, x_n \rangle + b) \geq r.\]

(12.9)

بمعنى آخر، نجمع بين متطلبات أن تكون الأمثلة على الأقل \(r\) بعيدة عن مستوى الفصل (في الاتجاهين الإيجابي والسلبي) في متباينة واحدة.

بما أننا مهتمون فقط بالاتجاه، نضيف افتراضًا إلى نموذجنا أن متجه المعلمة \(w\) له طول وحدة، أي \(\|w\| = 1\)، حيث نستخدم المعيار الإقليدي \(\|w\| = \sqrt{w^T w}\) (القسم 3.1). هذا الافتراض يسمح أيضًا بتفسير أكثر حدسية للمسافة \(r\) (12.8) حيث إنه عامل قياس لمتجه طوله 1.

ملاحظة. القارئ المطلع على عروض أخرى للهامش سيلاحظ أن تعريفنا لـ \(\|w\| = 1\) يختلف عن العرض القياسي إذا كانت SVM هي تلك المقدمة من Schölkopf و Smola (2002)، على سبيل المثال. في القسم 12.2.3، سنظهر تكافؤ كلا النهجين.

بجمع المتطلبات الثلاثة في مشكلة تحسين مقيدة واحدة، نحصل على الهدف

\[\max_{\boldsymbol{w},b,r} \underbrace{\boldsymbol{r}}_{\text{هامش}}\] (12.10) 
subject to \[\underbrace{y_{n}\langle\langle\boldsymbol{w},\boldsymbol{x}_{n} \rangle+b\rangle\geqslant r}_{\text{ملاءمة البيانات}}\ ,\ \left\|\boldsymbol{w }\right\|=1\ ,\quad r>0\,,\]

وهذا يعني أننا نريد تعظيم الهامش \(r\) مع ضمان أن تبقى البيانات على الجانب الصحيح من مستوى الفصل.

ملاحظة. مفهوم الهامش يظهر بشكل كبير في تعلم الآلة. استخدمه Vladimir Vapnik و Alexey Chervonenkis لإظهار أنه عندما يكون الهامش كبيرًا، تكون "تعقيد" فئة الوظائف منخفضًا، وبالتالي يكون التعلم ممكنًا [()]. اتضح أن المفهوم مفيد لتحليل الأخطاء التعميمية نظريًا بطرق مختلفة [(; )].

#### 12.2.2 اشتقاق تقليدي للهامش

في القسم السابق، اشتققنا (12.10) بملاحظة أننا مهتمون فقط باتجاه \(\boldsymbol{w}\) وليس طوله، مما أدى إلى افتراض أن \(\left\|\boldsymbol{w}\right\|=1\). في هذا القسم، نشتق مشكلة تعظيم الهامش بافتراض مختلف. بدلاً من اختيار أن متجه المعلمة طبيعي، نختار مقياسًا للبيانات. نختار هذا المقياس بحيث تكون قيمة المتنبئ \(\langle\boldsymbol{w},\boldsymbol{x}\rangle+b\) تساوي 1 عند أقرب مثال. دعنا نرمز أيضًا إلى المثال في مجموعة البيانات الأقرب إلى مستوى الفصل بـ \(\boldsymbol{x}_{a}\).

الشكل 12.5 مطابق للشكل 12.4، إلا أننا الآن قمنا بإعادة قياس المحاور بحيث يقع المثال \(\boldsymbol{x}_{a}\) بالضبط على الهامش، أي \(\langle\boldsymbol{w},\boldsymbol{x}_{a}\rangle+b=1\). بما أن \(\boldsymbol{x}^{\prime}_{a}\) هو الإسقاط العمودي لـ \(\boldsymbol{x}_{a}\) على مستوى الفصل، فلا بد أن يقع على مستوى الفصل بحكم التعريف، أي

\[\langle\boldsymbol{w},\boldsymbol{x}^{\prime}_{a}\rangle+b=0\,.\] (12.11)

الشكل 12.5: اشتقاق الهامش: \(r=\frac{1}{\|{\boldsymbol{w}}\|}\).

===== الصفحة 4 =====

#### 12.2.2 آلة ناقلات الدعم الأولية

باستبدال (12.8) في (12.11)، نحصل على

\[\left\langle{\bm{w}},{\bm{x}}_{a}-r\frac{{\bm{w}}}{\|{\bm{w}}\|}\right\rangle+b=0\,.\] (12.12)

باستغلال ثنائية الخطية للمنتج الداخلي (انظر القسم 3.2)، نحصل على

\[\left\langle{\bm{w}},{\bm{x}}_{a}\right\rangle+b-r\frac{\left\langle{\bm{w}},{\bm{w}}\right\rangle}{\|{\bm{w}}\|}=0\,.\] (12.13)

لاحظ أن الحد الأول هو 1 بافتراض المقياس، أي \(\left\langle{\bm{w}},{\bm{x}}_{a}\right\rangle+b=1\). من (3.16) في القسم 3.1، نعلم أن \(\left\langle{\bm{w}},{\bm{w}}\right\rangle=\|{\bm{w}}\|^{2}\). وبالتالي، فإن الحد الثاني يختزل إلى \(r\|{\bm{w}}\|\). باستخدام هذه التبسيطات، نحصل على

\[r=\frac{1}{\|{\bm{w}}\|}\,.\] (12.14)

هذا يعني أننا اشتققنا المسافة \(r\) بدلالة المتجه الطبيعي \({\bm{w}}\) لمستوى الفصل. للوهلة الأولى، تبدو هذه المعادلة غير بديهية حيث يبدو أننا اشتققنا المسافة من مستوى الفصل بدلالة طول المتجه \({\bm{w}}\)، لكننا لا نعرف هذا المتجه بعد. إحدى طرق التفكير في ذلك هي اعتبار المسافة \(r\) كمتغير مؤقت
